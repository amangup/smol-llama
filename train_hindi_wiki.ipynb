{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-16T06:13:14.675807Z",
     "start_time": "2024-12-16T06:13:13.745871Z"
    }
   },
   "source": [
    "from model import ModelConfig, LlamaModel\n",
    "from train import TrainerConfig, FileDataLoader, Trainer\n",
    "\n",
    "from transformers import AutoTokenizer"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T06:13:14.680027Z",
     "start_time": "2024-12-16T06:13:14.678406Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer_id = \"HuggingFaceTB/SmolLM2-135M\"",
   "id": "2f28fa23c987e72b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T06:13:14.942699Z",
     "start_time": "2024-12-16T06:13:14.724783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ],
   "id": "9bb4e51aa142abee",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T06:13:14.949885Z",
     "start_time": "2024-12-16T06:13:14.948274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_config = ModelConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=576,\n",
    "    d_head=64,\n",
    "    d_mlp_proj=1536,\n",
    "    n_layers=30,\n",
    "    n_kv_heads=3,\n",
    "    n_attn_heads=9,\n",
    "    rms_norm_eps=1e-5,\n",
    "    initializer_range=0.041666666666666664,\n",
    "    rope_theta=100000.0,\n",
    "    padding_idx=tokenizer.pad_token_id\n",
    ")"
   ],
   "id": "cde027092af8291e",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T06:13:14.994245Z",
     "start_time": "2024-12-16T06:13:14.992618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_config = TrainerConfig(\n",
    "    per_device_train_batch_size=8,\n",
    "    max_seq_len=2048,\n",
    "    num_epochs=1,\n",
    "    learning_rate=1e-3,\n",
    "    tokens_folder=\"wiki_hindi_tok\",\n",
    "    max_steps=2000\n",
    ")"
   ],
   "id": "809773e662327a12",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T06:13:19.812846Z",
     "start_time": "2024-12-16T06:13:17.865049Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = LlamaModel(model_config)\n",
    "dataloader = FileDataLoader(train_config, tokenizer)\n",
    "trainer = Trainer(train_config, model)"
   ],
   "id": "9a912a0ec92039d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train tokens             | 270,491,648\n",
      "Num Trainable Params           | 162,826,560\n",
      "Train device                   | cuda, NVIDIA GeForce RTX 3090, N=1\n",
      "Training precision             | torch.bfloat16\n",
      "Flash Attention                | True\n",
      "torch.compile()                | True\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T06:26:13.838827Z",
     "start_time": "2024-12-16T06:13:21.696455Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train(dataloader)",
   "id": "bc3b550223d43b9c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training steps                 | 2,000 \n",
      "Step: 0, Training Loss: 11.44452, Tokens/sec: 1523.6792020482028\n",
      "Step: 1, Training Loss: 9.92958, Tokens/sec: 1658.0446699343945\n",
      "Step: 2, Training Loss: 7.48547, Tokens/sec: 88535.48187497325\n",
      "Step: 3, Training Loss: 7.08707, Tokens/sec: 85621.38507734686\n",
      "Step: 4, Training Loss: 6.42159, Tokens/sec: 86666.16139748672\n",
      "Step: 5, Training Loss: 5.76335, Tokens/sec: 85075.0035268395\n",
      "Step: 6, Training Loss: 4.90985, Tokens/sec: 87301.0103515507\n",
      "Step: 7, Training Loss: 4.56522, Tokens/sec: 85954.65296049694\n",
      "Step: 8, Training Loss: 4.28419, Tokens/sec: 84144.34179420414\n",
      "Step: 9, Training Loss: 4.03407, Tokens/sec: 87916.10755802804\n",
      "Step: 10, Training Loss: 3.93829, Tokens/sec: 87618.75117636021\n",
      "Step: 11, Training Loss: 3.97180, Tokens/sec: 86250.11046215301\n",
      "Step: 12, Training Loss: 3.90830, Tokens/sec: 87801.91245281768\n",
      "Step: 13, Training Loss: 3.79044, Tokens/sec: 86141.00576033592\n",
      "Step: 14, Training Loss: 3.74311, Tokens/sec: 86699.7608479747\n",
      "Step: 15, Training Loss: 3.80275, Tokens/sec: 86302.27504602779\n",
      "Step: 16, Training Loss: 3.71701, Tokens/sec: 85810.60027142907\n",
      "Step: 17, Training Loss: 3.73183, Tokens/sec: 88221.0371650029\n",
      "Step: 18, Training Loss: 3.73413, Tokens/sec: 84956.36160288402\n",
      "Step: 19, Training Loss: 3.76456, Tokens/sec: 87860.08598725568\n",
      "Step: 20, Training Loss: 3.97386, Tokens/sec: 88377.17631056669\n",
      "Step: 21, Training Loss: 3.88590, Tokens/sec: 86049.32963501308\n",
      "Step: 22, Training Loss: 4.00396, Tokens/sec: 86190.34140449969\n",
      "Step: 23, Training Loss: 3.74094, Tokens/sec: 87624.3724998764\n",
      "Step: 24, Training Loss: 4.35054, Tokens/sec: 86211.53521921011\n",
      "Step: 25, Training Loss: 3.83411, Tokens/sec: 88217.7462513933\n",
      "Step: 26, Training Loss: 3.76492, Tokens/sec: 84706.26213734716\n",
      "Step: 27, Training Loss: 3.67480, Tokens/sec: 87950.44306278265\n",
      "Step: 28, Training Loss: 3.62959, Tokens/sec: 87648.30727367238\n",
      "Step: 29, Training Loss: 3.73641, Tokens/sec: 85415.44694092493\n",
      "Step: 30, Training Loss: 3.69601, Tokens/sec: 86845.00176170288\n",
      "Step: 31, Training Loss: 3.84770, Tokens/sec: 85874.60859545815\n",
      "Step: 32, Training Loss: 3.71600, Tokens/sec: 88112.67037686796\n",
      "Step: 33, Training Loss: 3.83148, Tokens/sec: 87036.96866770589\n",
      "Step: 34, Training Loss: 3.63362, Tokens/sec: 85081.34410784511\n",
      "Step: 35, Training Loss: 3.49506, Tokens/sec: 87581.07095658928\n",
      "Step: 36, Training Loss: 3.50662, Tokens/sec: 87593.3901567527\n",
      "Step: 37, Training Loss: 3.52145, Tokens/sec: 87220.42397768745\n",
      "Step: 38, Training Loss: 3.32258, Tokens/sec: 87257.8776319727\n",
      "Step: 39, Training Loss: 3.38114, Tokens/sec: 85221.58386890443\n",
      "Step: 40, Training Loss: 3.15178, Tokens/sec: 88050.69187647884\n",
      "Step: 41, Training Loss: 3.23690, Tokens/sec: 86653.47464746852\n",
      "Step: 42, Training Loss: 3.05739, Tokens/sec: 84310.02783648214\n",
      "Step: 43, Training Loss: 2.99847, Tokens/sec: 87904.06104689596\n",
      "Step: 44, Training Loss: 3.02560, Tokens/sec: 84583.39754743967\n",
      "Step: 45, Training Loss: 3.05549, Tokens/sec: 85936.8724623973\n",
      "Step: 46, Training Loss: 3.01678, Tokens/sec: 87881.98033622153\n",
      "Step: 47, Training Loss: 3.20853, Tokens/sec: 85618.87271815074\n",
      "Step: 48, Training Loss: 3.06911, Tokens/sec: 87079.05707928354\n",
      "Step: 49, Training Loss: 3.28046, Tokens/sec: 83864.2994379226\n",
      "Step: 50, Training Loss: 3.21355, Tokens/sec: 88286.24141201105\n",
      "Step: 51, Training Loss: 3.03575, Tokens/sec: 88347.47254966796\n",
      "Step: 52, Training Loss: 2.93351, Tokens/sec: 84538.51149209065\n",
      "Step: 53, Training Loss: 3.11480, Tokens/sec: 87884.0182008524\n",
      "Step: 54, Training Loss: 2.99967, Tokens/sec: 88211.86141829148\n",
      "Step: 55, Training Loss: 2.91035, Tokens/sec: 86113.21814463651\n",
      "Step: 56, Training Loss: 2.93091, Tokens/sec: 86271.18468781463\n",
      "Step: 57, Training Loss: 2.91930, Tokens/sec: 85092.50868161183\n",
      "Step: 58, Training Loss: 2.89471, Tokens/sec: 87684.85856539018\n",
      "Step: 59, Training Loss: 2.85819, Tokens/sec: 86649.05500653418\n",
      "Step: 60, Training Loss: 2.87321, Tokens/sec: 84466.23508029665\n",
      "Step: 61, Training Loss: 2.84676, Tokens/sec: 88030.8394214718\n",
      "Step: 62, Training Loss: 2.87203, Tokens/sec: 85276.89956861027\n",
      "Step: 63, Training Loss: 2.79279, Tokens/sec: 86844.69380057497\n",
      "Step: 64, Training Loss: 3.14661, Tokens/sec: 88537.40566760133\n",
      "Step: 65, Training Loss: 2.78401, Tokens/sec: 84952.72124772896\n",
      "Step: 66, Training Loss: 2.89083, Tokens/sec: 87434.85512304035\n",
      "Step: 67, Training Loss: 2.92061, Tokens/sec: 84786.13992677972\n",
      "Step: 68, Training Loss: 2.90609, Tokens/sec: 87461.09598605879\n",
      "Step: 69, Training Loss: 2.80746, Tokens/sec: 87216.88136840273\n",
      "Step: 70, Training Loss: 2.80370, Tokens/sec: 84913.35629405816\n",
      "Step: 71, Training Loss: 2.72793, Tokens/sec: 86096.2777389118\n",
      "Step: 72, Training Loss: 2.87998, Tokens/sec: 86178.26771010245\n",
      "Step: 73, Training Loss: 2.80483, Tokens/sec: 87099.13260752177\n",
      "Step: 74, Training Loss: 2.95122, Tokens/sec: 86078.18083390787\n",
      "Step: 75, Training Loss: 3.07131, Tokens/sec: 84026.60668656658\n",
      "Step: 76, Training Loss: 2.93361, Tokens/sec: 87041.75119687618\n",
      "Step: 77, Training Loss: 2.89090, Tokens/sec: 87829.87605959334\n",
      "Step: 78, Training Loss: 2.89766, Tokens/sec: 83796.2465618358\n",
      "Step: 79, Training Loss: 3.00479, Tokens/sec: 85750.70703452571\n",
      "Step: 80, Training Loss: 2.89339, Tokens/sec: 86626.52294208751\n",
      "Step: 81, Training Loss: 2.80007, Tokens/sec: 71343.93728184485\n",
      "Step: 82, Training Loss: 2.95242, Tokens/sec: 86918.0609961867\n",
      "Step: 83, Training Loss: 2.93406, Tokens/sec: 85562.68768302053\n",
      "Step: 84, Training Loss: 2.81517, Tokens/sec: 83385.36605855482\n",
      "Step: 85, Training Loss: 2.83474, Tokens/sec: 79589.4494064655\n",
      "Step: 86, Training Loss: 2.78719, Tokens/sec: 87077.94263484675\n",
      "Step: 87, Training Loss: 2.90744, Tokens/sec: 77813.72572840213\n",
      "Step: 88, Training Loss: 2.82939, Tokens/sec: 69187.04783892343\n",
      "Step: 89, Training Loss: 2.71649, Tokens/sec: 59546.42192252622\n",
      "Step: 90, Training Loss: 2.81722, Tokens/sec: 86199.01749286965\n",
      "Step: 91, Training Loss: 2.88450, Tokens/sec: 84947.24192512325\n",
      "Step: 92, Training Loss: 2.81921, Tokens/sec: 87266.70212069334\n",
      "Step: 93, Training Loss: 3.08517, Tokens/sec: 87981.30388765632\n",
      "Step: 94, Training Loss: 3.12204, Tokens/sec: 85714.78465955633\n",
      "Step: 95, Training Loss: 2.76345, Tokens/sec: 87424.8807172186\n",
      "Step: 96, Training Loss: 2.81772, Tokens/sec: 85691.38810430448\n",
      "Step: 97, Training Loss: 2.89511, Tokens/sec: 87717.57092094552\n",
      "Step: 98, Training Loss: 2.77998, Tokens/sec: 87523.66080922677\n",
      "Step: 99, Training Loss: 2.82469, Tokens/sec: 85409.6290426798\n",
      "Step: 100, Training Loss: 2.70475, Tokens/sec: 87097.17867238987\n",
      "Step: 101, Training Loss: 2.72933, Tokens/sec: 84350.22139924328\n",
      "Step: 102, Training Loss: 2.64923, Tokens/sec: 84580.1973336318\n",
      "Step: 103, Training Loss: 2.79784, Tokens/sec: 85110.13068831536\n",
      "Step: 104, Training Loss: 2.84407, Tokens/sec: 85405.6274181441\n",
      "Step: 105, Training Loss: 2.94419, Tokens/sec: 83078.57897006285\n",
      "Step: 106, Training Loss: 2.68977, Tokens/sec: 86765.10074806676\n",
      "Step: 107, Training Loss: 2.72636, Tokens/sec: 86071.18529272196\n",
      "Step: 108, Training Loss: 2.71738, Tokens/sec: 86485.2817009388\n",
      "Step: 109, Training Loss: 2.68850, Tokens/sec: 85595.91753496889\n",
      "Step: 110, Training Loss: 2.61263, Tokens/sec: 88041.135672604\n",
      "Step: 111, Training Loss: 2.63152, Tokens/sec: 86057.77259543297\n",
      "Step: 112, Training Loss: 2.64725, Tokens/sec: 55783.09839827186\n",
      "Step: 113, Training Loss: 2.75673, Tokens/sec: 69417.54212411553\n",
      "Step: 114, Training Loss: 2.69428, Tokens/sec: 87159.84691539436\n",
      "Step: 115, Training Loss: 2.72626, Tokens/sec: 85381.68471344262\n",
      "Step: 116, Training Loss: 2.68193, Tokens/sec: 86845.98181454008\n",
      "Step: 117, Training Loss: 2.62199, Tokens/sec: 85402.7577677378\n",
      "Step: 118, Training Loss: 2.64527, Tokens/sec: 87204.23942584729\n",
      "Step: 119, Training Loss: 2.82421, Tokens/sec: 87173.31502347093\n",
      "Step: 120, Training Loss: 2.60572, Tokens/sec: 85032.10444560216\n",
      "Step: 121, Training Loss: 2.63737, Tokens/sec: 88024.64892040181\n",
      "Step: 122, Training Loss: 2.88492, Tokens/sec: 86944.65784629298\n",
      "Step: 123, Training Loss: 2.64417, Tokens/sec: 86529.53474773881\n",
      "Step: 124, Training Loss: 2.80908, Tokens/sec: 87763.55649379863\n",
      "Step: 125, Training Loss: 2.59868, Tokens/sec: 85126.81435468796\n",
      "Step: 126, Training Loss: 2.61635, Tokens/sec: 86574.41637163401\n",
      "Step: 127, Training Loss: 2.64727, Tokens/sec: 86024.51928757518\n",
      "Step: 128, Training Loss: 2.71342, Tokens/sec: 86370.02054360733\n",
      "Step: 129, Training Loss: 2.61173, Tokens/sec: 87860.99815240254\n",
      "Step: 130, Training Loss: 2.75737, Tokens/sec: 79384.47495444606\n",
      "Step: 131, Training Loss: 2.72058, Tokens/sec: 85399.87985688625\n",
      "Step: 132, Training Loss: 2.57049, Tokens/sec: 87605.4654917974\n",
      "Step: 133, Training Loss: 2.62107, Tokens/sec: 82303.42532041721\n",
      "Step: 134, Training Loss: 2.78837, Tokens/sec: 64836.223676092675\n",
      "Step: 135, Training Loss: 2.71264, Tokens/sec: 85950.9292581916\n",
      "Step: 136, Training Loss: 2.62514, Tokens/sec: 85006.40401525311\n",
      "Step: 137, Training Loss: 2.63572, Tokens/sec: 87683.17154715037\n",
      "Step: 138, Training Loss: 2.72092, Tokens/sec: 86746.6655813276\n",
      "Step: 139, Training Loss: 2.62498, Tokens/sec: 84459.20214725721\n",
      "Step: 140, Training Loss: 2.60034, Tokens/sec: 87767.26354403119\n",
      "Step: 141, Training Loss: 2.57966, Tokens/sec: 87655.56997772554\n",
      "Step: 142, Training Loss: 2.63234, Tokens/sec: 84964.52181433083\n",
      "Step: 143, Training Loss: 2.54838, Tokens/sec: 68994.36373130068\n",
      "Step: 144, Training Loss: 2.54872, Tokens/sec: 86973.14263085806\n",
      "Step: 145, Training Loss: 2.56347, Tokens/sec: 74561.48017444079\n",
      "Step: 146, Training Loss: 2.56158, Tokens/sec: 86650.04484933842\n",
      "Step: 147, Training Loss: 2.57056, Tokens/sec: 87119.71677163274\n",
      "Step: 148, Training Loss: 2.67464, Tokens/sec: 84821.01748054933\n",
      "Step: 149, Training Loss: 2.70570, Tokens/sec: 87084.98336557945\n",
      "Step: 150, Training Loss: 2.58924, Tokens/sec: 82909.65272804312\n",
      "Step: 151, Training Loss: 2.66571, Tokens/sec: 75449.6528071742\n",
      "Step: 152, Training Loss: 2.55342, Tokens/sec: 87642.47565848466\n",
      "Step: 153, Training Loss: 2.54347, Tokens/sec: 84481.07417652414\n",
      "Step: 154, Training Loss: 2.68054, Tokens/sec: 87505.34965037882\n",
      "Step: 155, Training Loss: 2.51922, Tokens/sec: 87786.46717956956\n",
      "Step: 156, Training Loss: 2.49182, Tokens/sec: 84645.76766790033\n",
      "Step: 157, Training Loss: 2.58119, Tokens/sec: 86181.84340385444\n",
      "Step: 158, Training Loss: 2.58885, Tokens/sec: 85118.60215268112\n",
      "Step: 159, Training Loss: 2.59573, Tokens/sec: 88219.73891595405\n",
      "Step: 160, Training Loss: 2.59666, Tokens/sec: 86294.33809322359\n",
      "Step: 161, Training Loss: 2.48816, Tokens/sec: 84477.77803788656\n",
      "Step: 162, Training Loss: 2.57643, Tokens/sec: 87406.26719370576\n",
      "Step: 163, Training Loss: 2.49446, Tokens/sec: 87548.43504908281\n",
      "Step: 164, Training Loss: 2.64379, Tokens/sec: 85142.51520855584\n",
      "Step: 165, Training Loss: 2.76980, Tokens/sec: 60441.14666469286\n",
      "Step: 166, Training Loss: 2.49357, Tokens/sec: 87845.2607881921\n",
      "Step: 167, Training Loss: 2.70148, Tokens/sec: 85890.51584850867\n",
      "Step: 168, Training Loss: 2.51006, Tokens/sec: 86892.02050012453\n",
      "Step: 169, Training Loss: 2.57230, Tokens/sec: 81324.95466152266\n",
      "Step: 170, Training Loss: 2.51232, Tokens/sec: 87441.75862066068\n",
      "Step: 171, Training Loss: 2.57897, Tokens/sec: 87691.07551582136\n",
      "Step: 172, Training Loss: 2.47276, Tokens/sec: 84069.36142892309\n",
      "Step: 173, Training Loss: 2.42411, Tokens/sec: 86727.50967036454\n",
      "Step: 174, Training Loss: 2.48209, Tokens/sec: 84196.75778936257\n",
      "Step: 175, Training Loss: 2.44887, Tokens/sec: 86841.35517484287\n",
      "Step: 176, Training Loss: 2.42699, Tokens/sec: 87159.2306945444\n",
      "Step: 177, Training Loss: 2.52124, Tokens/sec: 84536.11375180467\n",
      "Step: 178, Training Loss: 2.52586, Tokens/sec: 87406.33061154716\n",
      "Step: 179, Training Loss: 2.51716, Tokens/sec: 86701.35977183055\n",
      "Step: 180, Training Loss: 2.53206, Tokens/sec: 83949.82663362459\n",
      "Step: 181, Training Loss: 2.49461, Tokens/sec: 87631.58155658905\n",
      "Step: 182, Training Loss: 2.52511, Tokens/sec: 83894.79582423635\n",
      "Step: 183, Training Loss: 2.49502, Tokens/sec: 87590.04569685936\n",
      "Step: 184, Training Loss: 2.48947, Tokens/sec: 87475.57555166089\n",
      "Step: 185, Training Loss: 2.42690, Tokens/sec: 85878.54760564855\n",
      "Step: 186, Training Loss: 2.43865, Tokens/sec: 87028.86365374106\n",
      "Step: 187, Training Loss: 2.45351, Tokens/sec: 87227.98745897003\n",
      "Step: 188, Training Loss: 2.38665, Tokens/sec: 85811.84880771594\n",
      "Step: 189, Training Loss: 2.31995, Tokens/sec: 85986.59995832187\n",
      "Step: 190, Training Loss: 2.60261, Tokens/sec: 83841.41594971014\n",
      "Step: 191, Training Loss: 2.47525, Tokens/sec: 85842.30532839327\n",
      "Step: 192, Training Loss: 2.52689, Tokens/sec: 86341.07806589287\n",
      "Step: 193, Training Loss: 2.50271, Tokens/sec: 85180.07441672622\n",
      "Step: 194, Training Loss: 2.59801, Tokens/sec: 87317.22991229797\n",
      "Step: 195, Training Loss: 2.52074, Tokens/sec: 84697.06165667766\n",
      "Step: 196, Training Loss: 2.61499, Tokens/sec: 87232.66096450188\n",
      "Step: 197, Training Loss: 2.39379, Tokens/sec: 88113.449422924\n",
      "Step: 198, Training Loss: 2.45212, Tokens/sec: 85674.64953607373\n",
      "Step: 199, Training Loss: 2.46487, Tokens/sec: 83917.53696369588\n",
      "Step: 200, Training Loss: 2.57472, Tokens/sec: 85878.89196624217\n",
      "Step: 201, Training Loss: 2.38713, Tokens/sec: 84060.62223881076\n",
      "Step: 202, Training Loss: 2.48023, Tokens/sec: 81631.72125068208\n",
      "Step: 203, Training Loss: 2.39423, Tokens/sec: 57963.223962016105\n",
      "Step: 204, Training Loss: 2.49011, Tokens/sec: 64274.01216440761\n",
      "Step: 205, Training Loss: 2.50638, Tokens/sec: 85841.40761322944\n",
      "Step: 206, Training Loss: 2.53455, Tokens/sec: 87565.3742695675\n",
      "Step: 207, Training Loss: 2.33736, Tokens/sec: 86948.6379594145\n",
      "Step: 208, Training Loss: 2.30727, Tokens/sec: 83831.46169457448\n",
      "Step: 209, Training Loss: 2.36302, Tokens/sec: 85684.15370980311\n",
      "Step: 210, Training Loss: 2.34286, Tokens/sec: 87191.68096622381\n",
      "Step: 211, Training Loss: 2.40758, Tokens/sec: 85712.83315171814\n",
      "Step: 212, Training Loss: 2.49733, Tokens/sec: 86324.58096256944\n",
      "Step: 213, Training Loss: 2.43928, Tokens/sec: 84479.04471353529\n",
      "Step: 214, Training Loss: 2.44452, Tokens/sec: 87561.7104542236\n",
      "Step: 215, Training Loss: 2.38875, Tokens/sec: 87277.26857183287\n",
      "Step: 216, Training Loss: 2.40203, Tokens/sec: 85033.80397146584\n",
      "Step: 217, Training Loss: 2.43778, Tokens/sec: 87407.15643614116\n",
      "Step: 218, Training Loss: 2.32144, Tokens/sec: 69854.09784028814\n",
      "Step: 219, Training Loss: 2.41212, Tokens/sec: 71365.46207810896\n",
      "Step: 220, Training Loss: 2.39782, Tokens/sec: 68022.39795091818\n",
      "Step: 221, Training Loss: 2.40990, Tokens/sec: 86444.48169403861\n",
      "Step: 222, Training Loss: 2.32014, Tokens/sec: 86688.63519600539\n",
      "Step: 223, Training Loss: 2.37330, Tokens/sec: 85666.29860481888\n",
      "Step: 224, Training Loss: 2.35257, Tokens/sec: 86724.67031771556\n",
      "Step: 225, Training Loss: 2.46522, Tokens/sec: 80934.37631140726\n",
      "Step: 226, Training Loss: 2.51076, Tokens/sec: 86797.97456139638\n",
      "Step: 227, Training Loss: 2.45924, Tokens/sec: 85737.6798230073\n",
      "Step: 228, Training Loss: 2.41255, Tokens/sec: 85198.64714106757\n",
      "Step: 229, Training Loss: 2.27499, Tokens/sec: 87817.29680029154\n",
      "Step: 230, Training Loss: 2.50576, Tokens/sec: 85326.34546352156\n",
      "Step: 231, Training Loss: 2.44060, Tokens/sec: 87085.56428181649\n",
      "Step: 232, Training Loss: 2.45953, Tokens/sec: 87058.59574275839\n",
      "Step: 233, Training Loss: 2.30761, Tokens/sec: 85490.15166145725\n",
      "Step: 234, Training Loss: 2.27385, Tokens/sec: 87042.02911534646\n",
      "Step: 235, Training Loss: 2.63144, Tokens/sec: 86988.29654819409\n",
      "Step: 236, Training Loss: 2.35353, Tokens/sec: 87237.38095474399\n",
      "Step: 237, Training Loss: 2.36348, Tokens/sec: 87319.89784847084\n",
      "Step: 238, Training Loss: 2.35161, Tokens/sec: 83893.35974419094\n",
      "Step: 239, Training Loss: 2.32676, Tokens/sec: 86975.3033923175\n",
      "Step: 240, Training Loss: 2.45364, Tokens/sec: 87332.65574322571\n",
      "Step: 241, Training Loss: 3.16714, Tokens/sec: 85064.29623120806\n",
      "Step: 242, Training Loss: 2.97093, Tokens/sec: 86011.36314742734\n",
      "Step: 243, Training Loss: 2.48896, Tokens/sec: 85763.72563607978\n",
      "Step: 244, Training Loss: 2.50951, Tokens/sec: 87454.82850435989\n",
      "Step: 245, Training Loss: 2.51750, Tokens/sec: 86305.4868777708\n",
      "Step: 246, Training Loss: 2.52678, Tokens/sec: 85605.33805305505\n",
      "Step: 247, Training Loss: 2.41754, Tokens/sec: 87498.77909545934\n",
      "Step: 248, Training Loss: 2.37452, Tokens/sec: 85117.28836605714\n",
      "Step: 249, Training Loss: 2.37978, Tokens/sec: 86121.27119836787\n",
      "Step: 250, Training Loss: 2.32753, Tokens/sec: 87498.50807163786\n",
      "Step: 251, Training Loss: 2.35185, Tokens/sec: 84887.27633811359\n",
      "Step: 252, Training Loss: 2.45313, Tokens/sec: 85918.80014489178\n",
      "Step: 253, Training Loss: 2.46298, Tokens/sec: 85605.42437955193\n",
      "Step: 254, Training Loss: 2.53022, Tokens/sec: 87166.7070491915\n",
      "Step: 255, Training Loss: 2.30653, Tokens/sec: 86122.56228672243\n",
      "Step: 256, Training Loss: 2.35326, Tokens/sec: 84419.62978098987\n",
      "Step: 257, Training Loss: 2.35069, Tokens/sec: 86661.54151668273\n",
      "Step: 258, Training Loss: 2.34740, Tokens/sec: 54495.10039956312\n",
      "Step: 259, Training Loss: 2.45502, Tokens/sec: 60546.96966109472\n",
      "Step: 260, Training Loss: 2.32758, Tokens/sec: 85096.43195659478\n",
      "Step: 261, Training Loss: 2.27264, Tokens/sec: 86603.59517370738\n",
      "Step: 262, Training Loss: 2.34991, Tokens/sec: 86611.08959715106\n",
      "Step: 263, Training Loss: 2.32031, Tokens/sec: 85164.2193715294\n",
      "Step: 264, Training Loss: 2.29644, Tokens/sec: 86055.51163602986\n",
      "Step: 265, Training Loss: 2.36161, Tokens/sec: 86362.91102794286\n",
      "Step: 266, Training Loss: 2.30833, Tokens/sec: 86347.41764220473\n",
      "Step: 267, Training Loss: 2.23503, Tokens/sec: 86056.45722730407\n",
      "Step: 268, Training Loss: 2.26196, Tokens/sec: 84061.91654679047\n",
      "Step: 269, Training Loss: 2.30465, Tokens/sec: 81528.45267513653\n",
      "Step: 270, Training Loss: 2.37435, Tokens/sec: 86822.65708563512\n",
      "Step: 271, Training Loss: 2.17964, Tokens/sec: 83904.49570758107\n",
      "Step: 272, Training Loss: 2.31867, Tokens/sec: 87451.40172850313\n",
      "Step: 273, Training Loss: 2.31221, Tokens/sec: 82681.33677459924\n",
      "Step: 274, Training Loss: 2.38214, Tokens/sec: 55257.71193109568\n",
      "Step: 275, Training Loss: 2.17683, Tokens/sec: 65664.76150534976\n",
      "Step: 276, Training Loss: 2.06738, Tokens/sec: 86025.43529373643\n",
      "Step: 277, Training Loss: 2.41155, Tokens/sec: 67629.59587412595\n",
      "Step: 278, Training Loss: 2.34042, Tokens/sec: 86925.12662719363\n",
      "Step: 279, Training Loss: 2.97717, Tokens/sec: 83392.11644534743\n",
      "Step: 280, Training Loss: 2.32293, Tokens/sec: 87158.08127804851\n",
      "Step: 281, Training Loss: 2.33656, Tokens/sec: 86920.80142906807\n",
      "Step: 282, Training Loss: 2.39631, Tokens/sec: 83651.03947639909\n",
      "Step: 283, Training Loss: 2.35055, Tokens/sec: 85596.25605358738\n",
      "Step: 284, Training Loss: 2.43713, Tokens/sec: 86165.25936057353\n",
      "Step: 285, Training Loss: 2.26206, Tokens/sec: 87339.87367774658\n",
      "Step: 286, Training Loss: 2.26795, Tokens/sec: 53519.018368478435\n",
      "Step: 287, Training Loss: 2.28445, Tokens/sec: 86797.12801921889\n",
      "Step: 288, Training Loss: 2.26138, Tokens/sec: 86494.133251712\n",
      "Step: 289, Training Loss: 2.22443, Tokens/sec: 85457.79549158855\n",
      "Step: 290, Training Loss: 2.27305, Tokens/sec: 85736.96644921204\n",
      "Step: 291, Training Loss: 2.21317, Tokens/sec: 86751.04190580535\n",
      "Step: 292, Training Loss: 2.48089, Tokens/sec: 87009.30983209414\n",
      "Step: 293, Training Loss: 2.23381, Tokens/sec: 86301.32949909309\n",
      "Step: 294, Training Loss: 2.24266, Tokens/sec: 86931.05503836842\n",
      "Step: 295, Training Loss: 2.34671, Tokens/sec: 84784.59287661861\n",
      "Step: 296, Training Loss: 2.55139, Tokens/sec: 85855.25188045406\n",
      "Step: 297, Training Loss: 2.27715, Tokens/sec: 81232.5311195113\n",
      "Step: 298, Training Loss: 2.20360, Tokens/sec: 80925.34336863307\n",
      "Step: 299, Training Loss: 2.22108, Tokens/sec: 81415.6641547838\n",
      "Step: 300, Training Loss: 2.26634, Tokens/sec: 84654.34245859839\n",
      "Step: 301, Training Loss: 2.23083, Tokens/sec: 86086.98317334593\n",
      "Step: 302, Training Loss: 2.19044, Tokens/sec: 82071.86372691536\n",
      "Step: 303, Training Loss: 2.23778, Tokens/sec: 86504.80299990338\n",
      "Step: 304, Training Loss: 2.34095, Tokens/sec: 86775.86365417065\n",
      "Step: 305, Training Loss: 2.38512, Tokens/sec: 86031.7177575634\n",
      "Step: 306, Training Loss: 2.30322, Tokens/sec: 86691.09651067934\n",
      "Step: 307, Training Loss: 2.24257, Tokens/sec: 87436.68705434003\n",
      "Step: 308, Training Loss: 2.23634, Tokens/sec: 86436.04340883576\n",
      "Step: 309, Training Loss: 2.22716, Tokens/sec: 86653.75696461333\n",
      "Step: 310, Training Loss: 2.26792, Tokens/sec: 86990.0760948175\n",
      "Step: 311, Training Loss: 2.32511, Tokens/sec: 87456.68181252058\n",
      "Step: 312, Training Loss: 2.16378, Tokens/sec: 87054.6518160303\n",
      "Step: 313, Training Loss: 2.25387, Tokens/sec: 85972.89645705596\n",
      "Step: 314, Training Loss: 2.22832, Tokens/sec: 87629.76254137549\n",
      "Step: 315, Training Loss: 2.14317, Tokens/sec: 87689.76278413448\n",
      "Step: 316, Training Loss: 2.09264, Tokens/sec: 86351.76560633144\n",
      "Step: 317, Training Loss: 2.23768, Tokens/sec: 87545.93463111875\n",
      "Step: 318, Training Loss: 2.25208, Tokens/sec: 87028.37825698422\n",
      "Step: 319, Training Loss: 2.19250, Tokens/sec: 87124.03537801774\n",
      "Step: 320, Training Loss: 2.22358, Tokens/sec: 86532.65611695846\n",
      "Step: 321, Training Loss: 2.25962, Tokens/sec: 86396.99645587827\n",
      "Step: 322, Training Loss: 2.23569, Tokens/sec: 87256.90126813519\n",
      "Step: 323, Training Loss: 2.31472, Tokens/sec: 85905.69118770886\n",
      "Step: 324, Training Loss: 2.49924, Tokens/sec: 85970.78656664079\n",
      "Step: 325, Training Loss: 2.46865, Tokens/sec: 87121.4692731982\n",
      "Step: 326, Training Loss: 2.26079, Tokens/sec: 86679.12883679065\n",
      "Step: 327, Training Loss: 2.32154, Tokens/sec: 86124.80866356991\n",
      "Step: 328, Training Loss: 2.22290, Tokens/sec: 87704.08577151452\n",
      "Step: 329, Training Loss: 2.11459, Tokens/sec: 86959.92876512866\n",
      "Step: 330, Training Loss: 2.12033, Tokens/sec: 85439.0975891673\n",
      "Step: 331, Training Loss: 2.25022, Tokens/sec: 87511.26819781726\n",
      "Step: 332, Training Loss: 2.21640, Tokens/sec: 86946.18229645364\n",
      "Step: 333, Training Loss: 2.14716, Tokens/sec: 86920.56394571479\n",
      "Step: 334, Training Loss: 2.23890, Tokens/sec: 87726.8949544298\n",
      "Step: 335, Training Loss: 2.20008, Tokens/sec: 87207.3836706796\n",
      "Step: 336, Training Loss: 2.36436, Tokens/sec: 87096.61982503704\n",
      "Step: 337, Training Loss: 2.62618, Tokens/sec: 87086.83723506003\n",
      "Step: 338, Training Loss: 2.37216, Tokens/sec: 86091.59311022809\n",
      "Step: 339, Training Loss: 2.27847, Tokens/sec: 86182.94318960058\n",
      "Step: 340, Training Loss: 2.22939, Tokens/sec: 85953.93867832863\n",
      "Step: 341, Training Loss: 2.16319, Tokens/sec: 86938.62147167989\n",
      "Step: 342, Training Loss: 2.18681, Tokens/sec: 86779.81223845364\n",
      "Step: 343, Training Loss: 2.22198, Tokens/sec: 86869.11504048873\n",
      "Step: 344, Training Loss: 2.27177, Tokens/sec: 86247.5329226496\n",
      "Step: 345, Training Loss: 2.14603, Tokens/sec: 86713.26244753724\n",
      "Step: 346, Training Loss: 2.21025, Tokens/sec: 87016.95551615293\n",
      "Step: 347, Training Loss: 2.21145, Tokens/sec: 86819.93939738275\n",
      "Step: 348, Training Loss: 2.17162, Tokens/sec: 87389.67753080264\n",
      "Step: 349, Training Loss: 2.70352, Tokens/sec: 87436.73884722209\n",
      "Step: 350, Training Loss: 2.23224, Tokens/sec: 85839.2951762919\n",
      "Step: 351, Training Loss: 2.20936, Tokens/sec: 86816.41130483603\n",
      "Step: 352, Training Loss: 2.16265, Tokens/sec: 86701.92043865017\n",
      "Step: 353, Training Loss: 2.26208, Tokens/sec: 86888.43724218379\n",
      "Step: 354, Training Loss: 2.20134, Tokens/sec: 86289.77413226113\n",
      "Step: 355, Training Loss: 2.43236, Tokens/sec: 87243.1801495893\n",
      "Step: 356, Training Loss: 2.21379, Tokens/sec: 87857.78869932244\n",
      "Step: 357, Training Loss: 2.10461, Tokens/sec: 86216.29143766085\n",
      "Step: 358, Training Loss: 2.14135, Tokens/sec: 87273.58280681026\n",
      "Step: 359, Training Loss: 2.16652, Tokens/sec: 86929.00117320997\n",
      "Step: 360, Training Loss: 2.16722, Tokens/sec: 87760.7739404172\n",
      "Step: 361, Training Loss: 2.35864, Tokens/sec: 86403.66685946923\n",
      "Step: 362, Training Loss: 2.45744, Tokens/sec: 87135.28188717634\n",
      "Step: 363, Training Loss: 2.08735, Tokens/sec: 87615.94921911081\n",
      "Step: 364, Training Loss: 2.16284, Tokens/sec: 85911.17818370512\n",
      "Step: 365, Training Loss: 2.16519, Tokens/sec: 86743.20681952825\n",
      "Step: 366, Training Loss: 2.07915, Tokens/sec: 86981.55820420511\n",
      "Step: 367, Training Loss: 2.10166, Tokens/sec: 85936.65204660613\n",
      "Step: 368, Training Loss: 2.27286, Tokens/sec: 86116.76037582787\n",
      "Step: 369, Training Loss: 2.09572, Tokens/sec: 87603.32483350136\n",
      "Step: 370, Training Loss: 2.02792, Tokens/sec: 87216.45980159006\n",
      "Step: 371, Training Loss: 1.97339, Tokens/sec: 86076.55823754038\n",
      "Step: 372, Training Loss: 2.07044, Tokens/sec: 86878.82389992033\n",
      "Step: 373, Training Loss: 1.95325, Tokens/sec: 87327.02710173774\n",
      "Step: 374, Training Loss: 1.92272, Tokens/sec: 86929.91255498199\n",
      "Step: 375, Training Loss: 2.12263, Tokens/sec: 87746.40752390285\n",
      "Step: 376, Training Loss: 2.13544, Tokens/sec: 86429.1705897563\n",
      "Step: 377, Training Loss: 2.05424, Tokens/sec: 86112.38037532174\n",
      "Step: 378, Training Loss: 2.11123, Tokens/sec: 85777.68048086583\n",
      "Step: 379, Training Loss: 2.07078, Tokens/sec: 85886.84316978922\n",
      "Step: 380, Training Loss: 2.01607, Tokens/sec: 85971.69105341945\n",
      "Step: 381, Training Loss: 2.09343, Tokens/sec: 86613.16509673302\n",
      "Step: 382, Training Loss: 2.14957, Tokens/sec: 86943.27232217182\n",
      "Step: 383, Training Loss: 2.33727, Tokens/sec: 85845.51944583062\n",
      "Step: 384, Training Loss: 2.16764, Tokens/sec: 87229.47170545663\n",
      "Step: 385, Training Loss: 2.20470, Tokens/sec: 86943.74338581746\n",
      "Step: 386, Training Loss: 2.24621, Tokens/sec: 86815.75761331232\n",
      "Step: 387, Training Loss: 2.25562, Tokens/sec: 87175.09055218232\n",
      "Step: 388, Training Loss: 2.32396, Tokens/sec: 87204.05330686753\n",
      "Step: 389, Training Loss: 2.26284, Tokens/sec: 87072.23248111237\n",
      "Step: 390, Training Loss: 2.24866, Tokens/sec: 87202.21625870824\n",
      "Step: 391, Training Loss: 2.24956, Tokens/sec: 85897.55680152708\n",
      "Step: 392, Training Loss: 2.34755, Tokens/sec: 86393.99875687044\n",
      "Step: 393, Training Loss: 2.35791, Tokens/sec: 87094.04283409621\n",
      "Step: 394, Training Loss: 2.36166, Tokens/sec: 85360.92536423082\n",
      "Step: 395, Training Loss: 2.06635, Tokens/sec: 86467.5383925556\n",
      "Step: 396, Training Loss: 2.08849, Tokens/sec: 87065.54729029907\n",
      "Step: 397, Training Loss: 2.24740, Tokens/sec: 86860.23768708766\n",
      "Step: 398, Training Loss: 1.99304, Tokens/sec: 85512.0762868797\n",
      "Step: 399, Training Loss: 1.98671, Tokens/sec: 87148.66220872609\n",
      "Step: 400, Training Loss: 2.08435, Tokens/sec: 86503.82378007926\n",
      "Step: 401, Training Loss: 2.39820, Tokens/sec: 86693.15841738513\n",
      "Step: 402, Training Loss: 2.22119, Tokens/sec: 86290.65762000375\n",
      "Step: 403, Training Loss: 2.04986, Tokens/sec: 87126.19159880345\n",
      "Step: 404, Training Loss: 1.99487, Tokens/sec: 87378.81542425424\n",
      "Step: 405, Training Loss: 1.99111, Tokens/sec: 85574.24846412703\n",
      "Step: 406, Training Loss: 2.12862, Tokens/sec: 86344.19995744828\n",
      "Step: 407, Training Loss: 2.10213, Tokens/sec: 86422.52314415772\n",
      "Step: 408, Training Loss: 2.02555, Tokens/sec: 86315.31934613224\n",
      "Step: 409, Training Loss: 2.06445, Tokens/sec: 86817.16805627788\n",
      "Step: 410, Training Loss: 2.13636, Tokens/sec: 86548.06197878806\n",
      "Step: 411, Training Loss: 2.01797, Tokens/sec: 86195.3424238539\n",
      "Step: 412, Training Loss: 2.08141, Tokens/sec: 85689.98307387931\n",
      "Step: 413, Training Loss: 1.97816, Tokens/sec: 86780.07791192993\n",
      "Step: 414, Training Loss: 2.04334, Tokens/sec: 87183.72247533925\n",
      "Step: 415, Training Loss: 2.07093, Tokens/sec: 86549.59907269751\n",
      "Step: 416, Training Loss: 2.63737, Tokens/sec: 86838.74540162891\n",
      "Step: 417, Training Loss: 2.07397, Tokens/sec: 86417.701299662\n",
      "Step: 418, Training Loss: 2.16440, Tokens/sec: 86343.20435011011\n",
      "Step: 419, Training Loss: 1.97616, Tokens/sec: 86411.6312256147\n",
      "Step: 420, Training Loss: 2.03627, Tokens/sec: 86005.4529592631\n",
      "Step: 421, Training Loss: 2.02149, Tokens/sec: 85763.4966806288\n",
      "Step: 422, Training Loss: 2.12669, Tokens/sec: 86243.46602167442\n",
      "Step: 423, Training Loss: 2.00470, Tokens/sec: 87222.55293019548\n",
      "Step: 424, Training Loss: 1.98419, Tokens/sec: 86467.38186758557\n",
      "Step: 425, Training Loss: 2.01258, Tokens/sec: 85798.07918459659\n",
      "Step: 426, Training Loss: 2.07240, Tokens/sec: 87120.55989374456\n",
      "Step: 427, Training Loss: 2.01485, Tokens/sec: 86432.73795328615\n",
      "Step: 428, Training Loss: 1.97746, Tokens/sec: 85426.39471244808\n",
      "Step: 429, Training Loss: 2.15599, Tokens/sec: 86495.76706777087\n",
      "Step: 430, Training Loss: 2.06023, Tokens/sec: 86949.92674856703\n",
      "Step: 431, Training Loss: 2.06374, Tokens/sec: 86920.20196065106\n",
      "Step: 432, Training Loss: 2.11183, Tokens/sec: 84949.43180994969\n",
      "Step: 433, Training Loss: 2.13302, Tokens/sec: 86579.46160587828\n",
      "Step: 434, Training Loss: 1.95899, Tokens/sec: 86757.69725875604\n",
      "Step: 435, Training Loss: 1.96642, Tokens/sec: 85758.42534321222\n",
      "Step: 436, Training Loss: 2.02646, Tokens/sec: 85664.5378675191\n",
      "Step: 437, Training Loss: 1.96358, Tokens/sec: 87304.3220753415\n",
      "Step: 438, Training Loss: 1.97848, Tokens/sec: 86774.49637185162\n",
      "Step: 439, Training Loss: 2.11305, Tokens/sec: 85581.06868694176\n",
      "Step: 440, Training Loss: 2.03706, Tokens/sec: 86121.96970373494\n",
      "Step: 441, Training Loss: 2.00960, Tokens/sec: 86357.44765392525\n",
      "Step: 442, Training Loss: 1.99311, Tokens/sec: 86194.2491281659\n",
      "Step: 443, Training Loss: 1.96826, Tokens/sec: 85660.89748403081\n",
      "Step: 444, Training Loss: 1.95193, Tokens/sec: 86650.14796009801\n",
      "Step: 445, Training Loss: 1.93450, Tokens/sec: 86918.63599704861\n",
      "Step: 446, Training Loss: 1.90430, Tokens/sec: 85774.09154320075\n",
      "Step: 447, Training Loss: 1.82605, Tokens/sec: 86194.58060402655\n",
      "Step: 448, Training Loss: 1.95934, Tokens/sec: 86806.68511702066\n",
      "Step: 449, Training Loss: 2.10775, Tokens/sec: 86327.64480628824\n",
      "Step: 450, Training Loss: 2.08822, Tokens/sec: 86761.93088622454\n",
      "Step: 451, Training Loss: 2.00493, Tokens/sec: 85705.0347286049\n",
      "Step: 452, Training Loss: 2.01278, Tokens/sec: 86448.27839325594\n",
      "Step: 453, Training Loss: 2.06337, Tokens/sec: 87016.71473294153\n",
      "Step: 454, Training Loss: 2.03894, Tokens/sec: 86452.16939329547\n",
      "Step: 455, Training Loss: 2.31749, Tokens/sec: 86688.58244675824\n",
      "Step: 456, Training Loss: 2.17668, Tokens/sec: 86373.70597118056\n",
      "Step: 457, Training Loss: 2.02383, Tokens/sec: 86542.86543424444\n",
      "Step: 458, Training Loss: 1.91299, Tokens/sec: 86107.18533528714\n",
      "Step: 459, Training Loss: 1.88096, Tokens/sec: 86737.76226761402\n",
      "Step: 460, Training Loss: 1.93993, Tokens/sec: 87219.95223206485\n",
      "Step: 461, Training Loss: 1.92144, Tokens/sec: 86818.62363667123\n",
      "Step: 462, Training Loss: 1.81359, Tokens/sec: 86148.33154560583\n",
      "Step: 463, Training Loss: 2.00521, Tokens/sec: 87288.37563708109\n",
      "Step: 464, Training Loss: 2.05195, Tokens/sec: 87300.15257093756\n",
      "Step: 465, Training Loss: 2.08380, Tokens/sec: 86067.59752417941\n",
      "Step: 466, Training Loss: 1.97196, Tokens/sec: 84118.18870512732\n",
      "Step: 467, Training Loss: 2.01932, Tokens/sec: 86781.30195726662\n",
      "Step: 468, Training Loss: 2.05752, Tokens/sec: 86089.52262586454\n",
      "Step: 469, Training Loss: 1.94747, Tokens/sec: 85780.82599019281\n",
      "Step: 470, Training Loss: 2.02557, Tokens/sec: 85805.7817563688\n",
      "Step: 471, Training Loss: 2.07920, Tokens/sec: 86244.40212901852\n",
      "Step: 472, Training Loss: 2.50890, Tokens/sec: 86679.11370214466\n",
      "Step: 473, Training Loss: 1.97593, Tokens/sec: 85454.26536021833\n",
      "Step: 474, Training Loss: 2.06988, Tokens/sec: 87314.78084329762\n",
      "Step: 475, Training Loss: 2.02913, Tokens/sec: 86879.69322790777\n",
      "Step: 476, Training Loss: 1.93340, Tokens/sec: 86321.27538639086\n",
      "Step: 477, Training Loss: 2.02042, Tokens/sec: 87058.49721125406\n",
      "Step: 478, Training Loss: 2.06035, Tokens/sec: 87073.03904849674\n",
      "Step: 479, Training Loss: 2.04588, Tokens/sec: 87022.9288196553\n",
      "Step: 480, Training Loss: 2.00045, Tokens/sec: 86290.42038644826\n",
      "Step: 481, Training Loss: 2.07057, Tokens/sec: 86315.71450883787\n",
      "Step: 482, Training Loss: 2.17440, Tokens/sec: 86436.71100355755\n",
      "Step: 483, Training Loss: 1.91038, Tokens/sec: 86263.09812207731\n",
      "Step: 484, Training Loss: 2.01894, Tokens/sec: 85917.26510253063\n",
      "Step: 485, Training Loss: 1.98346, Tokens/sec: 86503.35336074856\n",
      "Step: 486, Training Loss: 2.06668, Tokens/sec: 86684.97237955176\n",
      "Step: 487, Training Loss: 2.26733, Tokens/sec: 85679.90946260095\n",
      "Step: 488, Training Loss: 1.99820, Tokens/sec: 80644.26379502732\n",
      "Step: 489, Training Loss: 1.91629, Tokens/sec: 81602.34410959644\n",
      "Step: 490, Training Loss: 2.00229, Tokens/sec: 54067.32775612024\n",
      "Step: 491, Training Loss: 2.03258, Tokens/sec: 57017.162259636374\n",
      "Step: 492, Training Loss: 2.12325, Tokens/sec: 63169.377709416025\n",
      "Step: 493, Training Loss: 1.99435, Tokens/sec: 54193.02487717786\n",
      "Step: 494, Training Loss: 1.89115, Tokens/sec: 82204.94082129827\n",
      "Step: 495, Training Loss: 1.97176, Tokens/sec: 86793.000843614\n",
      "Step: 496, Training Loss: 2.22907, Tokens/sec: 86192.89557623143\n",
      "Step: 497, Training Loss: 1.95962, Tokens/sec: 84349.50400386476\n",
      "Step: 498, Training Loss: 2.01433, Tokens/sec: 86482.81745131266\n",
      "Step: 499, Training Loss: 1.85933, Tokens/sec: 83788.32163264195\n",
      "Step: 500, Training Loss: 2.02358, Tokens/sec: 85925.83312577779\n",
      "Step: 501, Training Loss: 1.95638, Tokens/sec: 85143.89393376831\n",
      "Step: 502, Training Loss: 2.01513, Tokens/sec: 74621.2809440899\n",
      "Step: 503, Training Loss: 2.03421, Tokens/sec: 69665.75235867858\n",
      "Step: 504, Training Loss: 1.95491, Tokens/sec: 87128.93590759073\n",
      "Step: 505, Training Loss: 2.00208, Tokens/sec: 86231.82765855429\n",
      "Step: 506, Training Loss: 1.84233, Tokens/sec: 83190.03523234303\n",
      "Step: 507, Training Loss: 1.92950, Tokens/sec: 84721.95017572408\n",
      "Step: 508, Training Loss: 1.99850, Tokens/sec: 86303.31426097939\n",
      "Step: 509, Training Loss: 1.97270, Tokens/sec: 55914.3361789876\n",
      "Step: 510, Training Loss: 1.97687, Tokens/sec: 60856.06920474583\n",
      "Step: 511, Training Loss: 1.99803, Tokens/sec: 86096.08727006736\n",
      "Step: 512, Training Loss: 1.94730, Tokens/sec: 60648.4907241037\n",
      "Step: 513, Training Loss: 1.81610, Tokens/sec: 82680.65541547454\n",
      "Step: 514, Training Loss: 1.97332, Tokens/sec: 86230.41347496478\n",
      "Step: 515, Training Loss: 1.87428, Tokens/sec: 87066.79189172896\n",
      "Step: 516, Training Loss: 1.91303, Tokens/sec: 84474.57318270188\n",
      "Step: 517, Training Loss: 2.05532, Tokens/sec: 86502.62719198095\n",
      "Step: 518, Training Loss: 1.95146, Tokens/sec: 84845.84182610331\n",
      "Step: 519, Training Loss: 2.04978, Tokens/sec: 86699.6406446503\n",
      "Step: 520, Training Loss: 2.08933, Tokens/sec: 86379.018838938\n",
      "Step: 521, Training Loss: 1.80405, Tokens/sec: 83734.41979087098\n",
      "Step: 522, Training Loss: 1.82558, Tokens/sec: 86913.0421514248\n",
      "Step: 523, Training Loss: 2.03254, Tokens/sec: 86146.71763078675\n",
      "Step: 524, Training Loss: 1.81863, Tokens/sec: 86629.87942363982\n",
      "Step: 525, Training Loss: 1.87464, Tokens/sec: 86359.79733840474\n",
      "Step: 526, Training Loss: 1.94410, Tokens/sec: 84711.90313538739\n",
      "Step: 527, Training Loss: 1.86542, Tokens/sec: 85805.662220834\n",
      "Step: 528, Training Loss: 1.80050, Tokens/sec: 86289.94046724838\n",
      "Step: 529, Training Loss: 2.02099, Tokens/sec: 83542.71021283632\n",
      "Step: 530, Training Loss: 1.95533, Tokens/sec: 84670.58666314646\n",
      "Step: 531, Training Loss: 1.95220, Tokens/sec: 86051.39547684207\n",
      "Step: 532, Training Loss: 1.90183, Tokens/sec: 85359.75840305597\n",
      "Step: 533, Training Loss: 1.90595, Tokens/sec: 84908.10513892001\n",
      "Step: 534, Training Loss: 1.90623, Tokens/sec: 83622.65979209395\n",
      "Step: 535, Training Loss: 1.85217, Tokens/sec: 86226.81604868316\n",
      "Step: 536, Training Loss: 2.10662, Tokens/sec: 85789.07573386715\n",
      "Step: 537, Training Loss: 2.17195, Tokens/sec: 83575.98970298056\n",
      "Step: 538, Training Loss: 1.94636, Tokens/sec: 85495.8993341993\n",
      "Step: 539, Training Loss: 1.95142, Tokens/sec: 83918.87673307135\n",
      "Step: 540, Training Loss: 1.79791, Tokens/sec: 85404.33858914099\n",
      "Step: 541, Training Loss: 1.88731, Tokens/sec: 84583.536409066\n",
      "Step: 542, Training Loss: 2.08915, Tokens/sec: 83806.26661386815\n",
      "Step: 543, Training Loss: 1.95507, Tokens/sec: 85876.45404857128\n",
      "Step: 544, Training Loss: 1.89941, Tokens/sec: 84556.97828179575\n",
      "Step: 545, Training Loss: 1.88539, Tokens/sec: 83912.90291461926\n",
      "Step: 546, Training Loss: 2.02045, Tokens/sec: 86408.84259208797\n",
      "Step: 547, Training Loss: 1.93024, Tokens/sec: 84330.69669300906\n",
      "Step: 548, Training Loss: 1.86972, Tokens/sec: 84822.82407497233\n",
      "Step: 549, Training Loss: 1.95257, Tokens/sec: 81370.88647595035\n",
      "Step: 550, Training Loss: 1.87000, Tokens/sec: 84113.49532840177\n",
      "Step: 551, Training Loss: 1.86675, Tokens/sec: 85492.52754392839\n",
      "Step: 552, Training Loss: 1.90928, Tokens/sec: 85261.75963994516\n",
      "Step: 553, Training Loss: 2.02392, Tokens/sec: 86062.7614063887\n",
      "Step: 554, Training Loss: 1.87666, Tokens/sec: 85997.9785619795\n",
      "Step: 555, Training Loss: 1.98036, Tokens/sec: 83585.00920458249\n",
      "Step: 556, Training Loss: 1.91159, Tokens/sec: 86238.82754642404\n",
      "Step: 557, Training Loss: 1.99502, Tokens/sec: 86689.08057027654\n",
      "Step: 558, Training Loss: 1.91618, Tokens/sec: 83628.19366610565\n",
      "Step: 559, Training Loss: 1.92129, Tokens/sec: 85381.780826212\n",
      "Step: 560, Training Loss: 1.86868, Tokens/sec: 86315.43257363814\n",
      "Step: 561, Training Loss: 1.94537, Tokens/sec: 86487.2799555978\n",
      "Step: 562, Training Loss: 2.02438, Tokens/sec: 85235.97209970566\n",
      "Step: 563, Training Loss: 1.94800, Tokens/sec: 81854.70859361812\n",
      "Step: 564, Training Loss: 1.98798, Tokens/sec: 85967.16070007256\n",
      "Step: 565, Training Loss: 1.84193, Tokens/sec: 86900.79003897985\n",
      "Step: 566, Training Loss: 1.94312, Tokens/sec: 83661.77627260909\n",
      "Step: 567, Training Loss: 1.96820, Tokens/sec: 86680.34407669301\n",
      "Step: 568, Training Loss: 1.86922, Tokens/sec: 86338.96326101877\n",
      "Step: 569, Training Loss: 2.19849, Tokens/sec: 86023.15254302093\n",
      "Step: 570, Training Loss: 2.26367, Tokens/sec: 85712.20000923536\n",
      "Step: 571, Training Loss: 2.04775, Tokens/sec: 84120.72260163527\n",
      "Step: 572, Training Loss: 1.98262, Tokens/sec: 85719.20951447226\n",
      "Step: 573, Training Loss: 1.83553, Tokens/sec: 86145.17352407801\n",
      "Step: 574, Training Loss: 1.97059, Tokens/sec: 83645.25617993211\n",
      "Step: 575, Training Loss: 1.88296, Tokens/sec: 85922.5111526495\n",
      "Step: 576, Training Loss: 1.74837, Tokens/sec: 82638.1753282855\n",
      "Step: 577, Training Loss: 1.86635, Tokens/sec: 85743.92302271572\n",
      "Step: 578, Training Loss: 1.93154, Tokens/sec: 86080.48866194686\n",
      "Step: 579, Training Loss: 1.91602, Tokens/sec: 83800.29124003739\n",
      "Step: 580, Training Loss: 1.93671, Tokens/sec: 85013.74982312719\n",
      "Step: 581, Training Loss: 1.89201, Tokens/sec: 85790.53118025478\n",
      "Step: 582, Training Loss: 1.83644, Tokens/sec: 84204.25427549577\n",
      "Step: 583, Training Loss: 1.90999, Tokens/sec: 85133.39533803341\n",
      "Step: 584, Training Loss: 2.29217, Tokens/sec: 82334.37118742813\n",
      "Step: 585, Training Loss: 2.20445, Tokens/sec: 85774.16114550138\n",
      "Step: 586, Training Loss: 2.19760, Tokens/sec: 86582.53578896778\n",
      "Step: 587, Training Loss: 1.93399, Tokens/sec: 82266.07238969846\n",
      "Step: 588, Training Loss: 1.87455, Tokens/sec: 85457.33102910369\n",
      "Step: 589, Training Loss: 1.90938, Tokens/sec: 86238.44080585746\n",
      "Step: 590, Training Loss: 1.86211, Tokens/sec: 84157.27054133746\n",
      "Step: 591, Training Loss: 1.83232, Tokens/sec: 84950.75319434765\n",
      "Step: 592, Training Loss: 1.83156, Tokens/sec: 83181.04797770715\n",
      "Step: 593, Training Loss: 1.82403, Tokens/sec: 86072.7032342277\n",
      "Step: 594, Training Loss: 2.05583, Tokens/sec: 85922.40931718181\n",
      "Step: 595, Training Loss: 1.95394, Tokens/sec: 81715.9715598483\n",
      "Step: 596, Training Loss: 1.83413, Tokens/sec: 85228.61533054679\n",
      "Step: 597, Training Loss: 1.84929, Tokens/sec: 85992.92775380748\n",
      "Step: 598, Training Loss: 1.80532, Tokens/sec: 85575.66400471979\n",
      "Step: 599, Training Loss: 1.86604, Tokens/sec: 84680.18185693261\n",
      "Step: 600, Training Loss: 1.80769, Tokens/sec: 83431.28811691538\n",
      "Step: 601, Training Loss: 1.79396, Tokens/sec: 86255.46762449443\n",
      "Step: 602, Training Loss: 1.94226, Tokens/sec: 86031.15533530487\n",
      "Step: 603, Training Loss: 1.88605, Tokens/sec: 83386.72496203175\n",
      "Step: 604, Training Loss: 1.83987, Tokens/sec: 86901.80822895734\n",
      "Step: 605, Training Loss: 1.81462, Tokens/sec: 86147.92160971803\n",
      "Step: 606, Training Loss: 2.16188, Tokens/sec: 86333.73038068601\n",
      "Step: 607, Training Loss: 2.15959, Tokens/sec: 85712.19507301648\n",
      "Step: 608, Training Loss: 1.89742, Tokens/sec: 82459.08534115231\n",
      "Step: 609, Training Loss: 1.80979, Tokens/sec: 86321.4814062544\n",
      "Step: 610, Training Loss: 1.84194, Tokens/sec: 86068.0790393987\n",
      "Step: 611, Training Loss: 1.84149, Tokens/sec: 83699.24164963579\n",
      "Step: 612, Training Loss: 1.87684, Tokens/sec: 86622.79850791754\n",
      "Step: 613, Training Loss: 1.92067, Tokens/sec: 84495.38119447355\n",
      "Step: 614, Training Loss: 1.78199, Tokens/sec: 86484.23581385489\n",
      "Step: 615, Training Loss: 1.98922, Tokens/sec: 84306.11296278195\n",
      "Step: 616, Training Loss: 1.97400, Tokens/sec: 84068.80150953765\n",
      "Step: 617, Training Loss: 2.05342, Tokens/sec: 86104.78602250475\n",
      "Step: 618, Training Loss: 1.82928, Tokens/sec: 86470.98159715247\n",
      "Step: 619, Training Loss: 1.93449, Tokens/sec: 83708.27793864094\n",
      "Step: 620, Training Loss: 1.79018, Tokens/sec: 86497.14429974242\n",
      "Step: 621, Training Loss: 1.86396, Tokens/sec: 84704.10010381122\n",
      "Step: 622, Training Loss: 1.83636, Tokens/sec: 85126.62328213021\n",
      "Step: 623, Training Loss: 1.78704, Tokens/sec: 84930.20019500905\n",
      "Step: 624, Training Loss: 1.85433, Tokens/sec: 83929.10369853856\n",
      "Step: 625, Training Loss: 1.78090, Tokens/sec: 85970.99678231477\n",
      "Step: 626, Training Loss: 1.83960, Tokens/sec: 84962.94577860003\n",
      "Step: 627, Training Loss: 1.94669, Tokens/sec: 84264.64564575198\n",
      "Step: 628, Training Loss: 1.87784, Tokens/sec: 86388.52689526019\n",
      "Step: 629, Training Loss: 1.82036, Tokens/sec: 83329.87142621154\n",
      "Step: 630, Training Loss: 1.81215, Tokens/sec: 85657.10200943402\n",
      "Step: 631, Training Loss: 1.76428, Tokens/sec: 86129.02647588447\n",
      "Step: 632, Training Loss: 1.84341, Tokens/sec: 84291.98835303744\n",
      "Step: 633, Training Loss: 2.06567, Tokens/sec: 84096.333644263\n",
      "Step: 634, Training Loss: 1.85305, Tokens/sec: 85229.89575798025\n",
      "Step: 635, Training Loss: 1.92319, Tokens/sec: 85499.95582343386\n",
      "Step: 636, Training Loss: 1.86621, Tokens/sec: 86337.98688178387\n",
      "Step: 637, Training Loss: 1.66370, Tokens/sec: 84024.6076186743\n",
      "Step: 638, Training Loss: 1.83753, Tokens/sec: 86007.50359732842\n",
      "Step: 639, Training Loss: 1.77662, Tokens/sec: 86403.63951745997\n",
      "Step: 640, Training Loss: 1.90187, Tokens/sec: 83594.2635269307\n",
      "Step: 641, Training Loss: 1.70936, Tokens/sec: 86639.64941750327\n",
      "Step: 642, Training Loss: 1.85270, Tokens/sec: 84408.30535045864\n",
      "Step: 643, Training Loss: 1.82979, Tokens/sec: 86764.85078889562\n",
      "Step: 644, Training Loss: 1.87665, Tokens/sec: 86139.51937517371\n",
      "Step: 645, Training Loss: 1.72810, Tokens/sec: 84335.1256547941\n",
      "Step: 646, Training Loss: 1.91287, Tokens/sec: 85985.74389649903\n",
      "Step: 647, Training Loss: 1.99435, Tokens/sec: 86156.57828463582\n",
      "Step: 648, Training Loss: 1.92414, Tokens/sec: 84796.90589898772\n",
      "Step: 649, Training Loss: 1.81128, Tokens/sec: 86589.38771508804\n",
      "Step: 650, Training Loss: 1.65985, Tokens/sec: 84415.21629367239\n",
      "Step: 651, Training Loss: 1.65909, Tokens/sec: 85869.57001504923\n",
      "Step: 652, Training Loss: 1.84307, Tokens/sec: 86958.73844511977\n",
      "Step: 653, Training Loss: 1.69963, Tokens/sec: 84355.69519953232\n",
      "Step: 654, Training Loss: 1.73365, Tokens/sec: 85490.12489454626\n",
      "Step: 655, Training Loss: 1.97707, Tokens/sec: 86387.74571231412\n",
      "Step: 656, Training Loss: 1.86614, Tokens/sec: 86421.13734220868\n",
      "Step: 657, Training Loss: 1.75884, Tokens/sec: 86625.85790736025\n",
      "Step: 658, Training Loss: 1.81423, Tokens/sec: 83775.08704432134\n",
      "Step: 659, Training Loss: 1.71719, Tokens/sec: 86473.29045568022\n",
      "Step: 660, Training Loss: 1.66494, Tokens/sec: 86021.55866779211\n",
      "Step: 661, Training Loss: 1.79215, Tokens/sec: 82563.48369155318\n",
      "Step: 662, Training Loss: 1.88094, Tokens/sec: 85468.99131990141\n",
      "Step: 663, Training Loss: 1.90376, Tokens/sec: 85854.58738775607\n",
      "Step: 664, Training Loss: 1.86190, Tokens/sec: 86093.03169697065\n",
      "Step: 665, Training Loss: 1.75327, Tokens/sec: 85432.36279067003\n",
      "Step: 666, Training Loss: 1.80719, Tokens/sec: 83958.58109779526\n",
      "Step: 667, Training Loss: 1.79897, Tokens/sec: 86408.60015238539\n",
      "Step: 668, Training Loss: 1.78102, Tokens/sec: 85820.6749763773\n",
      "Step: 669, Training Loss: 2.14004, Tokens/sec: 82799.48327821934\n",
      "Step: 670, Training Loss: 1.96034, Tokens/sec: 86092.18572949193\n",
      "Step: 671, Training Loss: 1.82783, Tokens/sec: 84159.06799513754\n",
      "Step: 672, Training Loss: 2.09343, Tokens/sec: 85778.20681036444\n",
      "Step: 673, Training Loss: 1.94155, Tokens/sec: 85634.4575386355\n",
      "Step: 674, Training Loss: 1.71791, Tokens/sec: 83659.57836945982\n",
      "Step: 675, Training Loss: 1.64115, Tokens/sec: 85723.8528094167\n",
      "Step: 676, Training Loss: 1.86159, Tokens/sec: 85450.974849149\n",
      "Step: 677, Training Loss: 1.95231, Tokens/sec: 84670.19897971308\n",
      "Step: 678, Training Loss: 1.79959, Tokens/sec: 86231.79135071088\n",
      "Step: 679, Training Loss: 1.77198, Tokens/sec: 83381.78780402022\n",
      "Step: 680, Training Loss: 2.08956, Tokens/sec: 86652.66162585093\n",
      "Step: 681, Training Loss: 1.91479, Tokens/sec: 86125.21023332213\n",
      "Step: 682, Training Loss: 1.67504, Tokens/sec: 84112.41188540224\n",
      "Step: 683, Training Loss: 1.80791, Tokens/sec: 84471.47135268383\n",
      "Step: 684, Training Loss: 1.85011, Tokens/sec: 86518.64142714199\n",
      "Step: 685, Training Loss: 1.96753, Tokens/sec: 85924.64030393267\n",
      "Step: 686, Training Loss: 1.74792, Tokens/sec: 85733.72680467849\n",
      "Step: 687, Training Loss: 1.83809, Tokens/sec: 83783.35308489947\n",
      "Step: 688, Training Loss: 2.18054, Tokens/sec: 86592.1417890103\n",
      "Step: 689, Training Loss: 1.83420, Tokens/sec: 85005.00107381537\n",
      "Step: 690, Training Loss: 1.71095, Tokens/sec: 82018.09610376877\n",
      "Step: 691, Training Loss: 1.66372, Tokens/sec: 86629.47084101591\n",
      "Step: 692, Training Loss: 1.79181, Tokens/sec: 86574.5540688247\n",
      "Step: 693, Training Loss: 1.81251, Tokens/sec: 85863.88583576208\n",
      "Step: 694, Training Loss: 1.83800, Tokens/sec: 87182.66798154237\n",
      "Step: 695, Training Loss: 1.75995, Tokens/sec: 84747.6713098304\n",
      "Step: 696, Training Loss: 1.80692, Tokens/sec: 86585.40610732084\n",
      "Step: 697, Training Loss: 1.83146, Tokens/sec: 85013.59851725024\n",
      "Step: 698, Training Loss: 1.79456, Tokens/sec: 84203.55537512555\n",
      "Step: 699, Training Loss: 1.85070, Tokens/sec: 86283.77245505364\n",
      "Step: 700, Training Loss: 1.99227, Tokens/sec: 83996.58757658454\n",
      "Step: 701, Training Loss: 1.79569, Tokens/sec: 85901.20201826858\n",
      "Step: 702, Training Loss: 1.84228, Tokens/sec: 86355.44993316504\n",
      "Step: 703, Training Loss: 1.78582, Tokens/sec: 84292.92290795753\n",
      "Step: 704, Training Loss: 2.09817, Tokens/sec: 85819.43337552482\n",
      "Step: 705, Training Loss: 1.76973, Tokens/sec: 85501.82804695753\n",
      "Step: 706, Training Loss: 1.81194, Tokens/sec: 83762.47371534615\n",
      "Step: 707, Training Loss: 1.70617, Tokens/sec: 85795.53488393508\n",
      "Step: 708, Training Loss: 1.74860, Tokens/sec: 83773.90478336206\n",
      "Step: 709, Training Loss: 1.78914, Tokens/sec: 85856.7649157737\n",
      "Step: 710, Training Loss: 1.89920, Tokens/sec: 86614.002560546\n",
      "Step: 711, Training Loss: 1.73725, Tokens/sec: 83017.92198013153\n",
      "Step: 712, Training Loss: 1.79895, Tokens/sec: 84836.0684872974\n",
      "Step: 713, Training Loss: 1.78049, Tokens/sec: 85818.2610364131\n",
      "Step: 714, Training Loss: 2.20431, Tokens/sec: 84408.47755946711\n",
      "Step: 715, Training Loss: 1.85703, Tokens/sec: 86573.89760375666\n",
      "Step: 716, Training Loss: 1.83121, Tokens/sec: 83995.41369997802\n",
      "Step: 717, Training Loss: 1.78067, Tokens/sec: 86278.93972652373\n",
      "Step: 718, Training Loss: 1.73095, Tokens/sec: 86183.3697864554\n",
      "Step: 719, Training Loss: 1.70136, Tokens/sec: 84098.21957352672\n",
      "Step: 720, Training Loss: 1.82418, Tokens/sec: 85405.42485271876\n",
      "Step: 721, Training Loss: 1.73762, Tokens/sec: 83932.02823228957\n",
      "Step: 722, Training Loss: 1.70335, Tokens/sec: 86905.80193462942\n",
      "Step: 723, Training Loss: 1.78712, Tokens/sec: 86537.1246840747\n",
      "Step: 724, Training Loss: 1.75393, Tokens/sec: 84445.35306224822\n",
      "Step: 725, Training Loss: 1.86123, Tokens/sec: 85938.75935993115\n",
      "Step: 726, Training Loss: 1.73460, Tokens/sec: 86159.88485444678\n",
      "Step: 727, Training Loss: 2.18487, Tokens/sec: 84047.90036779967\n",
      "Step: 728, Training Loss: 1.81549, Tokens/sec: 84818.23484293667\n",
      "Step: 729, Training Loss: 1.71297, Tokens/sec: 85462.47068891433\n",
      "Step: 730, Training Loss: 2.00885, Tokens/sec: 86345.74983992973\n",
      "Step: 731, Training Loss: 1.73227, Tokens/sec: 86350.26875675126\n",
      "Step: 732, Training Loss: 1.79177, Tokens/sec: 83063.62791957469\n",
      "Step: 733, Training Loss: 1.81288, Tokens/sec: 86388.73733929648\n",
      "Step: 734, Training Loss: 1.71413, Tokens/sec: 86144.66396749967\n",
      "Step: 735, Training Loss: 1.78108, Tokens/sec: 83127.69916447825\n",
      "Step: 736, Training Loss: 1.74456, Tokens/sec: 85440.27741559785\n",
      "Step: 737, Training Loss: 1.69129, Tokens/sec: 84191.3837688153\n",
      "Step: 738, Training Loss: 1.73656, Tokens/sec: 86371.00857670773\n",
      "Step: 739, Training Loss: 1.68451, Tokens/sec: 85677.83856658016\n",
      "Step: 740, Training Loss: 1.72906, Tokens/sec: 83002.91916651014\n",
      "Step: 741, Training Loss: 1.83922, Tokens/sec: 86173.62582427864\n",
      "Step: 742, Training Loss: 1.81931, Tokens/sec: 86108.72174181943\n",
      "Step: 743, Training Loss: 1.90898, Tokens/sec: 83568.7500132888\n",
      "Step: 744, Training Loss: 2.06164, Tokens/sec: 81131.29549500073\n",
      "Step: 745, Training Loss: 1.92502, Tokens/sec: 78682.90125043213\n",
      "Step: 746, Training Loss: 1.83748, Tokens/sec: 75130.65392159036\n",
      "Step: 747, Training Loss: 1.86698, Tokens/sec: 74724.64817741624\n",
      "Step: 748, Training Loss: 1.92525, Tokens/sec: 86862.02074995173\n",
      "Step: 749, Training Loss: 1.74444, Tokens/sec: 79495.7346444164\n",
      "Step: 750, Training Loss: 1.76045, Tokens/sec: 84519.1362823632\n",
      "Step: 751, Training Loss: 1.89427, Tokens/sec: 76393.12900287939\n",
      "Step: 752, Training Loss: 1.73797, Tokens/sec: 84067.91289783528\n",
      "Step: 753, Training Loss: 1.72453, Tokens/sec: 75924.74945411381\n",
      "Step: 754, Training Loss: 1.61998, Tokens/sec: 75419.1165139303\n",
      "Step: 755, Training Loss: 1.65969, Tokens/sec: 83333.40072769704\n",
      "Step: 756, Training Loss: 1.72807, Tokens/sec: 83943.93012690535\n",
      "Step: 757, Training Loss: 1.77567, Tokens/sec: 83990.2293877299\n",
      "Step: 758, Training Loss: 1.83916, Tokens/sec: 85523.45196789704\n",
      "Step: 759, Training Loss: 1.70854, Tokens/sec: 85240.61684874697\n",
      "Step: 760, Training Loss: 1.90081, Tokens/sec: 75313.234263571\n",
      "Step: 761, Training Loss: 1.77607, Tokens/sec: 82429.87576638772\n",
      "Step: 762, Training Loss: 1.73767, Tokens/sec: 78633.31270928458\n",
      "Step: 763, Training Loss: 1.77588, Tokens/sec: 78361.30254848847\n",
      "Step: 764, Training Loss: 1.97998, Tokens/sec: 74804.41446623846\n",
      "Step: 765, Training Loss: 1.74000, Tokens/sec: 76512.09133580087\n",
      "Step: 766, Training Loss: 1.81309, Tokens/sec: 82119.42785119537\n",
      "Step: 767, Training Loss: 1.82664, Tokens/sec: 85563.5018280865\n",
      "Step: 768, Training Loss: 1.87947, Tokens/sec: 86300.86855276412\n",
      "Step: 769, Training Loss: 1.65487, Tokens/sec: 84069.73457141555\n",
      "Step: 770, Training Loss: 1.73099, Tokens/sec: 86070.30132240792\n",
      "Step: 771, Training Loss: 1.74390, Tokens/sec: 87250.3619282902\n",
      "Step: 772, Training Loss: 1.68852, Tokens/sec: 84231.43221402966\n",
      "Step: 773, Training Loss: 1.75216, Tokens/sec: 86327.65572121216\n",
      "Step: 774, Training Loss: 1.63909, Tokens/sec: 85079.29453608331\n",
      "Step: 775, Training Loss: 1.55044, Tokens/sec: 85402.09581053123\n",
      "Step: 776, Training Loss: 1.85700, Tokens/sec: 86511.27216867785\n",
      "Step: 777, Training Loss: 2.23433, Tokens/sec: 85852.76716836053\n",
      "Step: 778, Training Loss: 1.69717, Tokens/sec: 86121.41650913299\n",
      "Step: 779, Training Loss: 1.76724, Tokens/sec: 84672.22582224765\n",
      "Step: 780, Training Loss: 1.81482, Tokens/sec: 74282.56997257484\n",
      "Step: 781, Training Loss: 1.84370, Tokens/sec: 85937.48053602291\n",
      "Step: 782, Training Loss: 1.74352, Tokens/sec: 83773.16203354056\n",
      "Step: 783, Training Loss: 2.09250, Tokens/sec: 87945.19335384094\n",
      "Step: 784, Training Loss: 1.68194, Tokens/sec: 87264.08112628564\n",
      "Step: 785, Training Loss: 1.59598, Tokens/sec: 81178.0134753424\n",
      "Step: 786, Training Loss: 1.72864, Tokens/sec: 84279.80547443514\n",
      "Step: 787, Training Loss: 1.79096, Tokens/sec: 85061.9895735391\n",
      "Step: 788, Training Loss: 1.92595, Tokens/sec: 83302.30893689423\n",
      "Step: 789, Training Loss: 1.90594, Tokens/sec: 85465.20034989441\n",
      "Step: 790, Training Loss: 1.95548, Tokens/sec: 84614.16405915593\n",
      "Step: 791, Training Loss: 1.77976, Tokens/sec: 87112.46431019336\n",
      "Step: 792, Training Loss: 1.70422, Tokens/sec: 87316.88927830038\n",
      "Step: 793, Training Loss: 1.73454, Tokens/sec: 84033.26691501975\n",
      "Step: 794, Training Loss: 1.79704, Tokens/sec: 75870.710289598\n",
      "Step: 795, Training Loss: 1.71874, Tokens/sec: 77274.9644098922\n",
      "Step: 796, Training Loss: 1.76339, Tokens/sec: 76596.94723766921\n",
      "Step: 797, Training Loss: 1.74292, Tokens/sec: 75190.04508805546\n",
      "Step: 798, Training Loss: 1.66652, Tokens/sec: 74027.21333107253\n",
      "Step: 799, Training Loss: 1.82083, Tokens/sec: 78861.4360907928\n",
      "Step: 800, Training Loss: 2.02278, Tokens/sec: 80760.07972138244\n",
      "Step: 801, Training Loss: 1.67240, Tokens/sec: 74524.99232632702\n",
      "Step: 802, Training Loss: 1.71286, Tokens/sec: 77577.70436429388\n",
      "Step: 803, Training Loss: 1.82022, Tokens/sec: 87185.7633439046\n",
      "Step: 804, Training Loss: 1.76215, Tokens/sec: 78140.5300981054\n",
      "Step: 805, Training Loss: 1.62273, Tokens/sec: 82714.77734638525\n",
      "Step: 806, Training Loss: 1.76065, Tokens/sec: 83019.6008397997\n",
      "Step: 807, Training Loss: 1.77193, Tokens/sec: 86853.62322517234\n",
      "Step: 808, Training Loss: 1.77023, Tokens/sec: 84162.40502459939\n",
      "Step: 809, Training Loss: 1.66674, Tokens/sec: 85915.17190932282\n",
      "Step: 810, Training Loss: 1.71923, Tokens/sec: 85253.58216003416\n",
      "Step: 811, Training Loss: 1.79371, Tokens/sec: 71963.754476539\n",
      "Step: 812, Training Loss: 1.81798, Tokens/sec: 87297.5839904764\n",
      "Step: 813, Training Loss: 1.88690, Tokens/sec: 84109.65611770107\n",
      "Step: 814, Training Loss: 1.83906, Tokens/sec: 82935.0037758951\n",
      "Step: 815, Training Loss: 1.83979, Tokens/sec: 86289.9550099756\n",
      "Step: 816, Training Loss: 1.83452, Tokens/sec: 84345.7569804913\n",
      "Step: 817, Training Loss: 1.65097, Tokens/sec: 86668.92448042078\n",
      "Step: 818, Training Loss: 2.01672, Tokens/sec: 86479.1386817254\n",
      "Step: 819, Training Loss: 1.86145, Tokens/sec: 85408.12371463339\n",
      "Step: 820, Training Loss: 1.84997, Tokens/sec: 86374.99644592534\n",
      "Step: 821, Training Loss: 1.76889, Tokens/sec: 83432.07367734607\n",
      "Step: 822, Training Loss: 1.70409, Tokens/sec: 87559.26262898248\n",
      "Step: 823, Training Loss: 1.77025, Tokens/sec: 87603.71501414082\n",
      "Step: 824, Training Loss: 1.73857, Tokens/sec: 83452.22541771816\n",
      "Step: 825, Training Loss: 1.67951, Tokens/sec: 86968.11604270247\n",
      "Step: 826, Training Loss: 1.77145, Tokens/sec: 86566.45169626606\n",
      "Step: 827, Training Loss: 1.72025, Tokens/sec: 85791.14347335776\n",
      "Step: 828, Training Loss: 1.70747, Tokens/sec: 86292.87368375447\n",
      "Step: 829, Training Loss: 1.76887, Tokens/sec: 84863.67565212217\n",
      "Step: 830, Training Loss: 1.85104, Tokens/sec: 87627.63381714006\n",
      "Step: 831, Training Loss: 1.65625, Tokens/sec: 86758.84119329856\n",
      "Step: 832, Training Loss: 1.54485, Tokens/sec: 73432.52122915484\n",
      "Step: 833, Training Loss: 1.99225, Tokens/sec: 80941.15633587442\n",
      "Step: 834, Training Loss: 1.73379, Tokens/sec: 83310.72128315749\n",
      "Step: 835, Training Loss: 1.70423, Tokens/sec: 82932.41948870424\n",
      "Step: 836, Training Loss: 1.91612, Tokens/sec: 86257.8340164943\n",
      "Step: 837, Training Loss: 2.04245, Tokens/sec: 85704.9177178496\n",
      "Step: 838, Training Loss: 1.75239, Tokens/sec: 86399.70732195515\n",
      "Step: 839, Training Loss: 1.76071, Tokens/sec: 86332.6108190702\n",
      "Step: 840, Training Loss: 1.79011, Tokens/sec: 84038.21514601496\n",
      "Step: 841, Training Loss: 1.75395, Tokens/sec: 72115.83969394186\n",
      "Step: 842, Training Loss: 1.86413, Tokens/sec: 76037.5090475652\n",
      "Step: 843, Training Loss: 1.70897, Tokens/sec: 83064.14294825583\n",
      "Step: 844, Training Loss: 1.66669, Tokens/sec: 72889.40441306862\n",
      "Step: 845, Training Loss: 1.58763, Tokens/sec: 86541.48765724707\n",
      "Step: 846, Training Loss: 1.52969, Tokens/sec: 83681.17621059119\n",
      "Step: 847, Training Loss: 1.75381, Tokens/sec: 87001.50656289332\n",
      "Step: 848, Training Loss: 1.87515, Tokens/sec: 87261.29809042563\n",
      "Step: 849, Training Loss: 1.65045, Tokens/sec: 85572.81867160162\n",
      "Step: 850, Training Loss: 1.65764, Tokens/sec: 84720.59515715587\n",
      "Step: 851, Training Loss: 1.74158, Tokens/sec: 83600.55721152063\n",
      "Step: 852, Training Loss: 1.75617, Tokens/sec: 86427.3833694857\n",
      "Step: 853, Training Loss: 1.66653, Tokens/sec: 86938.75940679\n",
      "Step: 854, Training Loss: 1.78878, Tokens/sec: 83055.52054026588\n",
      "Step: 855, Training Loss: 1.57404, Tokens/sec: 86517.6751407113\n",
      "Step: 856, Training Loss: 1.76018, Tokens/sec: 87496.44412429298\n",
      "Step: 857, Training Loss: 1.69039, Tokens/sec: 87010.96085245263\n",
      "Step: 858, Training Loss: 1.74207, Tokens/sec: 86261.54121251676\n",
      "Step: 859, Training Loss: 1.68625, Tokens/sec: 84560.02355013929\n",
      "Step: 860, Training Loss: 1.75466, Tokens/sec: 85852.36048567784\n",
      "Step: 861, Training Loss: 1.99101, Tokens/sec: 86317.16650825454\n",
      "Step: 862, Training Loss: 1.68027, Tokens/sec: 83866.92152029487\n",
      "Step: 863, Training Loss: 1.79884, Tokens/sec: 87245.94205542735\n",
      "Step: 864, Training Loss: 1.94040, Tokens/sec: 84415.34068591101\n",
      "Step: 865, Training Loss: 1.75277, Tokens/sec: 86667.13604371806\n",
      "Step: 866, Training Loss: 1.78604, Tokens/sec: 87198.94198145354\n",
      "Step: 867, Training Loss: 1.75815, Tokens/sec: 83778.97802404875\n",
      "Step: 868, Training Loss: 1.79579, Tokens/sec: 86539.76663928514\n",
      "Step: 869, Training Loss: 1.75869, Tokens/sec: 87369.6327925223\n",
      "Step: 870, Training Loss: 1.71561, Tokens/sec: 85534.41404919868\n",
      "Step: 871, Training Loss: 1.92046, Tokens/sec: 86933.28981995267\n",
      "Step: 872, Training Loss: 1.71468, Tokens/sec: 86026.12637242647\n",
      "Step: 873, Training Loss: 1.57801, Tokens/sec: 86397.64249059498\n",
      "Step: 874, Training Loss: 1.53469, Tokens/sec: 87052.42651684121\n",
      "Step: 875, Training Loss: 1.46528, Tokens/sec: 84010.83348004555\n",
      "Step: 876, Training Loss: 1.50409, Tokens/sec: 86378.76700054607\n",
      "Step: 877, Training Loss: 1.81577, Tokens/sec: 82846.66605602122\n",
      "Step: 878, Training Loss: 1.58577, Tokens/sec: 85381.0942727919\n",
      "Step: 879, Training Loss: 1.67359, Tokens/sec: 86658.67119089673\n",
      "Step: 880, Training Loss: 2.03546, Tokens/sec: 84581.98320614202\n",
      "Step: 881, Training Loss: 1.81530, Tokens/sec: 85220.2327691555\n",
      "Step: 882, Training Loss: 1.69464, Tokens/sec: 85187.54992796676\n",
      "Step: 883, Training Loss: 1.71896, Tokens/sec: 87567.51354346935\n",
      "Step: 884, Training Loss: 1.74929, Tokens/sec: 86644.03373352069\n",
      "Step: 885, Training Loss: 1.73383, Tokens/sec: 83257.29871121842\n",
      "Step: 886, Training Loss: 1.79802, Tokens/sec: 86319.54719976477\n",
      "Step: 887, Training Loss: 1.67809, Tokens/sec: 87367.07689096303\n",
      "Step: 888, Training Loss: 1.73114, Tokens/sec: 84947.40708861097\n",
      "Step: 889, Training Loss: 1.71968, Tokens/sec: 88114.36638071202\n",
      "Step: 890, Training Loss: 1.66375, Tokens/sec: 85673.7795136465\n",
      "Step: 891, Training Loss: 1.60884, Tokens/sec: 86192.80715354288\n",
      "Step: 892, Training Loss: 1.70094, Tokens/sec: 85770.33766897922\n",
      "Step: 893, Training Loss: 1.79728, Tokens/sec: 84614.86324254253\n",
      "Step: 894, Training Loss: 1.58965, Tokens/sec: 86455.6715908277\n",
      "Step: 895, Training Loss: 1.71907, Tokens/sec: 85683.90143036602\n",
      "Step: 896, Training Loss: 1.69347, Tokens/sec: 87189.07468168435\n",
      "Step: 897, Training Loss: 1.63264, Tokens/sec: 87905.9546655843\n",
      "Step: 898, Training Loss: 1.63999, Tokens/sec: 85289.7897866414\n",
      "Step: 899, Training Loss: 1.68982, Tokens/sec: 87206.92784832018\n",
      "Step: 900, Training Loss: 1.72568, Tokens/sec: 87934.78171898604\n",
      "Step: 901, Training Loss: 1.74221, Tokens/sec: 85574.35171339552\n",
      "Step: 902, Training Loss: 1.84984, Tokens/sec: 86121.3780313928\n",
      "Step: 903, Training Loss: 1.63879, Tokens/sec: 83780.14629209909\n",
      "Step: 904, Training Loss: 1.67671, Tokens/sec: 86478.47544969589\n",
      "Step: 905, Training Loss: 1.78260, Tokens/sec: 85935.43142858094\n",
      "Step: 906, Training Loss: 1.68688, Tokens/sec: 82838.07282307731\n",
      "Step: 907, Training Loss: 1.64330, Tokens/sec: 87314.3960219667\n",
      "Step: 908, Training Loss: 1.75955, Tokens/sec: 85991.89148504772\n",
      "Step: 909, Training Loss: 1.75326, Tokens/sec: 85767.5997066148\n",
      "Step: 910, Training Loss: 1.62532, Tokens/sec: 87942.26379984528\n",
      "Step: 911, Training Loss: 1.84211, Tokens/sec: 85518.68439283308\n",
      "Step: 912, Training Loss: 1.84984, Tokens/sec: 85463.28515628494\n",
      "Step: 913, Training Loss: 1.52161, Tokens/sec: 87903.5790427763\n",
      "Step: 914, Training Loss: 1.53744, Tokens/sec: 85472.86689796105\n",
      "Step: 915, Training Loss: 1.81161, Tokens/sec: 86255.8059349812\n",
      "Step: 916, Training Loss: 1.68375, Tokens/sec: 83558.85102455282\n",
      "Step: 917, Training Loss: 1.63167, Tokens/sec: 86619.41097798018\n",
      "Step: 918, Training Loss: 1.78256, Tokens/sec: 84742.17676186387\n",
      "Step: 919, Training Loss: 1.71095, Tokens/sec: 84220.73271152648\n",
      "Step: 920, Training Loss: 1.65454, Tokens/sec: 85889.21143929767\n",
      "Step: 921, Training Loss: 1.65784, Tokens/sec: 86540.62097014987\n",
      "Step: 922, Training Loss: 1.71567, Tokens/sec: 87052.1207813693\n",
      "Step: 923, Training Loss: 1.62539, Tokens/sec: 86390.23916865721\n",
      "Step: 924, Training Loss: 1.64991, Tokens/sec: 83996.67154747044\n",
      "Step: 925, Training Loss: 1.57365, Tokens/sec: 86218.36348843212\n",
      "Step: 926, Training Loss: 1.72706, Tokens/sec: 87101.13015946576\n",
      "Step: 927, Training Loss: 1.64423, Tokens/sec: 85164.73554546601\n",
      "Step: 928, Training Loss: 1.90142, Tokens/sec: 85835.4771332007\n",
      "Step: 929, Training Loss: 1.86120, Tokens/sec: 83511.53826101714\n",
      "Step: 930, Training Loss: 1.61569, Tokens/sec: 86524.62647216518\n",
      "Step: 931, Training Loss: 1.63773, Tokens/sec: 87450.69222713025\n",
      "Step: 932, Training Loss: 1.70559, Tokens/sec: 84001.75027465666\n",
      "Step: 933, Training Loss: 1.68384, Tokens/sec: 85847.7023592162\n",
      "Step: 934, Training Loss: 1.65553, Tokens/sec: 85706.01522611539\n",
      "Step: 935, Training Loss: 1.56204, Tokens/sec: 86376.0346828511\n",
      "Step: 936, Training Loss: 1.71449, Tokens/sec: 86237.26606678126\n",
      "Step: 937, Training Loss: 1.57838, Tokens/sec: 84677.24959391778\n",
      "Step: 938, Training Loss: 1.74403, Tokens/sec: 85670.43713231922\n",
      "Step: 939, Training Loss: 1.59999, Tokens/sec: 87488.32198396615\n",
      "Step: 940, Training Loss: 1.60918, Tokens/sec: 85044.36321560148\n",
      "Step: 941, Training Loss: 1.68315, Tokens/sec: 86328.80289907617\n",
      "Step: 942, Training Loss: 1.66848, Tokens/sec: 84119.4303693143\n",
      "Step: 943, Training Loss: 1.59194, Tokens/sec: 85798.90320951752\n",
      "Step: 944, Training Loss: 1.71224, Tokens/sec: 87429.49323045864\n",
      "Step: 945, Training Loss: 1.81025, Tokens/sec: 83284.2874080223\n",
      "Step: 946, Training Loss: 1.64227, Tokens/sec: 86650.11954826448\n",
      "Step: 947, Training Loss: 1.60374, Tokens/sec: 82828.27247031333\n",
      "Step: 948, Training Loss: 1.63487, Tokens/sec: 87024.14909451427\n",
      "Step: 949, Training Loss: 1.45257, Tokens/sec: 86660.12879191601\n",
      "Step: 950, Training Loss: 1.60217, Tokens/sec: 83794.63685540877\n",
      "Step: 951, Training Loss: 1.66569, Tokens/sec: 87156.58462371303\n",
      "Step: 952, Training Loss: 1.75674, Tokens/sec: 86987.66243394456\n",
      "Step: 953, Training Loss: 1.71873, Tokens/sec: 85507.23545586782\n",
      "Step: 954, Training Loss: 1.57187, Tokens/sec: 86812.03298452095\n",
      "Step: 955, Training Loss: 1.71648, Tokens/sec: 83568.94694656391\n",
      "Step: 956, Training Loss: 1.52317, Tokens/sec: 86170.51307948244\n",
      "Step: 957, Training Loss: 1.63868, Tokens/sec: 87100.14618913837\n",
      "Step: 958, Training Loss: 1.52479, Tokens/sec: 85260.30166737943\n",
      "Step: 959, Training Loss: 1.70945, Tokens/sec: 86374.84344309474\n",
      "Step: 960, Training Loss: 1.73671, Tokens/sec: 83681.9374195305\n",
      "Step: 961, Training Loss: 1.53304, Tokens/sec: 86152.32968715527\n",
      "Step: 962, Training Loss: 1.65869, Tokens/sec: 86173.09734840004\n",
      "Step: 963, Training Loss: 1.65739, Tokens/sec: 84287.20835238634\n",
      "Step: 964, Training Loss: 1.62794, Tokens/sec: 86398.43797770326\n",
      "Step: 965, Training Loss: 1.77915, Tokens/sec: 87415.8735584706\n",
      "Step: 966, Training Loss: 1.67427, Tokens/sec: 87501.97685724583\n",
      "Step: 967, Training Loss: 1.49400, Tokens/sec: 86420.99193035754\n",
      "Step: 968, Training Loss: 1.56368, Tokens/sec: 84815.26578693277\n",
      "Step: 969, Training Loss: 1.63804, Tokens/sec: 86255.21514806752\n",
      "Step: 970, Training Loss: 1.67140, Tokens/sec: 87722.14579873733\n",
      "Step: 971, Training Loss: 1.62390, Tokens/sec: 84776.36980383402\n",
      "Step: 972, Training Loss: 1.66284, Tokens/sec: 84893.53925941407\n",
      "Step: 973, Training Loss: 1.57580, Tokens/sec: 83791.70303070461\n",
      "Step: 974, Training Loss: 1.62797, Tokens/sec: 86434.57418599821\n",
      "Step: 975, Training Loss: 3.75169, Tokens/sec: 86160.7085908919\n",
      "Step: 976, Training Loss: 2.35471, Tokens/sec: 83902.65927296058\n",
      "Step: 977, Training Loss: 1.61773, Tokens/sec: 86389.69254479655\n",
      "Step: 978, Training Loss: 1.60018, Tokens/sec: 86032.4184266508\n",
      "Step: 979, Training Loss: 1.65406, Tokens/sec: 86008.19800484751\n",
      "Step: 980, Training Loss: 1.80841, Tokens/sec: 87716.30388604137\n",
      "Step: 981, Training Loss: 1.71663, Tokens/sec: 84679.38881648579\n",
      "Step: 982, Training Loss: 1.51441, Tokens/sec: 87501.70207340467\n",
      "Step: 983, Training Loss: 1.78264, Tokens/sec: 87662.28089108787\n",
      "Step: 984, Training Loss: 1.87082, Tokens/sec: 85339.33909088932\n",
      "Step: 985, Training Loss: 1.43493, Tokens/sec: 86982.6498651418\n",
      "Step: 986, Training Loss: 1.65899, Tokens/sec: 84351.18113042391\n",
      "Step: 987, Training Loss: 1.58062, Tokens/sec: 86395.43288619634\n",
      "Step: 988, Training Loss: 1.75306, Tokens/sec: 86467.74283012057\n",
      "Step: 989, Training Loss: 1.63276, Tokens/sec: 84372.38764510243\n",
      "Step: 990, Training Loss: 1.67426, Tokens/sec: 86705.25523241685\n",
      "Step: 991, Training Loss: 1.71468, Tokens/sec: 87241.05158256952\n",
      "Step: 992, Training Loss: 1.60930, Tokens/sec: 83414.80328346744\n",
      "Step: 993, Training Loss: 1.59577, Tokens/sec: 87801.9566818349\n",
      "Step: 994, Training Loss: 1.57919, Tokens/sec: 85714.45820792725\n",
      "Step: 995, Training Loss: 1.63046, Tokens/sec: 85882.62653398662\n",
      "Step: 996, Training Loss: 1.65905, Tokens/sec: 87648.64440523424\n",
      "Step: 997, Training Loss: 2.04782, Tokens/sec: 85647.31953089249\n",
      "Step: 998, Training Loss: 1.76319, Tokens/sec: 85639.44528676537\n",
      "Step: 999, Training Loss: 1.63535, Tokens/sec: 82125.43554302247\n",
      "Step: 1000, Training Loss: 1.57299, Tokens/sec: 86109.90610022403\n",
      "Step: 1001, Training Loss: 1.50137, Tokens/sec: 86664.45650726248\n",
      "Step: 1002, Training Loss: 1.67686, Tokens/sec: 83884.880432779\n",
      "Step: 1003, Training Loss: 1.72205, Tokens/sec: 86507.13878934162\n",
      "Step: 1004, Training Loss: 1.76303, Tokens/sec: 86109.68977474286\n",
      "Step: 1005, Training Loss: 1.92661, Tokens/sec: 87043.9685513137\n",
      "Step: 1006, Training Loss: 1.60013, Tokens/sec: 86867.15022642973\n",
      "Step: 1007, Training Loss: 1.54890, Tokens/sec: 84304.96295387781\n",
      "Step: 1008, Training Loss: 1.66333, Tokens/sec: 87514.4028351325\n",
      "Step: 1009, Training Loss: 1.53812, Tokens/sec: 86986.58588602972\n",
      "Step: 1010, Training Loss: 1.60542, Tokens/sec: 85154.66376937092\n",
      "Step: 1011, Training Loss: 1.81540, Tokens/sec: 86238.78760296502\n",
      "Step: 1012, Training Loss: 1.71739, Tokens/sec: 83345.39800209216\n",
      "Step: 1013, Training Loss: 1.70139, Tokens/sec: 86128.9562964747\n",
      "Step: 1014, Training Loss: 1.67509, Tokens/sec: 86352.36408783033\n",
      "Step: 1015, Training Loss: 1.52490, Tokens/sec: 84322.3105857322\n",
      "Step: 1016, Training Loss: 1.86489, Tokens/sec: 86609.34932706312\n",
      "Step: 1017, Training Loss: 1.64861, Tokens/sec: 86582.25485121232\n",
      "Step: 1018, Training Loss: 1.47742, Tokens/sec: 86637.79987783215\n",
      "Step: 1019, Training Loss: 1.68224, Tokens/sec: 87135.50849449984\n",
      "Step: 1020, Training Loss: 1.54067, Tokens/sec: 84075.6785374775\n",
      "Step: 1021, Training Loss: 1.57642, Tokens/sec: 86189.32122841505\n",
      "Step: 1022, Training Loss: 1.57443, Tokens/sec: 87722.78409491738\n",
      "Step: 1023, Training Loss: 1.77835, Tokens/sec: 70162.6602479043\n",
      "Step: 1024, Training Loss: 1.68086, Tokens/sec: 86472.85642121917\n",
      "Step: 1025, Training Loss: 1.62147, Tokens/sec: 85446.54867580147\n",
      "Step: 1026, Training Loss: 1.65700, Tokens/sec: 65760.08934270231\n",
      "Step: 1027, Training Loss: 1.61720, Tokens/sec: 80703.11286719942\n",
      "Step: 1028, Training Loss: 1.70530, Tokens/sec: 85280.72578326701\n",
      "Step: 1029, Training Loss: 1.59479, Tokens/sec: 84056.22119511096\n",
      "Step: 1030, Training Loss: 1.59349, Tokens/sec: 86370.86514895468\n",
      "Step: 1031, Training Loss: 1.66986, Tokens/sec: 86759.21562103264\n",
      "Step: 1032, Training Loss: 1.74200, Tokens/sec: 85695.29956803506\n",
      "Step: 1033, Training Loss: 1.73137, Tokens/sec: 87571.3262161463\n",
      "Step: 1034, Training Loss: 1.56872, Tokens/sec: 85002.78583291473\n",
      "Step: 1035, Training Loss: 1.62169, Tokens/sec: 86846.54343561114\n",
      "Step: 1036, Training Loss: 1.83211, Tokens/sec: 87508.97836140516\n",
      "Step: 1037, Training Loss: 1.94562, Tokens/sec: 85264.53640004834\n",
      "Step: 1038, Training Loss: 1.61091, Tokens/sec: 85152.23094751511\n",
      "Step: 1039, Training Loss: 1.60582, Tokens/sec: 82154.25587961251\n",
      "Step: 1040, Training Loss: 1.86001, Tokens/sec: 85855.96542358129\n",
      "Step: 1041, Training Loss: 1.57841, Tokens/sec: 85825.67005440433\n",
      "Step: 1042, Training Loss: 1.75137, Tokens/sec: 83698.75677273965\n",
      "Step: 1043, Training Loss: 1.81663, Tokens/sec: 85174.7636526004\n",
      "Step: 1044, Training Loss: 1.68462, Tokens/sec: 85666.79848642563\n",
      "Step: 1045, Training Loss: 1.78351, Tokens/sec: 86304.96814900123\n",
      "Step: 1046, Training Loss: 1.80172, Tokens/sec: 86360.92306360308\n",
      "Step: 1047, Training Loss: 1.75715, Tokens/sec: 83765.03718835894\n",
      "Step: 1048, Training Loss: 1.72409, Tokens/sec: 86586.42057919483\n",
      "Step: 1049, Training Loss: 1.69696, Tokens/sec: 87635.44715285565\n",
      "Step: 1050, Training Loss: 1.71326, Tokens/sec: 85247.61285995909\n",
      "Step: 1051, Training Loss: 1.58204, Tokens/sec: 85890.68919853229\n",
      "Step: 1052, Training Loss: 1.60674, Tokens/sec: 83270.96185424968\n",
      "Step: 1053, Training Loss: 1.68178, Tokens/sec: 85770.26897129754\n",
      "Step: 1054, Training Loss: 1.89501, Tokens/sec: 86603.45646816787\n",
      "Step: 1055, Training Loss: 1.66604, Tokens/sec: 84216.6780812787\n",
      "Step: 1056, Training Loss: 1.58954, Tokens/sec: 86341.836564716\n",
      "Step: 1057, Training Loss: 1.64059, Tokens/sec: 86261.24782092648\n",
      "Step: 1058, Training Loss: 1.51850, Tokens/sec: 86530.95144759922\n",
      "Step: 1059, Training Loss: 1.58421, Tokens/sec: 86847.9484345019\n",
      "Step: 1060, Training Loss: 1.63678, Tokens/sec: 83764.19866877142\n",
      "Step: 1061, Training Loss: 1.81355, Tokens/sec: 86888.15201249666\n",
      "Step: 1062, Training Loss: 1.75279, Tokens/sec: 85974.83771716463\n",
      "Step: 1063, Training Loss: 1.56229, Tokens/sec: 83280.53282585331\n",
      "Step: 1064, Training Loss: 1.91602, Tokens/sec: 85470.7333252646\n",
      "Step: 1065, Training Loss: 1.71917, Tokens/sec: 86002.72795142431\n",
      "Step: 1066, Training Loss: 1.54056, Tokens/sec: 86384.7026544553\n",
      "Step: 1067, Training Loss: 1.56721, Tokens/sec: 86290.50173476315\n",
      "Step: 1068, Training Loss: 1.47086, Tokens/sec: 83339.91547492561\n",
      "Step: 1069, Training Loss: 1.68108, Tokens/sec: 85482.45302100634\n",
      "Step: 1070, Training Loss: 1.62991, Tokens/sec: 86079.70444832827\n",
      "Step: 1071, Training Loss: 1.54007, Tokens/sec: 86591.50199265449\n",
      "Step: 1072, Training Loss: 1.45946, Tokens/sec: 86988.18431746769\n",
      "Step: 1073, Training Loss: 1.67535, Tokens/sec: 84286.80422630851\n",
      "Step: 1074, Training Loss: 1.85661, Tokens/sec: 85617.43293440645\n",
      "Step: 1075, Training Loss: 1.58555, Tokens/sec: 87009.65130816362\n",
      "Step: 1076, Training Loss: 1.68994, Tokens/sec: 82875.36985903597\n",
      "Step: 1077, Training Loss: 1.66366, Tokens/sec: 85686.5950645718\n",
      "Step: 1078, Training Loss: 1.88099, Tokens/sec: 84083.16857945544\n",
      "Step: 1079, Training Loss: 1.58422, Tokens/sec: 85500.10975824992\n",
      "Step: 1080, Training Loss: 1.48490, Tokens/sec: 84518.2084769986\n",
      "Step: 1081, Training Loss: 1.70528, Tokens/sec: 83708.1590444257\n",
      "Step: 1082, Training Loss: 1.83993, Tokens/sec: 85506.709764482\n",
      "Step: 1083, Training Loss: 1.76757, Tokens/sec: 86005.41955183978\n",
      "Step: 1084, Training Loss: 1.62669, Tokens/sec: 84789.88185971513\n",
      "Step: 1085, Training Loss: 1.62587, Tokens/sec: 86869.25644146929\n",
      "Step: 1086, Training Loss: 1.61062, Tokens/sec: 83230.86659135888\n",
      "Step: 1087, Training Loss: 1.70058, Tokens/sec: 85334.28445589782\n",
      "Step: 1088, Training Loss: 1.56764, Tokens/sec: 87320.70202853183\n",
      "Step: 1089, Training Loss: 1.65068, Tokens/sec: 85410.24214051466\n",
      "Step: 1090, Training Loss: 1.68836, Tokens/sec: 85310.42881370409\n",
      "Step: 1091, Training Loss: 1.64512, Tokens/sec: 83964.04678538762\n",
      "Step: 1092, Training Loss: 1.85336, Tokens/sec: 87210.88373054875\n",
      "Step: 1093, Training Loss: 1.60547, Tokens/sec: 86162.51606357761\n",
      "Step: 1094, Training Loss: 1.61447, Tokens/sec: 83610.07652802896\n",
      "Step: 1095, Training Loss: 1.68793, Tokens/sec: 86244.95871489709\n",
      "Step: 1096, Training Loss: 1.59559, Tokens/sec: 86766.84728297465\n",
      "Step: 1097, Training Loss: 1.48215, Tokens/sec: 84348.56254778137\n",
      "Step: 1098, Training Loss: 1.36422, Tokens/sec: 86169.52690773825\n",
      "Step: 1099, Training Loss: 1.55519, Tokens/sec: 85442.79757335043\n",
      "Step: 1100, Training Loss: 1.73502, Tokens/sec: 85745.53040875093\n",
      "Step: 1101, Training Loss: 1.66092, Tokens/sec: 87389.3125605608\n",
      "Step: 1102, Training Loss: 1.63822, Tokens/sec: 85361.79659998642\n",
      "Step: 1103, Training Loss: 2.42073, Tokens/sec: 87000.45924204409\n",
      "Step: 1104, Training Loss: 1.98898, Tokens/sec: 83830.19506475756\n",
      "Step: 1105, Training Loss: 1.74736, Tokens/sec: 85709.44019695431\n",
      "Step: 1106, Training Loss: 1.75408, Tokens/sec: 87127.9531630937\n",
      "Step: 1107, Training Loss: 1.86003, Tokens/sec: 82120.81619158367\n",
      "Step: 1108, Training Loss: 1.74987, Tokens/sec: 85294.33916244963\n",
      "Step: 1109, Training Loss: 1.78133, Tokens/sec: 86153.65025225989\n",
      "Step: 1110, Training Loss: 1.56542, Tokens/sec: 84395.90059156608\n",
      "Step: 1111, Training Loss: 1.68244, Tokens/sec: 85719.77190520184\n",
      "Step: 1112, Training Loss: 1.64872, Tokens/sec: 84148.83893723266\n",
      "Step: 1113, Training Loss: 1.75328, Tokens/sec: 86912.31185211768\n",
      "Step: 1114, Training Loss: 1.57049, Tokens/sec: 86831.48810245411\n",
      "Step: 1115, Training Loss: 1.72716, Tokens/sec: 84364.2860377027\n",
      "Step: 1116, Training Loss: 1.56652, Tokens/sec: 85359.849567513\n",
      "Step: 1117, Training Loss: 1.69755, Tokens/sec: 87567.70028408701\n",
      "Step: 1118, Training Loss: 1.71341, Tokens/sec: 85558.74497816977\n",
      "Step: 1119, Training Loss: 1.75355, Tokens/sec: 87679.55464652965\n",
      "Step: 1120, Training Loss: 1.68780, Tokens/sec: 85496.0394195184\n",
      "Step: 1121, Training Loss: 1.54704, Tokens/sec: 85419.67082070948\n",
      "Step: 1122, Training Loss: 1.69023, Tokens/sec: 87252.86686798024\n",
      "Step: 1123, Training Loss: 1.90916, Tokens/sec: 84630.95590742542\n",
      "Step: 1124, Training Loss: 1.64758, Tokens/sec: 86417.39773126543\n",
      "Step: 1125, Training Loss: 1.58050, Tokens/sec: 83686.54000917003\n",
      "Step: 1126, Training Loss: 2.16184, Tokens/sec: 87329.28228548402\n",
      "Step: 1127, Training Loss: 1.60340, Tokens/sec: 87798.56522710968\n",
      "Step: 1128, Training Loss: 1.70004, Tokens/sec: 84665.23030107286\n",
      "Step: 1129, Training Loss: 1.82362, Tokens/sec: 85920.49565131475\n",
      "Step: 1130, Training Loss: 1.59382, Tokens/sec: 86371.87824396473\n",
      "Step: 1131, Training Loss: 1.60995, Tokens/sec: 86987.44536880378\n",
      "Step: 1132, Training Loss: 1.63870, Tokens/sec: 85713.3116060498\n",
      "Step: 1133, Training Loss: 1.52462, Tokens/sec: 84185.80409208499\n",
      "Step: 1134, Training Loss: 1.58794, Tokens/sec: 83622.10452461669\n",
      "Step: 1135, Training Loss: 1.62047, Tokens/sec: 85610.18507731998\n",
      "Step: 1136, Training Loss: 1.63286, Tokens/sec: 84278.74462397928\n",
      "Step: 1137, Training Loss: 1.65884, Tokens/sec: 86440.30452978035\n",
      "Step: 1138, Training Loss: 1.60743, Tokens/sec: 83647.4473491427\n",
      "Step: 1139, Training Loss: 1.45981, Tokens/sec: 85416.34957152295\n",
      "Step: 1140, Training Loss: 1.58371, Tokens/sec: 87650.07125657106\n",
      "Step: 1141, Training Loss: 1.72638, Tokens/sec: 85454.56131112823\n",
      "Step: 1142, Training Loss: 1.88521, Tokens/sec: 86276.06243266341\n",
      "Step: 1143, Training Loss: 1.54434, Tokens/sec: 87666.50991933553\n",
      "Step: 1144, Training Loss: 1.50813, Tokens/sec: 87336.86233531959\n",
      "Step: 1145, Training Loss: 1.52461, Tokens/sec: 87103.97569180864\n",
      "Step: 1146, Training Loss: 1.48248, Tokens/sec: 84241.26641293493\n",
      "Step: 1147, Training Loss: 1.55687, Tokens/sec: 85916.31581150748\n",
      "Step: 1148, Training Loss: 1.82860, Tokens/sec: 86160.9292509152\n",
      "Step: 1149, Training Loss: 1.81230, Tokens/sec: 83596.95363408168\n",
      "Step: 1150, Training Loss: 2.05049, Tokens/sec: 86419.54646194013\n",
      "Step: 1151, Training Loss: 1.54356, Tokens/sec: 84046.48533859158\n",
      "Step: 1152, Training Loss: 1.72851, Tokens/sec: 85589.79334210673\n",
      "Step: 1153, Training Loss: 1.57405, Tokens/sec: 87215.01128388254\n",
      "Step: 1154, Training Loss: 1.58164, Tokens/sec: 83714.3569643768\n",
      "Step: 1155, Training Loss: 1.82495, Tokens/sec: 85819.99618267869\n",
      "Step: 1156, Training Loss: 1.59744, Tokens/sec: 86529.96203616449\n",
      "Step: 1157, Training Loss: 1.64592, Tokens/sec: 84582.56264937857\n",
      "Step: 1158, Training Loss: 1.84890, Tokens/sec: 71029.82635173386\n",
      "Step: 1159, Training Loss: 1.49216, Tokens/sec: 82663.92781748633\n",
      "Step: 1160, Training Loss: 1.61795, Tokens/sec: 83505.63420923977\n",
      "Step: 1161, Training Loss: 1.53784, Tokens/sec: 72791.70443576174\n",
      "Step: 1162, Training Loss: 1.80213, Tokens/sec: 80321.6186629824\n",
      "Step: 1163, Training Loss: 1.58739, Tokens/sec: 86310.14207422623\n",
      "Step: 1164, Training Loss: 1.49903, Tokens/sec: 83403.54981603436\n",
      "Step: 1165, Training Loss: 1.69992, Tokens/sec: 85790.67582811453\n",
      "Step: 1166, Training Loss: 1.54180, Tokens/sec: 81602.01449353938\n",
      "Step: 1167, Training Loss: 1.63447, Tokens/sec: 69357.90874442046\n",
      "Step: 1168, Training Loss: 1.60177, Tokens/sec: 81405.03096071613\n",
      "Step: 1169, Training Loss: 1.50741, Tokens/sec: 86358.13270095745\n",
      "Step: 1170, Training Loss: 1.68520, Tokens/sec: 63307.60521543121\n",
      "Step: 1171, Training Loss: 1.67420, Tokens/sec: 85628.60350336152\n",
      "Step: 1172, Training Loss: 1.84392, Tokens/sec: 84401.89990044925\n",
      "Step: 1173, Training Loss: 1.59007, Tokens/sec: 79156.7720496357\n",
      "Step: 1174, Training Loss: 1.52908, Tokens/sec: 76930.54343879934\n",
      "Step: 1175, Training Loss: 1.52203, Tokens/sec: 86060.19279261433\n",
      "Step: 1176, Training Loss: 1.68339, Tokens/sec: 81658.01048023674\n",
      "Step: 1177, Training Loss: 1.65085, Tokens/sec: 82168.70659860151\n",
      "Step: 1178, Training Loss: 1.51748, Tokens/sec: 81012.1883541542\n",
      "Step: 1179, Training Loss: 1.61114, Tokens/sec: 85396.24323585615\n",
      "Step: 1180, Training Loss: 1.53413, Tokens/sec: 84313.64325524241\n",
      "Step: 1181, Training Loss: 1.51657, Tokens/sec: 82590.62722156255\n",
      "Step: 1182, Training Loss: 1.56473, Tokens/sec: 85423.12683099364\n",
      "Step: 1183, Training Loss: 1.60343, Tokens/sec: 83918.18341886096\n",
      "Step: 1184, Training Loss: 1.57915, Tokens/sec: 60530.028177240325\n",
      "Step: 1185, Training Loss: 1.72054, Tokens/sec: 65837.09462916646\n",
      "Step: 1186, Training Loss: 1.62860, Tokens/sec: 85593.976352025\n",
      "Step: 1187, Training Loss: 1.65549, Tokens/sec: 85923.00456679327\n",
      "Step: 1188, Training Loss: 1.60668, Tokens/sec: 86557.67176277767\n",
      "Step: 1189, Training Loss: 1.74800, Tokens/sec: 85251.01992009986\n",
      "Step: 1190, Training Loss: 1.84433, Tokens/sec: 86076.50668461542\n",
      "Step: 1191, Training Loss: 1.61650, Tokens/sec: 87261.12938271626\n",
      "Step: 1192, Training Loss: 1.58080, Tokens/sec: 83923.50499561845\n",
      "Step: 1193, Training Loss: 1.74896, Tokens/sec: 86020.79856289661\n",
      "Step: 1194, Training Loss: 1.66705, Tokens/sec: 85182.62530463935\n",
      "Step: 1195, Training Loss: 1.65966, Tokens/sec: 84067.11920058847\n",
      "Step: 1196, Training Loss: 1.61014, Tokens/sec: 85288.62210456218\n",
      "Step: 1197, Training Loss: 1.92551, Tokens/sec: 83208.01936625004\n",
      "Step: 1198, Training Loss: 1.52663, Tokens/sec: 86423.87160530091\n",
      "Step: 1199, Training Loss: 1.69671, Tokens/sec: 86813.17421564095\n",
      "Step: 1200, Training Loss: 1.57296, Tokens/sec: 83722.3851359979\n",
      "Step: 1201, Training Loss: 1.75213, Tokens/sec: 86345.10913351078\n",
      "Step: 1202, Training Loss: 1.64664, Tokens/sec: 83948.1624213632\n",
      "Step: 1203, Training Loss: 1.56263, Tokens/sec: 82835.40536593938\n",
      "Step: 1204, Training Loss: 1.72123, Tokens/sec: 86746.3564826081\n",
      "Step: 1205, Training Loss: 1.49111, Tokens/sec: 83607.94662096507\n",
      "Step: 1206, Training Loss: 1.37041, Tokens/sec: 85959.32630709227\n",
      "Step: 1207, Training Loss: 1.38534, Tokens/sec: 85734.83627017523\n",
      "Step: 1208, Training Loss: 1.52620, Tokens/sec: 84227.7762127569\n",
      "Step: 1209, Training Loss: 1.54349, Tokens/sec: 86186.63261972717\n",
      "Step: 1210, Training Loss: 1.53203, Tokens/sec: 85755.43408514526\n",
      "Step: 1211, Training Loss: 1.60574, Tokens/sec: 86543.31205599308\n",
      "Step: 1212, Training Loss: 1.56101, Tokens/sec: 86462.89905660415\n",
      "Step: 1213, Training Loss: 1.68893, Tokens/sec: 83787.98354837579\n",
      "Step: 1214, Training Loss: 1.64066, Tokens/sec: 84804.76423116455\n",
      "Step: 1215, Training Loss: 1.67630, Tokens/sec: 86899.24136801058\n",
      "Step: 1216, Training Loss: 1.72597, Tokens/sec: 84455.92556135688\n",
      "Step: 1217, Training Loss: 1.65784, Tokens/sec: 86014.74889158798\n",
      "Step: 1218, Training Loss: 1.52992, Tokens/sec: 85073.61201547722\n",
      "Step: 1219, Training Loss: 1.47992, Tokens/sec: 86362.01376850094\n",
      "Step: 1220, Training Loss: 1.36444, Tokens/sec: 87347.86070372254\n",
      "Step: 1221, Training Loss: 1.79034, Tokens/sec: 82630.99217470558\n",
      "Step: 1222, Training Loss: 1.65773, Tokens/sec: 86269.77602820942\n",
      "Step: 1223, Training Loss: 1.62567, Tokens/sec: 86668.8910125811\n",
      "Step: 1224, Training Loss: 1.66890, Tokens/sec: 83461.53454385833\n",
      "Step: 1225, Training Loss: 1.73778, Tokens/sec: 85848.02443286614\n",
      "Step: 1226, Training Loss: 1.51118, Tokens/sec: 83828.36745470036\n",
      "Step: 1227, Training Loss: 2.09008, Tokens/sec: 86970.47783711727\n",
      "Step: 1228, Training Loss: 1.59923, Tokens/sec: 86208.6687659789\n",
      "Step: 1229, Training Loss: 1.60266, Tokens/sec: 83738.08658547979\n",
      "Step: 1230, Training Loss: 1.74691, Tokens/sec: 86302.09048160864\n",
      "Step: 1231, Training Loss: 1.69893, Tokens/sec: 84329.64714618301\n",
      "Step: 1232, Training Loss: 1.68371, Tokens/sec: 84300.53672398314\n",
      "Step: 1233, Training Loss: 1.69007, Tokens/sec: 85737.49003650119\n",
      "Step: 1234, Training Loss: 1.55186, Tokens/sec: 84148.06013202324\n",
      "Step: 1235, Training Loss: 1.71309, Tokens/sec: 87096.61473480874\n",
      "Step: 1236, Training Loss: 1.53263, Tokens/sec: 86309.79197043405\n",
      "Step: 1237, Training Loss: 1.56839, Tokens/sec: 83944.24194206501\n",
      "Step: 1238, Training Loss: 1.60394, Tokens/sec: 86203.78412793885\n",
      "Step: 1239, Training Loss: 1.66522, Tokens/sec: 83221.67310197733\n",
      "Step: 1240, Training Loss: 2.12424, Tokens/sec: 85343.33539215606\n",
      "Step: 1241, Training Loss: 1.58883, Tokens/sec: 86059.16213457836\n",
      "Step: 1242, Training Loss: 1.54426, Tokens/sec: 83864.75532544743\n",
      "Step: 1243, Training Loss: 1.68543, Tokens/sec: 86755.6428464673\n",
      "Step: 1244, Training Loss: 1.65842, Tokens/sec: 85954.7246607076\n",
      "Step: 1245, Training Loss: 1.52960, Tokens/sec: 83445.45977369357\n",
      "Step: 1246, Training Loss: 1.66649, Tokens/sec: 86121.73792575784\n",
      "Step: 1247, Training Loss: 1.60290, Tokens/sec: 85031.72712313014\n",
      "Step: 1248, Training Loss: 1.56186, Tokens/sec: 85704.25330597408\n",
      "Step: 1249, Training Loss: 1.70188, Tokens/sec: 86664.22591936757\n",
      "Step: 1250, Training Loss: 1.70109, Tokens/sec: 84387.97267505722\n",
      "Step: 1251, Training Loss: 1.51645, Tokens/sec: 86556.41789194116\n",
      "Step: 1252, Training Loss: 1.47758, Tokens/sec: 85079.29100331094\n",
      "Step: 1253, Training Loss: 1.65552, Tokens/sec: 85489.72699228855\n",
      "Step: 1254, Training Loss: 1.50108, Tokens/sec: 86630.21242886099\n",
      "Step: 1255, Training Loss: 1.40399, Tokens/sec: 84708.68837860595\n",
      "Step: 1256, Training Loss: 1.41283, Tokens/sec: 85683.94534449393\n",
      "Step: 1257, Training Loss: 1.44924, Tokens/sec: 86602.96665238168\n",
      "Step: 1258, Training Loss: 1.46193, Tokens/sec: 84273.04326261501\n",
      "Step: 1259, Training Loss: 1.81669, Tokens/sec: 86140.7874661839\n",
      "Step: 1260, Training Loss: 1.55648, Tokens/sec: 85228.05759514205\n",
      "Step: 1261, Training Loss: 1.49292, Tokens/sec: 85868.39045433805\n",
      "Step: 1262, Training Loss: 1.56828, Tokens/sec: 86473.63594630724\n",
      "Step: 1263, Training Loss: 1.64329, Tokens/sec: 83506.21602173553\n",
      "Step: 1264, Training Loss: 1.73101, Tokens/sec: 85644.34318138642\n",
      "Step: 1265, Training Loss: 1.58175, Tokens/sec: 86658.5621035775\n",
      "Step: 1266, Training Loss: 1.46654, Tokens/sec: 83155.33149049417\n",
      "Step: 1267, Training Loss: 1.61308, Tokens/sec: 85747.62342209206\n",
      "Step: 1268, Training Loss: 1.54855, Tokens/sec: 84698.48203786377\n",
      "Step: 1269, Training Loss: 1.56830, Tokens/sec: 86781.05971729165\n",
      "Step: 1270, Training Loss: 1.80147, Tokens/sec: 86506.26730669916\n",
      "Step: 1271, Training Loss: 1.47201, Tokens/sec: 82899.3719348993\n",
      "Step: 1272, Training Loss: 1.55438, Tokens/sec: 85733.17589683895\n",
      "Step: 1273, Training Loss: 1.56851, Tokens/sec: 86565.78118078451\n",
      "Step: 1274, Training Loss: 1.54521, Tokens/sec: 84385.53130703703\n",
      "Step: 1275, Training Loss: 1.59428, Tokens/sec: 86116.0085433621\n",
      "Step: 1276, Training Loss: 1.57986, Tokens/sec: 84018.69456798253\n",
      "Step: 1277, Training Loss: 1.45893, Tokens/sec: 86096.83106159388\n",
      "Step: 1278, Training Loss: 1.68314, Tokens/sec: 86654.657542022\n",
      "Step: 1279, Training Loss: 1.53093, Tokens/sec: 83966.03436561348\n",
      "Step: 1280, Training Loss: 1.59601, Tokens/sec: 85800.25654094593\n",
      "Step: 1281, Training Loss: 1.59887, Tokens/sec: 84098.63354894392\n",
      "Step: 1282, Training Loss: 1.53292, Tokens/sec: 85835.81305365097\n",
      "Step: 1283, Training Loss: 1.43306, Tokens/sec: 86155.12534400536\n",
      "Step: 1284, Training Loss: 1.45686, Tokens/sec: 84017.5101642919\n",
      "Step: 1285, Training Loss: 2.02781, Tokens/sec: 86807.72409657273\n",
      "Step: 1286, Training Loss: 1.54280, Tokens/sec: 86024.37249283442\n",
      "Step: 1287, Training Loss: 1.68854, Tokens/sec: 85812.01330200488\n",
      "Step: 1288, Training Loss: 1.70447, Tokens/sec: 86183.59056256605\n",
      "Step: 1289, Training Loss: 1.61652, Tokens/sec: 86846.92874624579\n",
      "Step: 1290, Training Loss: 1.66804, Tokens/sec: 86726.67735529448\n",
      "Step: 1291, Training Loss: 1.44309, Tokens/sec: 85988.77786705509\n",
      "Step: 1292, Training Loss: 1.49057, Tokens/sec: 84800.65229515405\n",
      "Step: 1293, Training Loss: 1.53377, Tokens/sec: 86112.36680127535\n",
      "Step: 1294, Training Loss: 1.55637, Tokens/sec: 86663.78400908282\n",
      "Step: 1295, Training Loss: 1.67808, Tokens/sec: 84851.53043376113\n",
      "Step: 1296, Training Loss: 1.57288, Tokens/sec: 86397.82382412755\n",
      "Step: 1297, Training Loss: 1.67224, Tokens/sec: 85253.9858493238\n",
      "Step: 1298, Training Loss: 1.56211, Tokens/sec: 84873.91700629822\n",
      "Step: 1299, Training Loss: 1.66048, Tokens/sec: 85454.93258875674\n",
      "Step: 1300, Training Loss: 1.50362, Tokens/sec: 84205.83431680172\n",
      "Step: 1301, Training Loss: 1.55987, Tokens/sec: 86811.00447746675\n",
      "Step: 1302, Training Loss: 1.56890, Tokens/sec: 83465.9402869765\n",
      "Step: 1303, Training Loss: 1.62639, Tokens/sec: 83908.18987597022\n",
      "Step: 1304, Training Loss: 1.58815, Tokens/sec: 86716.80465253929\n",
      "Step: 1305, Training Loss: 1.57722, Tokens/sec: 83689.79179506401\n",
      "Step: 1306, Training Loss: 1.79113, Tokens/sec: 86105.62861911095\n",
      "Step: 1307, Training Loss: 1.41654, Tokens/sec: 85022.56125899467\n",
      "Step: 1308, Training Loss: 1.48920, Tokens/sec: 83813.0424443025\n",
      "Step: 1309, Training Loss: 1.44651, Tokens/sec: 86702.66693629666\n",
      "Step: 1310, Training Loss: 1.58429, Tokens/sec: 85921.49820850824\n",
      "Step: 1311, Training Loss: 1.48009, Tokens/sec: 84210.41421477198\n",
      "Step: 1312, Training Loss: 1.49624, Tokens/sec: 85736.1530377647\n",
      "Step: 1313, Training Loss: 1.35473, Tokens/sec: 83520.96408430433\n",
      "Step: 1314, Training Loss: 1.47048, Tokens/sec: 86287.60594361695\n",
      "Step: 1315, Training Loss: 1.51736, Tokens/sec: 85743.68205354403\n",
      "Step: 1316, Training Loss: 1.55141, Tokens/sec: 82976.50330730091\n",
      "Step: 1317, Training Loss: 1.64941, Tokens/sec: 86559.62763598279\n",
      "Step: 1318, Training Loss: 1.55212, Tokens/sec: 85971.06535224711\n",
      "Step: 1319, Training Loss: 1.76976, Tokens/sec: 85702.90389509583\n",
      "Step: 1320, Training Loss: 1.69538, Tokens/sec: 84567.7860932396\n",
      "Step: 1321, Training Loss: 1.67642, Tokens/sec: 84182.68882616863\n",
      "Step: 1322, Training Loss: 1.79480, Tokens/sec: 85376.82348570136\n",
      "Step: 1323, Training Loss: 1.57794, Tokens/sec: 85216.23468870438\n",
      "Step: 1324, Training Loss: 1.40831, Tokens/sec: 84096.96127082453\n",
      "Step: 1325, Training Loss: 1.56348, Tokens/sec: 86457.78710116184\n",
      "Step: 1326, Training Loss: 1.69791, Tokens/sec: 85138.0426269318\n",
      "Step: 1327, Training Loss: 1.52873, Tokens/sec: 84161.33199121416\n",
      "Step: 1328, Training Loss: 1.66202, Tokens/sec: 86657.22234627111\n",
      "Step: 1329, Training Loss: 1.48619, Tokens/sec: 84051.72661386263\n",
      "Step: 1330, Training Loss: 1.71006, Tokens/sec: 84763.47774529092\n",
      "Step: 1331, Training Loss: 1.48642, Tokens/sec: 86518.49933696621\n",
      "Step: 1332, Training Loss: 1.66194, Tokens/sec: 84104.23146497249\n",
      "Step: 1333, Training Loss: 1.61949, Tokens/sec: 86671.66298317116\n",
      "Step: 1334, Training Loss: 1.54774, Tokens/sec: 84588.84924920749\n",
      "Step: 1335, Training Loss: 1.60906, Tokens/sec: 86510.50566474524\n",
      "Step: 1336, Training Loss: 1.64336, Tokens/sec: 86376.12666924679\n",
      "Step: 1337, Training Loss: 1.50205, Tokens/sec: 82937.60165886155\n",
      "Step: 1338, Training Loss: 1.41155, Tokens/sec: 85380.65956611867\n",
      "Step: 1339, Training Loss: 1.54165, Tokens/sec: 86333.42967591951\n",
      "Step: 1340, Training Loss: 1.52801, Tokens/sec: 83927.94288101706\n",
      "Step: 1341, Training Loss: 1.47820, Tokens/sec: 85087.11736335688\n",
      "Step: 1342, Training Loss: 1.44598, Tokens/sec: 84115.5621114855\n",
      "Step: 1343, Training Loss: 1.53294, Tokens/sec: 86777.01588299668\n",
      "Step: 1344, Training Loss: 1.43084, Tokens/sec: 86435.19752828043\n",
      "Step: 1345, Training Loss: 1.69908, Tokens/sec: 82279.42036976849\n",
      "Step: 1346, Training Loss: 1.88888, Tokens/sec: 85901.40018521575\n",
      "Step: 1347, Training Loss: 1.63630, Tokens/sec: 86090.57843675507\n",
      "Step: 1348, Training Loss: 1.49014, Tokens/sec: 85324.46181275803\n",
      "Step: 1349, Training Loss: 1.56578, Tokens/sec: 85367.67470092836\n",
      "Step: 1350, Training Loss: 1.67386, Tokens/sec: 82636.18425907985\n",
      "Step: 1351, Training Loss: 1.52958, Tokens/sec: 86368.89685706432\n",
      "Step: 1352, Training Loss: 1.73430, Tokens/sec: 86108.55927569096\n",
      "Step: 1353, Training Loss: 1.67665, Tokens/sec: 82981.08242100316\n",
      "Step: 1354, Training Loss: 1.52025, Tokens/sec: 85950.95315836494\n",
      "Step: 1355, Training Loss: 1.49289, Tokens/sec: 84336.75185095317\n",
      "Step: 1356, Training Loss: 1.41370, Tokens/sec: 84591.51902155385\n",
      "Step: 1357, Training Loss: 1.61065, Tokens/sec: 85354.8934467861\n",
      "Step: 1358, Training Loss: 1.47945, Tokens/sec: 82623.99943980468\n",
      "Step: 1359, Training Loss: 1.62593, Tokens/sec: 86335.12338733645\n",
      "Step: 1360, Training Loss: 1.45827, Tokens/sec: 86536.85821139533\n",
      "Step: 1361, Training Loss: 1.46035, Tokens/sec: 82800.45951477667\n",
      "Step: 1362, Training Loss: 1.53020, Tokens/sec: 85659.7214106205\n",
      "Step: 1363, Training Loss: 1.36523, Tokens/sec: 84781.60774750425\n",
      "Step: 1364, Training Loss: 1.48086, Tokens/sec: 85795.86824281626\n",
      "Step: 1365, Training Loss: 1.61117, Tokens/sec: 85123.55387154497\n",
      "Step: 1366, Training Loss: 1.49904, Tokens/sec: 83391.05235248411\n",
      "Step: 1367, Training Loss: 1.55933, Tokens/sec: 86356.00658885064\n",
      "Step: 1368, Training Loss: 1.79659, Tokens/sec: 86492.09861144108\n",
      "Step: 1369, Training Loss: 1.54564, Tokens/sec: 82048.21551625719\n",
      "Step: 1370, Training Loss: 1.56671, Tokens/sec: 86679.42369985349\n",
      "Step: 1371, Training Loss: 1.55079, Tokens/sec: 86585.60515637296\n",
      "Step: 1372, Training Loss: 1.46154, Tokens/sec: 86822.39023414394\n",
      "Step: 1373, Training Loss: 1.54197, Tokens/sec: 85031.28449732314\n",
      "Step: 1374, Training Loss: 1.41826, Tokens/sec: 83862.17029182984\n",
      "Step: 1375, Training Loss: 1.61478, Tokens/sec: 86674.93033963017\n",
      "Step: 1376, Training Loss: 1.60871, Tokens/sec: 85758.57302478545\n",
      "Step: 1377, Training Loss: 1.50746, Tokens/sec: 82977.64593542546\n",
      "Step: 1378, Training Loss: 1.54668, Tokens/sec: 86812.88165824172\n",
      "Step: 1379, Training Loss: 1.55195, Tokens/sec: 85651.54980524576\n",
      "Step: 1380, Training Loss: 1.61505, Tokens/sec: 85512.36594233477\n",
      "Step: 1381, Training Loss: 1.78158, Tokens/sec: 84914.91024673829\n",
      "Step: 1382, Training Loss: 1.57060, Tokens/sec: 83772.94786495996\n",
      "Step: 1383, Training Loss: 1.50011, Tokens/sec: 85737.96517519647\n",
      "Step: 1384, Training Loss: 1.50942, Tokens/sec: 85575.15803472\n",
      "Step: 1385, Training Loss: 1.57273, Tokens/sec: 84759.03875458962\n",
      "Step: 1386, Training Loss: 1.40441, Tokens/sec: 86510.62306087991\n",
      "Step: 1387, Training Loss: 1.31363, Tokens/sec: 83555.17988438737\n",
      "Step: 1388, Training Loss: 1.31063, Tokens/sec: 85576.75865391015\n",
      "Step: 1389, Training Loss: 1.56674, Tokens/sec: 85556.45297442781\n",
      "Step: 1390, Training Loss: 1.61432, Tokens/sec: 83169.12499118752\n",
      "Step: 1391, Training Loss: 1.53571, Tokens/sec: 86081.30454963907\n",
      "Step: 1392, Training Loss: 1.58582, Tokens/sec: 84101.346724054\n",
      "Step: 1393, Training Loss: 1.59299, Tokens/sec: 85084.33226259677\n",
      "Step: 1394, Training Loss: 1.49970, Tokens/sec: 85756.47319000431\n",
      "Step: 1395, Training Loss: 1.48512, Tokens/sec: 84195.86127935127\n",
      "Step: 1396, Training Loss: 1.57094, Tokens/sec: 85994.0922297109\n",
      "Step: 1397, Training Loss: 1.54150, Tokens/sec: 85346.94617161277\n",
      "Step: 1398, Training Loss: 1.53294, Tokens/sec: 83494.29962947841\n",
      "Step: 1399, Training Loss: 1.46991, Tokens/sec: 86523.80444318039\n",
      "Step: 1400, Training Loss: 1.56602, Tokens/sec: 84499.26311325705\n",
      "Step: 1401, Training Loss: 1.59151, Tokens/sec: 85875.64518997184\n",
      "Step: 1402, Training Loss: 1.64568, Tokens/sec: 86696.85495779589\n",
      "Step: 1403, Training Loss: 1.55751, Tokens/sec: 84810.43020739034\n",
      "Step: 1404, Training Loss: 1.55741, Tokens/sec: 85486.21206536125\n",
      "Step: 1405, Training Loss: 1.56757, Tokens/sec: 86352.25804527929\n",
      "Step: 1406, Training Loss: 1.51296, Tokens/sec: 84135.92789817399\n",
      "Step: 1407, Training Loss: 2.03270, Tokens/sec: 86712.11604492867\n",
      "Step: 1408, Training Loss: 1.57433, Tokens/sec: 85021.01439381068\n",
      "Step: 1409, Training Loss: 1.51735, Tokens/sec: 86225.66613512344\n",
      "Step: 1410, Training Loss: 1.52550, Tokens/sec: 86460.00993172674\n",
      "Step: 1411, Training Loss: 1.49973, Tokens/sec: 83367.10284449975\n",
      "Step: 1412, Training Loss: 1.45901, Tokens/sec: 84126.58868481092\n",
      "Step: 1413, Training Loss: 1.56359, Tokens/sec: 86608.06603402148\n",
      "Step: 1414, Training Loss: 1.51418, Tokens/sec: 83294.8019481539\n",
      "Step: 1415, Training Loss: 1.59744, Tokens/sec: 86696.91046756676\n",
      "Step: 1416, Training Loss: 1.57279, Tokens/sec: 83718.29535434821\n",
      "Step: 1417, Training Loss: 1.83122, Tokens/sec: 86642.16018390833\n",
      "Step: 1418, Training Loss: 1.73557, Tokens/sec: 86880.68328216564\n",
      "Step: 1419, Training Loss: 1.39326, Tokens/sec: 82423.93458087306\n",
      "Step: 1420, Training Loss: 1.44002, Tokens/sec: 85550.62925958469\n",
      "Step: 1421, Training Loss: 1.60875, Tokens/sec: 87062.38735895763\n",
      "Step: 1422, Training Loss: 1.48869, Tokens/sec: 82672.9655279914\n",
      "Step: 1423, Training Loss: 1.41635, Tokens/sec: 86116.75992463091\n",
      "Step: 1424, Training Loss: 1.37375, Tokens/sec: 83413.4341270612\n",
      "Step: 1425, Training Loss: 1.42515, Tokens/sec: 86145.50824775505\n",
      "Step: 1426, Training Loss: 1.74277, Tokens/sec: 86442.9501599736\n",
      "Step: 1427, Training Loss: 1.54498, Tokens/sec: 82942.46410338342\n",
      "Step: 1428, Training Loss: 1.60251, Tokens/sec: 85027.95631398424\n",
      "Step: 1429, Training Loss: 1.82823, Tokens/sec: 85851.49179923399\n",
      "Step: 1430, Training Loss: 1.53086, Tokens/sec: 83906.28925187499\n",
      "Step: 1431, Training Loss: 1.78428, Tokens/sec: 86437.20441182237\n",
      "Step: 1432, Training Loss: 1.52289, Tokens/sec: 83468.30321875181\n",
      "Step: 1433, Training Loss: 1.48640, Tokens/sec: 85639.06613768279\n",
      "Step: 1434, Training Loss: 1.48778, Tokens/sec: 87028.61032259035\n",
      "Step: 1435, Training Loss: 1.52588, Tokens/sec: 83539.2240721282\n",
      "Step: 1436, Training Loss: 1.76105, Tokens/sec: 85257.6139148784\n",
      "Step: 1437, Training Loss: 1.51494, Tokens/sec: 85156.72272570636\n",
      "Step: 1438, Training Loss: 1.47185, Tokens/sec: 87274.8198835359\n",
      "Step: 1439, Training Loss: 1.41937, Tokens/sec: 85884.74966028164\n",
      "Step: 1440, Training Loss: 1.49762, Tokens/sec: 83009.52071046179\n",
      "Step: 1441, Training Loss: 1.60264, Tokens/sec: 86083.80793465838\n",
      "Step: 1442, Training Loss: 1.52806, Tokens/sec: 86505.6333448002\n",
      "Step: 1443, Training Loss: 1.53327, Tokens/sec: 83307.68315411771\n",
      "Step: 1444, Training Loss: 1.69407, Tokens/sec: 86186.60133434717\n",
      "Step: 1445, Training Loss: 1.57784, Tokens/sec: 85153.43915547278\n",
      "Step: 1446, Training Loss: 1.49104, Tokens/sec: 86411.51637607922\n",
      "Step: 1447, Training Loss: 1.45094, Tokens/sec: 85451.75879075385\n",
      "Step: 1448, Training Loss: 1.44820, Tokens/sec: 84119.17857817076\n",
      "Step: 1449, Training Loss: 1.44332, Tokens/sec: 86091.70665998287\n",
      "Step: 1450, Training Loss: 1.56812, Tokens/sec: 86100.65112696901\n",
      "Step: 1451, Training Loss: 1.50585, Tokens/sec: 84684.6147826191\n",
      "Step: 1452, Training Loss: 1.55349, Tokens/sec: 86927.79048477308\n",
      "Step: 1453, Training Loss: 1.51819, Tokens/sec: 83923.89919897352\n",
      "Step: 1454, Training Loss: 1.46758, Tokens/sec: 86967.85060047616\n",
      "Step: 1455, Training Loss: 1.43053, Tokens/sec: 85731.32538469194\n",
      "Step: 1456, Training Loss: 1.64261, Tokens/sec: 83911.48168334746\n",
      "Step: 1457, Training Loss: 1.40881, Tokens/sec: 85983.61622713102\n",
      "Step: 1458, Training Loss: 1.40305, Tokens/sec: 84057.28421692036\n",
      "Step: 1459, Training Loss: 1.54307, Tokens/sec: 85012.7409894452\n",
      "Step: 1460, Training Loss: 1.44030, Tokens/sec: 86090.43549122446\n",
      "Step: 1461, Training Loss: 1.59866, Tokens/sec: 83122.79937502398\n",
      "Step: 1462, Training Loss: 1.42374, Tokens/sec: 85877.89445445774\n",
      "Step: 1463, Training Loss: 1.61927, Tokens/sec: 85912.20394648581\n",
      "Step: 1464, Training Loss: 1.38803, Tokens/sec: 83354.52130944857\n",
      "Step: 1465, Training Loss: 1.50944, Tokens/sec: 85830.78756670802\n",
      "Step: 1466, Training Loss: 1.42636, Tokens/sec: 84229.93522137389\n",
      "Step: 1467, Training Loss: 1.38384, Tokens/sec: 84736.16974066742\n",
      "Step: 1468, Training Loss: 1.74845, Tokens/sec: 85810.83262750474\n",
      "Step: 1469, Training Loss: 1.64098, Tokens/sec: 84432.29475082415\n",
      "Step: 1470, Training Loss: 1.54110, Tokens/sec: 85724.82611401494\n",
      "Step: 1471, Training Loss: 1.60251, Tokens/sec: 86426.95207464343\n",
      "Step: 1472, Training Loss: 1.42518, Tokens/sec: 83743.91739592011\n",
      "Step: 1473, Training Loss: 1.38463, Tokens/sec: 86739.72766468162\n",
      "Step: 1474, Training Loss: 1.45683, Tokens/sec: 84351.02218587954\n",
      "Step: 1475, Training Loss: 1.53550, Tokens/sec: 86157.13011797627\n",
      "Step: 1476, Training Loss: 1.51169, Tokens/sec: 87037.81758016274\n",
      "Step: 1477, Training Loss: 1.40464, Tokens/sec: 84660.72809928367\n",
      "Step: 1478, Training Loss: 1.47470, Tokens/sec: 85862.41350368573\n",
      "Step: 1479, Training Loss: 1.52480, Tokens/sec: 86775.17058601472\n",
      "Step: 1480, Training Loss: 1.60826, Tokens/sec: 85936.91708880407\n",
      "Step: 1481, Training Loss: 1.49542, Tokens/sec: 86976.42583042863\n",
      "Step: 1482, Training Loss: 1.55538, Tokens/sec: 83060.81706473227\n",
      "Step: 1483, Training Loss: 1.44980, Tokens/sec: 86807.50562950876\n",
      "Step: 1484, Training Loss: 1.55234, Tokens/sec: 86902.65451023578\n",
      "Step: 1485, Training Loss: 1.43829, Tokens/sec: 84210.63625366325\n",
      "Step: 1486, Training Loss: 1.45037, Tokens/sec: 84654.81004075857\n",
      "Step: 1487, Training Loss: 1.44374, Tokens/sec: 86435.53359717206\n",
      "Step: 1488, Training Loss: 1.45844, Tokens/sec: 85806.69894985353\n",
      "Step: 1489, Training Loss: 1.39284, Tokens/sec: 85830.83387909915\n",
      "Step: 1490, Training Loss: 1.86730, Tokens/sec: 83034.71825950648\n",
      "Step: 1491, Training Loss: 1.51682, Tokens/sec: 86689.2544087524\n",
      "Step: 1492, Training Loss: 1.55253, Tokens/sec: 86210.21741726929\n",
      "Step: 1493, Training Loss: 1.59607, Tokens/sec: 82947.44385778428\n",
      "Step: 1494, Training Loss: 1.54272, Tokens/sec: 85617.05174381786\n",
      "Step: 1495, Training Loss: 1.44365, Tokens/sec: 84886.71998278951\n",
      "Step: 1496, Training Loss: 1.47970, Tokens/sec: 85937.80553356354\n",
      "Step: 1497, Training Loss: 1.44669, Tokens/sec: 83824.45043951929\n",
      "Step: 1498, Training Loss: 1.35594, Tokens/sec: 83638.36484308499\n",
      "Step: 1499, Training Loss: 1.62296, Tokens/sec: 86190.53093139638\n",
      "Step: 1500, Training Loss: 1.44366, Tokens/sec: 80717.26751213321\n",
      "Step: 1501, Training Loss: 1.39306, Tokens/sec: 83713.77438909427\n",
      "Step: 1502, Training Loss: 1.60163, Tokens/sec: 86113.49196988778\n",
      "Step: 1503, Training Loss: 1.56832, Tokens/sec: 86553.36476933127\n",
      "Step: 1504, Training Loss: 1.63784, Tokens/sec: 84596.59393607012\n",
      "Step: 1505, Training Loss: 1.54429, Tokens/sec: 85829.83164052015\n",
      "Step: 1506, Training Loss: 1.54492, Tokens/sec: 83365.69495196787\n",
      "Step: 1507, Training Loss: 1.51180, Tokens/sec: 86233.76338474342\n",
      "Step: 1508, Training Loss: 1.65183, Tokens/sec: 86654.20793624723\n",
      "Step: 1509, Training Loss: 1.63271, Tokens/sec: 83690.83317054117\n",
      "Step: 1510, Training Loss: 1.73634, Tokens/sec: 85829.12617381169\n",
      "Step: 1511, Training Loss: 1.58435, Tokens/sec: 85951.17815528117\n",
      "Step: 1512, Training Loss: 1.72333, Tokens/sec: 86654.84499140651\n",
      "Step: 1513, Training Loss: 1.44854, Tokens/sec: 85525.40245286375\n",
      "Step: 1514, Training Loss: 1.56028, Tokens/sec: 83036.07459504437\n",
      "Step: 1515, Training Loss: 1.62571, Tokens/sec: 86424.25089909056\n",
      "Step: 1516, Training Loss: 1.32715, Tokens/sec: 86183.5270957505\n",
      "Step: 1517, Training Loss: 1.51390, Tokens/sec: 82918.20290685278\n",
      "Step: 1518, Training Loss: 1.32343, Tokens/sec: 86423.85929613716\n",
      "Step: 1519, Training Loss: 1.57480, Tokens/sec: 83877.57425004023\n",
      "Step: 1520, Training Loss: 1.57168, Tokens/sec: 86739.05078695038\n",
      "Step: 1521, Training Loss: 2.09967, Tokens/sec: 86142.84592070326\n",
      "Step: 1522, Training Loss: 1.47597, Tokens/sec: 83590.71508674986\n",
      "Step: 1523, Training Loss: 1.37012, Tokens/sec: 86723.64296451148\n",
      "Step: 1524, Training Loss: 1.49405, Tokens/sec: 86725.00175744364\n",
      "Step: 1525, Training Loss: 1.44831, Tokens/sec: 82799.07529917575\n",
      "Step: 1526, Training Loss: 1.44084, Tokens/sec: 86721.89908693802\n",
      "Step: 1527, Training Loss: 1.42566, Tokens/sec: 84502.7261116736\n",
      "Step: 1528, Training Loss: 1.45313, Tokens/sec: 85249.78499397081\n",
      "Step: 1529, Training Loss: 1.68790, Tokens/sec: 85256.60771336724\n",
      "Step: 1530, Training Loss: 1.51873, Tokens/sec: 84058.15880688347\n",
      "Step: 1531, Training Loss: 1.48261, Tokens/sec: 86480.48891021934\n",
      "Step: 1532, Training Loss: 1.53192, Tokens/sec: 85885.99269974837\n",
      "Step: 1533, Training Loss: 1.55370, Tokens/sec: 85467.58822694972\n",
      "Step: 1534, Training Loss: 1.65825, Tokens/sec: 86924.89880420771\n",
      "Step: 1535, Training Loss: 1.47775, Tokens/sec: 83372.34796828944\n",
      "Step: 1536, Training Loss: 1.90754, Tokens/sec: 87072.86921949487\n",
      "Step: 1537, Training Loss: 1.60093, Tokens/sec: 86147.16288876826\n",
      "Step: 1538, Training Loss: 1.47845, Tokens/sec: 84097.63768621364\n",
      "Step: 1539, Training Loss: 1.52189, Tokens/sec: 85665.16358933736\n",
      "Step: 1540, Training Loss: 1.48540, Tokens/sec: 84297.50100588823\n",
      "Step: 1541, Training Loss: 1.41737, Tokens/sec: 86305.80466498857\n",
      "Step: 1542, Training Loss: 1.33047, Tokens/sec: 86321.65150351245\n",
      "Step: 1543, Training Loss: 1.50448, Tokens/sec: 84115.44810291672\n",
      "Step: 1544, Training Loss: 1.46922, Tokens/sec: 85698.55020302391\n",
      "Step: 1545, Training Loss: 1.62186, Tokens/sec: 86231.06473796112\n",
      "Step: 1546, Training Loss: 1.75372, Tokens/sec: 83343.03905328561\n",
      "Step: 1547, Training Loss: 1.76222, Tokens/sec: 87044.17480177175\n",
      "Step: 1548, Training Loss: 1.53860, Tokens/sec: 83798.365073914\n",
      "Step: 1549, Training Loss: 1.38238, Tokens/sec: 85219.75759128\n",
      "Step: 1550, Training Loss: 1.52051, Tokens/sec: 86220.11530077783\n",
      "Step: 1551, Training Loss: 1.50019, Tokens/sec: 83478.88256658516\n",
      "Step: 1552, Training Loss: 1.43276, Tokens/sec: 85689.1835544231\n",
      "Step: 1553, Training Loss: 1.45499, Tokens/sec: 85795.3798835125\n",
      "Step: 1554, Training Loss: 1.44692, Tokens/sec: 83596.70837485997\n",
      "Step: 1555, Training Loss: 1.66894, Tokens/sec: 86685.45990649202\n",
      "Step: 1556, Training Loss: 1.50562, Tokens/sec: 82908.34876836903\n",
      "Step: 1557, Training Loss: 1.47677, Tokens/sec: 85775.2752518344\n",
      "Step: 1558, Training Loss: 1.63949, Tokens/sec: 86630.69201446915\n",
      "Step: 1559, Training Loss: 1.48115, Tokens/sec: 83548.93435821333\n",
      "Step: 1560, Training Loss: 1.57857, Tokens/sec: 86345.84949622082\n",
      "Step: 1561, Training Loss: 1.51924, Tokens/sec: 87286.9293785633\n",
      "Step: 1562, Training Loss: 1.54083, Tokens/sec: 85750.67875883407\n",
      "Step: 1563, Training Loss: 1.52126, Tokens/sec: 86743.51727250894\n",
      "Step: 1564, Training Loss: 1.48116, Tokens/sec: 82843.51716291561\n",
      "Step: 1565, Training Loss: 1.49054, Tokens/sec: 86728.14367511029\n",
      "Step: 1566, Training Loss: 1.71698, Tokens/sec: 86341.44525471705\n",
      "Step: 1567, Training Loss: 1.47620, Tokens/sec: 83425.65241079127\n",
      "Step: 1568, Training Loss: 1.41271, Tokens/sec: 86007.26882151769\n",
      "Step: 1569, Training Loss: 1.65198, Tokens/sec: 87045.80402564764\n",
      "Step: 1570, Training Loss: 1.46153, Tokens/sec: 85981.14483179433\n",
      "Step: 1571, Training Loss: 1.51681, Tokens/sec: 86132.67642075004\n",
      "Step: 1572, Training Loss: 1.51715, Tokens/sec: 83326.78528442468\n",
      "Step: 1573, Training Loss: 1.48911, Tokens/sec: 86773.11304892754\n",
      "Step: 1574, Training Loss: 1.60955, Tokens/sec: 85904.96645521198\n",
      "Step: 1575, Training Loss: 1.42287, Tokens/sec: 82424.97910748237\n",
      "Step: 1576, Training Loss: 1.48870, Tokens/sec: 85740.7936795735\n",
      "Step: 1577, Training Loss: 1.45065, Tokens/sec: 86825.10255194869\n",
      "Step: 1578, Training Loss: 1.52634, Tokens/sec: 86552.14348944063\n",
      "Step: 1579, Training Loss: 1.69943, Tokens/sec: 85664.95934768877\n",
      "Step: 1580, Training Loss: 1.50910, Tokens/sec: 82836.05451793365\n",
      "Step: 1581, Training Loss: 1.48340, Tokens/sec: 86071.55109428152\n",
      "Step: 1582, Training Loss: 1.43597, Tokens/sec: 87149.6718436346\n",
      "Step: 1583, Training Loss: 1.58555, Tokens/sec: 82166.48754864409\n",
      "Step: 1584, Training Loss: 1.52694, Tokens/sec: 86425.07012201477\n",
      "Step: 1585, Training Loss: 1.47700, Tokens/sec: 83652.55098854075\n",
      "Step: 1586, Training Loss: 1.72253, Tokens/sec: 86520.20442993319\n",
      "Step: 1587, Training Loss: 1.59652, Tokens/sec: 85632.04242764847\n",
      "Step: 1588, Training Loss: 1.42410, Tokens/sec: 83465.43727304267\n",
      "Step: 1589, Training Loss: 1.39373, Tokens/sec: 86349.51420245713\n",
      "Step: 1590, Training Loss: 1.44144, Tokens/sec: 86710.7902343633\n",
      "Step: 1591, Training Loss: 1.54806, Tokens/sec: 82469.49299272822\n",
      "Step: 1592, Training Loss: 1.54632, Tokens/sec: 84573.90372597123\n",
      "Step: 1593, Training Loss: 1.50520, Tokens/sec: 84669.23897618626\n",
      "Step: 1594, Training Loss: 1.50889, Tokens/sec: 86258.00658238266\n",
      "Step: 1595, Training Loss: 1.65721, Tokens/sec: 84880.66123157114\n",
      "Step: 1596, Training Loss: 1.72547, Tokens/sec: 82821.98567308886\n",
      "Step: 1597, Training Loss: 1.63547, Tokens/sec: 86499.67741601521\n",
      "Step: 1598, Training Loss: 1.55532, Tokens/sec: 86671.14122110853\n",
      "Step: 1599, Training Loss: 1.62379, Tokens/sec: 84193.86453202517\n",
      "Step: 1600, Training Loss: 1.77266, Tokens/sec: 86937.20799936056\n",
      "Step: 1601, Training Loss: 1.83617, Tokens/sec: 84545.68503558914\n",
      "Step: 1602, Training Loss: 1.50827, Tokens/sec: 86251.82224816018\n",
      "Step: 1603, Training Loss: 1.58998, Tokens/sec: 85221.95489553886\n",
      "Step: 1604, Training Loss: 2.21386, Tokens/sec: 86243.65941571214\n",
      "Step: 1605, Training Loss: 1.50284, Tokens/sec: 86694.86535829907\n",
      "Step: 1606, Training Loss: 1.47459, Tokens/sec: 85722.36688372682\n",
      "Step: 1607, Training Loss: 1.55821, Tokens/sec: 86245.88850241393\n",
      "Step: 1608, Training Loss: 1.49631, Tokens/sec: 86894.60030023816\n",
      "Step: 1609, Training Loss: 1.56608, Tokens/sec: 83533.50818305161\n",
      "Step: 1610, Training Loss: 1.42633, Tokens/sec: 85794.8075175066\n",
      "Step: 1611, Training Loss: 1.50100, Tokens/sec: 85565.61098693665\n",
      "Step: 1612, Training Loss: 1.58703, Tokens/sec: 86277.68529007593\n",
      "Step: 1613, Training Loss: 1.50559, Tokens/sec: 86199.1884673201\n",
      "Step: 1614, Training Loss: 1.59482, Tokens/sec: 84295.6221743974\n",
      "Step: 1615, Training Loss: 1.62994, Tokens/sec: 85265.95590918283\n",
      "Step: 1616, Training Loss: 1.51463, Tokens/sec: 86333.93646331572\n",
      "Step: 1617, Training Loss: 1.42993, Tokens/sec: 83246.20148004078\n",
      "Step: 1618, Training Loss: 1.45663, Tokens/sec: 85953.96122185481\n",
      "Step: 1619, Training Loss: 1.40780, Tokens/sec: 83429.35933475637\n",
      "Step: 1620, Training Loss: 1.40063, Tokens/sec: 83372.72131414212\n",
      "Step: 1621, Training Loss: 1.42198, Tokens/sec: 86693.201079158\n",
      "Step: 1622, Training Loss: 1.53042, Tokens/sec: 83860.24771674308\n",
      "Step: 1623, Training Loss: 1.50432, Tokens/sec: 86404.25694648342\n",
      "Step: 1624, Training Loss: 1.74654, Tokens/sec: 85961.67286796171\n",
      "Step: 1625, Training Loss: 1.53568, Tokens/sec: 83655.72196453325\n",
      "Step: 1626, Training Loss: 1.37890, Tokens/sec: 86106.461275322\n",
      "Step: 1627, Training Loss: 1.47914, Tokens/sec: 83185.99476174962\n",
      "Step: 1628, Training Loss: 1.54657, Tokens/sec: 85720.45315090698\n",
      "Step: 1629, Training Loss: 1.50882, Tokens/sec: 86673.61391954108\n",
      "Step: 1630, Training Loss: 1.63775, Tokens/sec: 83249.15052573447\n",
      "Step: 1631, Training Loss: 1.52860, Tokens/sec: 85591.43475275858\n",
      "Step: 1632, Training Loss: 1.42065, Tokens/sec: 86418.9689282089\n",
      "Step: 1633, Training Loss: 1.36653, Tokens/sec: 84198.5560645458\n",
      "Step: 1634, Training Loss: 1.50902, Tokens/sec: 85895.38576210082\n",
      "Step: 1635, Training Loss: 1.39766, Tokens/sec: 84687.34225439\n",
      "Step: 1636, Training Loss: 1.59889, Tokens/sec: 86369.77240174943\n",
      "Step: 1637, Training Loss: 1.55461, Tokens/sec: 86708.7646602154\n",
      "Step: 1638, Training Loss: 1.47934, Tokens/sec: 80771.55489592868\n",
      "Step: 1639, Training Loss: 1.51099, Tokens/sec: 86723.40976699367\n",
      "Step: 1640, Training Loss: 1.38087, Tokens/sec: 87009.74603086185\n",
      "Step: 1641, Training Loss: 1.43400, Tokens/sec: 84086.632943227\n",
      "Step: 1642, Training Loss: 1.41247, Tokens/sec: 83812.6913017881\n",
      "Step: 1643, Training Loss: 1.31852, Tokens/sec: 85067.30218111111\n",
      "Step: 1644, Training Loss: 1.47519, Tokens/sec: 86696.45033276748\n",
      "Step: 1645, Training Loss: 1.38995, Tokens/sec: 86251.9244133529\n",
      "Step: 1646, Training Loss: 1.53926, Tokens/sec: 83443.38244375959\n",
      "Step: 1647, Training Loss: 1.43378, Tokens/sec: 86787.45258199444\n",
      "Step: 1648, Training Loss: 1.54006, Tokens/sec: 85959.79353188242\n",
      "Step: 1649, Training Loss: 1.43234, Tokens/sec: 83636.27661468335\n",
      "Step: 1650, Training Loss: 1.40081, Tokens/sec: 85778.00113075292\n",
      "Step: 1651, Training Loss: 1.57755, Tokens/sec: 84039.3057332674\n",
      "Step: 1652, Training Loss: 1.51209, Tokens/sec: 85157.95982645919\n",
      "Step: 1653, Training Loss: 1.37373, Tokens/sec: 86227.82531485557\n",
      "Step: 1654, Training Loss: 1.37040, Tokens/sec: 83131.9992910174\n",
      "Step: 1655, Training Loss: 1.60375, Tokens/sec: 86324.1538799818\n",
      "Step: 1656, Training Loss: 1.64446, Tokens/sec: 86888.35798451952\n",
      "Step: 1657, Training Loss: 1.62311, Tokens/sec: 84024.50980277426\n",
      "Step: 1658, Training Loss: 1.46865, Tokens/sec: 85422.39641319407\n",
      "Step: 1659, Training Loss: 1.52393, Tokens/sec: 83451.26308286173\n",
      "Step: 1660, Training Loss: 1.41122, Tokens/sec: 86581.48846383067\n",
      "Step: 1661, Training Loss: 1.40978, Tokens/sec: 85603.67151324985\n",
      "Step: 1662, Training Loss: 1.55147, Tokens/sec: 72740.57384214317\n",
      "Step: 1663, Training Loss: 1.47591, Tokens/sec: 85060.34279488126\n",
      "Step: 1664, Training Loss: 1.48326, Tokens/sec: 69380.01499346517\n",
      "Step: 1665, Training Loss: 1.46272, Tokens/sec: 53957.764480955295\n",
      "Step: 1666, Training Loss: 1.54422, Tokens/sec: 86855.83514904734\n",
      "Step: 1667, Training Loss: 1.31982, Tokens/sec: 84432.9787463115\n",
      "Step: 1668, Training Loss: 1.34998, Tokens/sec: 85509.45207475993\n",
      "Step: 1669, Training Loss: 1.33980, Tokens/sec: 84699.63317693002\n",
      "Step: 1670, Training Loss: 1.69521, Tokens/sec: 86847.17963743814\n",
      "Step: 1671, Training Loss: 1.43384, Tokens/sec: 86332.53257549659\n",
      "Step: 1672, Training Loss: 1.41079, Tokens/sec: 84512.6359667265\n",
      "Step: 1673, Training Loss: 1.56732, Tokens/sec: 86697.28161045842\n",
      "Step: 1674, Training Loss: 1.63497, Tokens/sec: 85967.15934789852\n",
      "Step: 1675, Training Loss: 1.45980, Tokens/sec: 82467.77031025769\n",
      "Step: 1676, Training Loss: 1.61579, Tokens/sec: 74008.49486011578\n",
      "Step: 1677, Training Loss: 1.41118, Tokens/sec: 86823.03620544121\n",
      "Step: 1678, Training Loss: 1.43704, Tokens/sec: 85660.16299117432\n",
      "Step: 1679, Training Loss: 1.43630, Tokens/sec: 86014.35918709668\n",
      "Step: 1680, Training Loss: 1.46523, Tokens/sec: 85833.35644554574\n",
      "Step: 1681, Training Loss: 1.53290, Tokens/sec: 87609.91859183475\n",
      "Step: 1682, Training Loss: 1.48918, Tokens/sec: 86474.15260008122\n",
      "Step: 1683, Training Loss: 1.62673, Tokens/sec: 86126.53811690523\n",
      "Step: 1684, Training Loss: 1.45358, Tokens/sec: 85531.7152376235\n",
      "Step: 1685, Training Loss: 1.42665, Tokens/sec: 85062.82999150507\n",
      "Step: 1686, Training Loss: 1.55956, Tokens/sec: 87182.40261867408\n",
      "Step: 1687, Training Loss: 1.70626, Tokens/sec: 87515.61355540712\n",
      "Step: 1688, Training Loss: 1.47486, Tokens/sec: 86711.82325334895\n",
      "Step: 1689, Training Loss: 1.54358, Tokens/sec: 85331.82091693232\n",
      "Step: 1690, Training Loss: 1.44682, Tokens/sec: 83586.85862679682\n",
      "Step: 1691, Training Loss: 1.52107, Tokens/sec: 86797.5694503033\n",
      "Step: 1692, Training Loss: 1.55926, Tokens/sec: 83972.61957562587\n",
      "Step: 1693, Training Loss: 1.56963, Tokens/sec: 84165.96325696804\n",
      "Step: 1694, Training Loss: 1.39351, Tokens/sec: 86447.17319252803\n",
      "Step: 1695, Training Loss: 1.47165, Tokens/sec: 83559.60660194249\n",
      "Step: 1696, Training Loss: 1.60307, Tokens/sec: 57438.928052214666\n",
      "Step: 1697, Training Loss: 1.60585, Tokens/sec: 82942.52792427431\n",
      "Step: 1698, Training Loss: 1.48493, Tokens/sec: 66309.75822459933\n",
      "Step: 1699, Training Loss: 1.51957, Tokens/sec: 87310.55171909725\n",
      "Step: 1700, Training Loss: 1.55077, Tokens/sec: 84665.73081756376\n",
      "Step: 1701, Training Loss: 1.51645, Tokens/sec: 86174.51146642848\n",
      "Step: 1702, Training Loss: 1.49392, Tokens/sec: 86362.57324471972\n",
      "Step: 1703, Training Loss: 1.45515, Tokens/sec: 84093.52367957326\n",
      "Step: 1704, Training Loss: 1.45940, Tokens/sec: 86656.72917420782\n",
      "Step: 1705, Training Loss: 1.52924, Tokens/sec: 86411.1103082394\n",
      "Step: 1706, Training Loss: 1.61096, Tokens/sec: 84533.07979847878\n",
      "Step: 1707, Training Loss: 1.60116, Tokens/sec: 86919.55361669898\n",
      "Step: 1708, Training Loss: 1.62143, Tokens/sec: 84170.10251757626\n",
      "Step: 1709, Training Loss: 1.71535, Tokens/sec: 86867.56105078419\n",
      "Step: 1710, Training Loss: 1.52755, Tokens/sec: 87128.45217855234\n",
      "Step: 1711, Training Loss: 1.53197, Tokens/sec: 85443.16295515266\n",
      "Step: 1712, Training Loss: 1.50954, Tokens/sec: 87361.67439318517\n",
      "Step: 1713, Training Loss: 1.52947, Tokens/sec: 86656.12004689377\n",
      "Step: 1714, Training Loss: 1.95050, Tokens/sec: 87166.00772713863\n",
      "Step: 1715, Training Loss: 1.34970, Tokens/sec: 86807.6555642637\n",
      "Step: 1716, Training Loss: 1.57939, Tokens/sec: 82686.60279038096\n",
      "Step: 1717, Training Loss: 1.63148, Tokens/sec: 86122.14851829283\n",
      "Step: 1718, Training Loss: 2.04449, Tokens/sec: 86702.20674039179\n",
      "Step: 1719, Training Loss: 1.34924, Tokens/sec: 83164.43898336204\n",
      "Step: 1720, Training Loss: 1.56988, Tokens/sec: 78607.98047946145\n",
      "Step: 1721, Training Loss: 1.55532, Tokens/sec: 83644.98971423732\n",
      "Step: 1722, Training Loss: 1.68397, Tokens/sec: 76087.69800276074\n",
      "Step: 1723, Training Loss: 1.46109, Tokens/sec: 86151.96455665556\n",
      "Step: 1724, Training Loss: 1.41859, Tokens/sec: 86046.56072914555\n",
      "Step: 1725, Training Loss: 1.47967, Tokens/sec: 86523.94426676363\n",
      "Step: 1726, Training Loss: 1.48228, Tokens/sec: 87265.29608695029\n",
      "Step: 1727, Training Loss: 1.55738, Tokens/sec: 74919.58890588123\n",
      "Step: 1728, Training Loss: 1.45217, Tokens/sec: 85142.24884988913\n",
      "Step: 1729, Training Loss: 1.47529, Tokens/sec: 84733.70248988544\n",
      "Step: 1730, Training Loss: 1.53956, Tokens/sec: 81652.29356342215\n",
      "Step: 1731, Training Loss: 1.41472, Tokens/sec: 79362.20032872389\n",
      "Step: 1732, Training Loss: 1.37345, Tokens/sec: 86485.13652821498\n",
      "Step: 1733, Training Loss: 1.51257, Tokens/sec: 84499.96388068101\n",
      "Step: 1734, Training Loss: 1.53325, Tokens/sec: 86845.32721877118\n",
      "Step: 1735, Training Loss: 1.46199, Tokens/sec: 83724.2966881571\n",
      "Step: 1736, Training Loss: 1.51568, Tokens/sec: 85129.12806319361\n",
      "Step: 1737, Training Loss: 1.50595, Tokens/sec: 87681.24293564039\n",
      "Step: 1738, Training Loss: 1.55299, Tokens/sec: 71518.83083940769\n",
      "Step: 1739, Training Loss: 1.53668, Tokens/sec: 83856.54918390977\n",
      "Step: 1740, Training Loss: 1.63543, Tokens/sec: 86087.2654282672\n",
      "Step: 1741, Training Loss: 1.42990, Tokens/sec: 79394.53602802039\n",
      "Step: 1742, Training Loss: 1.63901, Tokens/sec: 84856.69108754967\n",
      "Step: 1743, Training Loss: 1.53053, Tokens/sec: 86038.41907320372\n",
      "Step: 1744, Training Loss: 1.62172, Tokens/sec: 84192.82487614709\n",
      "Step: 1745, Training Loss: 3.50745, Tokens/sec: 86189.79912181104\n",
      "Step: 1746, Training Loss: 1.45224, Tokens/sec: 83390.09142794496\n",
      "Step: 1747, Training Loss: 1.52938, Tokens/sec: 86131.18172105076\n",
      "Step: 1748, Training Loss: 1.49814, Tokens/sec: 86815.98256174689\n",
      "Step: 1749, Training Loss: 1.57923, Tokens/sec: 83434.06844442882\n",
      "Step: 1750, Training Loss: 1.52647, Tokens/sec: 86259.58516639468\n",
      "Step: 1751, Training Loss: 1.50205, Tokens/sec: 86341.57265735212\n",
      "Step: 1752, Training Loss: 1.41193, Tokens/sec: 86018.3909675704\n",
      "Step: 1753, Training Loss: 1.68451, Tokens/sec: 85466.14103797024\n",
      "Step: 1754, Training Loss: 1.80641, Tokens/sec: 83802.93504526556\n",
      "Step: 1755, Training Loss: 1.70135, Tokens/sec: 86330.57194157965\n",
      "Step: 1756, Training Loss: 1.53292, Tokens/sec: 74636.62625211476\n",
      "Step: 1757, Training Loss: 1.44367, Tokens/sec: 81479.3207223181\n",
      "Step: 1758, Training Loss: 1.63637, Tokens/sec: 74806.44835926502\n",
      "Step: 1759, Training Loss: 1.59130, Tokens/sec: 85048.84936413368\n",
      "Step: 1760, Training Loss: 1.37408, Tokens/sec: 83192.58532911805\n",
      "Step: 1761, Training Loss: 1.52342, Tokens/sec: 72688.05899062817\n",
      "Step: 1762, Training Loss: 1.54534, Tokens/sec: 86636.13046242685\n",
      "Step: 1763, Training Loss: 1.47803, Tokens/sec: 83381.7530043687\n",
      "Step: 1764, Training Loss: 1.38114, Tokens/sec: 79427.03670284961\n",
      "Step: 1765, Training Loss: 1.51740, Tokens/sec: 79838.43048349171\n",
      "Step: 1766, Training Loss: 1.51466, Tokens/sec: 65107.05751557231\n",
      "Step: 1767, Training Loss: 1.66765, Tokens/sec: 73005.02431007283\n",
      "Step: 1768, Training Loss: 1.75575, Tokens/sec: 84890.00984918073\n",
      "Step: 1769, Training Loss: 1.54394, Tokens/sec: 83031.50792409002\n",
      "Step: 1770, Training Loss: 1.41949, Tokens/sec: 85982.05269489766\n",
      "Step: 1771, Training Loss: 1.85745, Tokens/sec: 85862.9233247799\n",
      "Step: 1772, Training Loss: 1.51615, Tokens/sec: 85299.07020744082\n",
      "Step: 1773, Training Loss: 1.45303, Tokens/sec: 85946.79965543971\n",
      "Step: 1774, Training Loss: 1.51358, Tokens/sec: 83765.29628668806\n",
      "Step: 1775, Training Loss: 1.45122, Tokens/sec: 85716.6788556646\n",
      "Step: 1776, Training Loss: 2.05356, Tokens/sec: 85117.7018207406\n",
      "Step: 1777, Training Loss: 1.44146, Tokens/sec: 83687.67406638117\n",
      "Step: 1778, Training Loss: 1.34118, Tokens/sec: 86629.00775915066\n",
      "Step: 1779, Training Loss: 1.62442, Tokens/sec: 86236.61834295085\n",
      "Step: 1780, Training Loss: 1.55223, Tokens/sec: 84537.7852220432\n",
      "Step: 1781, Training Loss: 1.53147, Tokens/sec: 86332.29738554853\n",
      "Step: 1782, Training Loss: 1.65550, Tokens/sec: 84558.33461587685\n",
      "Step: 1783, Training Loss: 1.78493, Tokens/sec: 86061.69497270248\n",
      "Step: 1784, Training Loss: 1.95420, Tokens/sec: 84845.24163797364\n",
      "Step: 1785, Training Loss: 1.60777, Tokens/sec: 84294.29420407876\n",
      "Step: 1786, Training Loss: 1.44314, Tokens/sec: 86586.86170418614\n",
      "Step: 1787, Training Loss: 1.43324, Tokens/sec: 83862.47463187113\n",
      "Step: 1788, Training Loss: 1.57109, Tokens/sec: 84553.41221919701\n",
      "Step: 1789, Training Loss: 1.58763, Tokens/sec: 86117.64304137937\n",
      "Step: 1790, Training Loss: 1.60342, Tokens/sec: 83214.84208222298\n",
      "Step: 1791, Training Loss: 1.54494, Tokens/sec: 85520.54984292788\n",
      "Step: 1792, Training Loss: 1.60030, Tokens/sec: 84817.43921143263\n",
      "Step: 1793, Training Loss: 1.51584, Tokens/sec: 84047.56363938675\n",
      "Step: 1794, Training Loss: 1.76666, Tokens/sec: 85856.6470385968\n",
      "Step: 1795, Training Loss: 1.85808, Tokens/sec: 83631.77858930125\n",
      "Step: 1796, Training Loss: 1.81787, Tokens/sec: 85217.69469558429\n",
      "Step: 1797, Training Loss: 1.82268, Tokens/sec: 86094.51059593016\n",
      "Step: 1798, Training Loss: 1.66667, Tokens/sec: 83740.65883965693\n",
      "Step: 1799, Training Loss: 1.56258, Tokens/sec: 85252.0863200523\n",
      "Step: 1800, Training Loss: 1.61590, Tokens/sec: 84905.05674832163\n",
      "Step: 1801, Training Loss: 1.48857, Tokens/sec: 83036.96298904486\n",
      "Step: 1802, Training Loss: 1.54604, Tokens/sec: 86427.45631499258\n",
      "Step: 1803, Training Loss: 1.51506, Tokens/sec: 83461.91761358576\n",
      "Step: 1804, Training Loss: 1.39964, Tokens/sec: 84969.5071782099\n",
      "Step: 1805, Training Loss: 1.72981, Tokens/sec: 85958.34361460299\n",
      "Step: 1806, Training Loss: 1.55293, Tokens/sec: 83828.9726473608\n",
      "Step: 1807, Training Loss: 1.47001, Tokens/sec: 86264.87900702073\n",
      "Step: 1808, Training Loss: 1.47424, Tokens/sec: 84171.47327822038\n",
      "Step: 1809, Training Loss: 1.55443, Tokens/sec: 86442.88220241219\n",
      "Step: 1810, Training Loss: 1.40116, Tokens/sec: 86430.67063388723\n",
      "Step: 1811, Training Loss: 1.49182, Tokens/sec: 83791.94429697971\n",
      "Step: 1812, Training Loss: 1.50924, Tokens/sec: 86238.29418837119\n",
      "Step: 1813, Training Loss: 1.48438, Tokens/sec: 86821.52021138927\n",
      "Step: 1814, Training Loss: 1.35429, Tokens/sec: 84651.67615318777\n",
      "Step: 1815, Training Loss: 1.37009, Tokens/sec: 85683.94758437976\n",
      "Step: 1816, Training Loss: 1.45428, Tokens/sec: 84543.26071110045\n",
      "Step: 1817, Training Loss: 1.49347, Tokens/sec: 86611.74479078603\n",
      "Step: 1818, Training Loss: 1.55845, Tokens/sec: 81948.83000573951\n",
      "Step: 1819, Training Loss: 1.47247, Tokens/sec: 82697.12848573712\n",
      "Step: 1820, Training Loss: 1.46726, Tokens/sec: 85900.2215554219\n",
      "Step: 1821, Training Loss: 1.61274, Tokens/sec: 86364.2043633192\n",
      "Step: 1822, Training Loss: 1.68774, Tokens/sec: 83785.35868374942\n",
      "Step: 1823, Training Loss: 1.47727, Tokens/sec: 85053.33333240094\n",
      "Step: 1824, Training Loss: 1.68990, Tokens/sec: 84686.61955269729\n",
      "Step: 1825, Training Loss: 1.34889, Tokens/sec: 66146.44517014238\n",
      "Step: 1826, Training Loss: 1.30826, Tokens/sec: 74742.93896616752\n",
      "Step: 1827, Training Loss: 1.68996, Tokens/sec: 79307.34291159721\n",
      "Step: 1828, Training Loss: 1.63146, Tokens/sec: 83362.96499015209\n",
      "Step: 1829, Training Loss: 1.67056, Tokens/sec: 83807.26759336129\n",
      "Step: 1830, Training Loss: 1.34747, Tokens/sec: 85605.34789438406\n",
      "Step: 1831, Training Loss: 1.43299, Tokens/sec: 84878.65781564233\n",
      "Step: 1832, Training Loss: 1.40203, Tokens/sec: 83368.28934631156\n",
      "Step: 1833, Training Loss: 1.64216, Tokens/sec: 86489.45545327304\n",
      "Step: 1834, Training Loss: 1.53528, Tokens/sec: 84114.9432752805\n",
      "Step: 1835, Training Loss: 1.45074, Tokens/sec: 86196.20991982406\n",
      "Step: 1836, Training Loss: 1.62487, Tokens/sec: 85353.27221233502\n",
      "Step: 1837, Training Loss: 1.43481, Tokens/sec: 83803.9886699222\n",
      "Step: 1838, Training Loss: 1.58191, Tokens/sec: 85724.41391326401\n",
      "Step: 1839, Training Loss: 1.52530, Tokens/sec: 85872.45311494973\n",
      "Step: 1840, Training Loss: 1.50390, Tokens/sec: 83584.62201631941\n",
      "Step: 1841, Training Loss: 1.48272, Tokens/sec: 86098.29380557998\n",
      "Step: 1842, Training Loss: 1.52163, Tokens/sec: 83481.81792078161\n",
      "Step: 1843, Training Loss: 1.50050, Tokens/sec: 86090.1419063572\n",
      "Step: 1844, Training Loss: 1.53302, Tokens/sec: 85811.54723104062\n",
      "Step: 1845, Training Loss: 1.47185, Tokens/sec: 83003.40358721874\n",
      "Step: 1846, Training Loss: 1.43971, Tokens/sec: 86004.18523965222\n",
      "Step: 1847, Training Loss: 1.32192, Tokens/sec: 85809.33604634633\n",
      "Step: 1848, Training Loss: 1.46704, Tokens/sec: 84996.51296191245\n",
      "Step: 1849, Training Loss: 1.52042, Tokens/sec: 84504.84126499659\n",
      "Step: 1850, Training Loss: 1.58943, Tokens/sec: 83775.28366406397\n",
      "Step: 1851, Training Loss: 2.00448, Tokens/sec: 85081.10198783535\n",
      "Step: 1852, Training Loss: 1.49909, Tokens/sec: 85684.9034035786\n",
      "Step: 1853, Training Loss: 2.24394, Tokens/sec: 84006.48890427784\n",
      "Step: 1854, Training Loss: 1.50184, Tokens/sec: 86015.03519077977\n",
      "Step: 1855, Training Loss: 1.63174, Tokens/sec: 84752.34935788225\n",
      "Step: 1856, Training Loss: 1.41358, Tokens/sec: 84605.3077445027\n",
      "Step: 1857, Training Loss: 1.50548, Tokens/sec: 83743.7984035575\n",
      "Step: 1858, Training Loss: 1.45400, Tokens/sec: 83661.45202735672\n",
      "Step: 1859, Training Loss: 1.43407, Tokens/sec: 84663.97859326702\n",
      "Step: 1860, Training Loss: 1.30813, Tokens/sec: 86104.05521717713\n",
      "Step: 1861, Training Loss: 1.87115, Tokens/sec: 81050.68760804542\n",
      "Step: 1862, Training Loss: 1.44177, Tokens/sec: 84675.27283515762\n",
      "Step: 1863, Training Loss: 1.43705, Tokens/sec: 82997.96806432346\n",
      "Step: 1864, Training Loss: 1.36022, Tokens/sec: 84629.82936333834\n",
      "Step: 1865, Training Loss: 1.54167, Tokens/sec: 86287.76999867884\n",
      "Step: 1866, Training Loss: 1.46303, Tokens/sec: 83430.05436635275\n",
      "Step: 1867, Training Loss: 1.53698, Tokens/sec: 85538.3094081511\n",
      "Step: 1868, Training Loss: 1.41964, Tokens/sec: 85331.21116591594\n",
      "Step: 1869, Training Loss: 1.47175, Tokens/sec: 83653.2343670655\n",
      "Step: 1870, Training Loss: 1.51045, Tokens/sec: 85115.88441647476\n",
      "Step: 1871, Training Loss: 1.78596, Tokens/sec: 67493.70096123754\n",
      "Step: 1872, Training Loss: 1.54952, Tokens/sec: 76712.47310586231\n",
      "Step: 1873, Training Loss: 1.37594, Tokens/sec: 79135.46819638013\n",
      "Step: 1874, Training Loss: 1.69339, Tokens/sec: 67671.21803364002\n",
      "Step: 1875, Training Loss: 1.65227, Tokens/sec: 85928.13324102035\n",
      "Step: 1876, Training Loss: 1.58984, Tokens/sec: 86784.60378440283\n",
      "Step: 1877, Training Loss: 1.81979, Tokens/sec: 86888.50635806954\n",
      "Step: 1878, Training Loss: 1.44733, Tokens/sec: 85327.2079975989\n",
      "Step: 1879, Training Loss: 1.37917, Tokens/sec: 86792.49232889277\n",
      "Step: 1880, Training Loss: 1.68254, Tokens/sec: 86893.82929217577\n",
      "Step: 1881, Training Loss: 1.53111, Tokens/sec: 83754.124880447\n",
      "Step: 1882, Training Loss: 1.93030, Tokens/sec: 86952.19249415072\n",
      "Step: 1883, Training Loss: 1.50734, Tokens/sec: 82973.85466445179\n",
      "Step: 1884, Training Loss: 1.56529, Tokens/sec: 86288.28442723457\n",
      "Step: 1885, Training Loss: 1.54573, Tokens/sec: 86135.91096539581\n",
      "Step: 1886, Training Loss: 1.48596, Tokens/sec: 83083.1503973382\n",
      "Step: 1887, Training Loss: 1.43589, Tokens/sec: 86003.94732199385\n",
      "Step: 1888, Training Loss: 1.52704, Tokens/sec: 86230.53600973009\n",
      "Step: 1889, Training Loss: 1.50732, Tokens/sec: 85657.73613354923\n",
      "Step: 1890, Training Loss: 1.70994, Tokens/sec: 87390.02712695324\n",
      "Step: 1891, Training Loss: 1.48742, Tokens/sec: 85002.11903642387\n",
      "Step: 1892, Training Loss: 1.39191, Tokens/sec: 86356.01842498619\n",
      "Step: 1893, Training Loss: 1.42059, Tokens/sec: 86890.99516982913\n",
      "Step: 1894, Training Loss: 1.61124, Tokens/sec: 85138.08819797896\n",
      "Step: 1895, Training Loss: 1.52787, Tokens/sec: 87933.12471010126\n",
      "Step: 1896, Training Loss: 1.54227, Tokens/sec: 82879.16890944856\n",
      "Step: 1897, Training Loss: 1.62710, Tokens/sec: 86872.30932227403\n",
      "Step: 1898, Training Loss: 1.53674, Tokens/sec: 63126.34154231498\n",
      "Step: 1899, Training Loss: 1.62144, Tokens/sec: 85920.05904094102\n",
      "Step: 1900, Training Loss: 1.69064, Tokens/sec: 81933.90377606102\n",
      "Step: 1901, Training Loss: 1.47581, Tokens/sec: 86395.42149669031\n",
      "Step: 1902, Training Loss: 1.49295, Tokens/sec: 85857.18738612295\n",
      "Step: 1903, Training Loss: 1.68856, Tokens/sec: 62942.670820586696\n",
      "Step: 1904, Training Loss: 1.48988, Tokens/sec: 83348.37146463167\n",
      "Step: 1905, Training Loss: 1.55124, Tokens/sec: 69519.09988552071\n",
      "Step: 1906, Training Loss: 1.55940, Tokens/sec: 78977.96649886925\n",
      "Step: 1907, Training Loss: 1.63671, Tokens/sec: 87055.48627155619\n",
      "Step: 1908, Training Loss: 1.73261, Tokens/sec: 81425.2107477822\n",
      "Step: 1909, Training Loss: 1.53102, Tokens/sec: 82881.62242119394\n",
      "Step: 1910, Training Loss: 1.49353, Tokens/sec: 81016.36293101519\n",
      "Step: 1911, Training Loss: 1.56691, Tokens/sec: 81157.15721285813\n",
      "Step: 1912, Training Loss: 1.51480, Tokens/sec: 81528.22305436632\n",
      "Step: 1913, Training Loss: 1.54789, Tokens/sec: 85212.17759618806\n",
      "Step: 1914, Training Loss: 1.43377, Tokens/sec: 80260.44317667949\n",
      "Step: 1915, Training Loss: 1.40592, Tokens/sec: 79406.4099444738\n",
      "Step: 1916, Training Loss: 1.65225, Tokens/sec: 86875.68165363492\n",
      "Step: 1917, Training Loss: 1.59931, Tokens/sec: 83406.20006529774\n",
      "Step: 1918, Training Loss: 1.56234, Tokens/sec: 68675.27424614974\n",
      "Step: 1919, Training Loss: 1.50590, Tokens/sec: 86461.03835178839\n",
      "Step: 1920, Training Loss: 2.38199, Tokens/sec: 85177.68574974118\n",
      "Step: 1921, Training Loss: 1.63684, Tokens/sec: 67294.82066248976\n",
      "Step: 1922, Training Loss: 1.53997, Tokens/sec: 74605.36238134558\n",
      "Step: 1923, Training Loss: 1.63539, Tokens/sec: 73005.55683055142\n",
      "Step: 1924, Training Loss: 1.48641, Tokens/sec: 87082.7537404721\n",
      "Step: 1925, Training Loss: 1.56718, Tokens/sec: 81117.3225985253\n",
      "Step: 1926, Training Loss: 1.60151, Tokens/sec: 72418.004963821\n",
      "Step: 1927, Training Loss: 1.45143, Tokens/sec: 86087.19621867054\n",
      "Step: 1928, Training Loss: 1.39823, Tokens/sec: 74345.57808534699\n",
      "Step: 1929, Training Loss: 1.70282, Tokens/sec: 84873.12604637587\n",
      "Step: 1930, Training Loss: 1.48133, Tokens/sec: 86523.73819029192\n",
      "Step: 1931, Training Loss: 1.69285, Tokens/sec: 86737.14190063975\n",
      "Step: 1932, Training Loss: 1.90181, Tokens/sec: 83503.07765190762\n",
      "Step: 1933, Training Loss: 1.88806, Tokens/sec: 72155.32741527389\n",
      "Step: 1934, Training Loss: 1.74358, Tokens/sec: 86548.38749603872\n",
      "Step: 1935, Training Loss: 1.38766, Tokens/sec: 83275.9862146066\n",
      "Step: 1936, Training Loss: 1.43097, Tokens/sec: 73273.25474998016\n",
      "Step: 1937, Training Loss: 1.67794, Tokens/sec: 80446.7982967254\n",
      "Step: 1938, Training Loss: 1.51722, Tokens/sec: 87207.60647874748\n",
      "Step: 1939, Training Loss: 1.51969, Tokens/sec: 76389.97786811073\n",
      "Step: 1940, Training Loss: 1.62068, Tokens/sec: 83684.74003256291\n",
      "Step: 1941, Training Loss: 1.67485, Tokens/sec: 83531.92559146909\n",
      "Step: 1942, Training Loss: 1.69444, Tokens/sec: 85836.95709147168\n",
      "Step: 1943, Training Loss: 1.62233, Tokens/sec: 85139.12964667082\n",
      "Step: 1944, Training Loss: 1.51518, Tokens/sec: 83635.50342869855\n",
      "Step: 1945, Training Loss: 1.53714, Tokens/sec: 85736.87537190321\n",
      "Step: 1946, Training Loss: 1.45294, Tokens/sec: 82533.03599839294\n",
      "Step: 1947, Training Loss: 1.55780, Tokens/sec: 83933.38867156688\n",
      "Step: 1948, Training Loss: 1.64116, Tokens/sec: 85547.0177548263\n",
      "Step: 1949, Training Loss: 1.50323, Tokens/sec: 83683.82532746936\n",
      "Step: 1950, Training Loss: 1.49579, Tokens/sec: 85313.29492596604\n",
      "Step: 1951, Training Loss: 1.55895, Tokens/sec: 86095.75202179863\n",
      "Step: 1952, Training Loss: 1.67641, Tokens/sec: 84158.1757411143\n",
      "Step: 1953, Training Loss: 1.49859, Tokens/sec: 85622.12427071533\n",
      "Step: 1954, Training Loss: 1.51519, Tokens/sec: 84879.88949001329\n",
      "Step: 1955, Training Loss: 1.70567, Tokens/sec: 86363.60890744741\n",
      "Step: 1956, Training Loss: 1.61488, Tokens/sec: 86144.53669381676\n",
      "Step: 1957, Training Loss: 1.65137, Tokens/sec: 83397.7187571766\n",
      "Step: 1958, Training Loss: 1.58981, Tokens/sec: 86514.61103947661\n",
      "Step: 1959, Training Loss: 1.62451, Tokens/sec: 85826.5836265821\n",
      "Step: 1960, Training Loss: 1.54093, Tokens/sec: 83911.12670433731\n",
      "Step: 1961, Training Loss: 1.46911, Tokens/sec: 85877.86834486156\n",
      "Step: 1962, Training Loss: 1.53543, Tokens/sec: 85428.75681086826\n",
      "Step: 1963, Training Loss: 1.43876, Tokens/sec: 86263.18668573903\n",
      "Step: 1964, Training Loss: 1.56087, Tokens/sec: 85595.56336564657\n",
      "Step: 1965, Training Loss: 1.73052, Tokens/sec: 84302.12992156499\n",
      "Step: 1966, Training Loss: 1.58925, Tokens/sec: 86620.15330990708\n",
      "Step: 1967, Training Loss: 1.44125, Tokens/sec: 86513.12224299426\n",
      "Step: 1968, Training Loss: 1.43001, Tokens/sec: 83572.48970514775\n",
      "Step: 1969, Training Loss: 1.71432, Tokens/sec: 86132.57182309455\n",
      "Step: 1970, Training Loss: 1.44423, Tokens/sec: 85322.25299284993\n",
      "Step: 1971, Training Loss: 1.49367, Tokens/sec: 86102.81535934476\n",
      "Step: 1972, Training Loss: 1.52215, Tokens/sec: 86192.91054156094\n",
      "Step: 1973, Training Loss: 1.74816, Tokens/sec: 84277.43105765531\n",
      "Step: 1974, Training Loss: 1.51317, Tokens/sec: 87041.91489542475\n",
      "Step: 1975, Training Loss: 1.60982, Tokens/sec: 86791.99945586291\n",
      "Step: 1976, Training Loss: 1.47951, Tokens/sec: 84047.80379012947\n",
      "Step: 1977, Training Loss: 1.62726, Tokens/sec: 86659.50953494315\n",
      "Step: 1978, Training Loss: 1.80345, Tokens/sec: 84074.5952052264\n",
      "Step: 1979, Training Loss: 1.46874, Tokens/sec: 87239.21669308146\n",
      "Step: 1980, Training Loss: 1.46188, Tokens/sec: 85848.62000072685\n",
      "Step: 1981, Training Loss: 1.49889, Tokens/sec: 83873.29582984406\n",
      "Step: 1982, Training Loss: 1.45129, Tokens/sec: 85392.19835611674\n",
      "Step: 1983, Training Loss: 1.57016, Tokens/sec: 86147.66704018031\n",
      "Step: 1984, Training Loss: 1.52420, Tokens/sec: 84396.92440140633\n",
      "Step: 1985, Training Loss: 1.55003, Tokens/sec: 85369.93836331864\n",
      "Step: 1986, Training Loss: 1.49548, Tokens/sec: 83410.78851295332\n",
      "Step: 1987, Training Loss: 1.70305, Tokens/sec: 86456.96223550013\n",
      "Step: 1988, Training Loss: 1.56453, Tokens/sec: 85837.27323731853\n",
      "Step: 1989, Training Loss: 1.54911, Tokens/sec: 83155.18461783661\n",
      "Step: 1990, Training Loss: 1.52680, Tokens/sec: 86231.8930127495\n",
      "Step: 1991, Training Loss: 1.64127, Tokens/sec: 85599.87529636886\n",
      "Step: 1992, Training Loss: 1.49496, Tokens/sec: 84560.9439826043\n",
      "Step: 1993, Training Loss: 1.66601, Tokens/sec: 85010.40537056701\n",
      "Step: 1994, Training Loss: 1.50306, Tokens/sec: 83660.61600615627\n",
      "Step: 1995, Training Loss: 1.62271, Tokens/sec: 86666.6982265634\n",
      "Step: 1996, Training Loss: 1.44063, Tokens/sec: 84921.42197679648\n",
      "Step: 1997, Training Loss: 1.45986, Tokens/sec: 84434.73751790804\n",
      "Step: 1998, Training Loss: 1.50744, Tokens/sec: 86550.59853071441\n",
      "Step: 1999, Training Loss: 1.58144, Tokens/sec: 83308.13555596535\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T06:27:54.880836Z",
     "start_time": "2024-12-16T06:27:52.251011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_ids = tokenizer([\"  \"], return_tensors=\"pt\")['input_ids'].to(trainer.device)\n",
    "idx = model.generate(input_ids, temperature=0.25, top_k=50, max_new_tokens=256)\n",
    "print(tokenizer.batch_decode(idx)[0])"
   ],
   "id": "8b5596eda083de0a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          \n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "bc3bcc343073d67a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
