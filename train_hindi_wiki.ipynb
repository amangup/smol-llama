{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-16T06:13:14.675807Z",
     "start_time": "2024-12-16T06:13:13.745871Z"
    }
   },
   "source": [
    "from model import ModelConfig, LlamaModel\n",
    "from train import TrainerConfig, FileDataLoader, Trainer\n",
    "\n",
    "from transformers import AutoTokenizer"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T06:13:14.680027Z",
     "start_time": "2024-12-16T06:13:14.678406Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer_id = \"HuggingFaceTB/SmolLM2-135M\"",
   "id": "2f28fa23c987e72b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T06:13:14.942699Z",
     "start_time": "2024-12-16T06:13:14.724783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ],
   "id": "9bb4e51aa142abee",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T06:13:14.949885Z",
     "start_time": "2024-12-16T06:13:14.948274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_config = ModelConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=576,\n",
    "    d_head=64,\n",
    "    d_mlp_proj=1536,\n",
    "    n_layers=30,\n",
    "    n_kv_heads=3,\n",
    "    n_attn_heads=9,\n",
    "    rms_norm_eps=1e-5,\n",
    "    initializer_range=0.041666666666666664,\n",
    "    rope_theta=100000.0,\n",
    "    padding_idx=tokenizer.pad_token_id\n",
    ")"
   ],
   "id": "cde027092af8291e",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T06:13:14.994245Z",
     "start_time": "2024-12-16T06:13:14.992618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_config = TrainerConfig(\n",
    "    per_device_train_batch_size=8,\n",
    "    max_seq_len=2048,\n",
    "    num_epochs=1,\n",
    "    learning_rate=1e-3,\n",
    "    tokens_folder=\"wiki_hindi_tok\",\n",
    "    max_steps=2000\n",
    ")"
   ],
   "id": "809773e662327a12",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T06:13:19.812846Z",
     "start_time": "2024-12-16T06:13:17.865049Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = LlamaModel(model_config)\n",
    "dataloader = FileDataLoader(train_config, tokenizer)\n",
    "trainer = Trainer(train_config, model)"
   ],
   "id": "9a912a0ec92039d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train tokens             | 270,491,648\n",
      "Num Trainable Params           | 162,826,560\n",
      "Train device                   | cuda, NVIDIA GeForce RTX 3090, N=1\n",
      "Training precision             | torch.bfloat16\n",
      "Flash Attention                | True\n",
      "torch.compile()                | True\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T06:26:13.838827Z",
     "start_time": "2024-12-16T06:13:21.696455Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train(dataloader)",
   "id": "bc3b550223d43b9c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training steps                 | 2,000 \n",
      "Step: 0, Training Loss: 11.44452, Tokens/sec: 1523.6792020482028\n",
      "Step: 1, Training Loss: 9.92958, Tokens/sec: 1658.0446699343945\n",
      "Step: 2, Training Loss: 7.48547, Tokens/sec: 88535.48187497325\n",
      "Step: 3, Training Loss: 7.08707, Tokens/sec: 85621.38507734686\n",
      "Step: 4, Training Loss: 6.42159, Tokens/sec: 86666.16139748672\n",
      "Step: 5, Training Loss: 5.76335, Tokens/sec: 85075.0035268395\n",
      "Step: 6, Training Loss: 4.90985, Tokens/sec: 87301.0103515507\n",
      "Step: 7, Training Loss: 4.56522, Tokens/sec: 85954.65296049694\n",
      "Step: 8, Training Loss: 4.28419, Tokens/sec: 84144.34179420414\n",
      "Step: 9, Training Loss: 4.03407, Tokens/sec: 87916.10755802804\n",
      "Step: 10, Training Loss: 3.93829, Tokens/sec: 87618.75117636021\n",
      "Step: 11, Training Loss: 3.97180, Tokens/sec: 86250.11046215301\n",
      "Step: 12, Training Loss: 3.90830, Tokens/sec: 87801.91245281768\n",
      "Step: 13, Training Loss: 3.79044, Tokens/sec: 86141.00576033592\n",
      "Step: 14, Training Loss: 3.74311, Tokens/sec: 86699.7608479747\n",
      "Step: 15, Training Loss: 3.80275, Tokens/sec: 86302.27504602779\n",
      "Step: 16, Training Loss: 3.71701, Tokens/sec: 85810.60027142907\n",
      "Step: 17, Training Loss: 3.73183, Tokens/sec: 88221.0371650029\n",
      "Step: 18, Training Loss: 3.73413, Tokens/sec: 84956.36160288402\n",
      "Step: 19, Training Loss: 3.76456, Tokens/sec: 87860.08598725568\n",
      "Step: 20, Training Loss: 3.97386, Tokens/sec: 88377.17631056669\n",
      "Step: 21, Training Loss: 3.88590, Tokens/sec: 86049.32963501308\n",
      "Step: 22, Training Loss: 4.00396, Tokens/sec: 86190.34140449969\n",
      "Step: 23, Training Loss: 3.74094, Tokens/sec: 87624.3724998764\n",
      "Step: 24, Training Loss: 4.35054, Tokens/sec: 86211.53521921011\n",
      "Step: 25, Training Loss: 3.83411, Tokens/sec: 88217.7462513933\n",
      "Step: 26, Training Loss: 3.76492, Tokens/sec: 84706.26213734716\n",
      "Step: 27, Training Loss: 3.67480, Tokens/sec: 87950.44306278265\n",
      "Step: 28, Training Loss: 3.62959, Tokens/sec: 87648.30727367238\n",
      "Step: 29, Training Loss: 3.73641, Tokens/sec: 85415.44694092493\n",
      "Step: 30, Training Loss: 3.69601, Tokens/sec: 86845.00176170288\n",
      "Step: 31, Training Loss: 3.84770, Tokens/sec: 85874.60859545815\n",
      "Step: 32, Training Loss: 3.71600, Tokens/sec: 88112.67037686796\n",
      "Step: 33, Training Loss: 3.83148, Tokens/sec: 87036.96866770589\n",
      "Step: 34, Training Loss: 3.63362, Tokens/sec: 85081.34410784511\n",
      "Step: 35, Training Loss: 3.49506, Tokens/sec: 87581.07095658928\n",
      "Step: 36, Training Loss: 3.50662, Tokens/sec: 87593.3901567527\n",
      "Step: 37, Training Loss: 3.52145, Tokens/sec: 87220.42397768745\n",
      "Step: 38, Training Loss: 3.32258, Tokens/sec: 87257.8776319727\n",
      "Step: 39, Training Loss: 3.38114, Tokens/sec: 85221.58386890443\n",
      "Step: 40, Training Loss: 3.15178, Tokens/sec: 88050.69187647884\n",
      "Step: 41, Training Loss: 3.23690, Tokens/sec: 86653.47464746852\n",
      "Step: 42, Training Loss: 3.05739, Tokens/sec: 84310.02783648214\n",
      "Step: 43, Training Loss: 2.99847, Tokens/sec: 87904.06104689596\n",
      "Step: 44, Training Loss: 3.02560, Tokens/sec: 84583.39754743967\n",
      "Step: 45, Training Loss: 3.05549, Tokens/sec: 85936.8724623973\n",
      "Step: 46, Training Loss: 3.01678, Tokens/sec: 87881.98033622153\n",
      "Step: 47, Training Loss: 3.20853, Tokens/sec: 85618.87271815074\n",
      "Step: 48, Training Loss: 3.06911, Tokens/sec: 87079.05707928354\n",
      "Step: 49, Training Loss: 3.28046, Tokens/sec: 83864.2994379226\n",
      "Step: 50, Training Loss: 3.21355, Tokens/sec: 88286.24141201105\n",
      "Step: 51, Training Loss: 3.03575, Tokens/sec: 88347.47254966796\n",
      "Step: 52, Training Loss: 2.93351, Tokens/sec: 84538.51149209065\n",
      "Step: 53, Training Loss: 3.11480, Tokens/sec: 87884.0182008524\n",
      "Step: 54, Training Loss: 2.99967, Tokens/sec: 88211.86141829148\n",
      "Step: 55, Training Loss: 2.91035, Tokens/sec: 86113.21814463651\n",
      "Step: 56, Training Loss: 2.93091, Tokens/sec: 86271.18468781463\n",
      "Step: 57, Training Loss: 2.91930, Tokens/sec: 85092.50868161183\n",
      "Step: 58, Training Loss: 2.89471, Tokens/sec: 87684.85856539018\n",
      "Step: 59, Training Loss: 2.85819, Tokens/sec: 86649.05500653418\n",
      "Step: 60, Training Loss: 2.87321, Tokens/sec: 84466.23508029665\n",
      "Step: 61, Training Loss: 2.84676, Tokens/sec: 88030.8394214718\n",
      "Step: 62, Training Loss: 2.87203, Tokens/sec: 85276.89956861027\n",
      "Step: 63, Training Loss: 2.79279, Tokens/sec: 86844.69380057497\n",
      "Step: 64, Training Loss: 3.14661, Tokens/sec: 88537.40566760133\n",
      "Step: 65, Training Loss: 2.78401, Tokens/sec: 84952.72124772896\n",
      "Step: 66, Training Loss: 2.89083, Tokens/sec: 87434.85512304035\n",
      "Step: 67, Training Loss: 2.92061, Tokens/sec: 84786.13992677972\n",
      "Step: 68, Training Loss: 2.90609, Tokens/sec: 87461.09598605879\n",
      "Step: 69, Training Loss: 2.80746, Tokens/sec: 87216.88136840273\n",
      "Step: 70, Training Loss: 2.80370, Tokens/sec: 84913.35629405816\n",
      "Step: 71, Training Loss: 2.72793, Tokens/sec: 86096.2777389118\n",
      "Step: 72, Training Loss: 2.87998, Tokens/sec: 86178.26771010245\n",
      "Step: 73, Training Loss: 2.80483, Tokens/sec: 87099.13260752177\n",
      "Step: 74, Training Loss: 2.95122, Tokens/sec: 86078.18083390787\n",
      "Step: 75, Training Loss: 3.07131, Tokens/sec: 84026.60668656658\n",
      "Step: 76, Training Loss: 2.93361, Tokens/sec: 87041.75119687618\n",
      "Step: 77, Training Loss: 2.89090, Tokens/sec: 87829.87605959334\n",
      "Step: 78, Training Loss: 2.89766, Tokens/sec: 83796.2465618358\n",
      "Step: 79, Training Loss: 3.00479, Tokens/sec: 85750.70703452571\n",
      "Step: 80, Training Loss: 2.89339, Tokens/sec: 86626.52294208751\n",
      "Step: 81, Training Loss: 2.80007, Tokens/sec: 71343.93728184485\n",
      "Step: 82, Training Loss: 2.95242, Tokens/sec: 86918.0609961867\n",
      "Step: 83, Training Loss: 2.93406, Tokens/sec: 85562.68768302053\n",
      "Step: 84, Training Loss: 2.81517, Tokens/sec: 83385.36605855482\n",
      "Step: 85, Training Loss: 2.83474, Tokens/sec: 79589.4494064655\n",
      "Step: 86, Training Loss: 2.78719, Tokens/sec: 87077.94263484675\n",
      "Step: 87, Training Loss: 2.90744, Tokens/sec: 77813.72572840213\n",
      "Step: 88, Training Loss: 2.82939, Tokens/sec: 69187.04783892343\n",
      "Step: 89, Training Loss: 2.71649, Tokens/sec: 59546.42192252622\n",
      "Step: 90, Training Loss: 2.81722, Tokens/sec: 86199.01749286965\n",
      "Step: 91, Training Loss: 2.88450, Tokens/sec: 84947.24192512325\n",
      "Step: 92, Training Loss: 2.81921, Tokens/sec: 87266.70212069334\n",
      "Step: 93, Training Loss: 3.08517, Tokens/sec: 87981.30388765632\n",
      "Step: 94, Training Loss: 3.12204, Tokens/sec: 85714.78465955633\n",
      "Step: 95, Training Loss: 2.76345, Tokens/sec: 87424.8807172186\n",
      "Step: 96, Training Loss: 2.81772, Tokens/sec: 85691.38810430448\n",
      "Step: 97, Training Loss: 2.89511, Tokens/sec: 87717.57092094552\n",
      "Step: 98, Training Loss: 2.77998, Tokens/sec: 87523.66080922677\n",
      "Step: 99, Training Loss: 2.82469, Tokens/sec: 85409.6290426798\n",
      "Step: 100, Training Loss: 2.70475, Tokens/sec: 87097.17867238987\n",
      "Step: 101, Training Loss: 2.72933, Tokens/sec: 84350.22139924328\n",
      "Step: 102, Training Loss: 2.64923, Tokens/sec: 84580.1973336318\n",
      "Step: 103, Training Loss: 2.79784, Tokens/sec: 85110.13068831536\n",
      "Step: 104, Training Loss: 2.84407, Tokens/sec: 85405.6274181441\n",
      "Step: 105, Training Loss: 2.94419, Tokens/sec: 83078.57897006285\n",
      "Step: 106, Training Loss: 2.68977, Tokens/sec: 86765.10074806676\n",
      "Step: 107, Training Loss: 2.72636, Tokens/sec: 86071.18529272196\n",
      "Step: 108, Training Loss: 2.71738, Tokens/sec: 86485.2817009388\n",
      "Step: 109, Training Loss: 2.68850, Tokens/sec: 85595.91753496889\n",
      "Step: 110, Training Loss: 2.61263, Tokens/sec: 88041.135672604\n",
      "Step: 111, Training Loss: 2.63152, Tokens/sec: 86057.77259543297\n",
      "Step: 112, Training Loss: 2.64725, Tokens/sec: 55783.09839827186\n",
      "Step: 113, Training Loss: 2.75673, Tokens/sec: 69417.54212411553\n",
      "Step: 114, Training Loss: 2.69428, Tokens/sec: 87159.84691539436\n",
      "Step: 115, Training Loss: 2.72626, Tokens/sec: 85381.68471344262\n",
      "Step: 116, Training Loss: 2.68193, Tokens/sec: 86845.98181454008\n",
      "Step: 117, Training Loss: 2.62199, Tokens/sec: 85402.7577677378\n",
      "Step: 118, Training Loss: 2.64527, Tokens/sec: 87204.23942584729\n",
      "Step: 119, Training Loss: 2.82421, Tokens/sec: 87173.31502347093\n",
      "Step: 120, Training Loss: 2.60572, Tokens/sec: 85032.10444560216\n",
      "Step: 121, Training Loss: 2.63737, Tokens/sec: 88024.64892040181\n",
      "Step: 122, Training Loss: 2.88492, Tokens/sec: 86944.65784629298\n",
      "Step: 123, Training Loss: 2.64417, Tokens/sec: 86529.53474773881\n",
      "Step: 124, Training Loss: 2.80908, Tokens/sec: 87763.55649379863\n",
      "Step: 125, Training Loss: 2.59868, Tokens/sec: 85126.81435468796\n",
      "Step: 126, Training Loss: 2.61635, Tokens/sec: 86574.41637163401\n",
      "Step: 127, Training Loss: 2.64727, Tokens/sec: 86024.51928757518\n",
      "Step: 128, Training Loss: 2.71342, Tokens/sec: 86370.02054360733\n",
      "Step: 129, Training Loss: 2.61173, Tokens/sec: 87860.99815240254\n",
      "Step: 130, Training Loss: 2.75737, Tokens/sec: 79384.47495444606\n",
      "Step: 131, Training Loss: 2.72058, Tokens/sec: 85399.87985688625\n",
      "Step: 132, Training Loss: 2.57049, Tokens/sec: 87605.4654917974\n",
      "Step: 133, Training Loss: 2.62107, Tokens/sec: 82303.42532041721\n",
      "Step: 134, Training Loss: 2.78837, Tokens/sec: 64836.223676092675\n",
      "Step: 135, Training Loss: 2.71264, Tokens/sec: 85950.9292581916\n",
      "Step: 136, Training Loss: 2.62514, Tokens/sec: 85006.40401525311\n",
      "Step: 137, Training Loss: 2.63572, Tokens/sec: 87683.17154715037\n",
      "Step: 138, Training Loss: 2.72092, Tokens/sec: 86746.6655813276\n",
      "Step: 139, Training Loss: 2.62498, Tokens/sec: 84459.20214725721\n",
      "Step: 140, Training Loss: 2.60034, Tokens/sec: 87767.26354403119\n",
      "Step: 141, Training Loss: 2.57966, Tokens/sec: 87655.56997772554\n",
      "Step: 142, Training Loss: 2.63234, Tokens/sec: 84964.52181433083\n",
      "Step: 143, Training Loss: 2.54838, Tokens/sec: 68994.36373130068\n",
      "Step: 144, Training Loss: 2.54872, Tokens/sec: 86973.14263085806\n",
      "Step: 145, Training Loss: 2.56347, Tokens/sec: 74561.48017444079\n",
      "Step: 146, Training Loss: 2.56158, Tokens/sec: 86650.04484933842\n",
      "Step: 147, Training Loss: 2.57056, Tokens/sec: 87119.71677163274\n",
      "Step: 148, Training Loss: 2.67464, Tokens/sec: 84821.01748054933\n",
      "Step: 149, Training Loss: 2.70570, Tokens/sec: 87084.98336557945\n",
      "Step: 150, Training Loss: 2.58924, Tokens/sec: 82909.65272804312\n",
      "Step: 151, Training Loss: 2.66571, Tokens/sec: 75449.6528071742\n",
      "Step: 152, Training Loss: 2.55342, Tokens/sec: 87642.47565848466\n",
      "Step: 153, Training Loss: 2.54347, Tokens/sec: 84481.07417652414\n",
      "Step: 154, Training Loss: 2.68054, Tokens/sec: 87505.34965037882\n",
      "Step: 155, Training Loss: 2.51922, Tokens/sec: 87786.46717956956\n",
      "Step: 156, Training Loss: 2.49182, Tokens/sec: 84645.76766790033\n",
      "Step: 157, Training Loss: 2.58119, Tokens/sec: 86181.84340385444\n",
      "Step: 158, Training Loss: 2.58885, Tokens/sec: 85118.60215268112\n",
      "Step: 159, Training Loss: 2.59573, Tokens/sec: 88219.73891595405\n",
      "Step: 160, Training Loss: 2.59666, Tokens/sec: 86294.33809322359\n",
      "Step: 161, Training Loss: 2.48816, Tokens/sec: 84477.77803788656\n",
      "Step: 162, Training Loss: 2.57643, Tokens/sec: 87406.26719370576\n",
      "Step: 163, Training Loss: 2.49446, Tokens/sec: 87548.43504908281\n",
      "Step: 164, Training Loss: 2.64379, Tokens/sec: 85142.51520855584\n",
      "Step: 165, Training Loss: 2.76980, Tokens/sec: 60441.14666469286\n",
      "Step: 166, Training Loss: 2.49357, Tokens/sec: 87845.2607881921\n",
      "Step: 167, Training Loss: 2.70148, Tokens/sec: 85890.51584850867\n",
      "Step: 168, Training Loss: 2.51006, Tokens/sec: 86892.02050012453\n",
      "Step: 169, Training Loss: 2.57230, Tokens/sec: 81324.95466152266\n",
      "Step: 170, Training Loss: 2.51232, Tokens/sec: 87441.75862066068\n",
      "Step: 171, Training Loss: 2.57897, Tokens/sec: 87691.07551582136\n",
      "Step: 172, Training Loss: 2.47276, Tokens/sec: 84069.36142892309\n",
      "Step: 173, Training Loss: 2.42411, Tokens/sec: 86727.50967036454\n",
      "Step: 174, Training Loss: 2.48209, Tokens/sec: 84196.75778936257\n",
      "Step: 175, Training Loss: 2.44887, Tokens/sec: 86841.35517484287\n",
      "Step: 176, Training Loss: 2.42699, Tokens/sec: 87159.2306945444\n",
      "Step: 177, Training Loss: 2.52124, Tokens/sec: 84536.11375180467\n",
      "Step: 178, Training Loss: 2.52586, Tokens/sec: 87406.33061154716\n",
      "Step: 179, Training Loss: 2.51716, Tokens/sec: 86701.35977183055\n",
      "Step: 180, Training Loss: 2.53206, Tokens/sec: 83949.82663362459\n",
      "Step: 181, Training Loss: 2.49461, Tokens/sec: 87631.58155658905\n",
      "Step: 182, Training Loss: 2.52511, Tokens/sec: 83894.79582423635\n",
      "Step: 183, Training Loss: 2.49502, Tokens/sec: 87590.04569685936\n",
      "Step: 184, Training Loss: 2.48947, Tokens/sec: 87475.57555166089\n",
      "Step: 185, Training Loss: 2.42690, Tokens/sec: 85878.54760564855\n",
      "Step: 186, Training Loss: 2.43865, Tokens/sec: 87028.86365374106\n",
      "Step: 187, Training Loss: 2.45351, Tokens/sec: 87227.98745897003\n",
      "Step: 188, Training Loss: 2.38665, Tokens/sec: 85811.84880771594\n",
      "Step: 189, Training Loss: 2.31995, Tokens/sec: 85986.59995832187\n",
      "Step: 190, Training Loss: 2.60261, Tokens/sec: 83841.41594971014\n",
      "Step: 191, Training Loss: 2.47525, Tokens/sec: 85842.30532839327\n",
      "Step: 192, Training Loss: 2.52689, Tokens/sec: 86341.07806589287\n",
      "Step: 193, Training Loss: 2.50271, Tokens/sec: 85180.07441672622\n",
      "Step: 194, Training Loss: 2.59801, Tokens/sec: 87317.22991229797\n",
      "Step: 195, Training Loss: 2.52074, Tokens/sec: 84697.06165667766\n",
      "Step: 196, Training Loss: 2.61499, Tokens/sec: 87232.66096450188\n",
      "Step: 197, Training Loss: 2.39379, Tokens/sec: 88113.449422924\n",
      "Step: 198, Training Loss: 2.45212, Tokens/sec: 85674.64953607373\n",
      "Step: 199, Training Loss: 2.46487, Tokens/sec: 83917.53696369588\n",
      "Step: 200, Training Loss: 2.57472, Tokens/sec: 85878.89196624217\n",
      "Step: 201, Training Loss: 2.38713, Tokens/sec: 84060.62223881076\n",
      "Step: 202, Training Loss: 2.48023, Tokens/sec: 81631.72125068208\n",
      "Step: 203, Training Loss: 2.39423, Tokens/sec: 57963.223962016105\n",
      "Step: 204, Training Loss: 2.49011, Tokens/sec: 64274.01216440761\n",
      "Step: 205, Training Loss: 2.50638, Tokens/sec: 85841.40761322944\n",
      "Step: 206, Training Loss: 2.53455, Tokens/sec: 87565.3742695675\n",
      "Step: 207, Training Loss: 2.33736, Tokens/sec: 86948.6379594145\n",
      "Step: 208, Training Loss: 2.30727, Tokens/sec: 83831.46169457448\n",
      "Step: 209, Training Loss: 2.36302, Tokens/sec: 85684.15370980311\n",
      "Step: 210, Training Loss: 2.34286, Tokens/sec: 87191.68096622381\n",
      "Step: 211, Training Loss: 2.40758, Tokens/sec: 85712.83315171814\n",
      "Step: 212, Training Loss: 2.49733, Tokens/sec: 86324.58096256944\n",
      "Step: 213, Training Loss: 2.43928, Tokens/sec: 84479.04471353529\n",
      "Step: 214, Training Loss: 2.44452, Tokens/sec: 87561.7104542236\n",
      "Step: 215, Training Loss: 2.38875, Tokens/sec: 87277.26857183287\n",
      "Step: 216, Training Loss: 2.40203, Tokens/sec: 85033.80397146584\n",
      "Step: 217, Training Loss: 2.43778, Tokens/sec: 87407.15643614116\n",
      "Step: 218, Training Loss: 2.32144, Tokens/sec: 69854.09784028814\n",
      "Step: 219, Training Loss: 2.41212, Tokens/sec: 71365.46207810896\n",
      "Step: 220, Training Loss: 2.39782, Tokens/sec: 68022.39795091818\n",
      "Step: 221, Training Loss: 2.40990, Tokens/sec: 86444.48169403861\n",
      "Step: 222, Training Loss: 2.32014, Tokens/sec: 86688.63519600539\n",
      "Step: 223, Training Loss: 2.37330, Tokens/sec: 85666.29860481888\n",
      "Step: 224, Training Loss: 2.35257, Tokens/sec: 86724.67031771556\n",
      "Step: 225, Training Loss: 2.46522, Tokens/sec: 80934.37631140726\n",
      "Step: 226, Training Loss: 2.51076, Tokens/sec: 86797.97456139638\n",
      "Step: 227, Training Loss: 2.45924, Tokens/sec: 85737.6798230073\n",
      "Step: 228, Training Loss: 2.41255, Tokens/sec: 85198.64714106757\n",
      "Step: 229, Training Loss: 2.27499, Tokens/sec: 87817.29680029154\n",
      "Step: 230, Training Loss: 2.50576, Tokens/sec: 85326.34546352156\n",
      "Step: 231, Training Loss: 2.44060, Tokens/sec: 87085.56428181649\n",
      "Step: 232, Training Loss: 2.45953, Tokens/sec: 87058.59574275839\n",
      "Step: 233, Training Loss: 2.30761, Tokens/sec: 85490.15166145725\n",
      "Step: 234, Training Loss: 2.27385, Tokens/sec: 87042.02911534646\n",
      "Step: 235, Training Loss: 2.63144, Tokens/sec: 86988.29654819409\n",
      "Step: 236, Training Loss: 2.35353, Tokens/sec: 87237.38095474399\n",
      "Step: 237, Training Loss: 2.36348, Tokens/sec: 87319.89784847084\n",
      "Step: 238, Training Loss: 2.35161, Tokens/sec: 83893.35974419094\n",
      "Step: 239, Training Loss: 2.32676, Tokens/sec: 86975.3033923175\n",
      "Step: 240, Training Loss: 2.45364, Tokens/sec: 87332.65574322571\n",
      "Step: 241, Training Loss: 3.16714, Tokens/sec: 85064.29623120806\n",
      "Step: 242, Training Loss: 2.97093, Tokens/sec: 86011.36314742734\n",
      "Step: 243, Training Loss: 2.48896, Tokens/sec: 85763.72563607978\n",
      "Step: 244, Training Loss: 2.50951, Tokens/sec: 87454.82850435989\n",
      "Step: 245, Training Loss: 2.51750, Tokens/sec: 86305.4868777708\n",
      "Step: 246, Training Loss: 2.52678, Tokens/sec: 85605.33805305505\n",
      "Step: 247, Training Loss: 2.41754, Tokens/sec: 87498.77909545934\n",
      "Step: 248, Training Loss: 2.37452, Tokens/sec: 85117.28836605714\n",
      "Step: 249, Training Loss: 2.37978, Tokens/sec: 86121.27119836787\n",
      "Step: 250, Training Loss: 2.32753, Tokens/sec: 87498.50807163786\n",
      "Step: 251, Training Loss: 2.35185, Tokens/sec: 84887.27633811359\n",
      "Step: 252, Training Loss: 2.45313, Tokens/sec: 85918.80014489178\n",
      "Step: 253, Training Loss: 2.46298, Tokens/sec: 85605.42437955193\n",
      "Step: 254, Training Loss: 2.53022, Tokens/sec: 87166.7070491915\n",
      "Step: 255, Training Loss: 2.30653, Tokens/sec: 86122.56228672243\n",
      "Step: 256, Training Loss: 2.35326, Tokens/sec: 84419.62978098987\n",
      "Step: 257, Training Loss: 2.35069, Tokens/sec: 86661.54151668273\n",
      "Step: 258, Training Loss: 2.34740, Tokens/sec: 54495.10039956312\n",
      "Step: 259, Training Loss: 2.45502, Tokens/sec: 60546.96966109472\n",
      "Step: 260, Training Loss: 2.32758, Tokens/sec: 85096.43195659478\n",
      "Step: 261, Training Loss: 2.27264, Tokens/sec: 86603.59517370738\n",
      "Step: 262, Training Loss: 2.34991, Tokens/sec: 86611.08959715106\n",
      "Step: 263, Training Loss: 2.32031, Tokens/sec: 85164.2193715294\n",
      "Step: 264, Training Loss: 2.29644, Tokens/sec: 86055.51163602986\n",
      "Step: 265, Training Loss: 2.36161, Tokens/sec: 86362.91102794286\n",
      "Step: 266, Training Loss: 2.30833, Tokens/sec: 86347.41764220473\n",
      "Step: 267, Training Loss: 2.23503, Tokens/sec: 86056.45722730407\n",
      "Step: 268, Training Loss: 2.26196, Tokens/sec: 84061.91654679047\n",
      "Step: 269, Training Loss: 2.30465, Tokens/sec: 81528.45267513653\n",
      "Step: 270, Training Loss: 2.37435, Tokens/sec: 86822.65708563512\n",
      "Step: 271, Training Loss: 2.17964, Tokens/sec: 83904.49570758107\n",
      "Step: 272, Training Loss: 2.31867, Tokens/sec: 87451.40172850313\n",
      "Step: 273, Training Loss: 2.31221, Tokens/sec: 82681.33677459924\n",
      "Step: 274, Training Loss: 2.38214, Tokens/sec: 55257.71193109568\n",
      "Step: 275, Training Loss: 2.17683, Tokens/sec: 65664.76150534976\n",
      "Step: 276, Training Loss: 2.06738, Tokens/sec: 86025.43529373643\n",
      "Step: 277, Training Loss: 2.41155, Tokens/sec: 67629.59587412595\n",
      "Step: 278, Training Loss: 2.34042, Tokens/sec: 86925.12662719363\n",
      "Step: 279, Training Loss: 2.97717, Tokens/sec: 83392.11644534743\n",
      "Step: 280, Training Loss: 2.32293, Tokens/sec: 87158.08127804851\n",
      "Step: 281, Training Loss: 2.33656, Tokens/sec: 86920.80142906807\n",
      "Step: 282, Training Loss: 2.39631, Tokens/sec: 83651.03947639909\n",
      "Step: 283, Training Loss: 2.35055, Tokens/sec: 85596.25605358738\n",
      "Step: 284, Training Loss: 2.43713, Tokens/sec: 86165.25936057353\n",
      "Step: 285, Training Loss: 2.26206, Tokens/sec: 87339.87367774658\n",
      "Step: 286, Training Loss: 2.26795, Tokens/sec: 53519.018368478435\n",
      "Step: 287, Training Loss: 2.28445, Tokens/sec: 86797.12801921889\n",
      "Step: 288, Training Loss: 2.26138, Tokens/sec: 86494.133251712\n",
      "Step: 289, Training Loss: 2.22443, Tokens/sec: 85457.79549158855\n",
      "Step: 290, Training Loss: 2.27305, Tokens/sec: 85736.96644921204\n",
      "Step: 291, Training Loss: 2.21317, Tokens/sec: 86751.04190580535\n",
      "Step: 292, Training Loss: 2.48089, Tokens/sec: 87009.30983209414\n",
      "Step: 293, Training Loss: 2.23381, Tokens/sec: 86301.32949909309\n",
      "Step: 294, Training Loss: 2.24266, Tokens/sec: 86931.05503836842\n",
      "Step: 295, Training Loss: 2.34671, Tokens/sec: 84784.59287661861\n",
      "Step: 296, Training Loss: 2.55139, Tokens/sec: 85855.25188045406\n",
      "Step: 297, Training Loss: 2.27715, Tokens/sec: 81232.5311195113\n",
      "Step: 298, Training Loss: 2.20360, Tokens/sec: 80925.34336863307\n",
      "Step: 299, Training Loss: 2.22108, Tokens/sec: 81415.6641547838\n",
      "Step: 300, Training Loss: 2.26634, Tokens/sec: 84654.34245859839\n",
      "Step: 301, Training Loss: 2.23083, Tokens/sec: 86086.98317334593\n",
      "Step: 302, Training Loss: 2.19044, Tokens/sec: 82071.86372691536\n",
      "Step: 303, Training Loss: 2.23778, Tokens/sec: 86504.80299990338\n",
      "Step: 304, Training Loss: 2.34095, Tokens/sec: 86775.86365417065\n",
      "Step: 305, Training Loss: 2.38512, Tokens/sec: 86031.7177575634\n",
      "Step: 306, Training Loss: 2.30322, Tokens/sec: 86691.09651067934\n",
      "Step: 307, Training Loss: 2.24257, Tokens/sec: 87436.68705434003\n",
      "Step: 308, Training Loss: 2.23634, Tokens/sec: 86436.04340883576\n",
      "Step: 309, Training Loss: 2.22716, Tokens/sec: 86653.75696461333\n",
      "Step: 310, Training Loss: 2.26792, Tokens/sec: 86990.0760948175\n",
      "Step: 311, Training Loss: 2.32511, Tokens/sec: 87456.68181252058\n",
      "Step: 312, Training Loss: 2.16378, Tokens/sec: 87054.6518160303\n",
      "Step: 313, Training Loss: 2.25387, Tokens/sec: 85972.89645705596\n",
      "Step: 314, Training Loss: 2.22832, Tokens/sec: 87629.76254137549\n",
      "Step: 315, Training Loss: 2.14317, Tokens/sec: 87689.76278413448\n",
      "Step: 316, Training Loss: 2.09264, Tokens/sec: 86351.76560633144\n",
      "Step: 317, Training Loss: 2.23768, Tokens/sec: 87545.93463111875\n",
      "Step: 318, Training Loss: 2.25208, Tokens/sec: 87028.37825698422\n",
      "Step: 319, Training Loss: 2.19250, Tokens/sec: 87124.03537801774\n",
      "Step: 320, Training Loss: 2.22358, Tokens/sec: 86532.65611695846\n",
      "Step: 321, Training Loss: 2.25962, Tokens/sec: 86396.99645587827\n",
      "Step: 322, Training Loss: 2.23569, Tokens/sec: 87256.90126813519\n",
      "Step: 323, Training Loss: 2.31472, Tokens/sec: 85905.69118770886\n",
      "Step: 324, Training Loss: 2.49924, Tokens/sec: 85970.78656664079\n",
      "Step: 325, Training Loss: 2.46865, Tokens/sec: 87121.4692731982\n",
      "Step: 326, Training Loss: 2.26079, Tokens/sec: 86679.12883679065\n",
      "Step: 327, Training Loss: 2.32154, Tokens/sec: 86124.80866356991\n",
      "Step: 328, Training Loss: 2.22290, Tokens/sec: 87704.08577151452\n",
      "Step: 329, Training Loss: 2.11459, Tokens/sec: 86959.92876512866\n",
      "Step: 330, Training Loss: 2.12033, Tokens/sec: 85439.0975891673\n",
      "Step: 331, Training Loss: 2.25022, Tokens/sec: 87511.26819781726\n",
      "Step: 332, Training Loss: 2.21640, Tokens/sec: 86946.18229645364\n",
      "Step: 333, Training Loss: 2.14716, Tokens/sec: 86920.56394571479\n",
      "Step: 334, Training Loss: 2.23890, Tokens/sec: 87726.8949544298\n",
      "Step: 335, Training Loss: 2.20008, Tokens/sec: 87207.3836706796\n",
      "Step: 336, Training Loss: 2.36436, Tokens/sec: 87096.61982503704\n",
      "Step: 337, Training Loss: 2.62618, Tokens/sec: 87086.83723506003\n",
      "Step: 338, Training Loss: 2.37216, Tokens/sec: 86091.59311022809\n",
      "Step: 339, Training Loss: 2.27847, Tokens/sec: 86182.94318960058\n",
      "Step: 340, Training Loss: 2.22939, Tokens/sec: 85953.93867832863\n",
      "Step: 341, Training Loss: 2.16319, Tokens/sec: 86938.62147167989\n",
      "Step: 342, Training Loss: 2.18681, Tokens/sec: 86779.81223845364\n",
      "Step: 343, Training Loss: 2.22198, Tokens/sec: 86869.11504048873\n",
      "Step: 344, Training Loss: 2.27177, Tokens/sec: 86247.5329226496\n",
      "Step: 345, Training Loss: 2.14603, Tokens/sec: 86713.26244753724\n",
      "Step: 346, Training Loss: 2.21025, Tokens/sec: 87016.95551615293\n",
      "Step: 347, Training Loss: 2.21145, Tokens/sec: 86819.93939738275\n",
      "Step: 348, Training Loss: 2.17162, Tokens/sec: 87389.67753080264\n",
      "Step: 349, Training Loss: 2.70352, Tokens/sec: 87436.73884722209\n",
      "Step: 350, Training Loss: 2.23224, Tokens/sec: 85839.2951762919\n",
      "Step: 351, Training Loss: 2.20936, Tokens/sec: 86816.41130483603\n",
      "Step: 352, Training Loss: 2.16265, Tokens/sec: 86701.92043865017\n",
      "Step: 353, Training Loss: 2.26208, Tokens/sec: 86888.43724218379\n",
      "Step: 354, Training Loss: 2.20134, Tokens/sec: 86289.77413226113\n",
      "Step: 355, Training Loss: 2.43236, Tokens/sec: 87243.1801495893\n",
      "Step: 356, Training Loss: 2.21379, Tokens/sec: 87857.78869932244\n",
      "Step: 357, Training Loss: 2.10461, Tokens/sec: 86216.29143766085\n",
      "Step: 358, Training Loss: 2.14135, Tokens/sec: 87273.58280681026\n",
      "Step: 359, Training Loss: 2.16652, Tokens/sec: 86929.00117320997\n",
      "Step: 360, Training Loss: 2.16722, Tokens/sec: 87760.7739404172\n",
      "Step: 361, Training Loss: 2.35864, Tokens/sec: 86403.66685946923\n",
      "Step: 362, Training Loss: 2.45744, Tokens/sec: 87135.28188717634\n",
      "Step: 363, Training Loss: 2.08735, Tokens/sec: 87615.94921911081\n",
      "Step: 364, Training Loss: 2.16284, Tokens/sec: 85911.17818370512\n",
      "Step: 365, Training Loss: 2.16519, Tokens/sec: 86743.20681952825\n",
      "Step: 366, Training Loss: 2.07915, Tokens/sec: 86981.55820420511\n",
      "Step: 367, Training Loss: 2.10166, Tokens/sec: 85936.65204660613\n",
      "Step: 368, Training Loss: 2.27286, Tokens/sec: 86116.76037582787\n",
      "Step: 369, Training Loss: 2.09572, Tokens/sec: 87603.32483350136\n",
      "Step: 370, Training Loss: 2.02792, Tokens/sec: 87216.45980159006\n",
      "Step: 371, Training Loss: 1.97339, Tokens/sec: 86076.55823754038\n",
      "Step: 372, Training Loss: 2.07044, Tokens/sec: 86878.82389992033\n",
      "Step: 373, Training Loss: 1.95325, Tokens/sec: 87327.02710173774\n",
      "Step: 374, Training Loss: 1.92272, Tokens/sec: 86929.91255498199\n",
      "Step: 375, Training Loss: 2.12263, Tokens/sec: 87746.40752390285\n",
      "Step: 376, Training Loss: 2.13544, Tokens/sec: 86429.1705897563\n",
      "Step: 377, Training Loss: 2.05424, Tokens/sec: 86112.38037532174\n",
      "Step: 378, Training Loss: 2.11123, Tokens/sec: 85777.68048086583\n",
      "Step: 379, Training Loss: 2.07078, Tokens/sec: 85886.84316978922\n",
      "Step: 380, Training Loss: 2.01607, Tokens/sec: 85971.69105341945\n",
      "Step: 381, Training Loss: 2.09343, Tokens/sec: 86613.16509673302\n",
      "Step: 382, Training Loss: 2.14957, Tokens/sec: 86943.27232217182\n",
      "Step: 383, Training Loss: 2.33727, Tokens/sec: 85845.51944583062\n",
      "Step: 384, Training Loss: 2.16764, Tokens/sec: 87229.47170545663\n",
      "Step: 385, Training Loss: 2.20470, Tokens/sec: 86943.74338581746\n",
      "Step: 386, Training Loss: 2.24621, Tokens/sec: 86815.75761331232\n",
      "Step: 387, Training Loss: 2.25562, Tokens/sec: 87175.09055218232\n",
      "Step: 388, Training Loss: 2.32396, Tokens/sec: 87204.05330686753\n",
      "Step: 389, Training Loss: 2.26284, Tokens/sec: 87072.23248111237\n",
      "Step: 390, Training Loss: 2.24866, Tokens/sec: 87202.21625870824\n",
      "Step: 391, Training Loss: 2.24956, Tokens/sec: 85897.55680152708\n",
      "Step: 392, Training Loss: 2.34755, Tokens/sec: 86393.99875687044\n",
      "Step: 393, Training Loss: 2.35791, Tokens/sec: 87094.04283409621\n",
      "Step: 394, Training Loss: 2.36166, Tokens/sec: 85360.92536423082\n",
      "Step: 395, Training Loss: 2.06635, Tokens/sec: 86467.5383925556\n",
      "Step: 396, Training Loss: 2.08849, Tokens/sec: 87065.54729029907\n",
      "Step: 397, Training Loss: 2.24740, Tokens/sec: 86860.23768708766\n",
      "Step: 398, Training Loss: 1.99304, Tokens/sec: 85512.0762868797\n",
      "Step: 399, Training Loss: 1.98671, Tokens/sec: 87148.66220872609\n",
      "Step: 400, Training Loss: 2.08435, Tokens/sec: 86503.82378007926\n",
      "Step: 401, Training Loss: 2.39820, Tokens/sec: 86693.15841738513\n",
      "Step: 402, Training Loss: 2.22119, Tokens/sec: 86290.65762000375\n",
      "Step: 403, Training Loss: 2.04986, Tokens/sec: 87126.19159880345\n",
      "Step: 404, Training Loss: 1.99487, Tokens/sec: 87378.81542425424\n",
      "Step: 405, Training Loss: 1.99111, Tokens/sec: 85574.24846412703\n",
      "Step: 406, Training Loss: 2.12862, Tokens/sec: 86344.19995744828\n",
      "Step: 407, Training Loss: 2.10213, Tokens/sec: 86422.52314415772\n",
      "Step: 408, Training Loss: 2.02555, Tokens/sec: 86315.31934613224\n",
      "Step: 409, Training Loss: 2.06445, Tokens/sec: 86817.16805627788\n",
      "Step: 410, Training Loss: 2.13636, Tokens/sec: 86548.06197878806\n",
      "Step: 411, Training Loss: 2.01797, Tokens/sec: 86195.3424238539\n",
      "Step: 412, Training Loss: 2.08141, Tokens/sec: 85689.98307387931\n",
      "Step: 413, Training Loss: 1.97816, Tokens/sec: 86780.07791192993\n",
      "Step: 414, Training Loss: 2.04334, Tokens/sec: 87183.72247533925\n",
      "Step: 415, Training Loss: 2.07093, Tokens/sec: 86549.59907269751\n",
      "Step: 416, Training Loss: 2.63737, Tokens/sec: 86838.74540162891\n",
      "Step: 417, Training Loss: 2.07397, Tokens/sec: 86417.701299662\n",
      "Step: 418, Training Loss: 2.16440, Tokens/sec: 86343.20435011011\n",
      "Step: 419, Training Loss: 1.97616, Tokens/sec: 86411.6312256147\n",
      "Step: 420, Training Loss: 2.03627, Tokens/sec: 86005.4529592631\n",
      "Step: 421, Training Loss: 2.02149, Tokens/sec: 85763.4966806288\n",
      "Step: 422, Training Loss: 2.12669, Tokens/sec: 86243.46602167442\n",
      "Step: 423, Training Loss: 2.00470, Tokens/sec: 87222.55293019548\n",
      "Step: 424, Training Loss: 1.98419, Tokens/sec: 86467.38186758557\n",
      "Step: 425, Training Loss: 2.01258, Tokens/sec: 85798.07918459659\n",
      "Step: 426, Training Loss: 2.07240, Tokens/sec: 87120.55989374456\n",
      "Step: 427, Training Loss: 2.01485, Tokens/sec: 86432.73795328615\n",
      "Step: 428, Training Loss: 1.97746, Tokens/sec: 85426.39471244808\n",
      "Step: 429, Training Loss: 2.15599, Tokens/sec: 86495.76706777087\n",
      "Step: 430, Training Loss: 2.06023, Tokens/sec: 86949.92674856703\n",
      "Step: 431, Training Loss: 2.06374, Tokens/sec: 86920.20196065106\n",
      "Step: 432, Training Loss: 2.11183, Tokens/sec: 84949.43180994969\n",
      "Step: 433, Training Loss: 2.13302, Tokens/sec: 86579.46160587828\n",
      "Step: 434, Training Loss: 1.95899, Tokens/sec: 86757.69725875604\n",
      "Step: 435, Training Loss: 1.96642, Tokens/sec: 85758.42534321222\n",
      "Step: 436, Training Loss: 2.02646, Tokens/sec: 85664.5378675191\n",
      "Step: 437, Training Loss: 1.96358, Tokens/sec: 87304.3220753415\n",
      "Step: 438, Training Loss: 1.97848, Tokens/sec: 86774.49637185162\n",
      "Step: 439, Training Loss: 2.11305, Tokens/sec: 85581.06868694176\n",
      "Step: 440, Training Loss: 2.03706, Tokens/sec: 86121.96970373494\n",
      "Step: 441, Training Loss: 2.00960, Tokens/sec: 86357.44765392525\n",
      "Step: 442, Training Loss: 1.99311, Tokens/sec: 86194.2491281659\n",
      "Step: 443, Training Loss: 1.96826, Tokens/sec: 85660.89748403081\n",
      "Step: 444, Training Loss: 1.95193, Tokens/sec: 86650.14796009801\n",
      "Step: 445, Training Loss: 1.93450, Tokens/sec: 86918.63599704861\n",
      "Step: 446, Training Loss: 1.90430, Tokens/sec: 85774.09154320075\n",
      "Step: 447, Training Loss: 1.82605, Tokens/sec: 86194.58060402655\n",
      "Step: 448, Training Loss: 1.95934, Tokens/sec: 86806.68511702066\n",
      "Step: 449, Training Loss: 2.10775, Tokens/sec: 86327.64480628824\n",
      "Step: 450, Training Loss: 2.08822, Tokens/sec: 86761.93088622454\n",
      "Step: 451, Training Loss: 2.00493, Tokens/sec: 85705.0347286049\n",
      "Step: 452, Training Loss: 2.01278, Tokens/sec: 86448.27839325594\n",
      "Step: 453, Training Loss: 2.06337, Tokens/sec: 87016.71473294153\n",
      "Step: 454, Training Loss: 2.03894, Tokens/sec: 86452.16939329547\n",
      "Step: 455, Training Loss: 2.31749, Tokens/sec: 86688.58244675824\n",
      "Step: 456, Training Loss: 2.17668, Tokens/sec: 86373.70597118056\n",
      "Step: 457, Training Loss: 2.02383, Tokens/sec: 86542.86543424444\n",
      "Step: 458, Training Loss: 1.91299, Tokens/sec: 86107.18533528714\n",
      "Step: 459, Training Loss: 1.88096, Tokens/sec: 86737.76226761402\n",
      "Step: 460, Training Loss: 1.93993, Tokens/sec: 87219.95223206485\n",
      "Step: 461, Training Loss: 1.92144, Tokens/sec: 86818.62363667123\n",
      "Step: 462, Training Loss: 1.81359, Tokens/sec: 86148.33154560583\n",
      "Step: 463, Training Loss: 2.00521, Tokens/sec: 87288.37563708109\n",
      "Step: 464, Training Loss: 2.05195, Tokens/sec: 87300.15257093756\n",
      "Step: 465, Training Loss: 2.08380, Tokens/sec: 86067.59752417941\n",
      "Step: 466, Training Loss: 1.97196, Tokens/sec: 84118.18870512732\n",
      "Step: 467, Training Loss: 2.01932, Tokens/sec: 86781.30195726662\n",
      "Step: 468, Training Loss: 2.05752, Tokens/sec: 86089.52262586454\n",
      "Step: 469, Training Loss: 1.94747, Tokens/sec: 85780.82599019281\n",
      "Step: 470, Training Loss: 2.02557, Tokens/sec: 85805.7817563688\n",
      "Step: 471, Training Loss: 2.07920, Tokens/sec: 86244.40212901852\n",
      "Step: 472, Training Loss: 2.50890, Tokens/sec: 86679.11370214466\n",
      "Step: 473, Training Loss: 1.97593, Tokens/sec: 85454.26536021833\n",
      "Step: 474, Training Loss: 2.06988, Tokens/sec: 87314.78084329762\n",
      "Step: 475, Training Loss: 2.02913, Tokens/sec: 86879.69322790777\n",
      "Step: 476, Training Loss: 1.93340, Tokens/sec: 86321.27538639086\n",
      "Step: 477, Training Loss: 2.02042, Tokens/sec: 87058.49721125406\n",
      "Step: 478, Training Loss: 2.06035, Tokens/sec: 87073.03904849674\n",
      "Step: 479, Training Loss: 2.04588, Tokens/sec: 87022.9288196553\n",
      "Step: 480, Training Loss: 2.00045, Tokens/sec: 86290.42038644826\n",
      "Step: 481, Training Loss: 2.07057, Tokens/sec: 86315.71450883787\n",
      "Step: 482, Training Loss: 2.17440, Tokens/sec: 86436.71100355755\n",
      "Step: 483, Training Loss: 1.91038, Tokens/sec: 86263.09812207731\n",
      "Step: 484, Training Loss: 2.01894, Tokens/sec: 85917.26510253063\n",
      "Step: 485, Training Loss: 1.98346, Tokens/sec: 86503.35336074856\n",
      "Step: 486, Training Loss: 2.06668, Tokens/sec: 86684.97237955176\n",
      "Step: 487, Training Loss: 2.26733, Tokens/sec: 85679.90946260095\n",
      "Step: 488, Training Loss: 1.99820, Tokens/sec: 80644.26379502732\n",
      "Step: 489, Training Loss: 1.91629, Tokens/sec: 81602.34410959644\n",
      "Step: 490, Training Loss: 2.00229, Tokens/sec: 54067.32775612024\n",
      "Step: 491, Training Loss: 2.03258, Tokens/sec: 57017.162259636374\n",
      "Step: 492, Training Loss: 2.12325, Tokens/sec: 63169.377709416025\n",
      "Step: 493, Training Loss: 1.99435, Tokens/sec: 54193.02487717786\n",
      "Step: 494, Training Loss: 1.89115, Tokens/sec: 82204.94082129827\n",
      "Step: 495, Training Loss: 1.97176, Tokens/sec: 86793.000843614\n",
      "Step: 496, Training Loss: 2.22907, Tokens/sec: 86192.89557623143\n",
      "Step: 497, Training Loss: 1.95962, Tokens/sec: 84349.50400386476\n",
      "Step: 498, Training Loss: 2.01433, Tokens/sec: 86482.81745131266\n",
      "Step: 499, Training Loss: 1.85933, Tokens/sec: 83788.32163264195\n",
      "Step: 500, Training Loss: 2.02358, Tokens/sec: 85925.83312577779\n",
      "Step: 501, Training Loss: 1.95638, Tokens/sec: 85143.89393376831\n",
      "Step: 502, Training Loss: 2.01513, Tokens/sec: 74621.2809440899\n",
      "Step: 503, Training Loss: 2.03421, Tokens/sec: 69665.75235867858\n",
      "Step: 504, Training Loss: 1.95491, Tokens/sec: 87128.93590759073\n",
      "Step: 505, Training Loss: 2.00208, Tokens/sec: 86231.82765855429\n",
      "Step: 506, Training Loss: 1.84233, Tokens/sec: 83190.03523234303\n",
      "Step: 507, Training Loss: 1.92950, Tokens/sec: 84721.95017572408\n",
      "Step: 508, Training Loss: 1.99850, Tokens/sec: 86303.31426097939\n",
      "Step: 509, Training Loss: 1.97270, Tokens/sec: 55914.3361789876\n",
      "Step: 510, Training Loss: 1.97687, Tokens/sec: 60856.06920474583\n",
      "Step: 511, Training Loss: 1.99803, Tokens/sec: 86096.08727006736\n",
      "Step: 512, Training Loss: 1.94730, Tokens/sec: 60648.4907241037\n",
      "Step: 513, Training Loss: 1.81610, Tokens/sec: 82680.65541547454\n",
      "Step: 514, Training Loss: 1.97332, Tokens/sec: 86230.41347496478\n",
      "Step: 515, Training Loss: 1.87428, Tokens/sec: 87066.79189172896\n",
      "Step: 516, Training Loss: 1.91303, Tokens/sec: 84474.57318270188\n",
      "Step: 517, Training Loss: 2.05532, Tokens/sec: 86502.62719198095\n",
      "Step: 518, Training Loss: 1.95146, Tokens/sec: 84845.84182610331\n",
      "Step: 519, Training Loss: 2.04978, Tokens/sec: 86699.6406446503\n",
      "Step: 520, Training Loss: 2.08933, Tokens/sec: 86379.018838938\n",
      "Step: 521, Training Loss: 1.80405, Tokens/sec: 83734.41979087098\n",
      "Step: 522, Training Loss: 1.82558, Tokens/sec: 86913.0421514248\n",
      "Step: 523, Training Loss: 2.03254, Tokens/sec: 86146.71763078675\n",
      "Step: 524, Training Loss: 1.81863, Tokens/sec: 86629.87942363982\n",
      "Step: 525, Training Loss: 1.87464, Tokens/sec: 86359.79733840474\n",
      "Step: 526, Training Loss: 1.94410, Tokens/sec: 84711.90313538739\n",
      "Step: 527, Training Loss: 1.86542, Tokens/sec: 85805.662220834\n",
      "Step: 528, Training Loss: 1.80050, Tokens/sec: 86289.94046724838\n",
      "Step: 529, Training Loss: 2.02099, Tokens/sec: 83542.71021283632\n",
      "Step: 530, Training Loss: 1.95533, Tokens/sec: 84670.58666314646\n",
      "Step: 531, Training Loss: 1.95220, Tokens/sec: 86051.39547684207\n",
      "Step: 532, Training Loss: 1.90183, Tokens/sec: 85359.75840305597\n",
      "Step: 533, Training Loss: 1.90595, Tokens/sec: 84908.10513892001\n",
      "Step: 534, Training Loss: 1.90623, Tokens/sec: 83622.65979209395\n",
      "Step: 535, Training Loss: 1.85217, Tokens/sec: 86226.81604868316\n",
      "Step: 536, Training Loss: 2.10662, Tokens/sec: 85789.07573386715\n",
      "Step: 537, Training Loss: 2.17195, Tokens/sec: 83575.98970298056\n",
      "Step: 538, Training Loss: 1.94636, Tokens/sec: 85495.8993341993\n",
      "Step: 539, Training Loss: 1.95142, Tokens/sec: 83918.87673307135\n",
      "Step: 540, Training Loss: 1.79791, Tokens/sec: 85404.33858914099\n",
      "Step: 541, Training Loss: 1.88731, Tokens/sec: 84583.536409066\n",
      "Step: 542, Training Loss: 2.08915, Tokens/sec: 83806.26661386815\n",
      "Step: 543, Training Loss: 1.95507, Tokens/sec: 85876.45404857128\n",
      "Step: 544, Training Loss: 1.89941, Tokens/sec: 84556.97828179575\n",
      "Step: 545, Training Loss: 1.88539, Tokens/sec: 83912.90291461926\n",
      "Step: 546, Training Loss: 2.02045, Tokens/sec: 86408.84259208797\n",
      "Step: 547, Training Loss: 1.93024, Tokens/sec: 84330.69669300906\n",
      "Step: 548, Training Loss: 1.86972, Tokens/sec: 84822.82407497233\n",
      "Step: 549, Training Loss: 1.95257, Tokens/sec: 81370.88647595035\n",
      "Step: 550, Training Loss: 1.87000, Tokens/sec: 84113.49532840177\n",
      "Step: 551, Training Loss: 1.86675, Tokens/sec: 85492.52754392839\n",
      "Step: 552, Training Loss: 1.90928, Tokens/sec: 85261.75963994516\n",
      "Step: 553, Training Loss: 2.02392, Tokens/sec: 86062.7614063887\n",
      "Step: 554, Training Loss: 1.87666, Tokens/sec: 85997.9785619795\n",
      "Step: 555, Training Loss: 1.98036, Tokens/sec: 83585.00920458249\n",
      "Step: 556, Training Loss: 1.91159, Tokens/sec: 86238.82754642404\n",
      "Step: 557, Training Loss: 1.99502, Tokens/sec: 86689.08057027654\n",
      "Step: 558, Training Loss: 1.91618, Tokens/sec: 83628.19366610565\n",
      "Step: 559, Training Loss: 1.92129, Tokens/sec: 85381.780826212\n",
      "Step: 560, Training Loss: 1.86868, Tokens/sec: 86315.43257363814\n",
      "Step: 561, Training Loss: 1.94537, Tokens/sec: 86487.2799555978\n",
      "Step: 562, Training Loss: 2.02438, Tokens/sec: 85235.97209970566\n",
      "Step: 563, Training Loss: 1.94800, Tokens/sec: 81854.70859361812\n",
      "Step: 564, Training Loss: 1.98798, Tokens/sec: 85967.16070007256\n",
      "Step: 565, Training Loss: 1.84193, Tokens/sec: 86900.79003897985\n",
      "Step: 566, Training Loss: 1.94312, Tokens/sec: 83661.77627260909\n",
      "Step: 567, Training Loss: 1.96820, Tokens/sec: 86680.34407669301\n",
      "Step: 568, Training Loss: 1.86922, Tokens/sec: 86338.96326101877\n",
      "Step: 569, Training Loss: 2.19849, Tokens/sec: 86023.15254302093\n",
      "Step: 570, Training Loss: 2.26367, Tokens/sec: 85712.20000923536\n",
      "Step: 571, Training Loss: 2.04775, Tokens/sec: 84120.72260163527\n",
      "Step: 572, Training Loss: 1.98262, Tokens/sec: 85719.20951447226\n",
      "Step: 573, Training Loss: 1.83553, Tokens/sec: 86145.17352407801\n",
      "Step: 574, Training Loss: 1.97059, Tokens/sec: 83645.25617993211\n",
      "Step: 575, Training Loss: 1.88296, Tokens/sec: 85922.5111526495\n",
      "Step: 576, Training Loss: 1.74837, Tokens/sec: 82638.1753282855\n",
      "Step: 577, Training Loss: 1.86635, Tokens/sec: 85743.92302271572\n",
      "Step: 578, Training Loss: 1.93154, Tokens/sec: 86080.48866194686\n",
      "Step: 579, Training Loss: 1.91602, Tokens/sec: 83800.29124003739\n",
      "Step: 580, Training Loss: 1.93671, Tokens/sec: 85013.74982312719\n",
      "Step: 581, Training Loss: 1.89201, Tokens/sec: 85790.53118025478\n",
      "Step: 582, Training Loss: 1.83644, Tokens/sec: 84204.25427549577\n",
      "Step: 583, Training Loss: 1.90999, Tokens/sec: 85133.39533803341\n",
      "Step: 584, Training Loss: 2.29217, Tokens/sec: 82334.37118742813\n",
      "Step: 585, Training Loss: 2.20445, Tokens/sec: 85774.16114550138\n",
      "Step: 586, Training Loss: 2.19760, Tokens/sec: 86582.53578896778\n",
      "Step: 587, Training Loss: 1.93399, Tokens/sec: 82266.07238969846\n",
      "Step: 588, Training Loss: 1.87455, Tokens/sec: 85457.33102910369\n",
      "Step: 589, Training Loss: 1.90938, Tokens/sec: 86238.44080585746\n",
      "Step: 590, Training Loss: 1.86211, Tokens/sec: 84157.27054133746\n",
      "Step: 591, Training Loss: 1.83232, Tokens/sec: 84950.75319434765\n",
      "Step: 592, Training Loss: 1.83156, Tokens/sec: 83181.04797770715\n",
      "Step: 593, Training Loss: 1.82403, Tokens/sec: 86072.7032342277\n",
      "Step: 594, Training Loss: 2.05583, Tokens/sec: 85922.40931718181\n",
      "Step: 595, Training Loss: 1.95394, Tokens/sec: 81715.9715598483\n",
      "Step: 596, Training Loss: 1.83413, Tokens/sec: 85228.61533054679\n",
      "Step: 597, Training Loss: 1.84929, Tokens/sec: 85992.92775380748\n",
      "Step: 598, Training Loss: 1.80532, Tokens/sec: 85575.66400471979\n",
      "Step: 599, Training Loss: 1.86604, Tokens/sec: 84680.18185693261\n",
      "Step: 600, Training Loss: 1.80769, Tokens/sec: 83431.28811691538\n",
      "Step: 601, Training Loss: 1.79396, Tokens/sec: 86255.46762449443\n",
      "Step: 602, Training Loss: 1.94226, Tokens/sec: 86031.15533530487\n",
      "Step: 603, Training Loss: 1.88605, Tokens/sec: 83386.72496203175\n",
      "Step: 604, Training Loss: 1.83987, Tokens/sec: 86901.80822895734\n",
      "Step: 605, Training Loss: 1.81462, Tokens/sec: 86147.92160971803\n",
      "Step: 606, Training Loss: 2.16188, Tokens/sec: 86333.73038068601\n",
      "Step: 607, Training Loss: 2.15959, Tokens/sec: 85712.19507301648\n",
      "Step: 608, Training Loss: 1.89742, Tokens/sec: 82459.08534115231\n",
      "Step: 609, Training Loss: 1.80979, Tokens/sec: 86321.4814062544\n",
      "Step: 610, Training Loss: 1.84194, Tokens/sec: 86068.0790393987\n",
      "Step: 611, Training Loss: 1.84149, Tokens/sec: 83699.24164963579\n",
      "Step: 612, Training Loss: 1.87684, Tokens/sec: 86622.79850791754\n",
      "Step: 613, Training Loss: 1.92067, Tokens/sec: 84495.38119447355\n",
      "Step: 614, Training Loss: 1.78199, Tokens/sec: 86484.23581385489\n",
      "Step: 615, Training Loss: 1.98922, Tokens/sec: 84306.11296278195\n",
      "Step: 616, Training Loss: 1.97400, Tokens/sec: 84068.80150953765\n",
      "Step: 617, Training Loss: 2.05342, Tokens/sec: 86104.78602250475\n",
      "Step: 618, Training Loss: 1.82928, Tokens/sec: 86470.98159715247\n",
      "Step: 619, Training Loss: 1.93449, Tokens/sec: 83708.27793864094\n",
      "Step: 620, Training Loss: 1.79018, Tokens/sec: 86497.14429974242\n",
      "Step: 621, Training Loss: 1.86396, Tokens/sec: 84704.10010381122\n",
      "Step: 622, Training Loss: 1.83636, Tokens/sec: 85126.62328213021\n",
      "Step: 623, Training Loss: 1.78704, Tokens/sec: 84930.20019500905\n",
      "Step: 624, Training Loss: 1.85433, Tokens/sec: 83929.10369853856\n",
      "Step: 625, Training Loss: 1.78090, Tokens/sec: 85970.99678231477\n",
      "Step: 626, Training Loss: 1.83960, Tokens/sec: 84962.94577860003\n",
      "Step: 627, Training Loss: 1.94669, Tokens/sec: 84264.64564575198\n",
      "Step: 628, Training Loss: 1.87784, Tokens/sec: 86388.52689526019\n",
      "Step: 629, Training Loss: 1.82036, Tokens/sec: 83329.87142621154\n",
      "Step: 630, Training Loss: 1.81215, Tokens/sec: 85657.10200943402\n",
      "Step: 631, Training Loss: 1.76428, Tokens/sec: 86129.02647588447\n",
      "Step: 632, Training Loss: 1.84341, Tokens/sec: 84291.98835303744\n",
      "Step: 633, Training Loss: 2.06567, Tokens/sec: 84096.333644263\n",
      "Step: 634, Training Loss: 1.85305, Tokens/sec: 85229.89575798025\n",
      "Step: 635, Training Loss: 1.92319, Tokens/sec: 85499.95582343386\n",
      "Step: 636, Training Loss: 1.86621, Tokens/sec: 86337.98688178387\n",
      "Step: 637, Training Loss: 1.66370, Tokens/sec: 84024.6076186743\n",
      "Step: 638, Training Loss: 1.83753, Tokens/sec: 86007.50359732842\n",
      "Step: 639, Training Loss: 1.77662, Tokens/sec: 86403.63951745997\n",
      "Step: 640, Training Loss: 1.90187, Tokens/sec: 83594.2635269307\n",
      "Step: 641, Training Loss: 1.70936, Tokens/sec: 86639.64941750327\n",
      "Step: 642, Training Loss: 1.85270, Tokens/sec: 84408.30535045864\n",
      "Step: 643, Training Loss: 1.82979, Tokens/sec: 86764.85078889562\n",
      "Step: 644, Training Loss: 1.87665, Tokens/sec: 86139.51937517371\n",
      "Step: 645, Training Loss: 1.72810, Tokens/sec: 84335.1256547941\n",
      "Step: 646, Training Loss: 1.91287, Tokens/sec: 85985.74389649903\n",
      "Step: 647, Training Loss: 1.99435, Tokens/sec: 86156.57828463582\n",
      "Step: 648, Training Loss: 1.92414, Tokens/sec: 84796.90589898772\n",
      "Step: 649, Training Loss: 1.81128, Tokens/sec: 86589.38771508804\n",
      "Step: 650, Training Loss: 1.65985, Tokens/sec: 84415.21629367239\n",
      "Step: 651, Training Loss: 1.65909, Tokens/sec: 85869.57001504923\n",
      "Step: 652, Training Loss: 1.84307, Tokens/sec: 86958.73844511977\n",
      "Step: 653, Training Loss: 1.69963, Tokens/sec: 84355.69519953232\n",
      "Step: 654, Training Loss: 1.73365, Tokens/sec: 85490.12489454626\n",
      "Step: 655, Training Loss: 1.97707, Tokens/sec: 86387.74571231412\n",
      "Step: 656, Training Loss: 1.86614, Tokens/sec: 86421.13734220868\n",
      "Step: 657, Training Loss: 1.75884, Tokens/sec: 86625.85790736025\n",
      "Step: 658, Training Loss: 1.81423, Tokens/sec: 83775.08704432134\n",
      "Step: 659, Training Loss: 1.71719, Tokens/sec: 86473.29045568022\n",
      "Step: 660, Training Loss: 1.66494, Tokens/sec: 86021.55866779211\n",
      "Step: 661, Training Loss: 1.79215, Tokens/sec: 82563.48369155318\n",
      "Step: 662, Training Loss: 1.88094, Tokens/sec: 85468.99131990141\n",
      "Step: 663, Training Loss: 1.90376, Tokens/sec: 85854.58738775607\n",
      "Step: 664, Training Loss: 1.86190, Tokens/sec: 86093.03169697065\n",
      "Step: 665, Training Loss: 1.75327, Tokens/sec: 85432.36279067003\n",
      "Step: 666, Training Loss: 1.80719, Tokens/sec: 83958.58109779526\n",
      "Step: 667, Training Loss: 1.79897, Tokens/sec: 86408.60015238539\n",
      "Step: 668, Training Loss: 1.78102, Tokens/sec: 85820.6749763773\n",
      "Step: 669, Training Loss: 2.14004, Tokens/sec: 82799.48327821934\n",
      "Step: 670, Training Loss: 1.96034, Tokens/sec: 86092.18572949193\n",
      "Step: 671, Training Loss: 1.82783, Tokens/sec: 84159.06799513754\n",
      "Step: 672, Training Loss: 2.09343, Tokens/sec: 85778.20681036444\n",
      "Step: 673, Training Loss: 1.94155, Tokens/sec: 85634.4575386355\n",
      "Step: 674, Training Loss: 1.71791, Tokens/sec: 83659.57836945982\n",
      "Step: 675, Training Loss: 1.64115, Tokens/sec: 85723.8528094167\n",
      "Step: 676, Training Loss: 1.86159, Tokens/sec: 85450.974849149\n",
      "Step: 677, Training Loss: 1.95231, Tokens/sec: 84670.19897971308\n",
      "Step: 678, Training Loss: 1.79959, Tokens/sec: 86231.79135071088\n",
      "Step: 679, Training Loss: 1.77198, Tokens/sec: 83381.78780402022\n",
      "Step: 680, Training Loss: 2.08956, Tokens/sec: 86652.66162585093\n",
      "Step: 681, Training Loss: 1.91479, Tokens/sec: 86125.21023332213\n",
      "Step: 682, Training Loss: 1.67504, Tokens/sec: 84112.41188540224\n",
      "Step: 683, Training Loss: 1.80791, Tokens/sec: 84471.47135268383\n",
      "Step: 684, Training Loss: 1.85011, Tokens/sec: 86518.64142714199\n",
      "Step: 685, Training Loss: 1.96753, Tokens/sec: 85924.64030393267\n",
      "Step: 686, Training Loss: 1.74792, Tokens/sec: 85733.72680467849\n",
      "Step: 687, Training Loss: 1.83809, Tokens/sec: 83783.35308489947\n",
      "Step: 688, Training Loss: 2.18054, Tokens/sec: 86592.1417890103\n",
      "Step: 689, Training Loss: 1.83420, Tokens/sec: 85005.00107381537\n",
      "Step: 690, Training Loss: 1.71095, Tokens/sec: 82018.09610376877\n",
      "Step: 691, Training Loss: 1.66372, Tokens/sec: 86629.47084101591\n",
      "Step: 692, Training Loss: 1.79181, Tokens/sec: 86574.5540688247\n",
      "Step: 693, Training Loss: 1.81251, Tokens/sec: 85863.88583576208\n",
      "Step: 694, Training Loss: 1.83800, Tokens/sec: 87182.66798154237\n",
      "Step: 695, Training Loss: 1.75995, Tokens/sec: 84747.6713098304\n",
      "Step: 696, Training Loss: 1.80692, Tokens/sec: 86585.40610732084\n",
      "Step: 697, Training Loss: 1.83146, Tokens/sec: 85013.59851725024\n",
      "Step: 698, Training Loss: 1.79456, Tokens/sec: 84203.55537512555\n",
      "Step: 699, Training Loss: 1.85070, Tokens/sec: 86283.77245505364\n",
      "Step: 700, Training Loss: 1.99227, Tokens/sec: 83996.58757658454\n",
      "Step: 701, Training Loss: 1.79569, Tokens/sec: 85901.20201826858\n",
      "Step: 702, Training Loss: 1.84228, Tokens/sec: 86355.44993316504\n",
      "Step: 703, Training Loss: 1.78582, Tokens/sec: 84292.92290795753\n",
      "Step: 704, Training Loss: 2.09817, Tokens/sec: 85819.43337552482\n",
      "Step: 705, Training Loss: 1.76973, Tokens/sec: 85501.82804695753\n",
      "Step: 706, Training Loss: 1.81194, Tokens/sec: 83762.47371534615\n",
      "Step: 707, Training Loss: 1.70617, Tokens/sec: 85795.53488393508\n",
      "Step: 708, Training Loss: 1.74860, Tokens/sec: 83773.90478336206\n",
      "Step: 709, Training Loss: 1.78914, Tokens/sec: 85856.7649157737\n",
      "Step: 710, Training Loss: 1.89920, Tokens/sec: 86614.002560546\n",
      "Step: 711, Training Loss: 1.73725, Tokens/sec: 83017.92198013153\n",
      "Step: 712, Training Loss: 1.79895, Tokens/sec: 84836.0684872974\n",
      "Step: 713, Training Loss: 1.78049, Tokens/sec: 85818.2610364131\n",
      "Step: 714, Training Loss: 2.20431, Tokens/sec: 84408.47755946711\n",
      "Step: 715, Training Loss: 1.85703, Tokens/sec: 86573.89760375666\n",
      "Step: 716, Training Loss: 1.83121, Tokens/sec: 83995.41369997802\n",
      "Step: 717, Training Loss: 1.78067, Tokens/sec: 86278.93972652373\n",
      "Step: 718, Training Loss: 1.73095, Tokens/sec: 86183.3697864554\n",
      "Step: 719, Training Loss: 1.70136, Tokens/sec: 84098.21957352672\n",
      "Step: 720, Training Loss: 1.82418, Tokens/sec: 85405.42485271876\n",
      "Step: 721, Training Loss: 1.73762, Tokens/sec: 83932.02823228957\n",
      "Step: 722, Training Loss: 1.70335, Tokens/sec: 86905.80193462942\n",
      "Step: 723, Training Loss: 1.78712, Tokens/sec: 86537.1246840747\n",
      "Step: 724, Training Loss: 1.75393, Tokens/sec: 84445.35306224822\n",
      "Step: 725, Training Loss: 1.86123, Tokens/sec: 85938.75935993115\n",
      "Step: 726, Training Loss: 1.73460, Tokens/sec: 86159.88485444678\n",
      "Step: 727, Training Loss: 2.18487, Tokens/sec: 84047.90036779967\n",
      "Step: 728, Training Loss: 1.81549, Tokens/sec: 84818.23484293667\n",
      "Step: 729, Training Loss: 1.71297, Tokens/sec: 85462.47068891433\n",
      "Step: 730, Training Loss: 2.00885, Tokens/sec: 86345.74983992973\n",
      "Step: 731, Training Loss: 1.73227, Tokens/sec: 86350.26875675126\n",
      "Step: 732, Training Loss: 1.79177, Tokens/sec: 83063.62791957469\n",
      "Step: 733, Training Loss: 1.81288, Tokens/sec: 86388.73733929648\n",
      "Step: 734, Training Loss: 1.71413, Tokens/sec: 86144.66396749967\n",
      "Step: 735, Training Loss: 1.78108, Tokens/sec: 83127.69916447825\n",
      "Step: 736, Training Loss: 1.74456, Tokens/sec: 85440.27741559785\n",
      "Step: 737, Training Loss: 1.69129, Tokens/sec: 84191.3837688153\n",
      "Step: 738, Training Loss: 1.73656, Tokens/sec: 86371.00857670773\n",
      "Step: 739, Training Loss: 1.68451, Tokens/sec: 85677.83856658016\n",
      "Step: 740, Training Loss: 1.72906, Tokens/sec: 83002.91916651014\n",
      "Step: 741, Training Loss: 1.83922, Tokens/sec: 86173.62582427864\n",
      "Step: 742, Training Loss: 1.81931, Tokens/sec: 86108.72174181943\n",
      "Step: 743, Training Loss: 1.90898, Tokens/sec: 83568.7500132888\n",
      "Step: 744, Training Loss: 2.06164, Tokens/sec: 81131.29549500073\n",
      "Step: 745, Training Loss: 1.92502, Tokens/sec: 78682.90125043213\n",
      "Step: 746, Training Loss: 1.83748, Tokens/sec: 75130.65392159036\n",
      "Step: 747, Training Loss: 1.86698, Tokens/sec: 74724.64817741624\n",
      "Step: 748, Training Loss: 1.92525, Tokens/sec: 86862.02074995173\n",
      "Step: 749, Training Loss: 1.74444, Tokens/sec: 79495.7346444164\n",
      "Step: 750, Training Loss: 1.76045, Tokens/sec: 84519.1362823632\n",
      "Step: 751, Training Loss: 1.89427, Tokens/sec: 76393.12900287939\n",
      "Step: 752, Training Loss: 1.73797, Tokens/sec: 84067.91289783528\n",
      "Step: 753, Training Loss: 1.72453, Tokens/sec: 75924.74945411381\n",
      "Step: 754, Training Loss: 1.61998, Tokens/sec: 75419.1165139303\n",
      "Step: 755, Training Loss: 1.65969, Tokens/sec: 83333.40072769704\n",
      "Step: 756, Training Loss: 1.72807, Tokens/sec: 83943.93012690535\n",
      "Step: 757, Training Loss: 1.77567, Tokens/sec: 83990.2293877299\n",
      "Step: 758, Training Loss: 1.83916, Tokens/sec: 85523.45196789704\n",
      "Step: 759, Training Loss: 1.70854, Tokens/sec: 85240.61684874697\n",
      "Step: 760, Training Loss: 1.90081, Tokens/sec: 75313.234263571\n",
      "Step: 761, Training Loss: 1.77607, Tokens/sec: 82429.87576638772\n",
      "Step: 762, Training Loss: 1.73767, Tokens/sec: 78633.31270928458\n",
      "Step: 763, Training Loss: 1.77588, Tokens/sec: 78361.30254848847\n",
      "Step: 764, Training Loss: 1.97998, Tokens/sec: 74804.41446623846\n",
      "Step: 765, Training Loss: 1.74000, Tokens/sec: 76512.09133580087\n",
      "Step: 766, Training Loss: 1.81309, Tokens/sec: 82119.42785119537\n",
      "Step: 767, Training Loss: 1.82664, Tokens/sec: 85563.5018280865\n",
      "Step: 768, Training Loss: 1.87947, Tokens/sec: 86300.86855276412\n",
      "Step: 769, Training Loss: 1.65487, Tokens/sec: 84069.73457141555\n",
      "Step: 770, Training Loss: 1.73099, Tokens/sec: 86070.30132240792\n",
      "Step: 771, Training Loss: 1.74390, Tokens/sec: 87250.3619282902\n",
      "Step: 772, Training Loss: 1.68852, Tokens/sec: 84231.43221402966\n",
      "Step: 773, Training Loss: 1.75216, Tokens/sec: 86327.65572121216\n",
      "Step: 774, Training Loss: 1.63909, Tokens/sec: 85079.29453608331\n",
      "Step: 775, Training Loss: 1.55044, Tokens/sec: 85402.09581053123\n",
      "Step: 776, Training Loss: 1.85700, Tokens/sec: 86511.27216867785\n",
      "Step: 777, Training Loss: 2.23433, Tokens/sec: 85852.76716836053\n",
      "Step: 778, Training Loss: 1.69717, Tokens/sec: 86121.41650913299\n",
      "Step: 779, Training Loss: 1.76724, Tokens/sec: 84672.22582224765\n",
      "Step: 780, Training Loss: 1.81482, Tokens/sec: 74282.56997257484\n",
      "Step: 781, Training Loss: 1.84370, Tokens/sec: 85937.48053602291\n",
      "Step: 782, Training Loss: 1.74352, Tokens/sec: 83773.16203354056\n",
      "Step: 783, Training Loss: 2.09250, Tokens/sec: 87945.19335384094\n",
      "Step: 784, Training Loss: 1.68194, Tokens/sec: 87264.08112628564\n",
      "Step: 785, Training Loss: 1.59598, Tokens/sec: 81178.0134753424\n",
      "Step: 786, Training Loss: 1.72864, Tokens/sec: 84279.80547443514\n",
      "Step: 787, Training Loss: 1.79096, Tokens/sec: 85061.9895735391\n",
      "Step: 788, Training Loss: 1.92595, Tokens/sec: 83302.30893689423\n",
      "Step: 789, Training Loss: 1.90594, Tokens/sec: 85465.20034989441\n",
      "Step: 790, Training Loss: 1.95548, Tokens/sec: 84614.16405915593\n",
      "Step: 791, Training Loss: 1.77976, Tokens/sec: 87112.46431019336\n",
      "Step: 792, Training Loss: 1.70422, Tokens/sec: 87316.88927830038\n",
      "Step: 793, Training Loss: 1.73454, Tokens/sec: 84033.26691501975\n",
      "Step: 794, Training Loss: 1.79704, Tokens/sec: 75870.710289598\n",
      "Step: 795, Training Loss: 1.71874, Tokens/sec: 77274.9644098922\n",
      "Step: 796, Training Loss: 1.76339, Tokens/sec: 76596.94723766921\n",
      "Step: 797, Training Loss: 1.74292, Tokens/sec: 75190.04508805546\n",
      "Step: 798, Training Loss: 1.66652, Tokens/sec: 74027.21333107253\n",
      "Step: 799, Training Loss: 1.82083, Tokens/sec: 78861.4360907928\n",
      "Step: 800, Training Loss: 2.02278, Tokens/sec: 80760.07972138244\n",
      "Step: 801, Training Loss: 1.67240, Tokens/sec: 74524.99232632702\n",
      "Step: 802, Training Loss: 1.71286, Tokens/sec: 77577.70436429388\n",
      "Step: 803, Training Loss: 1.82022, Tokens/sec: 87185.7633439046\n",
      "Step: 804, Training Loss: 1.76215, Tokens/sec: 78140.5300981054\n",
      "Step: 805, Training Loss: 1.62273, Tokens/sec: 82714.77734638525\n",
      "Step: 806, Training Loss: 1.76065, Tokens/sec: 83019.6008397997\n",
      "Step: 807, Training Loss: 1.77193, Tokens/sec: 86853.62322517234\n",
      "Step: 808, Training Loss: 1.77023, Tokens/sec: 84162.40502459939\n",
      "Step: 809, Training Loss: 1.66674, Tokens/sec: 85915.17190932282\n",
      "Step: 810, Training Loss: 1.71923, Tokens/sec: 85253.58216003416\n",
      "Step: 811, Training Loss: 1.79371, Tokens/sec: 71963.754476539\n",
      "Step: 812, Training Loss: 1.81798, Tokens/sec: 87297.5839904764\n",
      "Step: 813, Training Loss: 1.88690, Tokens/sec: 84109.65611770107\n",
      "Step: 814, Training Loss: 1.83906, Tokens/sec: 82935.0037758951\n",
      "Step: 815, Training Loss: 1.83979, Tokens/sec: 86289.9550099756\n",
      "Step: 816, Training Loss: 1.83452, Tokens/sec: 84345.7569804913\n",
      "Step: 817, Training Loss: 1.65097, Tokens/sec: 86668.92448042078\n",
      "Step: 818, Training Loss: 2.01672, Tokens/sec: 86479.1386817254\n",
      "Step: 819, Training Loss: 1.86145, Tokens/sec: 85408.12371463339\n",
      "Step: 820, Training Loss: 1.84997, Tokens/sec: 86374.99644592534\n",
      "Step: 821, Training Loss: 1.76889, Tokens/sec: 83432.07367734607\n",
      "Step: 822, Training Loss: 1.70409, Tokens/sec: 87559.26262898248\n",
      "Step: 823, Training Loss: 1.77025, Tokens/sec: 87603.71501414082\n",
      "Step: 824, Training Loss: 1.73857, Tokens/sec: 83452.22541771816\n",
      "Step: 825, Training Loss: 1.67951, Tokens/sec: 86968.11604270247\n",
      "Step: 826, Training Loss: 1.77145, Tokens/sec: 86566.45169626606\n",
      "Step: 827, Training Loss: 1.72025, Tokens/sec: 85791.14347335776\n",
      "Step: 828, Training Loss: 1.70747, Tokens/sec: 86292.87368375447\n",
      "Step: 829, Training Loss: 1.76887, Tokens/sec: 84863.67565212217\n",
      "Step: 830, Training Loss: 1.85104, Tokens/sec: 87627.63381714006\n",
      "Step: 831, Training Loss: 1.65625, Tokens/sec: 86758.84119329856\n",
      "Step: 832, Training Loss: 1.54485, Tokens/sec: 73432.52122915484\n",
      "Step: 833, Training Loss: 1.99225, Tokens/sec: 80941.15633587442\n",
      "Step: 834, Training Loss: 1.73379, Tokens/sec: 83310.72128315749\n",
      "Step: 835, Training Loss: 1.70423, Tokens/sec: 82932.41948870424\n",
      "Step: 836, Training Loss: 1.91612, Tokens/sec: 86257.8340164943\n",
      "Step: 837, Training Loss: 2.04245, Tokens/sec: 85704.9177178496\n",
      "Step: 838, Training Loss: 1.75239, Tokens/sec: 86399.70732195515\n",
      "Step: 839, Training Loss: 1.76071, Tokens/sec: 86332.6108190702\n",
      "Step: 840, Training Loss: 1.79011, Tokens/sec: 84038.21514601496\n",
      "Step: 841, Training Loss: 1.75395, Tokens/sec: 72115.83969394186\n",
      "Step: 842, Training Loss: 1.86413, Tokens/sec: 76037.5090475652\n",
      "Step: 843, Training Loss: 1.70897, Tokens/sec: 83064.14294825583\n",
      "Step: 844, Training Loss: 1.66669, Tokens/sec: 72889.40441306862\n",
      "Step: 845, Training Loss: 1.58763, Tokens/sec: 86541.48765724707\n",
      "Step: 846, Training Loss: 1.52969, Tokens/sec: 83681.17621059119\n",
      "Step: 847, Training Loss: 1.75381, Tokens/sec: 87001.50656289332\n",
      "Step: 848, Training Loss: 1.87515, Tokens/sec: 87261.29809042563\n",
      "Step: 849, Training Loss: 1.65045, Tokens/sec: 85572.81867160162\n",
      "Step: 850, Training Loss: 1.65764, Tokens/sec: 84720.59515715587\n",
      "Step: 851, Training Loss: 1.74158, Tokens/sec: 83600.55721152063\n",
      "Step: 852, Training Loss: 1.75617, Tokens/sec: 86427.3833694857\n",
      "Step: 853, Training Loss: 1.66653, Tokens/sec: 86938.75940679\n",
      "Step: 854, Training Loss: 1.78878, Tokens/sec: 83055.52054026588\n",
      "Step: 855, Training Loss: 1.57404, Tokens/sec: 86517.6751407113\n",
      "Step: 856, Training Loss: 1.76018, Tokens/sec: 87496.44412429298\n",
      "Step: 857, Training Loss: 1.69039, Tokens/sec: 87010.96085245263\n",
      "Step: 858, Training Loss: 1.74207, Tokens/sec: 86261.54121251676\n",
      "Step: 859, Training Loss: 1.68625, Tokens/sec: 84560.02355013929\n",
      "Step: 860, Training Loss: 1.75466, Tokens/sec: 85852.36048567784\n",
      "Step: 861, Training Loss: 1.99101, Tokens/sec: 86317.16650825454\n",
      "Step: 862, Training Loss: 1.68027, Tokens/sec: 83866.92152029487\n",
      "Step: 863, Training Loss: 1.79884, Tokens/sec: 87245.94205542735\n",
      "Step: 864, Training Loss: 1.94040, Tokens/sec: 84415.34068591101\n",
      "Step: 865, Training Loss: 1.75277, Tokens/sec: 86667.13604371806\n",
      "Step: 866, Training Loss: 1.78604, Tokens/sec: 87198.94198145354\n",
      "Step: 867, Training Loss: 1.75815, Tokens/sec: 83778.97802404875\n",
      "Step: 868, Training Loss: 1.79579, Tokens/sec: 86539.76663928514\n",
      "Step: 869, Training Loss: 1.75869, Tokens/sec: 87369.6327925223\n",
      "Step: 870, Training Loss: 1.71561, Tokens/sec: 85534.41404919868\n",
      "Step: 871, Training Loss: 1.92046, Tokens/sec: 86933.28981995267\n",
      "Step: 872, Training Loss: 1.71468, Tokens/sec: 86026.12637242647\n",
      "Step: 873, Training Loss: 1.57801, Tokens/sec: 86397.64249059498\n",
      "Step: 874, Training Loss: 1.53469, Tokens/sec: 87052.42651684121\n",
      "Step: 875, Training Loss: 1.46528, Tokens/sec: 84010.83348004555\n",
      "Step: 876, Training Loss: 1.50409, Tokens/sec: 86378.76700054607\n",
      "Step: 877, Training Loss: 1.81577, Tokens/sec: 82846.66605602122\n",
      "Step: 878, Training Loss: 1.58577, Tokens/sec: 85381.0942727919\n",
      "Step: 879, Training Loss: 1.67359, Tokens/sec: 86658.67119089673\n",
      "Step: 880, Training Loss: 2.03546, Tokens/sec: 84581.98320614202\n",
      "Step: 881, Training Loss: 1.81530, Tokens/sec: 85220.2327691555\n",
      "Step: 882, Training Loss: 1.69464, Tokens/sec: 85187.54992796676\n",
      "Step: 883, Training Loss: 1.71896, Tokens/sec: 87567.51354346935\n",
      "Step: 884, Training Loss: 1.74929, Tokens/sec: 86644.03373352069\n",
      "Step: 885, Training Loss: 1.73383, Tokens/sec: 83257.29871121842\n",
      "Step: 886, Training Loss: 1.79802, Tokens/sec: 86319.54719976477\n",
      "Step: 887, Training Loss: 1.67809, Tokens/sec: 87367.07689096303\n",
      "Step: 888, Training Loss: 1.73114, Tokens/sec: 84947.40708861097\n",
      "Step: 889, Training Loss: 1.71968, Tokens/sec: 88114.36638071202\n",
      "Step: 890, Training Loss: 1.66375, Tokens/sec: 85673.7795136465\n",
      "Step: 891, Training Loss: 1.60884, Tokens/sec: 86192.80715354288\n",
      "Step: 892, Training Loss: 1.70094, Tokens/sec: 85770.33766897922\n",
      "Step: 893, Training Loss: 1.79728, Tokens/sec: 84614.86324254253\n",
      "Step: 894, Training Loss: 1.58965, Tokens/sec: 86455.6715908277\n",
      "Step: 895, Training Loss: 1.71907, Tokens/sec: 85683.90143036602\n",
      "Step: 896, Training Loss: 1.69347, Tokens/sec: 87189.07468168435\n",
      "Step: 897, Training Loss: 1.63264, Tokens/sec: 87905.9546655843\n",
      "Step: 898, Training Loss: 1.63999, Tokens/sec: 85289.7897866414\n",
      "Step: 899, Training Loss: 1.68982, Tokens/sec: 87206.92784832018\n",
      "Step: 900, Training Loss: 1.72568, Tokens/sec: 87934.78171898604\n",
      "Step: 901, Training Loss: 1.74221, Tokens/sec: 85574.35171339552\n",
      "Step: 902, Training Loss: 1.84984, Tokens/sec: 86121.3780313928\n",
      "Step: 903, Training Loss: 1.63879, Tokens/sec: 83780.14629209909\n",
      "Step: 904, Training Loss: 1.67671, Tokens/sec: 86478.47544969589\n",
      "Step: 905, Training Loss: 1.78260, Tokens/sec: 85935.43142858094\n",
      "Step: 906, Training Loss: 1.68688, Tokens/sec: 82838.07282307731\n",
      "Step: 907, Training Loss: 1.64330, Tokens/sec: 87314.3960219667\n",
      "Step: 908, Training Loss: 1.75955, Tokens/sec: 85991.89148504772\n",
      "Step: 909, Training Loss: 1.75326, Tokens/sec: 85767.5997066148\n",
      "Step: 910, Training Loss: 1.62532, Tokens/sec: 87942.26379984528\n",
      "Step: 911, Training Loss: 1.84211, Tokens/sec: 85518.68439283308\n",
      "Step: 912, Training Loss: 1.84984, Tokens/sec: 85463.28515628494\n",
      "Step: 913, Training Loss: 1.52161, Tokens/sec: 87903.5790427763\n",
      "Step: 914, Training Loss: 1.53744, Tokens/sec: 85472.86689796105\n",
      "Step: 915, Training Loss: 1.81161, Tokens/sec: 86255.8059349812\n",
      "Step: 916, Training Loss: 1.68375, Tokens/sec: 83558.85102455282\n",
      "Step: 917, Training Loss: 1.63167, Tokens/sec: 86619.41097798018\n",
      "Step: 918, Training Loss: 1.78256, Tokens/sec: 84742.17676186387\n",
      "Step: 919, Training Loss: 1.71095, Tokens/sec: 84220.73271152648\n",
      "Step: 920, Training Loss: 1.65454, Tokens/sec: 85889.21143929767\n",
      "Step: 921, Training Loss: 1.65784, Tokens/sec: 86540.62097014987\n",
      "Step: 922, Training Loss: 1.71567, Tokens/sec: 87052.1207813693\n",
      "Step: 923, Training Loss: 1.62539, Tokens/sec: 86390.23916865721\n",
      "Step: 924, Training Loss: 1.64991, Tokens/sec: 83996.67154747044\n",
      "Step: 925, Training Loss: 1.57365, Tokens/sec: 86218.36348843212\n",
      "Step: 926, Training Loss: 1.72706, Tokens/sec: 87101.13015946576\n",
      "Step: 927, Training Loss: 1.64423, Tokens/sec: 85164.73554546601\n",
      "Step: 928, Training Loss: 1.90142, Tokens/sec: 85835.4771332007\n",
      "Step: 929, Training Loss: 1.86120, Tokens/sec: 83511.53826101714\n",
      "Step: 930, Training Loss: 1.61569, Tokens/sec: 86524.62647216518\n",
      "Step: 931, Training Loss: 1.63773, Tokens/sec: 87450.69222713025\n",
      "Step: 932, Training Loss: 1.70559, Tokens/sec: 84001.75027465666\n",
      "Step: 933, Training Loss: 1.68384, Tokens/sec: 85847.7023592162\n",
      "Step: 934, Training Loss: 1.65553, Tokens/sec: 85706.01522611539\n",
      "Step: 935, Training Loss: 1.56204, Tokens/sec: 86376.0346828511\n",
      "Step: 936, Training Loss: 1.71449, Tokens/sec: 86237.26606678126\n",
      "Step: 937, Training Loss: 1.57838, Tokens/sec: 84677.24959391778\n",
      "Step: 938, Training Loss: 1.74403, Tokens/sec: 85670.43713231922\n",
      "Step: 939, Training Loss: 1.59999, Tokens/sec: 87488.32198396615\n",
      "Step: 940, Training Loss: 1.60918, Tokens/sec: 85044.36321560148\n",
      "Step: 941, Training Loss: 1.68315, Tokens/sec: 86328.80289907617\n",
      "Step: 942, Training Loss: 1.66848, Tokens/sec: 84119.4303693143\n",
      "Step: 943, Training Loss: 1.59194, Tokens/sec: 85798.90320951752\n",
      "Step: 944, Training Loss: 1.71224, Tokens/sec: 87429.49323045864\n",
      "Step: 945, Training Loss: 1.81025, Tokens/sec: 83284.2874080223\n",
      "Step: 946, Training Loss: 1.64227, Tokens/sec: 86650.11954826448\n",
      "Step: 947, Training Loss: 1.60374, Tokens/sec: 82828.27247031333\n",
      "Step: 948, Training Loss: 1.63487, Tokens/sec: 87024.14909451427\n",
      "Step: 949, Training Loss: 1.45257, Tokens/sec: 86660.12879191601\n",
      "Step: 950, Training Loss: 1.60217, Tokens/sec: 83794.63685540877\n",
      "Step: 951, Training Loss: 1.66569, Tokens/sec: 87156.58462371303\n",
      "Step: 952, Training Loss: 1.75674, Tokens/sec: 86987.66243394456\n",
      "Step: 953, Training Loss: 1.71873, Tokens/sec: 85507.23545586782\n",
      "Step: 954, Training Loss: 1.57187, Tokens/sec: 86812.03298452095\n",
      "Step: 955, Training Loss: 1.71648, Tokens/sec: 83568.94694656391\n",
      "Step: 956, Training Loss: 1.52317, Tokens/sec: 86170.51307948244\n",
      "Step: 957, Training Loss: 1.63868, Tokens/sec: 87100.14618913837\n",
      "Step: 958, Training Loss: 1.52479, Tokens/sec: 85260.30166737943\n",
      "Step: 959, Training Loss: 1.70945, Tokens/sec: 86374.84344309474\n",
      "Step: 960, Training Loss: 1.73671, Tokens/sec: 83681.9374195305\n",
      "Step: 961, Training Loss: 1.53304, Tokens/sec: 86152.32968715527\n",
      "Step: 962, Training Loss: 1.65869, Tokens/sec: 86173.09734840004\n",
      "Step: 963, Training Loss: 1.65739, Tokens/sec: 84287.20835238634\n",
      "Step: 964, Training Loss: 1.62794, Tokens/sec: 86398.43797770326\n",
      "Step: 965, Training Loss: 1.77915, Tokens/sec: 87415.8735584706\n",
      "Step: 966, Training Loss: 1.67427, Tokens/sec: 87501.97685724583\n",
      "Step: 967, Training Loss: 1.49400, Tokens/sec: 86420.99193035754\n",
      "Step: 968, Training Loss: 1.56368, Tokens/sec: 84815.26578693277\n",
      "Step: 969, Training Loss: 1.63804, Tokens/sec: 86255.21514806752\n",
      "Step: 970, Training Loss: 1.67140, Tokens/sec: 87722.14579873733\n",
      "Step: 971, Training Loss: 1.62390, Tokens/sec: 84776.36980383402\n",
      "Step: 972, Training Loss: 1.66284, Tokens/sec: 84893.53925941407\n",
      "Step: 973, Training Loss: 1.57580, Tokens/sec: 83791.70303070461\n",
      "Step: 974, Training Loss: 1.62797, Tokens/sec: 86434.57418599821\n",
      "Step: 975, Training Loss: 3.75169, Tokens/sec: 86160.7085908919\n",
      "Step: 976, Training Loss: 2.35471, Tokens/sec: 83902.65927296058\n",
      "Step: 977, Training Loss: 1.61773, Tokens/sec: 86389.69254479655\n",
      "Step: 978, Training Loss: 1.60018, Tokens/sec: 86032.4184266508\n",
      "Step: 979, Training Loss: 1.65406, Tokens/sec: 86008.19800484751\n",
      "Step: 980, Training Loss: 1.80841, Tokens/sec: 87716.30388604137\n",
      "Step: 981, Training Loss: 1.71663, Tokens/sec: 84679.38881648579\n",
      "Step: 982, Training Loss: 1.51441, Tokens/sec: 87501.70207340467\n",
      "Step: 983, Training Loss: 1.78264, Tokens/sec: 87662.28089108787\n",
      "Step: 984, Training Loss: 1.87082, Tokens/sec: 85339.33909088932\n",
      "Step: 985, Training Loss: 1.43493, Tokens/sec: 86982.6498651418\n",
      "Step: 986, Training Loss: 1.65899, Tokens/sec: 84351.18113042391\n",
      "Step: 987, Training Loss: 1.58062, Tokens/sec: 86395.43288619634\n",
      "Step: 988, Training Loss: 1.75306, Tokens/sec: 86467.74283012057\n",
      "Step: 989, Training Loss: 1.63276, Tokens/sec: 84372.38764510243\n",
      "Step: 990, Training Loss: 1.67426, Tokens/sec: 86705.25523241685\n",
      "Step: 991, Training Loss: 1.71468, Tokens/sec: 87241.05158256952\n",
      "Step: 992, Training Loss: 1.60930, Tokens/sec: 83414.80328346744\n",
      "Step: 993, Training Loss: 1.59577, Tokens/sec: 87801.9566818349\n",
      "Step: 994, Training Loss: 1.57919, Tokens/sec: 85714.45820792725\n",
      "Step: 995, Training Loss: 1.63046, Tokens/sec: 85882.62653398662\n",
      "Step: 996, Training Loss: 1.65905, Tokens/sec: 87648.64440523424\n",
      "Step: 997, Training Loss: 2.04782, Tokens/sec: 85647.31953089249\n",
      "Step: 998, Training Loss: 1.76319, Tokens/sec: 85639.44528676537\n",
      "Step: 999, Training Loss: 1.63535, Tokens/sec: 82125.43554302247\n",
      "Step: 1000, Training Loss: 1.57299, Tokens/sec: 86109.90610022403\n",
      "Step: 1001, Training Loss: 1.50137, Tokens/sec: 86664.45650726248\n",
      "Step: 1002, Training Loss: 1.67686, Tokens/sec: 83884.880432779\n",
      "Step: 1003, Training Loss: 1.72205, Tokens/sec: 86507.13878934162\n",
      "Step: 1004, Training Loss: 1.76303, Tokens/sec: 86109.68977474286\n",
      "Step: 1005, Training Loss: 1.92661, Tokens/sec: 87043.9685513137\n",
      "Step: 1006, Training Loss: 1.60013, Tokens/sec: 86867.15022642973\n",
      "Step: 1007, Training Loss: 1.54890, Tokens/sec: 84304.96295387781\n",
      "Step: 1008, Training Loss: 1.66333, Tokens/sec: 87514.4028351325\n",
      "Step: 1009, Training Loss: 1.53812, Tokens/sec: 86986.58588602972\n",
      "Step: 1010, Training Loss: 1.60542, Tokens/sec: 85154.66376937092\n",
      "Step: 1011, Training Loss: 1.81540, Tokens/sec: 86238.78760296502\n",
      "Step: 1012, Training Loss: 1.71739, Tokens/sec: 83345.39800209216\n",
      "Step: 1013, Training Loss: 1.70139, Tokens/sec: 86128.9562964747\n",
      "Step: 1014, Training Loss: 1.67509, Tokens/sec: 86352.36408783033\n",
      "Step: 1015, Training Loss: 1.52490, Tokens/sec: 84322.3105857322\n",
      "Step: 1016, Training Loss: 1.86489, Tokens/sec: 86609.34932706312\n",
      "Step: 1017, Training Loss: 1.64861, Tokens/sec: 86582.25485121232\n",
      "Step: 1018, Training Loss: 1.47742, Tokens/sec: 86637.79987783215\n",
      "Step: 1019, Training Loss: 1.68224, Tokens/sec: 87135.50849449984\n",
      "Step: 1020, Training Loss: 1.54067, Tokens/sec: 84075.6785374775\n",
      "Step: 1021, Training Loss: 1.57642, Tokens/sec: 86189.32122841505\n",
      "Step: 1022, Training Loss: 1.57443, Tokens/sec: 87722.78409491738\n",
      "Step: 1023, Training Loss: 1.77835, Tokens/sec: 70162.6602479043\n",
      "Step: 1024, Training Loss: 1.68086, Tokens/sec: 86472.85642121917\n",
      "Step: 1025, Training Loss: 1.62147, Tokens/sec: 85446.54867580147\n",
      "Step: 1026, Training Loss: 1.65700, Tokens/sec: 65760.08934270231\n",
      "Step: 1027, Training Loss: 1.61720, Tokens/sec: 80703.11286719942\n",
      "Step: 1028, Training Loss: 1.70530, Tokens/sec: 85280.72578326701\n",
      "Step: 1029, Training Loss: 1.59479, Tokens/sec: 84056.22119511096\n",
      "Step: 1030, Training Loss: 1.59349, Tokens/sec: 86370.86514895468\n",
      "Step: 1031, Training Loss: 1.66986, Tokens/sec: 86759.21562103264\n",
      "Step: 1032, Training Loss: 1.74200, Tokens/sec: 85695.29956803506\n",
      "Step: 1033, Training Loss: 1.73137, Tokens/sec: 87571.3262161463\n",
      "Step: 1034, Training Loss: 1.56872, Tokens/sec: 85002.78583291473\n",
      "Step: 1035, Training Loss: 1.62169, Tokens/sec: 86846.54343561114\n",
      "Step: 1036, Training Loss: 1.83211, Tokens/sec: 87508.97836140516\n",
      "Step: 1037, Training Loss: 1.94562, Tokens/sec: 85264.53640004834\n",
      "Step: 1038, Training Loss: 1.61091, Tokens/sec: 85152.23094751511\n",
      "Step: 1039, Training Loss: 1.60582, Tokens/sec: 82154.25587961251\n",
      "Step: 1040, Training Loss: 1.86001, Tokens/sec: 85855.96542358129\n",
      "Step: 1041, Training Loss: 1.57841, Tokens/sec: 85825.67005440433\n",
      "Step: 1042, Training Loss: 1.75137, Tokens/sec: 83698.75677273965\n",
      "Step: 1043, Training Loss: 1.81663, Tokens/sec: 85174.7636526004\n",
      "Step: 1044, Training Loss: 1.68462, Tokens/sec: 85666.79848642563\n",
      "Step: 1045, Training Loss: 1.78351, Tokens/sec: 86304.96814900123\n",
      "Step: 1046, Training Loss: 1.80172, Tokens/sec: 86360.92306360308\n",
      "Step: 1047, Training Loss: 1.75715, Tokens/sec: 83765.03718835894\n",
      "Step: 1048, Training Loss: 1.72409, Tokens/sec: 86586.42057919483\n",
      "Step: 1049, Training Loss: 1.69696, Tokens/sec: 87635.44715285565\n",
      "Step: 1050, Training Loss: 1.71326, Tokens/sec: 85247.61285995909\n",
      "Step: 1051, Training Loss: 1.58204, Tokens/sec: 85890.68919853229\n",
      "Step: 1052, Training Loss: 1.60674, Tokens/sec: 83270.96185424968\n",
      "Step: 1053, Training Loss: 1.68178, Tokens/sec: 85770.26897129754\n",
      "Step: 1054, Training Loss: 1.89501, Tokens/sec: 86603.45646816787\n",
      "Step: 1055, Training Loss: 1.66604, Tokens/sec: 84216.6780812787\n",
      "Step: 1056, Training Loss: 1.58954, Tokens/sec: 86341.836564716\n",
      "Step: 1057, Training Loss: 1.64059, Tokens/sec: 86261.24782092648\n",
      "Step: 1058, Training Loss: 1.51850, Tokens/sec: 86530.95144759922\n",
      "Step: 1059, Training Loss: 1.58421, Tokens/sec: 86847.9484345019\n",
      "Step: 1060, Training Loss: 1.63678, Tokens/sec: 83764.19866877142\n",
      "Step: 1061, Training Loss: 1.81355, Tokens/sec: 86888.15201249666\n",
      "Step: 1062, Training Loss: 1.75279, Tokens/sec: 85974.83771716463\n",
      "Step: 1063, Training Loss: 1.56229, Tokens/sec: 83280.53282585331\n",
      "Step: 1064, Training Loss: 1.91602, Tokens/sec: 85470.7333252646\n",
      "Step: 1065, Training Loss: 1.71917, Tokens/sec: 86002.72795142431\n",
      "Step: 1066, Training Loss: 1.54056, Tokens/sec: 86384.7026544553\n",
      "Step: 1067, Training Loss: 1.56721, Tokens/sec: 86290.50173476315\n",
      "Step: 1068, Training Loss: 1.47086, Tokens/sec: 83339.91547492561\n",
      "Step: 1069, Training Loss: 1.68108, Tokens/sec: 85482.45302100634\n",
      "Step: 1070, Training Loss: 1.62991, Tokens/sec: 86079.70444832827\n",
      "Step: 1071, Training Loss: 1.54007, Tokens/sec: 86591.50199265449\n",
      "Step: 1072, Training Loss: 1.45946, Tokens/sec: 86988.18431746769\n",
      "Step: 1073, Training Loss: 1.67535, Tokens/sec: 84286.80422630851\n",
      "Step: 1074, Training Loss: 1.85661, Tokens/sec: 85617.43293440645\n",
      "Step: 1075, Training Loss: 1.58555, Tokens/sec: 87009.65130816362\n",
      "Step: 1076, Training Loss: 1.68994, Tokens/sec: 82875.36985903597\n",
      "Step: 1077, Training Loss: 1.66366, Tokens/sec: 85686.5950645718\n",
      "Step: 1078, Training Loss: 1.88099, Tokens/sec: 84083.16857945544\n",
      "Step: 1079, Training Loss: 1.58422, Tokens/sec: 85500.10975824992\n",
      "Step: 1080, Training Loss: 1.48490, Tokens/sec: 84518.2084769986\n",
      "Step: 1081, Training Loss: 1.70528, Tokens/sec: 83708.1590444257\n",
      "Step: 1082, Training Loss: 1.83993, Tokens/sec: 85506.709764482\n",
      "Step: 1083, Training Loss: 1.76757, Tokens/sec: 86005.41955183978\n",
      "Step: 1084, Training Loss: 1.62669, Tokens/sec: 84789.88185971513\n",
      "Step: 1085, Training Loss: 1.62587, Tokens/sec: 86869.25644146929\n",
      "Step: 1086, Training Loss: 1.61062, Tokens/sec: 83230.86659135888\n",
      "Step: 1087, Training Loss: 1.70058, Tokens/sec: 85334.28445589782\n",
      "Step: 1088, Training Loss: 1.56764, Tokens/sec: 87320.70202853183\n",
      "Step: 1089, Training Loss: 1.65068, Tokens/sec: 85410.24214051466\n",
      "Step: 1090, Training Loss: 1.68836, Tokens/sec: 85310.42881370409\n",
      "Step: 1091, Training Loss: 1.64512, Tokens/sec: 83964.04678538762\n",
      "Step: 1092, Training Loss: 1.85336, Tokens/sec: 87210.88373054875\n",
      "Step: 1093, Training Loss: 1.60547, Tokens/sec: 86162.51606357761\n",
      "Step: 1094, Training Loss: 1.61447, Tokens/sec: 83610.07652802896\n",
      "Step: 1095, Training Loss: 1.68793, Tokens/sec: 86244.95871489709\n",
      "Step: 1096, Training Loss: 1.59559, Tokens/sec: 86766.84728297465\n",
      "Step: 1097, Training Loss: 1.48215, Tokens/sec: 84348.56254778137\n",
      "Step: 1098, Training Loss: 1.36422, Tokens/sec: 86169.52690773825\n",
      "Step: 1099, Training Loss: 1.55519, Tokens/sec: 85442.79757335043\n",
      "Step: 1100, Training Loss: 1.73502, Tokens/sec: 85745.53040875093\n",
      "Step: 1101, Training Loss: 1.66092, Tokens/sec: 87389.3125605608\n",
      "Step: 1102, Training Loss: 1.63822, Tokens/sec: 85361.79659998642\n",
      "Step: 1103, Training Loss: 2.42073, Tokens/sec: 87000.45924204409\n",
      "Step: 1104, Training Loss: 1.98898, Tokens/sec: 83830.19506475756\n",
      "Step: 1105, Training Loss: 1.74736, Tokens/sec: 85709.44019695431\n",
      "Step: 1106, Training Loss: 1.75408, Tokens/sec: 87127.9531630937\n",
      "Step: 1107, Training Loss: 1.86003, Tokens/sec: 82120.81619158367\n",
      "Step: 1108, Training Loss: 1.74987, Tokens/sec: 85294.33916244963\n",
      "Step: 1109, Training Loss: 1.78133, Tokens/sec: 86153.65025225989\n",
      "Step: 1110, Training Loss: 1.56542, Tokens/sec: 84395.90059156608\n",
      "Step: 1111, Training Loss: 1.68244, Tokens/sec: 85719.77190520184\n",
      "Step: 1112, Training Loss: 1.64872, Tokens/sec: 84148.83893723266\n",
      "Step: 1113, Training Loss: 1.75328, Tokens/sec: 86912.31185211768\n",
      "Step: 1114, Training Loss: 1.57049, Tokens/sec: 86831.48810245411\n",
      "Step: 1115, Training Loss: 1.72716, Tokens/sec: 84364.2860377027\n",
      "Step: 1116, Training Loss: 1.56652, Tokens/sec: 85359.849567513\n",
      "Step: 1117, Training Loss: 1.69755, Tokens/sec: 87567.70028408701\n",
      "Step: 1118, Training Loss: 1.71341, Tokens/sec: 85558.74497816977\n",
      "Step: 1119, Training Loss: 1.75355, Tokens/sec: 87679.55464652965\n",
      "Step: 1120, Training Loss: 1.68780, Tokens/sec: 85496.0394195184\n",
      "Step: 1121, Training Loss: 1.54704, Tokens/sec: 85419.67082070948\n",
      "Step: 1122, Training Loss: 1.69023, Tokens/sec: 87252.86686798024\n",
      "Step: 1123, Training Loss: 1.90916, Tokens/sec: 84630.95590742542\n",
      "Step: 1124, Training Loss: 1.64758, Tokens/sec: 86417.39773126543\n",
      "Step: 1125, Training Loss: 1.58050, Tokens/sec: 83686.54000917003\n",
      "Step: 1126, Training Loss: 2.16184, Tokens/sec: 87329.28228548402\n",
      "Step: 1127, Training Loss: 1.60340, Tokens/sec: 87798.56522710968\n",
      "Step: 1128, Training Loss: 1.70004, Tokens/sec: 84665.23030107286\n",
      "Step: 1129, Training Loss: 1.82362, Tokens/sec: 85920.49565131475\n",
      "Step: 1130, Training Loss: 1.59382, Tokens/sec: 86371.87824396473\n",
      "Step: 1131, Training Loss: 1.60995, Tokens/sec: 86987.44536880378\n",
      "Step: 1132, Training Loss: 1.63870, Tokens/sec: 85713.3116060498\n",
      "Step: 1133, Training Loss: 1.52462, Tokens/sec: 84185.80409208499\n",
      "Step: 1134, Training Loss: 1.58794, Tokens/sec: 83622.10452461669\n",
      "Step: 1135, Training Loss: 1.62047, Tokens/sec: 85610.18507731998\n",
      "Step: 1136, Training Loss: 1.63286, Tokens/sec: 84278.74462397928\n",
      "Step: 1137, Training Loss: 1.65884, Tokens/sec: 86440.30452978035\n",
      "Step: 1138, Training Loss: 1.60743, Tokens/sec: 83647.4473491427\n",
      "Step: 1139, Training Loss: 1.45981, Tokens/sec: 85416.34957152295\n",
      "Step: 1140, Training Loss: 1.58371, Tokens/sec: 87650.07125657106\n",
      "Step: 1141, Training Loss: 1.72638, Tokens/sec: 85454.56131112823\n",
      "Step: 1142, Training Loss: 1.88521, Tokens/sec: 86276.06243266341\n",
      "Step: 1143, Training Loss: 1.54434, Tokens/sec: 87666.50991933553\n",
      "Step: 1144, Training Loss: 1.50813, Tokens/sec: 87336.86233531959\n",
      "Step: 1145, Training Loss: 1.52461, Tokens/sec: 87103.97569180864\n",
      "Step: 1146, Training Loss: 1.48248, Tokens/sec: 84241.26641293493\n",
      "Step: 1147, Training Loss: 1.55687, Tokens/sec: 85916.31581150748\n",
      "Step: 1148, Training Loss: 1.82860, Tokens/sec: 86160.9292509152\n",
      "Step: 1149, Training Loss: 1.81230, Tokens/sec: 83596.95363408168\n",
      "Step: 1150, Training Loss: 2.05049, Tokens/sec: 86419.54646194013\n",
      "Step: 1151, Training Loss: 1.54356, Tokens/sec: 84046.48533859158\n",
      "Step: 1152, Training Loss: 1.72851, Tokens/sec: 85589.79334210673\n",
      "Step: 1153, Training Loss: 1.57405, Tokens/sec: 87215.01128388254\n",
      "Step: 1154, Training Loss: 1.58164, Tokens/sec: 83714.3569643768\n",
      "Step: 1155, Training Loss: 1.82495, Tokens/sec: 85819.99618267869\n",
      "Step: 1156, Training Loss: 1.59744, Tokens/sec: 86529.96203616449\n",
      "Step: 1157, Training Loss: 1.64592, Tokens/sec: 84582.56264937857\n",
      "Step: 1158, Training Loss: 1.84890, Tokens/sec: 71029.82635173386\n",
      "Step: 1159, Training Loss: 1.49216, Tokens/sec: 82663.92781748633\n",
      "Step: 1160, Training Loss: 1.61795, Tokens/sec: 83505.63420923977\n",
      "Step: 1161, Training Loss: 1.53784, Tokens/sec: 72791.70443576174\n",
      "Step: 1162, Training Loss: 1.80213, Tokens/sec: 80321.6186629824\n",
      "Step: 1163, Training Loss: 1.58739, Tokens/sec: 86310.14207422623\n",
      "Step: 1164, Training Loss: 1.49903, Tokens/sec: 83403.54981603436\n",
      "Step: 1165, Training Loss: 1.69992, Tokens/sec: 85790.67582811453\n",
      "Step: 1166, Training Loss: 1.54180, Tokens/sec: 81602.01449353938\n",
      "Step: 1167, Training Loss: 1.63447, Tokens/sec: 69357.90874442046\n",
      "Step: 1168, Training Loss: 1.60177, Tokens/sec: 81405.03096071613\n",
      "Step: 1169, Training Loss: 1.50741, Tokens/sec: 86358.13270095745\n",
      "Step: 1170, Training Loss: 1.68520, Tokens/sec: 63307.60521543121\n",
      "Step: 1171, Training Loss: 1.67420, Tokens/sec: 85628.60350336152\n",
      "Step: 1172, Training Loss: 1.84392, Tokens/sec: 84401.89990044925\n",
      "Step: 1173, Training Loss: 1.59007, Tokens/sec: 79156.7720496357\n",
      "Step: 1174, Training Loss: 1.52908, Tokens/sec: 76930.54343879934\n",
      "Step: 1175, Training Loss: 1.52203, Tokens/sec: 86060.19279261433\n",
      "Step: 1176, Training Loss: 1.68339, Tokens/sec: 81658.01048023674\n",
      "Step: 1177, Training Loss: 1.65085, Tokens/sec: 82168.70659860151\n",
      "Step: 1178, Training Loss: 1.51748, Tokens/sec: 81012.1883541542\n",
      "Step: 1179, Training Loss: 1.61114, Tokens/sec: 85396.24323585615\n",
      "Step: 1180, Training Loss: 1.53413, Tokens/sec: 84313.64325524241\n",
      "Step: 1181, Training Loss: 1.51657, Tokens/sec: 82590.62722156255\n",
      "Step: 1182, Training Loss: 1.56473, Tokens/sec: 85423.12683099364\n",
      "Step: 1183, Training Loss: 1.60343, Tokens/sec: 83918.18341886096\n",
      "Step: 1184, Training Loss: 1.57915, Tokens/sec: 60530.028177240325\n",
      "Step: 1185, Training Loss: 1.72054, Tokens/sec: 65837.09462916646\n",
      "Step: 1186, Training Loss: 1.62860, Tokens/sec: 85593.976352025\n",
      "Step: 1187, Training Loss: 1.65549, Tokens/sec: 85923.00456679327\n",
      "Step: 1188, Training Loss: 1.60668, Tokens/sec: 86557.67176277767\n",
      "Step: 1189, Training Loss: 1.74800, Tokens/sec: 85251.01992009986\n",
      "Step: 1190, Training Loss: 1.84433, Tokens/sec: 86076.50668461542\n",
      "Step: 1191, Training Loss: 1.61650, Tokens/sec: 87261.12938271626\n",
      "Step: 1192, Training Loss: 1.58080, Tokens/sec: 83923.50499561845\n",
      "Step: 1193, Training Loss: 1.74896, Tokens/sec: 86020.79856289661\n",
      "Step: 1194, Training Loss: 1.66705, Tokens/sec: 85182.62530463935\n",
      "Step: 1195, Training Loss: 1.65966, Tokens/sec: 84067.11920058847\n",
      "Step: 1196, Training Loss: 1.61014, Tokens/sec: 85288.62210456218\n",
      "Step: 1197, Training Loss: 1.92551, Tokens/sec: 83208.01936625004\n",
      "Step: 1198, Training Loss: 1.52663, Tokens/sec: 86423.87160530091\n",
      "Step: 1199, Training Loss: 1.69671, Tokens/sec: 86813.17421564095\n",
      "Step: 1200, Training Loss: 1.57296, Tokens/sec: 83722.3851359979\n",
      "Step: 1201, Training Loss: 1.75213, Tokens/sec: 86345.10913351078\n",
      "Step: 1202, Training Loss: 1.64664, Tokens/sec: 83948.1624213632\n",
      "Step: 1203, Training Loss: 1.56263, Tokens/sec: 82835.40536593938\n",
      "Step: 1204, Training Loss: 1.72123, Tokens/sec: 86746.3564826081\n",
      "Step: 1205, Training Loss: 1.49111, Tokens/sec: 83607.94662096507\n",
      "Step: 1206, Training Loss: 1.37041, Tokens/sec: 85959.32630709227\n",
      "Step: 1207, Training Loss: 1.38534, Tokens/sec: 85734.83627017523\n",
      "Step: 1208, Training Loss: 1.52620, Tokens/sec: 84227.7762127569\n",
      "Step: 1209, Training Loss: 1.54349, Tokens/sec: 86186.63261972717\n",
      "Step: 1210, Training Loss: 1.53203, Tokens/sec: 85755.43408514526\n",
      "Step: 1211, Training Loss: 1.60574, Tokens/sec: 86543.31205599308\n",
      "Step: 1212, Training Loss: 1.56101, Tokens/sec: 86462.89905660415\n",
      "Step: 1213, Training Loss: 1.68893, Tokens/sec: 83787.98354837579\n",
      "Step: 1214, Training Loss: 1.64066, Tokens/sec: 84804.76423116455\n",
      "Step: 1215, Training Loss: 1.67630, Tokens/sec: 86899.24136801058\n",
      "Step: 1216, Training Loss: 1.72597, Tokens/sec: 84455.92556135688\n",
      "Step: 1217, Training Loss: 1.65784, Tokens/sec: 86014.74889158798\n",
      "Step: 1218, Training Loss: 1.52992, Tokens/sec: 85073.61201547722\n",
      "Step: 1219, Training Loss: 1.47992, Tokens/sec: 86362.01376850094\n",
      "Step: 1220, Training Loss: 1.36444, Tokens/sec: 87347.86070372254\n",
      "Step: 1221, Training Loss: 1.79034, Tokens/sec: 82630.99217470558\n",
      "Step: 1222, Training Loss: 1.65773, Tokens/sec: 86269.77602820942\n",
      "Step: 1223, Training Loss: 1.62567, Tokens/sec: 86668.8910125811\n",
      "Step: 1224, Training Loss: 1.66890, Tokens/sec: 83461.53454385833\n",
      "Step: 1225, Training Loss: 1.73778, Tokens/sec: 85848.02443286614\n",
      "Step: 1226, Training Loss: 1.51118, Tokens/sec: 83828.36745470036\n",
      "Step: 1227, Training Loss: 2.09008, Tokens/sec: 86970.47783711727\n",
      "Step: 1228, Training Loss: 1.59923, Tokens/sec: 86208.6687659789\n",
      "Step: 1229, Training Loss: 1.60266, Tokens/sec: 83738.08658547979\n",
      "Step: 1230, Training Loss: 1.74691, Tokens/sec: 86302.09048160864\n",
      "Step: 1231, Training Loss: 1.69893, Tokens/sec: 84329.64714618301\n",
      "Step: 1232, Training Loss: 1.68371, Tokens/sec: 84300.53672398314\n",
      "Step: 1233, Training Loss: 1.69007, Tokens/sec: 85737.49003650119\n",
      "Step: 1234, Training Loss: 1.55186, Tokens/sec: 84148.06013202324\n",
      "Step: 1235, Training Loss: 1.71309, Tokens/sec: 87096.61473480874\n",
      "Step: 1236, Training Loss: 1.53263, Tokens/sec: 86309.79197043405\n",
      "Step: 1237, Training Loss: 1.56839, Tokens/sec: 83944.24194206501\n",
      "Step: 1238, Training Loss: 1.60394, Tokens/sec: 86203.78412793885\n",
      "Step: 1239, Training Loss: 1.66522, Tokens/sec: 83221.67310197733\n",
      "Step: 1240, Training Loss: 2.12424, Tokens/sec: 85343.33539215606\n",
      "Step: 1241, Training Loss: 1.58883, Tokens/sec: 86059.16213457836\n",
      "Step: 1242, Training Loss: 1.54426, Tokens/sec: 83864.75532544743\n",
      "Step: 1243, Training Loss: 1.68543, Tokens/sec: 86755.6428464673\n",
      "Step: 1244, Training Loss: 1.65842, Tokens/sec: 85954.7246607076\n",
      "Step: 1245, Training Loss: 1.52960, Tokens/sec: 83445.45977369357\n",
      "Step: 1246, Training Loss: 1.66649, Tokens/sec: 86121.73792575784\n",
      "Step: 1247, Training Loss: 1.60290, Tokens/sec: 85031.72712313014\n",
      "Step: 1248, Training Loss: 1.56186, Tokens/sec: 85704.25330597408\n",
      "Step: 1249, Training Loss: 1.70188, Tokens/sec: 86664.22591936757\n",
      "Step: 1250, Training Loss: 1.70109, Tokens/sec: 84387.97267505722\n",
      "Step: 1251, Training Loss: 1.51645, Tokens/sec: 86556.41789194116\n",
      "Step: 1252, Training Loss: 1.47758, Tokens/sec: 85079.29100331094\n",
      "Step: 1253, Training Loss: 1.65552, Tokens/sec: 85489.72699228855\n",
      "Step: 1254, Training Loss: 1.50108, Tokens/sec: 86630.21242886099\n",
      "Step: 1255, Training Loss: 1.40399, Tokens/sec: 84708.68837860595\n",
      "Step: 1256, Training Loss: 1.41283, Tokens/sec: 85683.94534449393\n",
      "Step: 1257, Training Loss: 1.44924, Tokens/sec: 86602.96665238168\n",
      "Step: 1258, Training Loss: 1.46193, Tokens/sec: 84273.04326261501\n",
      "Step: 1259, Training Loss: 1.81669, Tokens/sec: 86140.7874661839\n",
      "Step: 1260, Training Loss: 1.55648, Tokens/sec: 85228.05759514205\n",
      "Step: 1261, Training Loss: 1.49292, Tokens/sec: 85868.39045433805\n",
      "Step: 1262, Training Loss: 1.56828, Tokens/sec: 86473.63594630724\n",
      "Step: 1263, Training Loss: 1.64329, Tokens/sec: 83506.21602173553\n",
      "Step: 1264, Training Loss: 1.73101, Tokens/sec: 85644.34318138642\n",
      "Step: 1265, Training Loss: 1.58175, Tokens/sec: 86658.5621035775\n",
      "Step: 1266, Training Loss: 1.46654, Tokens/sec: 83155.33149049417\n",
      "Step: 1267, Training Loss: 1.61308, Tokens/sec: 85747.62342209206\n",
      "Step: 1268, Training Loss: 1.54855, Tokens/sec: 84698.48203786377\n",
      "Step: 1269, Training Loss: 1.56830, Tokens/sec: 86781.05971729165\n",
      "Step: 1270, Training Loss: 1.80147, Tokens/sec: 86506.26730669916\n",
      "Step: 1271, Training Loss: 1.47201, Tokens/sec: 82899.3719348993\n",
      "Step: 1272, Training Loss: 1.55438, Tokens/sec: 85733.17589683895\n",
      "Step: 1273, Training Loss: 1.56851, Tokens/sec: 86565.78118078451\n",
      "Step: 1274, Training Loss: 1.54521, Tokens/sec: 84385.53130703703\n",
      "Step: 1275, Training Loss: 1.59428, Tokens/sec: 86116.0085433621\n",
      "Step: 1276, Training Loss: 1.57986, Tokens/sec: 84018.69456798253\n",
      "Step: 1277, Training Loss: 1.45893, Tokens/sec: 86096.83106159388\n",
      "Step: 1278, Training Loss: 1.68314, Tokens/sec: 86654.657542022\n",
      "Step: 1279, Training Loss: 1.53093, Tokens/sec: 83966.03436561348\n",
      "Step: 1280, Training Loss: 1.59601, Tokens/sec: 85800.25654094593\n",
      "Step: 1281, Training Loss: 1.59887, Tokens/sec: 84098.63354894392\n",
      "Step: 1282, Training Loss: 1.53292, Tokens/sec: 85835.81305365097\n",
      "Step: 1283, Training Loss: 1.43306, Tokens/sec: 86155.12534400536\n",
      "Step: 1284, Training Loss: 1.45686, Tokens/sec: 84017.5101642919\n",
      "Step: 1285, Training Loss: 2.02781, Tokens/sec: 86807.72409657273\n",
      "Step: 1286, Training Loss: 1.54280, Tokens/sec: 86024.37249283442\n",
      "Step: 1287, Training Loss: 1.68854, Tokens/sec: 85812.01330200488\n",
      "Step: 1288, Training Loss: 1.70447, Tokens/sec: 86183.59056256605\n",
      "Step: 1289, Training Loss: 1.61652, Tokens/sec: 86846.92874624579\n",
      "Step: 1290, Training Loss: 1.66804, Tokens/sec: 86726.67735529448\n",
      "Step: 1291, Training Loss: 1.44309, Tokens/sec: 85988.77786705509\n",
      "Step: 1292, Training Loss: 1.49057, Tokens/sec: 84800.65229515405\n",
      "Step: 1293, Training Loss: 1.53377, Tokens/sec: 86112.36680127535\n",
      "Step: 1294, Training Loss: 1.55637, Tokens/sec: 86663.78400908282\n",
      "Step: 1295, Training Loss: 1.67808, Tokens/sec: 84851.53043376113\n",
      "Step: 1296, Training Loss: 1.57288, Tokens/sec: 86397.82382412755\n",
      "Step: 1297, Training Loss: 1.67224, Tokens/sec: 85253.9858493238\n",
      "Step: 1298, Training Loss: 1.56211, Tokens/sec: 84873.91700629822\n",
      "Step: 1299, Training Loss: 1.66048, Tokens/sec: 85454.93258875674\n",
      "Step: 1300, Training Loss: 1.50362, Tokens/sec: 84205.83431680172\n",
      "Step: 1301, Training Loss: 1.55987, Tokens/sec: 86811.00447746675\n",
      "Step: 1302, Training Loss: 1.56890, Tokens/sec: 83465.9402869765\n",
      "Step: 1303, Training Loss: 1.62639, Tokens/sec: 83908.18987597022\n",
      "Step: 1304, Training Loss: 1.58815, Tokens/sec: 86716.80465253929\n",
      "Step: 1305, Training Loss: 1.57722, Tokens/sec: 83689.79179506401\n",
      "Step: 1306, Training Loss: 1.79113, Tokens/sec: 86105.62861911095\n",
      "Step: 1307, Training Loss: 1.41654, Tokens/sec: 85022.56125899467\n",
      "Step: 1308, Training Loss: 1.48920, Tokens/sec: 83813.0424443025\n",
      "Step: 1309, Training Loss: 1.44651, Tokens/sec: 86702.66693629666\n",
      "Step: 1310, Training Loss: 1.58429, Tokens/sec: 85921.49820850824\n",
      "Step: 1311, Training Loss: 1.48009, Tokens/sec: 84210.41421477198\n",
      "Step: 1312, Training Loss: 1.49624, Tokens/sec: 85736.1530377647\n",
      "Step: 1313, Training Loss: 1.35473, Tokens/sec: 83520.96408430433\n",
      "Step: 1314, Training Loss: 1.47048, Tokens/sec: 86287.60594361695\n",
      "Step: 1315, Training Loss: 1.51736, Tokens/sec: 85743.68205354403\n",
      "Step: 1316, Training Loss: 1.55141, Tokens/sec: 82976.50330730091\n",
      "Step: 1317, Training Loss: 1.64941, Tokens/sec: 86559.62763598279\n",
      "Step: 1318, Training Loss: 1.55212, Tokens/sec: 85971.06535224711\n",
      "Step: 1319, Training Loss: 1.76976, Tokens/sec: 85702.90389509583\n",
      "Step: 1320, Training Loss: 1.69538, Tokens/sec: 84567.7860932396\n",
      "Step: 1321, Training Loss: 1.67642, Tokens/sec: 84182.68882616863\n",
      "Step: 1322, Training Loss: 1.79480, Tokens/sec: 85376.82348570136\n",
      "Step: 1323, Training Loss: 1.57794, Tokens/sec: 85216.23468870438\n",
      "Step: 1324, Training Loss: 1.40831, Tokens/sec: 84096.96127082453\n",
      "Step: 1325, Training Loss: 1.56348, Tokens/sec: 86457.78710116184\n",
      "Step: 1326, Training Loss: 1.69791, Tokens/sec: 85138.0426269318\n",
      "Step: 1327, Training Loss: 1.52873, Tokens/sec: 84161.33199121416\n",
      "Step: 1328, Training Loss: 1.66202, Tokens/sec: 86657.22234627111\n",
      "Step: 1329, Training Loss: 1.48619, Tokens/sec: 84051.72661386263\n",
      "Step: 1330, Training Loss: 1.71006, Tokens/sec: 84763.47774529092\n",
      "Step: 1331, Training Loss: 1.48642, Tokens/sec: 86518.49933696621\n",
      "Step: 1332, Training Loss: 1.66194, Tokens/sec: 84104.23146497249\n",
      "Step: 1333, Training Loss: 1.61949, Tokens/sec: 86671.66298317116\n",
      "Step: 1334, Training Loss: 1.54774, Tokens/sec: 84588.84924920749\n",
      "Step: 1335, Training Loss: 1.60906, Tokens/sec: 86510.50566474524\n",
      "Step: 1336, Training Loss: 1.64336, Tokens/sec: 86376.12666924679\n",
      "Step: 1337, Training Loss: 1.50205, Tokens/sec: 82937.60165886155\n",
      "Step: 1338, Training Loss: 1.41155, Tokens/sec: 85380.65956611867\n",
      "Step: 1339, Training Loss: 1.54165, Tokens/sec: 86333.42967591951\n",
      "Step: 1340, Training Loss: 1.52801, Tokens/sec: 83927.94288101706\n",
      "Step: 1341, Training Loss: 1.47820, Tokens/sec: 85087.11736335688\n",
      "Step: 1342, Training Loss: 1.44598, Tokens/sec: 84115.5621114855\n",
      "Step: 1343, Training Loss: 1.53294, Tokens/sec: 86777.01588299668\n",
      "Step: 1344, Training Loss: 1.43084, Tokens/sec: 86435.19752828043\n",
      "Step: 1345, Training Loss: 1.69908, Tokens/sec: 82279.42036976849\n",
      "Step: 1346, Training Loss: 1.88888, Tokens/sec: 85901.40018521575\n",
      "Step: 1347, Training Loss: 1.63630, Tokens/sec: 86090.57843675507\n",
      "Step: 1348, Training Loss: 1.49014, Tokens/sec: 85324.46181275803\n",
      "Step: 1349, Training Loss: 1.56578, Tokens/sec: 85367.67470092836\n",
      "Step: 1350, Training Loss: 1.67386, Tokens/sec: 82636.18425907985\n",
      "Step: 1351, Training Loss: 1.52958, Tokens/sec: 86368.89685706432\n",
      "Step: 1352, Training Loss: 1.73430, Tokens/sec: 86108.55927569096\n",
      "Step: 1353, Training Loss: 1.67665, Tokens/sec: 82981.08242100316\n",
      "Step: 1354, Training Loss: 1.52025, Tokens/sec: 85950.95315836494\n",
      "Step: 1355, Training Loss: 1.49289, Tokens/sec: 84336.75185095317\n",
      "Step: 1356, Training Loss: 1.41370, Tokens/sec: 84591.51902155385\n",
      "Step: 1357, Training Loss: 1.61065, Tokens/sec: 85354.8934467861\n",
      "Step: 1358, Training Loss: 1.47945, Tokens/sec: 82623.99943980468\n",
      "Step: 1359, Training Loss: 1.62593, Tokens/sec: 86335.12338733645\n",
      "Step: 1360, Training Loss: 1.45827, Tokens/sec: 86536.85821139533\n",
      "Step: 1361, Training Loss: 1.46035, Tokens/sec: 82800.45951477667\n",
      "Step: 1362, Training Loss: 1.53020, Tokens/sec: 85659.7214106205\n",
      "Step: 1363, Training Loss: 1.36523, Tokens/sec: 84781.60774750425\n",
      "Step: 1364, Training Loss: 1.48086, Tokens/sec: 85795.86824281626\n",
      "Step: 1365, Training Loss: 1.61117, Tokens/sec: 85123.55387154497\n",
      "Step: 1366, Training Loss: 1.49904, Tokens/sec: 83391.05235248411\n",
      "Step: 1367, Training Loss: 1.55933, Tokens/sec: 86356.00658885064\n",
      "Step: 1368, Training Loss: 1.79659, Tokens/sec: 86492.09861144108\n",
      "Step: 1369, Training Loss: 1.54564, Tokens/sec: 82048.21551625719\n",
      "Step: 1370, Training Loss: 1.56671, Tokens/sec: 86679.42369985349\n",
      "Step: 1371, Training Loss: 1.55079, Tokens/sec: 86585.60515637296\n",
      "Step: 1372, Training Loss: 1.46154, Tokens/sec: 86822.39023414394\n",
      "Step: 1373, Training Loss: 1.54197, Tokens/sec: 85031.28449732314\n",
      "Step: 1374, Training Loss: 1.41826, Tokens/sec: 83862.17029182984\n",
      "Step: 1375, Training Loss: 1.61478, Tokens/sec: 86674.93033963017\n",
      "Step: 1376, Training Loss: 1.60871, Tokens/sec: 85758.57302478545\n",
      "Step: 1377, Training Loss: 1.50746, Tokens/sec: 82977.64593542546\n",
      "Step: 1378, Training Loss: 1.54668, Tokens/sec: 86812.88165824172\n",
      "Step: 1379, Training Loss: 1.55195, Tokens/sec: 85651.54980524576\n",
      "Step: 1380, Training Loss: 1.61505, Tokens/sec: 85512.36594233477\n",
      "Step: 1381, Training Loss: 1.78158, Tokens/sec: 84914.91024673829\n",
      "Step: 1382, Training Loss: 1.57060, Tokens/sec: 83772.94786495996\n",
      "Step: 1383, Training Loss: 1.50011, Tokens/sec: 85737.96517519647\n",
      "Step: 1384, Training Loss: 1.50942, Tokens/sec: 85575.15803472\n",
      "Step: 1385, Training Loss: 1.57273, Tokens/sec: 84759.03875458962\n",
      "Step: 1386, Training Loss: 1.40441, Tokens/sec: 86510.62306087991\n",
      "Step: 1387, Training Loss: 1.31363, Tokens/sec: 83555.17988438737\n",
      "Step: 1388, Training Loss: 1.31063, Tokens/sec: 85576.75865391015\n",
      "Step: 1389, Training Loss: 1.56674, Tokens/sec: 85556.45297442781\n",
      "Step: 1390, Training Loss: 1.61432, Tokens/sec: 83169.12499118752\n",
      "Step: 1391, Training Loss: 1.53571, Tokens/sec: 86081.30454963907\n",
      "Step: 1392, Training Loss: 1.58582, Tokens/sec: 84101.346724054\n",
      "Step: 1393, Training Loss: 1.59299, Tokens/sec: 85084.33226259677\n",
      "Step: 1394, Training Loss: 1.49970, Tokens/sec: 85756.47319000431\n",
      "Step: 1395, Training Loss: 1.48512, Tokens/sec: 84195.86127935127\n",
      "Step: 1396, Training Loss: 1.57094, Tokens/sec: 85994.0922297109\n",
      "Step: 1397, Training Loss: 1.54150, Tokens/sec: 85346.94617161277\n",
      "Step: 1398, Training Loss: 1.53294, Tokens/sec: 83494.29962947841\n",
      "Step: 1399, Training Loss: 1.46991, Tokens/sec: 86523.80444318039\n",
      "Step: 1400, Training Loss: 1.56602, Tokens/sec: 84499.26311325705\n",
      "Step: 1401, Training Loss: 1.59151, Tokens/sec: 85875.64518997184\n",
      "Step: 1402, Training Loss: 1.64568, Tokens/sec: 86696.85495779589\n",
      "Step: 1403, Training Loss: 1.55751, Tokens/sec: 84810.43020739034\n",
      "Step: 1404, Training Loss: 1.55741, Tokens/sec: 85486.21206536125\n",
      "Step: 1405, Training Loss: 1.56757, Tokens/sec: 86352.25804527929\n",
      "Step: 1406, Training Loss: 1.51296, Tokens/sec: 84135.92789817399\n",
      "Step: 1407, Training Loss: 2.03270, Tokens/sec: 86712.11604492867\n",
      "Step: 1408, Training Loss: 1.57433, Tokens/sec: 85021.01439381068\n",
      "Step: 1409, Training Loss: 1.51735, Tokens/sec: 86225.66613512344\n",
      "Step: 1410, Training Loss: 1.52550, Tokens/sec: 86460.00993172674\n",
      "Step: 1411, Training Loss: 1.49973, Tokens/sec: 83367.10284449975\n",
      "Step: 1412, Training Loss: 1.45901, Tokens/sec: 84126.58868481092\n",
      "Step: 1413, Training Loss: 1.56359, Tokens/sec: 86608.06603402148\n",
      "Step: 1414, Training Loss: 1.51418, Tokens/sec: 83294.8019481539\n",
      "Step: 1415, Training Loss: 1.59744, Tokens/sec: 86696.91046756676\n",
      "Step: 1416, Training Loss: 1.57279, Tokens/sec: 83718.29535434821\n",
      "Step: 1417, Training Loss: 1.83122, Tokens/sec: 86642.16018390833\n",
      "Step: 1418, Training Loss: 1.73557, Tokens/sec: 86880.68328216564\n",
      "Step: 1419, Training Loss: 1.39326, Tokens/sec: 82423.93458087306\n",
      "Step: 1420, Training Loss: 1.44002, Tokens/sec: 85550.62925958469\n",
      "Step: 1421, Training Loss: 1.60875, Tokens/sec: 87062.38735895763\n",
      "Step: 1422, Training Loss: 1.48869, Tokens/sec: 82672.9655279914\n",
      "Step: 1423, Training Loss: 1.41635, Tokens/sec: 86116.75992463091\n",
      "Step: 1424, Training Loss: 1.37375, Tokens/sec: 83413.4341270612\n",
      "Step: 1425, Training Loss: 1.42515, Tokens/sec: 86145.50824775505\n",
      "Step: 1426, Training Loss: 1.74277, Tokens/sec: 86442.9501599736\n",
      "Step: 1427, Training Loss: 1.54498, Tokens/sec: 82942.46410338342\n",
      "Step: 1428, Training Loss: 1.60251, Tokens/sec: 85027.95631398424\n",
      "Step: 1429, Training Loss: 1.82823, Tokens/sec: 85851.49179923399\n",
      "Step: 1430, Training Loss: 1.53086, Tokens/sec: 83906.28925187499\n",
      "Step: 1431, Training Loss: 1.78428, Tokens/sec: 86437.20441182237\n",
      "Step: 1432, Training Loss: 1.52289, Tokens/sec: 83468.30321875181\n",
      "Step: 1433, Training Loss: 1.48640, Tokens/sec: 85639.06613768279\n",
      "Step: 1434, Training Loss: 1.48778, Tokens/sec: 87028.61032259035\n",
      "Step: 1435, Training Loss: 1.52588, Tokens/sec: 83539.2240721282\n",
      "Step: 1436, Training Loss: 1.76105, Tokens/sec: 85257.6139148784\n",
      "Step: 1437, Training Loss: 1.51494, Tokens/sec: 85156.72272570636\n",
      "Step: 1438, Training Loss: 1.47185, Tokens/sec: 87274.8198835359\n",
      "Step: 1439, Training Loss: 1.41937, Tokens/sec: 85884.74966028164\n",
      "Step: 1440, Training Loss: 1.49762, Tokens/sec: 83009.52071046179\n",
      "Step: 1441, Training Loss: 1.60264, Tokens/sec: 86083.80793465838\n",
      "Step: 1442, Training Loss: 1.52806, Tokens/sec: 86505.6333448002\n",
      "Step: 1443, Training Loss: 1.53327, Tokens/sec: 83307.68315411771\n",
      "Step: 1444, Training Loss: 1.69407, Tokens/sec: 86186.60133434717\n",
      "Step: 1445, Training Loss: 1.57784, Tokens/sec: 85153.43915547278\n",
      "Step: 1446, Training Loss: 1.49104, Tokens/sec: 86411.51637607922\n",
      "Step: 1447, Training Loss: 1.45094, Tokens/sec: 85451.75879075385\n",
      "Step: 1448, Training Loss: 1.44820, Tokens/sec: 84119.17857817076\n",
      "Step: 1449, Training Loss: 1.44332, Tokens/sec: 86091.70665998287\n",
      "Step: 1450, Training Loss: 1.56812, Tokens/sec: 86100.65112696901\n",
      "Step: 1451, Training Loss: 1.50585, Tokens/sec: 84684.6147826191\n",
      "Step: 1452, Training Loss: 1.55349, Tokens/sec: 86927.79048477308\n",
      "Step: 1453, Training Loss: 1.51819, Tokens/sec: 83923.89919897352\n",
      "Step: 1454, Training Loss: 1.46758, Tokens/sec: 86967.85060047616\n",
      "Step: 1455, Training Loss: 1.43053, Tokens/sec: 85731.32538469194\n",
      "Step: 1456, Training Loss: 1.64261, Tokens/sec: 83911.48168334746\n",
      "Step: 1457, Training Loss: 1.40881, Tokens/sec: 85983.61622713102\n",
      "Step: 1458, Training Loss: 1.40305, Tokens/sec: 84057.28421692036\n",
      "Step: 1459, Training Loss: 1.54307, Tokens/sec: 85012.7409894452\n",
      "Step: 1460, Training Loss: 1.44030, Tokens/sec: 86090.43549122446\n",
      "Step: 1461, Training Loss: 1.59866, Tokens/sec: 83122.79937502398\n",
      "Step: 1462, Training Loss: 1.42374, Tokens/sec: 85877.89445445774\n",
      "Step: 1463, Training Loss: 1.61927, Tokens/sec: 85912.20394648581\n",
      "Step: 1464, Training Loss: 1.38803, Tokens/sec: 83354.52130944857\n",
      "Step: 1465, Training Loss: 1.50944, Tokens/sec: 85830.78756670802\n",
      "Step: 1466, Training Loss: 1.42636, Tokens/sec: 84229.93522137389\n",
      "Step: 1467, Training Loss: 1.38384, Tokens/sec: 84736.16974066742\n",
      "Step: 1468, Training Loss: 1.74845, Tokens/sec: 85810.83262750474\n",
      "Step: 1469, Training Loss: 1.64098, Tokens/sec: 84432.29475082415\n",
      "Step: 1470, Training Loss: 1.54110, Tokens/sec: 85724.82611401494\n",
      "Step: 1471, Training Loss: 1.60251, Tokens/sec: 86426.95207464343\n",
      "Step: 1472, Training Loss: 1.42518, Tokens/sec: 83743.91739592011\n",
      "Step: 1473, Training Loss: 1.38463, Tokens/sec: 86739.72766468162\n",
      "Step: 1474, Training Loss: 1.45683, Tokens/sec: 84351.02218587954\n",
      "Step: 1475, Training Loss: 1.53550, Tokens/sec: 86157.13011797627\n",
      "Step: 1476, Training Loss: 1.51169, Tokens/sec: 87037.81758016274\n",
      "Step: 1477, Training Loss: 1.40464, Tokens/sec: 84660.72809928367\n",
      "Step: 1478, Training Loss: 1.47470, Tokens/sec: 85862.41350368573\n",
      "Step: 1479, Training Loss: 1.52480, Tokens/sec: 86775.17058601472\n",
      "Step: 1480, Training Loss: 1.60826, Tokens/sec: 85936.91708880407\n",
      "Step: 1481, Training Loss: 1.49542, Tokens/sec: 86976.42583042863\n",
      "Step: 1482, Training Loss: 1.55538, Tokens/sec: 83060.81706473227\n",
      "Step: 1483, Training Loss: 1.44980, Tokens/sec: 86807.50562950876\n",
      "Step: 1484, Training Loss: 1.55234, Tokens/sec: 86902.65451023578\n",
      "Step: 1485, Training Loss: 1.43829, Tokens/sec: 84210.63625366325\n",
      "Step: 1486, Training Loss: 1.45037, Tokens/sec: 84654.81004075857\n",
      "Step: 1487, Training Loss: 1.44374, Tokens/sec: 86435.53359717206\n",
      "Step: 1488, Training Loss: 1.45844, Tokens/sec: 85806.69894985353\n",
      "Step: 1489, Training Loss: 1.39284, Tokens/sec: 85830.83387909915\n",
      "Step: 1490, Training Loss: 1.86730, Tokens/sec: 83034.71825950648\n",
      "Step: 1491, Training Loss: 1.51682, Tokens/sec: 86689.2544087524\n",
      "Step: 1492, Training Loss: 1.55253, Tokens/sec: 86210.21741726929\n",
      "Step: 1493, Training Loss: 1.59607, Tokens/sec: 82947.44385778428\n",
      "Step: 1494, Training Loss: 1.54272, Tokens/sec: 85617.05174381786\n",
      "Step: 1495, Training Loss: 1.44365, Tokens/sec: 84886.71998278951\n",
      "Step: 1496, Training Loss: 1.47970, Tokens/sec: 85937.80553356354\n",
      "Step: 1497, Training Loss: 1.44669, Tokens/sec: 83824.45043951929\n",
      "Step: 1498, Training Loss: 1.35594, Tokens/sec: 83638.36484308499\n",
      "Step: 1499, Training Loss: 1.62296, Tokens/sec: 86190.53093139638\n",
      "Step: 1500, Training Loss: 1.44366, Tokens/sec: 80717.26751213321\n",
      "Step: 1501, Training Loss: 1.39306, Tokens/sec: 83713.77438909427\n",
      "Step: 1502, Training Loss: 1.60163, Tokens/sec: 86113.49196988778\n",
      "Step: 1503, Training Loss: 1.56832, Tokens/sec: 86553.36476933127\n",
      "Step: 1504, Training Loss: 1.63784, Tokens/sec: 84596.59393607012\n",
      "Step: 1505, Training Loss: 1.54429, Tokens/sec: 85829.83164052015\n",
      "Step: 1506, Training Loss: 1.54492, Tokens/sec: 83365.69495196787\n",
      "Step: 1507, Training Loss: 1.51180, Tokens/sec: 86233.76338474342\n",
      "Step: 1508, Training Loss: 1.65183, Tokens/sec: 86654.20793624723\n",
      "Step: 1509, Training Loss: 1.63271, Tokens/sec: 83690.83317054117\n",
      "Step: 1510, Training Loss: 1.73634, Tokens/sec: 85829.12617381169\n",
      "Step: 1511, Training Loss: 1.58435, Tokens/sec: 85951.17815528117\n",
      "Step: 1512, Training Loss: 1.72333, Tokens/sec: 86654.84499140651\n",
      "Step: 1513, Training Loss: 1.44854, Tokens/sec: 85525.40245286375\n",
      "Step: 1514, Training Loss: 1.56028, Tokens/sec: 83036.07459504437\n",
      "Step: 1515, Training Loss: 1.62571, Tokens/sec: 86424.25089909056\n",
      "Step: 1516, Training Loss: 1.32715, Tokens/sec: 86183.5270957505\n",
      "Step: 1517, Training Loss: 1.51390, Tokens/sec: 82918.20290685278\n",
      "Step: 1518, Training Loss: 1.32343, Tokens/sec: 86423.85929613716\n",
      "Step: 1519, Training Loss: 1.57480, Tokens/sec: 83877.57425004023\n",
      "Step: 1520, Training Loss: 1.57168, Tokens/sec: 86739.05078695038\n",
      "Step: 1521, Training Loss: 2.09967, Tokens/sec: 86142.84592070326\n",
      "Step: 1522, Training Loss: 1.47597, Tokens/sec: 83590.71508674986\n",
      "Step: 1523, Training Loss: 1.37012, Tokens/sec: 86723.64296451148\n",
      "Step: 1524, Training Loss: 1.49405, Tokens/sec: 86725.00175744364\n",
      "Step: 1525, Training Loss: 1.44831, Tokens/sec: 82799.07529917575\n",
      "Step: 1526, Training Loss: 1.44084, Tokens/sec: 86721.89908693802\n",
      "Step: 1527, Training Loss: 1.42566, Tokens/sec: 84502.7261116736\n",
      "Step: 1528, Training Loss: 1.45313, Tokens/sec: 85249.78499397081\n",
      "Step: 1529, Training Loss: 1.68790, Tokens/sec: 85256.60771336724\n",
      "Step: 1530, Training Loss: 1.51873, Tokens/sec: 84058.15880688347\n",
      "Step: 1531, Training Loss: 1.48261, Tokens/sec: 86480.48891021934\n",
      "Step: 1532, Training Loss: 1.53192, Tokens/sec: 85885.99269974837\n",
      "Step: 1533, Training Loss: 1.55370, Tokens/sec: 85467.58822694972\n",
      "Step: 1534, Training Loss: 1.65825, Tokens/sec: 86924.89880420771\n",
      "Step: 1535, Training Loss: 1.47775, Tokens/sec: 83372.34796828944\n",
      "Step: 1536, Training Loss: 1.90754, Tokens/sec: 87072.86921949487\n",
      "Step: 1537, Training Loss: 1.60093, Tokens/sec: 86147.16288876826\n",
      "Step: 1538, Training Loss: 1.47845, Tokens/sec: 84097.63768621364\n",
      "Step: 1539, Training Loss: 1.52189, Tokens/sec: 85665.16358933736\n",
      "Step: 1540, Training Loss: 1.48540, Tokens/sec: 84297.50100588823\n",
      "Step: 1541, Training Loss: 1.41737, Tokens/sec: 86305.80466498857\n",
      "Step: 1542, Training Loss: 1.33047, Tokens/sec: 86321.65150351245\n",
      "Step: 1543, Training Loss: 1.50448, Tokens/sec: 84115.44810291672\n",
      "Step: 1544, Training Loss: 1.46922, Tokens/sec: 85698.55020302391\n",
      "Step: 1545, Training Loss: 1.62186, Tokens/sec: 86231.06473796112\n",
      "Step: 1546, Training Loss: 1.75372, Tokens/sec: 83343.03905328561\n",
      "Step: 1547, Training Loss: 1.76222, Tokens/sec: 87044.17480177175\n",
      "Step: 1548, Training Loss: 1.53860, Tokens/sec: 83798.365073914\n",
      "Step: 1549, Training Loss: 1.38238, Tokens/sec: 85219.75759128\n",
      "Step: 1550, Training Loss: 1.52051, Tokens/sec: 86220.11530077783\n",
      "Step: 1551, Training Loss: 1.50019, Tokens/sec: 83478.88256658516\n",
      "Step: 1552, Training Loss: 1.43276, Tokens/sec: 85689.1835544231\n",
      "Step: 1553, Training Loss: 1.45499, Tokens/sec: 85795.3798835125\n",
      "Step: 1554, Training Loss: 1.44692, Tokens/sec: 83596.70837485997\n",
      "Step: 1555, Training Loss: 1.66894, Tokens/sec: 86685.45990649202\n",
      "Step: 1556, Training Loss: 1.50562, Tokens/sec: 82908.34876836903\n",
      "Step: 1557, Training Loss: 1.47677, Tokens/sec: 85775.2752518344\n",
      "Step: 1558, Training Loss: 1.63949, Tokens/sec: 86630.69201446915\n",
      "Step: 1559, Training Loss: 1.48115, Tokens/sec: 83548.93435821333\n",
      "Step: 1560, Training Loss: 1.57857, Tokens/sec: 86345.84949622082\n",
      "Step: 1561, Training Loss: 1.51924, Tokens/sec: 87286.9293785633\n",
      "Step: 1562, Training Loss: 1.54083, Tokens/sec: 85750.67875883407\n",
      "Step: 1563, Training Loss: 1.52126, Tokens/sec: 86743.51727250894\n",
      "Step: 1564, Training Loss: 1.48116, Tokens/sec: 82843.51716291561\n",
      "Step: 1565, Training Loss: 1.49054, Tokens/sec: 86728.14367511029\n",
      "Step: 1566, Training Loss: 1.71698, Tokens/sec: 86341.44525471705\n",
      "Step: 1567, Training Loss: 1.47620, Tokens/sec: 83425.65241079127\n",
      "Step: 1568, Training Loss: 1.41271, Tokens/sec: 86007.26882151769\n",
      "Step: 1569, Training Loss: 1.65198, Tokens/sec: 87045.80402564764\n",
      "Step: 1570, Training Loss: 1.46153, Tokens/sec: 85981.14483179433\n",
      "Step: 1571, Training Loss: 1.51681, Tokens/sec: 86132.67642075004\n",
      "Step: 1572, Training Loss: 1.51715, Tokens/sec: 83326.78528442468\n",
      "Step: 1573, Training Loss: 1.48911, Tokens/sec: 86773.11304892754\n",
      "Step: 1574, Training Loss: 1.60955, Tokens/sec: 85904.96645521198\n",
      "Step: 1575, Training Loss: 1.42287, Tokens/sec: 82424.97910748237\n",
      "Step: 1576, Training Loss: 1.48870, Tokens/sec: 85740.7936795735\n",
      "Step: 1577, Training Loss: 1.45065, Tokens/sec: 86825.10255194869\n",
      "Step: 1578, Training Loss: 1.52634, Tokens/sec: 86552.14348944063\n",
      "Step: 1579, Training Loss: 1.69943, Tokens/sec: 85664.95934768877\n",
      "Step: 1580, Training Loss: 1.50910, Tokens/sec: 82836.05451793365\n",
      "Step: 1581, Training Loss: 1.48340, Tokens/sec: 86071.55109428152\n",
      "Step: 1582, Training Loss: 1.43597, Tokens/sec: 87149.6718436346\n",
      "Step: 1583, Training Loss: 1.58555, Tokens/sec: 82166.48754864409\n",
      "Step: 1584, Training Loss: 1.52694, Tokens/sec: 86425.07012201477\n",
      "Step: 1585, Training Loss: 1.47700, Tokens/sec: 83652.55098854075\n",
      "Step: 1586, Training Loss: 1.72253, Tokens/sec: 86520.20442993319\n",
      "Step: 1587, Training Loss: 1.59652, Tokens/sec: 85632.04242764847\n",
      "Step: 1588, Training Loss: 1.42410, Tokens/sec: 83465.43727304267\n",
      "Step: 1589, Training Loss: 1.39373, Tokens/sec: 86349.51420245713\n",
      "Step: 1590, Training Loss: 1.44144, Tokens/sec: 86710.7902343633\n",
      "Step: 1591, Training Loss: 1.54806, Tokens/sec: 82469.49299272822\n",
      "Step: 1592, Training Loss: 1.54632, Tokens/sec: 84573.90372597123\n",
      "Step: 1593, Training Loss: 1.50520, Tokens/sec: 84669.23897618626\n",
      "Step: 1594, Training Loss: 1.50889, Tokens/sec: 86258.00658238266\n",
      "Step: 1595, Training Loss: 1.65721, Tokens/sec: 84880.66123157114\n",
      "Step: 1596, Training Loss: 1.72547, Tokens/sec: 82821.98567308886\n",
      "Step: 1597, Training Loss: 1.63547, Tokens/sec: 86499.67741601521\n",
      "Step: 1598, Training Loss: 1.55532, Tokens/sec: 86671.14122110853\n",
      "Step: 1599, Training Loss: 1.62379, Tokens/sec: 84193.86453202517\n",
      "Step: 1600, Training Loss: 1.77266, Tokens/sec: 86937.20799936056\n",
      "Step: 1601, Training Loss: 1.83617, Tokens/sec: 84545.68503558914\n",
      "Step: 1602, Training Loss: 1.50827, Tokens/sec: 86251.82224816018\n",
      "Step: 1603, Training Loss: 1.58998, Tokens/sec: 85221.95489553886\n",
      "Step: 1604, Training Loss: 2.21386, Tokens/sec: 86243.65941571214\n",
      "Step: 1605, Training Loss: 1.50284, Tokens/sec: 86694.86535829907\n",
      "Step: 1606, Training Loss: 1.47459, Tokens/sec: 85722.36688372682\n",
      "Step: 1607, Training Loss: 1.55821, Tokens/sec: 86245.88850241393\n",
      "Step: 1608, Training Loss: 1.49631, Tokens/sec: 86894.60030023816\n",
      "Step: 1609, Training Loss: 1.56608, Tokens/sec: 83533.50818305161\n",
      "Step: 1610, Training Loss: 1.42633, Tokens/sec: 85794.8075175066\n",
      "Step: 1611, Training Loss: 1.50100, Tokens/sec: 85565.61098693665\n",
      "Step: 1612, Training Loss: 1.58703, Tokens/sec: 86277.68529007593\n",
      "Step: 1613, Training Loss: 1.50559, Tokens/sec: 86199.1884673201\n",
      "Step: 1614, Training Loss: 1.59482, Tokens/sec: 84295.6221743974\n",
      "Step: 1615, Training Loss: 1.62994, Tokens/sec: 85265.95590918283\n",
      "Step: 1616, Training Loss: 1.51463, Tokens/sec: 86333.93646331572\n",
      "Step: 1617, Training Loss: 1.42993, Tokens/sec: 83246.20148004078\n",
      "Step: 1618, Training Loss: 1.45663, Tokens/sec: 85953.96122185481\n",
      "Step: 1619, Training Loss: 1.40780, Tokens/sec: 83429.35933475637\n",
      "Step: 1620, Training Loss: 1.40063, Tokens/sec: 83372.72131414212\n",
      "Step: 1621, Training Loss: 1.42198, Tokens/sec: 86693.201079158\n",
      "Step: 1622, Training Loss: 1.53042, Tokens/sec: 83860.24771674308\n",
      "Step: 1623, Training Loss: 1.50432, Tokens/sec: 86404.25694648342\n",
      "Step: 1624, Training Loss: 1.74654, Tokens/sec: 85961.67286796171\n",
      "Step: 1625, Training Loss: 1.53568, Tokens/sec: 83655.72196453325\n",
      "Step: 1626, Training Loss: 1.37890, Tokens/sec: 86106.461275322\n",
      "Step: 1627, Training Loss: 1.47914, Tokens/sec: 83185.99476174962\n",
      "Step: 1628, Training Loss: 1.54657, Tokens/sec: 85720.45315090698\n",
      "Step: 1629, Training Loss: 1.50882, Tokens/sec: 86673.61391954108\n",
      "Step: 1630, Training Loss: 1.63775, Tokens/sec: 83249.15052573447\n",
      "Step: 1631, Training Loss: 1.52860, Tokens/sec: 85591.43475275858\n",
      "Step: 1632, Training Loss: 1.42065, Tokens/sec: 86418.9689282089\n",
      "Step: 1633, Training Loss: 1.36653, Tokens/sec: 84198.5560645458\n",
      "Step: 1634, Training Loss: 1.50902, Tokens/sec: 85895.38576210082\n",
      "Step: 1635, Training Loss: 1.39766, Tokens/sec: 84687.34225439\n",
      "Step: 1636, Training Loss: 1.59889, Tokens/sec: 86369.77240174943\n",
      "Step: 1637, Training Loss: 1.55461, Tokens/sec: 86708.7646602154\n",
      "Step: 1638, Training Loss: 1.47934, Tokens/sec: 80771.55489592868\n",
      "Step: 1639, Training Loss: 1.51099, Tokens/sec: 86723.40976699367\n",
      "Step: 1640, Training Loss: 1.38087, Tokens/sec: 87009.74603086185\n",
      "Step: 1641, Training Loss: 1.43400, Tokens/sec: 84086.632943227\n",
      "Step: 1642, Training Loss: 1.41247, Tokens/sec: 83812.6913017881\n",
      "Step: 1643, Training Loss: 1.31852, Tokens/sec: 85067.30218111111\n",
      "Step: 1644, Training Loss: 1.47519, Tokens/sec: 86696.45033276748\n",
      "Step: 1645, Training Loss: 1.38995, Tokens/sec: 86251.9244133529\n",
      "Step: 1646, Training Loss: 1.53926, Tokens/sec: 83443.38244375959\n",
      "Step: 1647, Training Loss: 1.43378, Tokens/sec: 86787.45258199444\n",
      "Step: 1648, Training Loss: 1.54006, Tokens/sec: 85959.79353188242\n",
      "Step: 1649, Training Loss: 1.43234, Tokens/sec: 83636.27661468335\n",
      "Step: 1650, Training Loss: 1.40081, Tokens/sec: 85778.00113075292\n",
      "Step: 1651, Training Loss: 1.57755, Tokens/sec: 84039.3057332674\n",
      "Step: 1652, Training Loss: 1.51209, Tokens/sec: 85157.95982645919\n",
      "Step: 1653, Training Loss: 1.37373, Tokens/sec: 86227.82531485557\n",
      "Step: 1654, Training Loss: 1.37040, Tokens/sec: 83131.9992910174\n",
      "Step: 1655, Training Loss: 1.60375, Tokens/sec: 86324.1538799818\n",
      "Step: 1656, Training Loss: 1.64446, Tokens/sec: 86888.35798451952\n",
      "Step: 1657, Training Loss: 1.62311, Tokens/sec: 84024.50980277426\n",
      "Step: 1658, Training Loss: 1.46865, Tokens/sec: 85422.39641319407\n",
      "Step: 1659, Training Loss: 1.52393, Tokens/sec: 83451.26308286173\n",
      "Step: 1660, Training Loss: 1.41122, Tokens/sec: 86581.48846383067\n",
      "Step: 1661, Training Loss: 1.40978, Tokens/sec: 85603.67151324985\n",
      "Step: 1662, Training Loss: 1.55147, Tokens/sec: 72740.57384214317\n",
      "Step: 1663, Training Loss: 1.47591, Tokens/sec: 85060.34279488126\n",
      "Step: 1664, Training Loss: 1.48326, Tokens/sec: 69380.01499346517\n",
      "Step: 1665, Training Loss: 1.46272, Tokens/sec: 53957.764480955295\n",
      "Step: 1666, Training Loss: 1.54422, Tokens/sec: 86855.83514904734\n",
      "Step: 1667, Training Loss: 1.31982, Tokens/sec: 84432.9787463115\n",
      "Step: 1668, Training Loss: 1.34998, Tokens/sec: 85509.45207475993\n",
      "Step: 1669, Training Loss: 1.33980, Tokens/sec: 84699.63317693002\n",
      "Step: 1670, Training Loss: 1.69521, Tokens/sec: 86847.17963743814\n",
      "Step: 1671, Training Loss: 1.43384, Tokens/sec: 86332.53257549659\n",
      "Step: 1672, Training Loss: 1.41079, Tokens/sec: 84512.6359667265\n",
      "Step: 1673, Training Loss: 1.56732, Tokens/sec: 86697.28161045842\n",
      "Step: 1674, Training Loss: 1.63497, Tokens/sec: 85967.15934789852\n",
      "Step: 1675, Training Loss: 1.45980, Tokens/sec: 82467.77031025769\n",
      "Step: 1676, Training Loss: 1.61579, Tokens/sec: 74008.49486011578\n",
      "Step: 1677, Training Loss: 1.41118, Tokens/sec: 86823.03620544121\n",
      "Step: 1678, Training Loss: 1.43704, Tokens/sec: 85660.16299117432\n",
      "Step: 1679, Training Loss: 1.43630, Tokens/sec: 86014.35918709668\n",
      "Step: 1680, Training Loss: 1.46523, Tokens/sec: 85833.35644554574\n",
      "Step: 1681, Training Loss: 1.53290, Tokens/sec: 87609.91859183475\n",
      "Step: 1682, Training Loss: 1.48918, Tokens/sec: 86474.15260008122\n",
      "Step: 1683, Training Loss: 1.62673, Tokens/sec: 86126.53811690523\n",
      "Step: 1684, Training Loss: 1.45358, Tokens/sec: 85531.7152376235\n",
      "Step: 1685, Training Loss: 1.42665, Tokens/sec: 85062.82999150507\n",
      "Step: 1686, Training Loss: 1.55956, Tokens/sec: 87182.40261867408\n",
      "Step: 1687, Training Loss: 1.70626, Tokens/sec: 87515.61355540712\n",
      "Step: 1688, Training Loss: 1.47486, Tokens/sec: 86711.82325334895\n",
      "Step: 1689, Training Loss: 1.54358, Tokens/sec: 85331.82091693232\n",
      "Step: 1690, Training Loss: 1.44682, Tokens/sec: 83586.85862679682\n",
      "Step: 1691, Training Loss: 1.52107, Tokens/sec: 86797.5694503033\n",
      "Step: 1692, Training Loss: 1.55926, Tokens/sec: 83972.61957562587\n",
      "Step: 1693, Training Loss: 1.56963, Tokens/sec: 84165.96325696804\n",
      "Step: 1694, Training Loss: 1.39351, Tokens/sec: 86447.17319252803\n",
      "Step: 1695, Training Loss: 1.47165, Tokens/sec: 83559.60660194249\n",
      "Step: 1696, Training Loss: 1.60307, Tokens/sec: 57438.928052214666\n",
      "Step: 1697, Training Loss: 1.60585, Tokens/sec: 82942.52792427431\n",
      "Step: 1698, Training Loss: 1.48493, Tokens/sec: 66309.75822459933\n",
      "Step: 1699, Training Loss: 1.51957, Tokens/sec: 87310.55171909725\n",
      "Step: 1700, Training Loss: 1.55077, Tokens/sec: 84665.73081756376\n",
      "Step: 1701, Training Loss: 1.51645, Tokens/sec: 86174.51146642848\n",
      "Step: 1702, Training Loss: 1.49392, Tokens/sec: 86362.57324471972\n",
      "Step: 1703, Training Loss: 1.45515, Tokens/sec: 84093.52367957326\n",
      "Step: 1704, Training Loss: 1.45940, Tokens/sec: 86656.72917420782\n",
      "Step: 1705, Training Loss: 1.52924, Tokens/sec: 86411.1103082394\n",
      "Step: 1706, Training Loss: 1.61096, Tokens/sec: 84533.07979847878\n",
      "Step: 1707, Training Loss: 1.60116, Tokens/sec: 86919.55361669898\n",
      "Step: 1708, Training Loss: 1.62143, Tokens/sec: 84170.10251757626\n",
      "Step: 1709, Training Loss: 1.71535, Tokens/sec: 86867.56105078419\n",
      "Step: 1710, Training Loss: 1.52755, Tokens/sec: 87128.45217855234\n",
      "Step: 1711, Training Loss: 1.53197, Tokens/sec: 85443.16295515266\n",
      "Step: 1712, Training Loss: 1.50954, Tokens/sec: 87361.67439318517\n",
      "Step: 1713, Training Loss: 1.52947, Tokens/sec: 86656.12004689377\n",
      "Step: 1714, Training Loss: 1.95050, Tokens/sec: 87166.00772713863\n",
      "Step: 1715, Training Loss: 1.34970, Tokens/sec: 86807.6555642637\n",
      "Step: 1716, Training Loss: 1.57939, Tokens/sec: 82686.60279038096\n",
      "Step: 1717, Training Loss: 1.63148, Tokens/sec: 86122.14851829283\n",
      "Step: 1718, Training Loss: 2.04449, Tokens/sec: 86702.20674039179\n",
      "Step: 1719, Training Loss: 1.34924, Tokens/sec: 83164.43898336204\n",
      "Step: 1720, Training Loss: 1.56988, Tokens/sec: 78607.98047946145\n",
      "Step: 1721, Training Loss: 1.55532, Tokens/sec: 83644.98971423732\n",
      "Step: 1722, Training Loss: 1.68397, Tokens/sec: 76087.69800276074\n",
      "Step: 1723, Training Loss: 1.46109, Tokens/sec: 86151.96455665556\n",
      "Step: 1724, Training Loss: 1.41859, Tokens/sec: 86046.56072914555\n",
      "Step: 1725, Training Loss: 1.47967, Tokens/sec: 86523.94426676363\n",
      "Step: 1726, Training Loss: 1.48228, Tokens/sec: 87265.29608695029\n",
      "Step: 1727, Training Loss: 1.55738, Tokens/sec: 74919.58890588123\n",
      "Step: 1728, Training Loss: 1.45217, Tokens/sec: 85142.24884988913\n",
      "Step: 1729, Training Loss: 1.47529, Tokens/sec: 84733.70248988544\n",
      "Step: 1730, Training Loss: 1.53956, Tokens/sec: 81652.29356342215\n",
      "Step: 1731, Training Loss: 1.41472, Tokens/sec: 79362.20032872389\n",
      "Step: 1732, Training Loss: 1.37345, Tokens/sec: 86485.13652821498\n",
      "Step: 1733, Training Loss: 1.51257, Tokens/sec: 84499.96388068101\n",
      "Step: 1734, Training Loss: 1.53325, Tokens/sec: 86845.32721877118\n",
      "Step: 1735, Training Loss: 1.46199, Tokens/sec: 83724.2966881571\n",
      "Step: 1736, Training Loss: 1.51568, Tokens/sec: 85129.12806319361\n",
      "Step: 1737, Training Loss: 1.50595, Tokens/sec: 87681.24293564039\n",
      "Step: 1738, Training Loss: 1.55299, Tokens/sec: 71518.83083940769\n",
      "Step: 1739, Training Loss: 1.53668, Tokens/sec: 83856.54918390977\n",
      "Step: 1740, Training Loss: 1.63543, Tokens/sec: 86087.2654282672\n",
      "Step: 1741, Training Loss: 1.42990, Tokens/sec: 79394.53602802039\n",
      "Step: 1742, Training Loss: 1.63901, Tokens/sec: 84856.69108754967\n",
      "Step: 1743, Training Loss: 1.53053, Tokens/sec: 86038.41907320372\n",
      "Step: 1744, Training Loss: 1.62172, Tokens/sec: 84192.82487614709\n",
      "Step: 1745, Training Loss: 3.50745, Tokens/sec: 86189.79912181104\n",
      "Step: 1746, Training Loss: 1.45224, Tokens/sec: 83390.09142794496\n",
      "Step: 1747, Training Loss: 1.52938, Tokens/sec: 86131.18172105076\n",
      "Step: 1748, Training Loss: 1.49814, Tokens/sec: 86815.98256174689\n",
      "Step: 1749, Training Loss: 1.57923, Tokens/sec: 83434.06844442882\n",
      "Step: 1750, Training Loss: 1.52647, Tokens/sec: 86259.58516639468\n",
      "Step: 1751, Training Loss: 1.50205, Tokens/sec: 86341.57265735212\n",
      "Step: 1752, Training Loss: 1.41193, Tokens/sec: 86018.3909675704\n",
      "Step: 1753, Training Loss: 1.68451, Tokens/sec: 85466.14103797024\n",
      "Step: 1754, Training Loss: 1.80641, Tokens/sec: 83802.93504526556\n",
      "Step: 1755, Training Loss: 1.70135, Tokens/sec: 86330.57194157965\n",
      "Step: 1756, Training Loss: 1.53292, Tokens/sec: 74636.62625211476\n",
      "Step: 1757, Training Loss: 1.44367, Tokens/sec: 81479.3207223181\n",
      "Step: 1758, Training Loss: 1.63637, Tokens/sec: 74806.44835926502\n",
      "Step: 1759, Training Loss: 1.59130, Tokens/sec: 85048.84936413368\n",
      "Step: 1760, Training Loss: 1.37408, Tokens/sec: 83192.58532911805\n",
      "Step: 1761, Training Loss: 1.52342, Tokens/sec: 72688.05899062817\n",
      "Step: 1762, Training Loss: 1.54534, Tokens/sec: 86636.13046242685\n",
      "Step: 1763, Training Loss: 1.47803, Tokens/sec: 83381.7530043687\n",
      "Step: 1764, Training Loss: 1.38114, Tokens/sec: 79427.03670284961\n",
      "Step: 1765, Training Loss: 1.51740, Tokens/sec: 79838.43048349171\n",
      "Step: 1766, Training Loss: 1.51466, Tokens/sec: 65107.05751557231\n",
      "Step: 1767, Training Loss: 1.66765, Tokens/sec: 73005.02431007283\n",
      "Step: 1768, Training Loss: 1.75575, Tokens/sec: 84890.00984918073\n",
      "Step: 1769, Training Loss: 1.54394, Tokens/sec: 83031.50792409002\n",
      "Step: 1770, Training Loss: 1.41949, Tokens/sec: 85982.05269489766\n",
      "Step: 1771, Training Loss: 1.85745, Tokens/sec: 85862.9233247799\n",
      "Step: 1772, Training Loss: 1.51615, Tokens/sec: 85299.07020744082\n",
      "Step: 1773, Training Loss: 1.45303, Tokens/sec: 85946.79965543971\n",
      "Step: 1774, Training Loss: 1.51358, Tokens/sec: 83765.29628668806\n",
      "Step: 1775, Training Loss: 1.45122, Tokens/sec: 85716.6788556646\n",
      "Step: 1776, Training Loss: 2.05356, Tokens/sec: 85117.7018207406\n",
      "Step: 1777, Training Loss: 1.44146, Tokens/sec: 83687.67406638117\n",
      "Step: 1778, Training Loss: 1.34118, Tokens/sec: 86629.00775915066\n",
      "Step: 1779, Training Loss: 1.62442, Tokens/sec: 86236.61834295085\n",
      "Step: 1780, Training Loss: 1.55223, Tokens/sec: 84537.7852220432\n",
      "Step: 1781, Training Loss: 1.53147, Tokens/sec: 86332.29738554853\n",
      "Step: 1782, Training Loss: 1.65550, Tokens/sec: 84558.33461587685\n",
      "Step: 1783, Training Loss: 1.78493, Tokens/sec: 86061.69497270248\n",
      "Step: 1784, Training Loss: 1.95420, Tokens/sec: 84845.24163797364\n",
      "Step: 1785, Training Loss: 1.60777, Tokens/sec: 84294.29420407876\n",
      "Step: 1786, Training Loss: 1.44314, Tokens/sec: 86586.86170418614\n",
      "Step: 1787, Training Loss: 1.43324, Tokens/sec: 83862.47463187113\n",
      "Step: 1788, Training Loss: 1.57109, Tokens/sec: 84553.41221919701\n",
      "Step: 1789, Training Loss: 1.58763, Tokens/sec: 86117.64304137937\n",
      "Step: 1790, Training Loss: 1.60342, Tokens/sec: 83214.84208222298\n",
      "Step: 1791, Training Loss: 1.54494, Tokens/sec: 85520.54984292788\n",
      "Step: 1792, Training Loss: 1.60030, Tokens/sec: 84817.43921143263\n",
      "Step: 1793, Training Loss: 1.51584, Tokens/sec: 84047.56363938675\n",
      "Step: 1794, Training Loss: 1.76666, Tokens/sec: 85856.6470385968\n",
      "Step: 1795, Training Loss: 1.85808, Tokens/sec: 83631.77858930125\n",
      "Step: 1796, Training Loss: 1.81787, Tokens/sec: 85217.69469558429\n",
      "Step: 1797, Training Loss: 1.82268, Tokens/sec: 86094.51059593016\n",
      "Step: 1798, Training Loss: 1.66667, Tokens/sec: 83740.65883965693\n",
      "Step: 1799, Training Loss: 1.56258, Tokens/sec: 85252.0863200523\n",
      "Step: 1800, Training Loss: 1.61590, Tokens/sec: 84905.05674832163\n",
      "Step: 1801, Training Loss: 1.48857, Tokens/sec: 83036.96298904486\n",
      "Step: 1802, Training Loss: 1.54604, Tokens/sec: 86427.45631499258\n",
      "Step: 1803, Training Loss: 1.51506, Tokens/sec: 83461.91761358576\n",
      "Step: 1804, Training Loss: 1.39964, Tokens/sec: 84969.5071782099\n",
      "Step: 1805, Training Loss: 1.72981, Tokens/sec: 85958.34361460299\n",
      "Step: 1806, Training Loss: 1.55293, Tokens/sec: 83828.9726473608\n",
      "Step: 1807, Training Loss: 1.47001, Tokens/sec: 86264.87900702073\n",
      "Step: 1808, Training Loss: 1.47424, Tokens/sec: 84171.47327822038\n",
      "Step: 1809, Training Loss: 1.55443, Tokens/sec: 86442.88220241219\n",
      "Step: 1810, Training Loss: 1.40116, Tokens/sec: 86430.67063388723\n",
      "Step: 1811, Training Loss: 1.49182, Tokens/sec: 83791.94429697971\n",
      "Step: 1812, Training Loss: 1.50924, Tokens/sec: 86238.29418837119\n",
      "Step: 1813, Training Loss: 1.48438, Tokens/sec: 86821.52021138927\n",
      "Step: 1814, Training Loss: 1.35429, Tokens/sec: 84651.67615318777\n",
      "Step: 1815, Training Loss: 1.37009, Tokens/sec: 85683.94758437976\n",
      "Step: 1816, Training Loss: 1.45428, Tokens/sec: 84543.26071110045\n",
      "Step: 1817, Training Loss: 1.49347, Tokens/sec: 86611.74479078603\n",
      "Step: 1818, Training Loss: 1.55845, Tokens/sec: 81948.83000573951\n",
      "Step: 1819, Training Loss: 1.47247, Tokens/sec: 82697.12848573712\n",
      "Step: 1820, Training Loss: 1.46726, Tokens/sec: 85900.2215554219\n",
      "Step: 1821, Training Loss: 1.61274, Tokens/sec: 86364.2043633192\n",
      "Step: 1822, Training Loss: 1.68774, Tokens/sec: 83785.35868374942\n",
      "Step: 1823, Training Loss: 1.47727, Tokens/sec: 85053.33333240094\n",
      "Step: 1824, Training Loss: 1.68990, Tokens/sec: 84686.61955269729\n",
      "Step: 1825, Training Loss: 1.34889, Tokens/sec: 66146.44517014238\n",
      "Step: 1826, Training Loss: 1.30826, Tokens/sec: 74742.93896616752\n",
      "Step: 1827, Training Loss: 1.68996, Tokens/sec: 79307.34291159721\n",
      "Step: 1828, Training Loss: 1.63146, Tokens/sec: 83362.96499015209\n",
      "Step: 1829, Training Loss: 1.67056, Tokens/sec: 83807.26759336129\n",
      "Step: 1830, Training Loss: 1.34747, Tokens/sec: 85605.34789438406\n",
      "Step: 1831, Training Loss: 1.43299, Tokens/sec: 84878.65781564233\n",
      "Step: 1832, Training Loss: 1.40203, Tokens/sec: 83368.28934631156\n",
      "Step: 1833, Training Loss: 1.64216, Tokens/sec: 86489.45545327304\n",
      "Step: 1834, Training Loss: 1.53528, Tokens/sec: 84114.9432752805\n",
      "Step: 1835, Training Loss: 1.45074, Tokens/sec: 86196.20991982406\n",
      "Step: 1836, Training Loss: 1.62487, Tokens/sec: 85353.27221233502\n",
      "Step: 1837, Training Loss: 1.43481, Tokens/sec: 83803.9886699222\n",
      "Step: 1838, Training Loss: 1.58191, Tokens/sec: 85724.41391326401\n",
      "Step: 1839, Training Loss: 1.52530, Tokens/sec: 85872.45311494973\n",
      "Step: 1840, Training Loss: 1.50390, Tokens/sec: 83584.62201631941\n",
      "Step: 1841, Training Loss: 1.48272, Tokens/sec: 86098.29380557998\n",
      "Step: 1842, Training Loss: 1.52163, Tokens/sec: 83481.81792078161\n",
      "Step: 1843, Training Loss: 1.50050, Tokens/sec: 86090.1419063572\n",
      "Step: 1844, Training Loss: 1.53302, Tokens/sec: 85811.54723104062\n",
      "Step: 1845, Training Loss: 1.47185, Tokens/sec: 83003.40358721874\n",
      "Step: 1846, Training Loss: 1.43971, Tokens/sec: 86004.18523965222\n",
      "Step: 1847, Training Loss: 1.32192, Tokens/sec: 85809.33604634633\n",
      "Step: 1848, Training Loss: 1.46704, Tokens/sec: 84996.51296191245\n",
      "Step: 1849, Training Loss: 1.52042, Tokens/sec: 84504.84126499659\n",
      "Step: 1850, Training Loss: 1.58943, Tokens/sec: 83775.28366406397\n",
      "Step: 1851, Training Loss: 2.00448, Tokens/sec: 85081.10198783535\n",
      "Step: 1852, Training Loss: 1.49909, Tokens/sec: 85684.9034035786\n",
      "Step: 1853, Training Loss: 2.24394, Tokens/sec: 84006.48890427784\n",
      "Step: 1854, Training Loss: 1.50184, Tokens/sec: 86015.03519077977\n",
      "Step: 1855, Training Loss: 1.63174, Tokens/sec: 84752.34935788225\n",
      "Step: 1856, Training Loss: 1.41358, Tokens/sec: 84605.3077445027\n",
      "Step: 1857, Training Loss: 1.50548, Tokens/sec: 83743.7984035575\n",
      "Step: 1858, Training Loss: 1.45400, Tokens/sec: 83661.45202735672\n",
      "Step: 1859, Training Loss: 1.43407, Tokens/sec: 84663.97859326702\n",
      "Step: 1860, Training Loss: 1.30813, Tokens/sec: 86104.05521717713\n",
      "Step: 1861, Training Loss: 1.87115, Tokens/sec: 81050.68760804542\n",
      "Step: 1862, Training Loss: 1.44177, Tokens/sec: 84675.27283515762\n",
      "Step: 1863, Training Loss: 1.43705, Tokens/sec: 82997.96806432346\n",
      "Step: 1864, Training Loss: 1.36022, Tokens/sec: 84629.82936333834\n",
      "Step: 1865, Training Loss: 1.54167, Tokens/sec: 86287.76999867884\n",
      "Step: 1866, Training Loss: 1.46303, Tokens/sec: 83430.05436635275\n",
      "Step: 1867, Training Loss: 1.53698, Tokens/sec: 85538.3094081511\n",
      "Step: 1868, Training Loss: 1.41964, Tokens/sec: 85331.21116591594\n",
      "Step: 1869, Training Loss: 1.47175, Tokens/sec: 83653.2343670655\n",
      "Step: 1870, Training Loss: 1.51045, Tokens/sec: 85115.88441647476\n",
      "Step: 1871, Training Loss: 1.78596, Tokens/sec: 67493.70096123754\n",
      "Step: 1872, Training Loss: 1.54952, Tokens/sec: 76712.47310586231\n",
      "Step: 1873, Training Loss: 1.37594, Tokens/sec: 79135.46819638013\n",
      "Step: 1874, Training Loss: 1.69339, Tokens/sec: 67671.21803364002\n",
      "Step: 1875, Training Loss: 1.65227, Tokens/sec: 85928.13324102035\n",
      "Step: 1876, Training Loss: 1.58984, Tokens/sec: 86784.60378440283\n",
      "Step: 1877, Training Loss: 1.81979, Tokens/sec: 86888.50635806954\n",
      "Step: 1878, Training Loss: 1.44733, Tokens/sec: 85327.2079975989\n",
      "Step: 1879, Training Loss: 1.37917, Tokens/sec: 86792.49232889277\n",
      "Step: 1880, Training Loss: 1.68254, Tokens/sec: 86893.82929217577\n",
      "Step: 1881, Training Loss: 1.53111, Tokens/sec: 83754.124880447\n",
      "Step: 1882, Training Loss: 1.93030, Tokens/sec: 86952.19249415072\n",
      "Step: 1883, Training Loss: 1.50734, Tokens/sec: 82973.85466445179\n",
      "Step: 1884, Training Loss: 1.56529, Tokens/sec: 86288.28442723457\n",
      "Step: 1885, Training Loss: 1.54573, Tokens/sec: 86135.91096539581\n",
      "Step: 1886, Training Loss: 1.48596, Tokens/sec: 83083.1503973382\n",
      "Step: 1887, Training Loss: 1.43589, Tokens/sec: 86003.94732199385\n",
      "Step: 1888, Training Loss: 1.52704, Tokens/sec: 86230.53600973009\n",
      "Step: 1889, Training Loss: 1.50732, Tokens/sec: 85657.73613354923\n",
      "Step: 1890, Training Loss: 1.70994, Tokens/sec: 87390.02712695324\n",
      "Step: 1891, Training Loss: 1.48742, Tokens/sec: 85002.11903642387\n",
      "Step: 1892, Training Loss: 1.39191, Tokens/sec: 86356.01842498619\n",
      "Step: 1893, Training Loss: 1.42059, Tokens/sec: 86890.99516982913\n",
      "Step: 1894, Training Loss: 1.61124, Tokens/sec: 85138.08819797896\n",
      "Step: 1895, Training Loss: 1.52787, Tokens/sec: 87933.12471010126\n",
      "Step: 1896, Training Loss: 1.54227, Tokens/sec: 82879.16890944856\n",
      "Step: 1897, Training Loss: 1.62710, Tokens/sec: 86872.30932227403\n",
      "Step: 1898, Training Loss: 1.53674, Tokens/sec: 63126.34154231498\n",
      "Step: 1899, Training Loss: 1.62144, Tokens/sec: 85920.05904094102\n",
      "Step: 1900, Training Loss: 1.69064, Tokens/sec: 81933.90377606102\n",
      "Step: 1901, Training Loss: 1.47581, Tokens/sec: 86395.42149669031\n",
      "Step: 1902, Training Loss: 1.49295, Tokens/sec: 85857.18738612295\n",
      "Step: 1903, Training Loss: 1.68856, Tokens/sec: 62942.670820586696\n",
      "Step: 1904, Training Loss: 1.48988, Tokens/sec: 83348.37146463167\n",
      "Step: 1905, Training Loss: 1.55124, Tokens/sec: 69519.09988552071\n",
      "Step: 1906, Training Loss: 1.55940, Tokens/sec: 78977.96649886925\n",
      "Step: 1907, Training Loss: 1.63671, Tokens/sec: 87055.48627155619\n",
      "Step: 1908, Training Loss: 1.73261, Tokens/sec: 81425.2107477822\n",
      "Step: 1909, Training Loss: 1.53102, Tokens/sec: 82881.62242119394\n",
      "Step: 1910, Training Loss: 1.49353, Tokens/sec: 81016.36293101519\n",
      "Step: 1911, Training Loss: 1.56691, Tokens/sec: 81157.15721285813\n",
      "Step: 1912, Training Loss: 1.51480, Tokens/sec: 81528.22305436632\n",
      "Step: 1913, Training Loss: 1.54789, Tokens/sec: 85212.17759618806\n",
      "Step: 1914, Training Loss: 1.43377, Tokens/sec: 80260.44317667949\n",
      "Step: 1915, Training Loss: 1.40592, Tokens/sec: 79406.4099444738\n",
      "Step: 1916, Training Loss: 1.65225, Tokens/sec: 86875.68165363492\n",
      "Step: 1917, Training Loss: 1.59931, Tokens/sec: 83406.20006529774\n",
      "Step: 1918, Training Loss: 1.56234, Tokens/sec: 68675.27424614974\n",
      "Step: 1919, Training Loss: 1.50590, Tokens/sec: 86461.03835178839\n",
      "Step: 1920, Training Loss: 2.38199, Tokens/sec: 85177.68574974118\n",
      "Step: 1921, Training Loss: 1.63684, Tokens/sec: 67294.82066248976\n",
      "Step: 1922, Training Loss: 1.53997, Tokens/sec: 74605.36238134558\n",
      "Step: 1923, Training Loss: 1.63539, Tokens/sec: 73005.55683055142\n",
      "Step: 1924, Training Loss: 1.48641, Tokens/sec: 87082.7537404721\n",
      "Step: 1925, Training Loss: 1.56718, Tokens/sec: 81117.3225985253\n",
      "Step: 1926, Training Loss: 1.60151, Tokens/sec: 72418.004963821\n",
      "Step: 1927, Training Loss: 1.45143, Tokens/sec: 86087.19621867054\n",
      "Step: 1928, Training Loss: 1.39823, Tokens/sec: 74345.57808534699\n",
      "Step: 1929, Training Loss: 1.70282, Tokens/sec: 84873.12604637587\n",
      "Step: 1930, Training Loss: 1.48133, Tokens/sec: 86523.73819029192\n",
      "Step: 1931, Training Loss: 1.69285, Tokens/sec: 86737.14190063975\n",
      "Step: 1932, Training Loss: 1.90181, Tokens/sec: 83503.07765190762\n",
      "Step: 1933, Training Loss: 1.88806, Tokens/sec: 72155.32741527389\n",
      "Step: 1934, Training Loss: 1.74358, Tokens/sec: 86548.38749603872\n",
      "Step: 1935, Training Loss: 1.38766, Tokens/sec: 83275.9862146066\n",
      "Step: 1936, Training Loss: 1.43097, Tokens/sec: 73273.25474998016\n",
      "Step: 1937, Training Loss: 1.67794, Tokens/sec: 80446.7982967254\n",
      "Step: 1938, Training Loss: 1.51722, Tokens/sec: 87207.60647874748\n",
      "Step: 1939, Training Loss: 1.51969, Tokens/sec: 76389.97786811073\n",
      "Step: 1940, Training Loss: 1.62068, Tokens/sec: 83684.74003256291\n",
      "Step: 1941, Training Loss: 1.67485, Tokens/sec: 83531.92559146909\n",
      "Step: 1942, Training Loss: 1.69444, Tokens/sec: 85836.95709147168\n",
      "Step: 1943, Training Loss: 1.62233, Tokens/sec: 85139.12964667082\n",
      "Step: 1944, Training Loss: 1.51518, Tokens/sec: 83635.50342869855\n",
      "Step: 1945, Training Loss: 1.53714, Tokens/sec: 85736.87537190321\n",
      "Step: 1946, Training Loss: 1.45294, Tokens/sec: 82533.03599839294\n",
      "Step: 1947, Training Loss: 1.55780, Tokens/sec: 83933.38867156688\n",
      "Step: 1948, Training Loss: 1.64116, Tokens/sec: 85547.0177548263\n",
      "Step: 1949, Training Loss: 1.50323, Tokens/sec: 83683.82532746936\n",
      "Step: 1950, Training Loss: 1.49579, Tokens/sec: 85313.29492596604\n",
      "Step: 1951, Training Loss: 1.55895, Tokens/sec: 86095.75202179863\n",
      "Step: 1952, Training Loss: 1.67641, Tokens/sec: 84158.1757411143\n",
      "Step: 1953, Training Loss: 1.49859, Tokens/sec: 85622.12427071533\n",
      "Step: 1954, Training Loss: 1.51519, Tokens/sec: 84879.88949001329\n",
      "Step: 1955, Training Loss: 1.70567, Tokens/sec: 86363.60890744741\n",
      "Step: 1956, Training Loss: 1.61488, Tokens/sec: 86144.53669381676\n",
      "Step: 1957, Training Loss: 1.65137, Tokens/sec: 83397.7187571766\n",
      "Step: 1958, Training Loss: 1.58981, Tokens/sec: 86514.61103947661\n",
      "Step: 1959, Training Loss: 1.62451, Tokens/sec: 85826.5836265821\n",
      "Step: 1960, Training Loss: 1.54093, Tokens/sec: 83911.12670433731\n",
      "Step: 1961, Training Loss: 1.46911, Tokens/sec: 85877.86834486156\n",
      "Step: 1962, Training Loss: 1.53543, Tokens/sec: 85428.75681086826\n",
      "Step: 1963, Training Loss: 1.43876, Tokens/sec: 86263.18668573903\n",
      "Step: 1964, Training Loss: 1.56087, Tokens/sec: 85595.56336564657\n",
      "Step: 1965, Training Loss: 1.73052, Tokens/sec: 84302.12992156499\n",
      "Step: 1966, Training Loss: 1.58925, Tokens/sec: 86620.15330990708\n",
      "Step: 1967, Training Loss: 1.44125, Tokens/sec: 86513.12224299426\n",
      "Step: 1968, Training Loss: 1.43001, Tokens/sec: 83572.48970514775\n",
      "Step: 1969, Training Loss: 1.71432, Tokens/sec: 86132.57182309455\n",
      "Step: 1970, Training Loss: 1.44423, Tokens/sec: 85322.25299284993\n",
      "Step: 1971, Training Loss: 1.49367, Tokens/sec: 86102.81535934476\n",
      "Step: 1972, Training Loss: 1.52215, Tokens/sec: 86192.91054156094\n",
      "Step: 1973, Training Loss: 1.74816, Tokens/sec: 84277.43105765531\n",
      "Step: 1974, Training Loss: 1.51317, Tokens/sec: 87041.91489542475\n",
      "Step: 1975, Training Loss: 1.60982, Tokens/sec: 86791.99945586291\n",
      "Step: 1976, Training Loss: 1.47951, Tokens/sec: 84047.80379012947\n",
      "Step: 1977, Training Loss: 1.62726, Tokens/sec: 86659.50953494315\n",
      "Step: 1978, Training Loss: 1.80345, Tokens/sec: 84074.5952052264\n",
      "Step: 1979, Training Loss: 1.46874, Tokens/sec: 87239.21669308146\n",
      "Step: 1980, Training Loss: 1.46188, Tokens/sec: 85848.62000072685\n",
      "Step: 1981, Training Loss: 1.49889, Tokens/sec: 83873.29582984406\n",
      "Step: 1982, Training Loss: 1.45129, Tokens/sec: 85392.19835611674\n",
      "Step: 1983, Training Loss: 1.57016, Tokens/sec: 86147.66704018031\n",
      "Step: 1984, Training Loss: 1.52420, Tokens/sec: 84396.92440140633\n",
      "Step: 1985, Training Loss: 1.55003, Tokens/sec: 85369.93836331864\n",
      "Step: 1986, Training Loss: 1.49548, Tokens/sec: 83410.78851295332\n",
      "Step: 1987, Training Loss: 1.70305, Tokens/sec: 86456.96223550013\n",
      "Step: 1988, Training Loss: 1.56453, Tokens/sec: 85837.27323731853\n",
      "Step: 1989, Training Loss: 1.54911, Tokens/sec: 83155.18461783661\n",
      "Step: 1990, Training Loss: 1.52680, Tokens/sec: 86231.8930127495\n",
      "Step: 1991, Training Loss: 1.64127, Tokens/sec: 85599.87529636886\n",
      "Step: 1992, Training Loss: 1.49496, Tokens/sec: 84560.9439826043\n",
      "Step: 1993, Training Loss: 1.66601, Tokens/sec: 85010.40537056701\n",
      "Step: 1994, Training Loss: 1.50306, Tokens/sec: 83660.61600615627\n",
      "Step: 1995, Training Loss: 1.62271, Tokens/sec: 86666.6982265634\n",
      "Step: 1996, Training Loss: 1.44063, Tokens/sec: 84921.42197679648\n",
      "Step: 1997, Training Loss: 1.45986, Tokens/sec: 84434.73751790804\n",
      "Step: 1998, Training Loss: 1.50744, Tokens/sec: 86550.59853071441\n",
      "Step: 1999, Training Loss: 1.58144, Tokens/sec: 83308.13555596535\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T06:27:54.880836Z",
     "start_time": "2024-12-16T06:27:52.251011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_ids = tokenizer([\"आज की चर्चा\"], return_tensors=\"pt\")['input_ids'].to(trainer.device)\n",
    "idx = model.generate(input_ids, temperature=0.25, top_k=50, max_new_tokens=256)\n",
    "print(tokenizer.batch_decode(idx)[0])"
   ],
   "id": "8b5596eda083de0a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "आज की चर्चा मिलता है। इसके प्रमुख प्रभावित होने के लिए प्रति प्राप्त होते हैं। इसका प्राचीन प्रकाशित होता है। इसके अतिरिक्त प्रसारण में ही प्रतिभाग्य प्राप्त होता है। इसके प्रभावित हैं। इस प्रकार के अनुसार अपने प्रतिपत्ति में ही है। विश्वास म\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "bc3bcc343073d67a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
