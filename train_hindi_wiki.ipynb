{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:22:48.329931Z",
     "start_time": "2024-12-16T09:22:47.413234Z"
    }
   },
   "outputs": [],
   "source": [
    "from model import ModelConfig, LlamaModel\n",
    "from train import TrainerConfig, FileDataLoader, Trainer\n",
    "\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f28fa23c987e72b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:22:48.333961Z",
     "start_time": "2024-12-16T09:22:48.332382Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer_id = \"HuggingFaceTB/SmolLM-360M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bb4e51aa142abee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:22:48.533405Z",
     "start_time": "2024-12-16T09:22:48.376114Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b38fa5fd-ff52-4feb-b870-133de2909c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61db1d95-147f-4511-af23-f80fa052c441",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tokenizer([\"Aman\", \"You are a doofus\"], return_tensors='pt', padding=\"longest\", padding_side='left')['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe947da0-6eea-424b-90ec-5d83b0c99d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Aman', 'You are a doofus']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(x, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cde027092af8291e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:22:48.540534Z",
     "start_time": "2024-12-16T09:22:48.538895Z"
    }
   },
   "outputs": [],
   "source": [
    "model_config = ModelConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=960,\n",
    "    d_head=64,\n",
    "    d_mlp_proj=2560,\n",
    "    n_layers=32,\n",
    "    n_kv_heads=5,\n",
    "    n_attn_heads=15,\n",
    "    rms_norm_eps=1e-5,\n",
    "    initializer_range=0.02,\n",
    "    rope_theta=100000.0,\n",
    "    padding_idx=tokenizer.pad_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0897594b27eb59f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:22:48.582151Z",
     "start_time": "2024-12-16T09:22:48.580277Z"
    }
   },
   "outputs": [],
   "source": [
    "train_config = TrainerConfig(\n",
    "    per_device_train_batch_size=32,\n",
    "    grad_accumulation_steps=4,\n",
    "    max_seq_len=2048,\n",
    "    num_epochs=2,\n",
    "    eval_interval_steps=500,\n",
    "    learning_rate=1e-3,\n",
    "    grad_clip_norm=1.0,\n",
    "    tokens_folder=\"wiki_hindi_tok/\",\n",
    "    log_dir=\"runs/hindi_wiki\",\n",
    "    warmup_ratio=0.01,\n",
    "    val_size=0.005,\n",
    "    checkpoint_save_interval=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6504e357e2012d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:22:50.546015Z",
     "start_time": "2024-12-16T09:22:48.624998Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens                   | 270,505,984\n",
      "Shard range rank:0             | (0,131423)\n",
      "Num Trainable Params           | 409,007,040\n",
      "Train device                   | cuda, NVIDIA H200, N=4\n",
      "Training precision             | torch.bfloat16\n",
      "Flash Attention                | True\n",
      "torch.compile()                | True\n",
      "DistributedDataParallel        | False\n",
      "Batch size                     | 262,144\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LlamaModel(model_config)\n",
    "dataloader = FileDataLoader(train_config, tokenizer)\n",
    "trainer = Trainer(train_config, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c853027a7a843745",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-16T09:22:50.552519Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training steps                 | 2,054 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:674.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:674.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Training Loss: 11.00659, LR: 0.0000500, Tokens/sec: 32287.57\n",
      "Step: 1, Training Loss: 9.38478, LR: 0.0000975, Tokens/sec: 630610.14\n",
      "Step: 2, Training Loss: 8.16895, LR: 0.0001450, Tokens/sec: 624619.20\n",
      "Step: 3, Training Loss: 7.52400, LR: 0.0001925, Tokens/sec: 624507.36\n",
      "Computing Eval loss, steps: 21\n",
      "Step: 3, Eval Loss: 7.51354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:674.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 4, Training Loss: 7.45392, LR: 0.0002400, Tokens/sec: 628849.92\n",
      "Step: 5, Training Loss: 7.14768, LR: 0.0002875, Tokens/sec: 625545.73\n",
      "Step: 6, Training Loss: 6.66279, LR: 0.0003350, Tokens/sec: 626470.59\n",
      "Step: 7, Training Loss: 6.22066, LR: 0.0003825, Tokens/sec: 624861.38\n",
      "Step: 8, Training Loss: 5.75518, LR: 0.0004300, Tokens/sec: 626854.53\n",
      "Step: 9, Training Loss: 5.30798, LR: 0.0004775, Tokens/sec: 627772.67\n",
      "Step: 10, Training Loss: 4.84102, LR: 0.0005250, Tokens/sec: 623923.06\n",
      "Step: 11, Training Loss: 4.45114, LR: 0.0005725, Tokens/sec: 625755.42\n",
      "Step: 12, Training Loss: 4.16633, LR: 0.0006200, Tokens/sec: 624366.42\n",
      "Step: 13, Training Loss: 3.99765, LR: 0.0006675, Tokens/sec: 625337.98\n",
      "Step: 14, Training Loss: 3.84446, LR: 0.0007150, Tokens/sec: 625368.88\n",
      "Step: 15, Training Loss: 3.78879, LR: 0.0007625, Tokens/sec: 623409.40\n",
      "Step: 16, Training Loss: 3.74289, LR: 0.0008100, Tokens/sec: 624871.62\n",
      "Step: 17, Training Loss: 3.74924, LR: 0.0008575, Tokens/sec: 623160.72\n",
      "Step: 18, Training Loss: 3.72142, LR: 0.0009050, Tokens/sec: 622146.39\n",
      "Step: 19, Training Loss: 3.63705, LR: 0.0009525, Tokens/sec: 622134.58\n",
      "Step: 20, Training Loss: 3.51977, LR: 0.0010000, Tokens/sec: 622928.65\n",
      "Step: 21, Training Loss: 3.34742, LR: 0.0010000, Tokens/sec: 621043.78\n",
      "Step: 22, Training Loss: 3.16780, LR: 0.0010000, Tokens/sec: 619866.11\n",
      "Step: 23, Training Loss: 3.07595, LR: 0.0010000, Tokens/sec: 619954.04\n",
      "Step: 24, Training Loss: 3.18943, LR: 0.0010000, Tokens/sec: 619674.80\n",
      "Step: 25, Training Loss: 3.17113, LR: 0.0010000, Tokens/sec: 619888.92\n",
      "Step: 26, Training Loss: 3.11950, LR: 0.0010000, Tokens/sec: 619754.45\n",
      "Step: 27, Training Loss: 3.04345, LR: 0.0010000, Tokens/sec: 621449.74\n",
      "Step: 28, Training Loss: 3.08270, LR: 0.0010000, Tokens/sec: 620739.50\n",
      "Step: 29, Training Loss: 3.00264, LR: 0.0010000, Tokens/sec: 621737.30\n",
      "Step: 30, Training Loss: 2.95601, LR: 0.0009999, Tokens/sec: 621201.59\n",
      "Step: 31, Training Loss: 2.99528, LR: 0.0009999, Tokens/sec: 621568.67\n",
      "Step: 32, Training Loss: 3.07787, LR: 0.0009999, Tokens/sec: 619701.38\n",
      "Step: 33, Training Loss: 3.27965, LR: 0.0009999, Tokens/sec: 621740.63\n",
      "Step: 34, Training Loss: 3.13455, LR: 0.0009999, Tokens/sec: 622603.21\n",
      "Step: 35, Training Loss: 2.94368, LR: 0.0009999, Tokens/sec: 621038.15\n",
      "Step: 36, Training Loss: 2.91291, LR: 0.0009999, Tokens/sec: 621249.04\n",
      "Step: 37, Training Loss: 2.95120, LR: 0.0009998, Tokens/sec: 621939.80\n",
      "Step: 38, Training Loss: 2.89412, LR: 0.0009998, Tokens/sec: 620986.54\n",
      "Step: 39, Training Loss: 3.00849, LR: 0.0009998, Tokens/sec: 622976.67\n",
      "Step: 40, Training Loss: 2.92996, LR: 0.0009998, Tokens/sec: 622705.43\n",
      "Step: 41, Training Loss: 2.97413, LR: 0.0009998, Tokens/sec: 622118.34\n",
      "Step: 42, Training Loss: 2.89430, LR: 0.0009997, Tokens/sec: 620386.00\n",
      "Step: 43, Training Loss: 2.82928, LR: 0.0009997, Tokens/sec: 620765.51\n",
      "Step: 44, Training Loss: 2.84111, LR: 0.0009997, Tokens/sec: 621873.23\n",
      "Step: 45, Training Loss: 2.80170, LR: 0.0009997, Tokens/sec: 620852.22\n",
      "Step: 46, Training Loss: 2.78735, LR: 0.0009996, Tokens/sec: 621039.70\n",
      "Step: 47, Training Loss: 2.82023, LR: 0.0009996, Tokens/sec: 621768.33\n",
      "Step: 48, Training Loss: 2.80662, LR: 0.0009996, Tokens/sec: 621568.09\n",
      "Step: 49, Training Loss: 2.91735, LR: 0.0009995, Tokens/sec: 621394.29\n",
      "Step: 50, Training Loss: 2.86722, LR: 0.0009995, Tokens/sec: 623206.66\n",
      "Step: 51, Training Loss: 2.83748, LR: 0.0009995, Tokens/sec: 620885.45\n",
      "Step: 52, Training Loss: 2.81532, LR: 0.0009995, Tokens/sec: 621842.77\n",
      "Step: 53, Training Loss: 2.79322, LR: 0.0009994, Tokens/sec: 621370.30\n",
      "Step: 54, Training Loss: 2.77307, LR: 0.0009994, Tokens/sec: 621170.06\n",
      "Step: 55, Training Loss: 2.83475, LR: 0.0009993, Tokens/sec: 620660.53\n",
      "Step: 56, Training Loss: 2.88888, LR: 0.0009993, Tokens/sec: 621225.14\n",
      "Step: 57, Training Loss: 2.76162, LR: 0.0009993, Tokens/sec: 621890.21\n",
      "Step: 58, Training Loss: 2.78256, LR: 0.0009992, Tokens/sec: 621047.99\n",
      "Step: 59, Training Loss: 2.92796, LR: 0.0009992, Tokens/sec: 622339.86\n",
      "Step: 60, Training Loss: 2.75493, LR: 0.0009991, Tokens/sec: 619326.30\n",
      "Step: 61, Training Loss: 2.71047, LR: 0.0009991, Tokens/sec: 618787.73\n",
      "Step: 62, Training Loss: 2.71839, LR: 0.0009991, Tokens/sec: 620356.27\n",
      "Step: 63, Training Loss: 2.80829, LR: 0.0009990, Tokens/sec: 621302.67\n",
      "Step: 64, Training Loss: 2.67558, LR: 0.0009990, Tokens/sec: 620576.82\n",
      "Step: 65, Training Loss: 2.74260, LR: 0.0009989, Tokens/sec: 620290.08\n",
      "Step: 66, Training Loss: 2.69818, LR: 0.0009989, Tokens/sec: 620626.85\n",
      "Step: 67, Training Loss: 2.62654, LR: 0.0009988, Tokens/sec: 620654.14\n",
      "Step: 68, Training Loss: 2.68005, LR: 0.0009988, Tokens/sec: 621493.28\n",
      "Step: 69, Training Loss: 2.62776, LR: 0.0009987, Tokens/sec: 620536.20\n",
      "Step: 70, Training Loss: 2.61420, LR: 0.0009987, Tokens/sec: 621105.94\n",
      "Step: 71, Training Loss: 2.62674, LR: 0.0009986, Tokens/sec: 620621.08\n",
      "Step: 72, Training Loss: 2.70077, LR: 0.0009985, Tokens/sec: 620139.44\n",
      "Step: 73, Training Loss: 2.62299, LR: 0.0009985, Tokens/sec: 621396.37\n",
      "Step: 74, Training Loss: 2.64133, LR: 0.0009984, Tokens/sec: 620341.69\n",
      "Step: 75, Training Loss: 2.61273, LR: 0.0009984, Tokens/sec: 621488.12\n",
      "Step: 76, Training Loss: 2.65084, LR: 0.0009983, Tokens/sec: 620889.07\n",
      "Step: 77, Training Loss: 2.67811, LR: 0.0009983, Tokens/sec: 621326.25\n",
      "Step: 78, Training Loss: 2.64334, LR: 0.0009982, Tokens/sec: 622190.36\n",
      "Step: 79, Training Loss: 2.64728, LR: 0.0009981, Tokens/sec: 621600.18\n",
      "Step: 80, Training Loss: 2.64052, LR: 0.0009981, Tokens/sec: 620998.03\n",
      "Step: 81, Training Loss: 2.58258, LR: 0.0009980, Tokens/sec: 622518.83\n",
      "Step: 82, Training Loss: 2.59274, LR: 0.0009979, Tokens/sec: 620431.59\n",
      "Step: 83, Training Loss: 2.58145, LR: 0.0009979, Tokens/sec: 619931.05\n",
      "Step: 84, Training Loss: 2.62200, LR: 0.0009978, Tokens/sec: 619717.66\n",
      "Step: 85, Training Loss: 2.62300, LR: 0.0009977, Tokens/sec: 619165.28\n",
      "Step: 86, Training Loss: 2.61610, LR: 0.0009977, Tokens/sec: 621396.23\n",
      "Step: 87, Training Loss: 2.62842, LR: 0.0009976, Tokens/sec: 621015.40\n",
      "Step: 88, Training Loss: 2.64671, LR: 0.0009975, Tokens/sec: 621130.53\n",
      "Step: 89, Training Loss: 2.58859, LR: 0.0009974, Tokens/sec: 620927.09\n",
      "Step: 90, Training Loss: 2.57990, LR: 0.0009974, Tokens/sec: 621010.71\n",
      "Step: 91, Training Loss: 2.57977, LR: 0.0009973, Tokens/sec: 620233.99\n",
      "Step: 92, Training Loss: 2.70013, LR: 0.0009972, Tokens/sec: 620649.27\n",
      "Step: 93, Training Loss: 2.63478, LR: 0.0009971, Tokens/sec: 620681.49\n",
      "Step: 94, Training Loss: 2.63403, LR: 0.0009971, Tokens/sec: 620669.96\n",
      "Step: 95, Training Loss: 2.53452, LR: 0.0009970, Tokens/sec: 620396.62\n",
      "Step: 96, Training Loss: 2.60787, LR: 0.0009969, Tokens/sec: 621431.49\n",
      "Step: 97, Training Loss: 2.70233, LR: 0.0009968, Tokens/sec: 618945.40\n",
      "Step: 98, Training Loss: 2.63310, LR: 0.0009967, Tokens/sec: 619088.20\n",
      "Step: 99, Training Loss: 2.60672, LR: 0.0009967, Tokens/sec: 620573.23\n",
      "Step: 100, Training Loss: 2.60230, LR: 0.0009966, Tokens/sec: 620702.40\n",
      "Step: 101, Training Loss: 2.56495, LR: 0.0009965, Tokens/sec: 621152.49\n",
      "Step: 102, Training Loss: 2.55428, LR: 0.0009964, Tokens/sec: 621524.90\n",
      "Step: 103, Training Loss: 2.55799, LR: 0.0009963, Tokens/sec: 620994.30\n",
      "Step: 104, Training Loss: 2.53253, LR: 0.0009962, Tokens/sec: 621202.43\n",
      "Step: 105, Training Loss: 2.53581, LR: 0.0009961, Tokens/sec: 620545.61\n",
      "Step: 106, Training Loss: 2.51533, LR: 0.0009960, Tokens/sec: 621660.20\n",
      "Step: 107, Training Loss: 2.56205, LR: 0.0009959, Tokens/sec: 621209.22\n",
      "Step: 108, Training Loss: 2.47845, LR: 0.0009958, Tokens/sec: 620655.01\n",
      "Step: 109, Training Loss: 2.45626, LR: 0.0009958, Tokens/sec: 621497.07\n",
      "Step: 110, Training Loss: 2.46686, LR: 0.0009957, Tokens/sec: 621372.47\n",
      "Step: 111, Training Loss: 2.49604, LR: 0.0009956, Tokens/sec: 621426.55\n",
      "Step: 112, Training Loss: 2.47351, LR: 0.0009955, Tokens/sec: 622240.94\n",
      "Step: 113, Training Loss: 2.46810, LR: 0.0009954, Tokens/sec: 620461.30\n",
      "Step: 114, Training Loss: 2.43167, LR: 0.0009953, Tokens/sec: 621566.23\n",
      "Step: 115, Training Loss: 2.43021, LR: 0.0009952, Tokens/sec: 621585.98\n",
      "Step: 116, Training Loss: 2.39423, LR: 0.0009951, Tokens/sec: 621427.99\n",
      "Step: 117, Training Loss: 2.42551, LR: 0.0009950, Tokens/sec: 622764.26\n",
      "Step: 118, Training Loss: 2.48852, LR: 0.0009949, Tokens/sec: 620826.43\n",
      "Step: 119, Training Loss: 2.46607, LR: 0.0009947, Tokens/sec: 621632.11\n",
      "Step: 120, Training Loss: 2.45080, LR: 0.0009946, Tokens/sec: 621827.08\n",
      "Step: 121, Training Loss: 2.46581, LR: 0.0009945, Tokens/sec: 621085.83\n",
      "Step: 122, Training Loss: 2.39205, LR: 0.0009944, Tokens/sec: 622133.10\n",
      "Step: 123, Training Loss: 2.46082, LR: 0.0009943, Tokens/sec: 621020.99\n",
      "Step: 124, Training Loss: 2.43686, LR: 0.0009942, Tokens/sec: 619870.68\n",
      "Step: 125, Training Loss: 2.35393, LR: 0.0009941, Tokens/sec: 621641.69\n",
      "Step: 126, Training Loss: 2.42632, LR: 0.0009940, Tokens/sec: 620315.31\n",
      "Step: 127, Training Loss: 2.79748, LR: 0.0009939, Tokens/sec: 618393.75\n",
      "Step: 128, Training Loss: 2.49339, LR: 0.0009938, Tokens/sec: 620889.73\n",
      "Step: 129, Training Loss: 2.44884, LR: 0.0009936, Tokens/sec: 620071.57\n",
      "Step: 130, Training Loss: 2.40266, LR: 0.0009935, Tokens/sec: 619210.44\n",
      "Step: 131, Training Loss: 2.38343, LR: 0.0009934, Tokens/sec: 622965.56\n",
      "Step: 132, Training Loss: 2.37374, LR: 0.0009933, Tokens/sec: 620564.62\n",
      "Step: 133, Training Loss: 2.54952, LR: 0.0009932, Tokens/sec: 620716.36\n",
      "Step: 134, Training Loss: 2.38083, LR: 0.0009930, Tokens/sec: 621106.58\n",
      "Step: 135, Training Loss: 2.63675, LR: 0.0009929, Tokens/sec: 620696.25\n",
      "Step: 136, Training Loss: 2.47443, LR: 0.0009928, Tokens/sec: 620788.10\n",
      "Step: 137, Training Loss: 2.38871, LR: 0.0009927, Tokens/sec: 621217.03\n",
      "Step: 138, Training Loss: 2.29538, LR: 0.0009925, Tokens/sec: 619585.60\n",
      "Step: 139, Training Loss: 2.43848, LR: 0.0009924, Tokens/sec: 620072.26\n",
      "Step: 140, Training Loss: 2.29596, LR: 0.0009923, Tokens/sec: 621091.19\n",
      "Step: 141, Training Loss: 2.49421, LR: 0.0009922, Tokens/sec: 621679.31\n",
      "Step: 142, Training Loss: 2.29142, LR: 0.0009920, Tokens/sec: 619459.34\n",
      "Step: 143, Training Loss: 2.26570, LR: 0.0009919, Tokens/sec: 619836.51\n",
      "Step: 144, Training Loss: 2.24205, LR: 0.0009918, Tokens/sec: 619845.63\n",
      "Step: 145, Training Loss: 2.32763, LR: 0.0009916, Tokens/sec: 619611.63\n",
      "Step: 146, Training Loss: 2.26433, LR: 0.0009915, Tokens/sec: 618755.84\n",
      "Step: 147, Training Loss: 2.50167, LR: 0.0009914, Tokens/sec: 618905.38\n",
      "Step: 148, Training Loss: 2.37648, LR: 0.0009912, Tokens/sec: 620917.89\n",
      "Step: 149, Training Loss: 2.43041, LR: 0.0009911, Tokens/sec: 618641.80\n",
      "Step: 150, Training Loss: 2.25297, LR: 0.0009910, Tokens/sec: 621008.05\n",
      "Step: 151, Training Loss: 2.23521, LR: 0.0009908, Tokens/sec: 619822.58\n",
      "Step: 152, Training Loss: 2.17254, LR: 0.0009907, Tokens/sec: 619437.91\n",
      "Step: 153, Training Loss: 2.26627, LR: 0.0009905, Tokens/sec: 620470.20\n",
      "Step: 154, Training Loss: 2.25150, LR: 0.0009904, Tokens/sec: 618880.58\n",
      "Step: 155, Training Loss: 2.04167, LR: 0.0009903, Tokens/sec: 620229.67\n",
      "Step: 156, Training Loss: 2.11375, LR: 0.0009901, Tokens/sec: 619502.83\n",
      "Step: 157, Training Loss: 2.10291, LR: 0.0009900, Tokens/sec: 619036.07\n",
      "Step: 158, Training Loss: 2.19086, LR: 0.0009898, Tokens/sec: 619520.93\n",
      "Step: 159, Training Loss: 2.03224, LR: 0.0009897, Tokens/sec: 621583.47\n",
      "Step: 160, Training Loss: 2.19280, LR: 0.0009895, Tokens/sec: 621073.93\n",
      "Step: 161, Training Loss: 2.32846, LR: 0.0009894, Tokens/sec: 620757.73\n",
      "Step: 162, Training Loss: 2.29791, LR: 0.0009892, Tokens/sec: 619979.40\n",
      "Step: 163, Training Loss: 2.27493, LR: 0.0009891, Tokens/sec: 621162.54\n",
      "Step: 164, Training Loss: 2.23718, LR: 0.0009889, Tokens/sec: 618903.40\n",
      "Step: 165, Training Loss: 2.37507, LR: 0.0009888, Tokens/sec: 619535.14\n",
      "Step: 166, Training Loss: 2.21018, LR: 0.0009886, Tokens/sec: 620275.00\n",
      "Step: 167, Training Loss: 2.25456, LR: 0.0009885, Tokens/sec: 620001.78\n",
      "Step: 168, Training Loss: 2.21139, LR: 0.0009883, Tokens/sec: 621223.97\n",
      "Step: 169, Training Loss: 2.20491, LR: 0.0009881, Tokens/sec: 619026.94\n",
      "Step: 170, Training Loss: 2.25604, LR: 0.0009880, Tokens/sec: 620505.02\n",
      "Step: 171, Training Loss: 2.36479, LR: 0.0009878, Tokens/sec: 620340.81\n",
      "Step: 172, Training Loss: 2.12457, LR: 0.0009877, Tokens/sec: 620401.67\n",
      "Step: 173, Training Loss: 2.17302, LR: 0.0009875, Tokens/sec: 621040.15\n",
      "Step: 174, Training Loss: 2.27116, LR: 0.0009873, Tokens/sec: 619486.70\n",
      "Step: 175, Training Loss: 2.16541, LR: 0.0009872, Tokens/sec: 619747.44\n",
      "Step: 176, Training Loss: 2.20742, LR: 0.0009870, Tokens/sec: 621001.32\n",
      "Step: 177, Training Loss: 2.18590, LR: 0.0009868, Tokens/sec: 619302.44\n",
      "Step: 178, Training Loss: 2.14460, LR: 0.0009867, Tokens/sec: 620173.97\n",
      "Step: 179, Training Loss: 2.23150, LR: 0.0009865, Tokens/sec: 619154.23\n",
      "Step: 180, Training Loss: 2.06051, LR: 0.0009863, Tokens/sec: 620491.34\n",
      "Step: 181, Training Loss: 2.06597, LR: 0.0009862, Tokens/sec: 619312.18\n",
      "Step: 182, Training Loss: 2.10760, LR: 0.0009860, Tokens/sec: 620563.96\n",
      "Step: 183, Training Loss: 2.08959, LR: 0.0009858, Tokens/sec: 619760.53\n",
      "Step: 184, Training Loss: 2.12086, LR: 0.0009856, Tokens/sec: 620593.09\n",
      "Step: 185, Training Loss: 2.11841, LR: 0.0009855, Tokens/sec: 619963.75\n",
      "Step: 186, Training Loss: 2.09483, LR: 0.0009853, Tokens/sec: 620720.51\n",
      "Step: 187, Training Loss: 2.09442, LR: 0.0009851, Tokens/sec: 619851.85\n",
      "Step: 188, Training Loss: 2.13177, LR: 0.0009849, Tokens/sec: 621588.33\n",
      "Step: 189, Training Loss: 2.02219, LR: 0.0009848, Tokens/sec: 620039.63\n",
      "Step: 190, Training Loss: 2.20843, LR: 0.0009846, Tokens/sec: 620520.72\n",
      "Step: 191, Training Loss: 2.14017, LR: 0.0009844, Tokens/sec: 619893.39\n",
      "Step: 192, Training Loss: 2.09662, LR: 0.0009842, Tokens/sec: 620575.33\n",
      "Step: 193, Training Loss: 2.07056, LR: 0.0009840, Tokens/sec: 619584.23\n",
      "Step: 194, Training Loss: 2.69810, LR: 0.0009838, Tokens/sec: 619409.05\n",
      "Step: 195, Training Loss: 2.10169, LR: 0.0009837, Tokens/sec: 620210.16\n",
      "Step: 196, Training Loss: 2.17753, LR: 0.0009835, Tokens/sec: 622222.26\n",
      "Step: 197, Training Loss: 2.23630, LR: 0.0009833, Tokens/sec: 619645.92\n",
      "Step: 198, Training Loss: 2.06165, LR: 0.0009831, Tokens/sec: 620036.58\n",
      "Step: 199, Training Loss: 2.08360, LR: 0.0009829, Tokens/sec: 620331.80\n",
      "Step: 200, Training Loss: 2.07598, LR: 0.0009827, Tokens/sec: 619870.70\n",
      "Step: 201, Training Loss: 2.13431, LR: 0.0009825, Tokens/sec: 619876.63\n",
      "Step: 202, Training Loss: 2.05484, LR: 0.0009823, Tokens/sec: 620139.29\n",
      "Step: 203, Training Loss: 2.35687, LR: 0.0009821, Tokens/sec: 621068.90\n",
      "Step: 204, Training Loss: 2.22470, LR: 0.0009819, Tokens/sec: 618557.58\n",
      "Step: 205, Training Loss: 2.21814, LR: 0.0009818, Tokens/sec: 620627.79\n",
      "Step: 206, Training Loss: 2.12673, LR: 0.0009816, Tokens/sec: 621062.69\n",
      "Step: 207, Training Loss: 2.14165, LR: 0.0009814, Tokens/sec: 619282.81\n",
      "Step: 208, Training Loss: 2.36362, LR: 0.0009812, Tokens/sec: 619781.57\n",
      "Step: 209, Training Loss: 2.12753, LR: 0.0009810, Tokens/sec: 620358.06\n",
      "Step: 210, Training Loss: 2.14132, LR: 0.0009808, Tokens/sec: 620324.99\n",
      "Step: 211, Training Loss: 2.32222, LR: 0.0009806, Tokens/sec: 619183.48\n",
      "Step: 212, Training Loss: 2.23668, LR: 0.0009804, Tokens/sec: 618263.51\n",
      "Step: 213, Training Loss: 2.16900, LR: 0.0009802, Tokens/sec: 619730.50\n",
      "Step: 214, Training Loss: 2.06372, LR: 0.0009799, Tokens/sec: 620641.87\n",
      "Step: 215, Training Loss: 1.98356, LR: 0.0009797, Tokens/sec: 619995.68\n",
      "Step: 216, Training Loss: 2.02601, LR: 0.0009795, Tokens/sec: 620725.11\n",
      "Step: 217, Training Loss: 2.15241, LR: 0.0009793, Tokens/sec: 619642.81\n",
      "Step: 218, Training Loss: 2.06725, LR: 0.0009791, Tokens/sec: 617952.62\n",
      "Step: 219, Training Loss: 2.14306, LR: 0.0009789, Tokens/sec: 618466.55\n",
      "Step: 220, Training Loss: 1.99847, LR: 0.0009787, Tokens/sec: 620515.44\n",
      "Step: 221, Training Loss: 2.00176, LR: 0.0009785, Tokens/sec: 619749.68\n",
      "Step: 222, Training Loss: 2.13411, LR: 0.0009783, Tokens/sec: 620387.16\n",
      "Step: 223, Training Loss: 2.04305, LR: 0.0009781, Tokens/sec: 620338.04\n",
      "Step: 224, Training Loss: 2.18114, LR: 0.0009778, Tokens/sec: 618738.69\n",
      "Step: 225, Training Loss: 5.57159, LR: 0.0009776, Tokens/sec: 620994.04\n",
      "Step: 226, Training Loss: 2.10014, LR: 0.0009774, Tokens/sec: 619612.57\n",
      "Step: 227, Training Loss: 2.06986, LR: 0.0009772, Tokens/sec: 619770.29\n",
      "Step: 228, Training Loss: 2.02262, LR: 0.0009770, Tokens/sec: 619209.66\n",
      "Step: 229, Training Loss: 1.98680, LR: 0.0009768, Tokens/sec: 618544.45\n",
      "Step: 230, Training Loss: 2.02221, LR: 0.0009765, Tokens/sec: 618760.32\n",
      "Step: 231, Training Loss: 2.00338, LR: 0.0009763, Tokens/sec: 619276.59\n",
      "Step: 232, Training Loss: 2.13383, LR: 0.0009761, Tokens/sec: 619271.89\n",
      "Step: 233, Training Loss: 2.02981, LR: 0.0009759, Tokens/sec: 618938.10\n",
      "Step: 234, Training Loss: 1.86996, LR: 0.0009756, Tokens/sec: 619997.73\n",
      "Step: 235, Training Loss: 1.81260, LR: 0.0009754, Tokens/sec: 619102.35\n",
      "Step: 236, Training Loss: 1.89398, LR: 0.0009752, Tokens/sec: 617390.31\n",
      "Step: 237, Training Loss: 1.78801, LR: 0.0009750, Tokens/sec: 619336.92\n",
      "Step: 238, Training Loss: 2.19302, LR: 0.0009747, Tokens/sec: 619259.98\n",
      "Step: 239, Training Loss: 2.16104, LR: 0.0009745, Tokens/sec: 619745.32\n",
      "Step: 240, Training Loss: 2.09968, LR: 0.0009743, Tokens/sec: 619367.70\n",
      "Step: 241, Training Loss: 2.11671, LR: 0.0009740, Tokens/sec: 621264.45\n",
      "Step: 242, Training Loss: 2.10294, LR: 0.0009738, Tokens/sec: 619941.70\n",
      "Step: 243, Training Loss: 2.09287, LR: 0.0009736, Tokens/sec: 618961.13\n",
      "Step: 244, Training Loss: 2.08503, LR: 0.0009733, Tokens/sec: 619645.60\n",
      "Step: 245, Training Loss: 2.09068, LR: 0.0009731, Tokens/sec: 619141.37\n",
      "Step: 246, Training Loss: 2.04806, LR: 0.0009729, Tokens/sec: 620028.33\n",
      "Step: 247, Training Loss: 1.93469, LR: 0.0009726, Tokens/sec: 619719.37\n",
      "Step: 248, Training Loss: 2.16605, LR: 0.0009724, Tokens/sec: 618015.82\n",
      "Step: 249, Training Loss: 1.94477, LR: 0.0009721, Tokens/sec: 620058.80\n",
      "Step: 250, Training Loss: 1.98091, LR: 0.0009719, Tokens/sec: 620425.27\n",
      "Step: 251, Training Loss: 1.95067, LR: 0.0009717, Tokens/sec: 617880.66\n",
      "Step: 252, Training Loss: 1.99325, LR: 0.0009714, Tokens/sec: 620840.46\n",
      "Step: 253, Training Loss: 2.02870, LR: 0.0009712, Tokens/sec: 619725.77\n",
      "Step: 254, Training Loss: 1.95223, LR: 0.0009709, Tokens/sec: 619103.34\n",
      "Step: 255, Training Loss: 2.05523, LR: 0.0009707, Tokens/sec: 619181.60\n",
      "Step: 256, Training Loss: 1.93665, LR: 0.0009704, Tokens/sec: 619517.19\n",
      "Step: 257, Training Loss: 2.03862, LR: 0.0009702, Tokens/sec: 619684.82\n",
      "Step: 258, Training Loss: 1.88961, LR: 0.0009699, Tokens/sec: 619242.86\n",
      "Step: 259, Training Loss: 1.83685, LR: 0.0009697, Tokens/sec: 619554.85\n",
      "Step: 260, Training Loss: 2.02971, LR: 0.0009694, Tokens/sec: 619084.42\n",
      "Step: 261, Training Loss: 2.00766, LR: 0.0009692, Tokens/sec: 619500.00\n",
      "Step: 262, Training Loss: 2.08514, LR: 0.0009689, Tokens/sec: 620866.96\n",
      "Step: 263, Training Loss: 1.99690, LR: 0.0009687, Tokens/sec: 619175.23\n",
      "Step: 264, Training Loss: 1.93331, LR: 0.0009684, Tokens/sec: 619963.32\n",
      "Step: 265, Training Loss: 1.74824, LR: 0.0009682, Tokens/sec: 619579.44\n",
      "Step: 266, Training Loss: 1.60367, LR: 0.0009679, Tokens/sec: 618858.71\n",
      "Step: 267, Training Loss: 1.62794, LR: 0.0009676, Tokens/sec: 618707.50\n",
      "Step: 268, Training Loss: 1.24378, LR: 0.0009674, Tokens/sec: 619554.68\n",
      "Step: 269, Training Loss: 0.85486, LR: 0.0009671, Tokens/sec: 619497.98\n",
      "Step: 270, Training Loss: 1.30246, LR: 0.0009669, Tokens/sec: 620118.96\n",
      "Step: 271, Training Loss: 1.25192, LR: 0.0009666, Tokens/sec: 619795.35\n",
      "Step: 272, Training Loss: 1.87256, LR: 0.0009663, Tokens/sec: 620852.40\n",
      "Step: 273, Training Loss: 2.16195, LR: 0.0009661, Tokens/sec: 620523.51\n",
      "Step: 274, Training Loss: 2.13727, LR: 0.0009658, Tokens/sec: 619946.21\n",
      "Step: 275, Training Loss: 2.10268, LR: 0.0009655, Tokens/sec: 619911.24\n",
      "Step: 276, Training Loss: 2.20300, LR: 0.0009653, Tokens/sec: 619413.40\n",
      "Step: 277, Training Loss: 2.08679, LR: 0.0009650, Tokens/sec: 619445.37\n",
      "Step: 278, Training Loss: 2.03479, LR: 0.0009647, Tokens/sec: 620164.54\n",
      "Step: 279, Training Loss: 2.08011, LR: 0.0009645, Tokens/sec: 619620.64\n",
      "Step: 280, Training Loss: 2.09747, LR: 0.0009642, Tokens/sec: 619592.25\n",
      "Step: 281, Training Loss: 2.13442, LR: 0.0009639, Tokens/sec: 620496.88\n",
      "Step: 282, Training Loss: 2.08610, LR: 0.0009637, Tokens/sec: 620040.33\n",
      "Step: 283, Training Loss: 2.15572, LR: 0.0009634, Tokens/sec: 621727.01\n",
      "Step: 284, Training Loss: 2.03926, LR: 0.0009631, Tokens/sec: 621071.97\n",
      "Step: 285, Training Loss: 2.04498, LR: 0.0009628, Tokens/sec: 620475.18\n",
      "Step: 286, Training Loss: 2.01850, LR: 0.0009626, Tokens/sec: 620262.73\n",
      "Step: 287, Training Loss: 2.14982, LR: 0.0009623, Tokens/sec: 620676.80\n",
      "Step: 288, Training Loss: 2.02482, LR: 0.0009620, Tokens/sec: 619395.05\n",
      "Step: 289, Training Loss: 1.96683, LR: 0.0009617, Tokens/sec: 619585.78\n",
      "Step: 290, Training Loss: 2.07159, LR: 0.0009614, Tokens/sec: 618783.93\n",
      "Step: 291, Training Loss: 2.00248, LR: 0.0009612, Tokens/sec: 620003.98\n",
      "Step: 292, Training Loss: 2.01326, LR: 0.0009609, Tokens/sec: 619532.17\n",
      "Step: 293, Training Loss: 1.94874, LR: 0.0009606, Tokens/sec: 618998.37\n",
      "Step: 294, Training Loss: 1.97922, LR: 0.0009603, Tokens/sec: 620141.99\n",
      "Step: 295, Training Loss: 1.98491, LR: 0.0009600, Tokens/sec: 620030.85\n",
      "Step: 296, Training Loss: 1.93936, LR: 0.0009597, Tokens/sec: 619311.15\n",
      "Step: 297, Training Loss: 1.93869, LR: 0.0009594, Tokens/sec: 620373.31\n",
      "Step: 298, Training Loss: 1.97995, LR: 0.0009592, Tokens/sec: 619025.29\n",
      "Step: 299, Training Loss: 1.91818, LR: 0.0009589, Tokens/sec: 620249.63\n",
      "Step: 300, Training Loss: 1.94215, LR: 0.0009586, Tokens/sec: 619787.72\n",
      "Step: 301, Training Loss: 1.96357, LR: 0.0009583, Tokens/sec: 619536.12\n",
      "Step: 302, Training Loss: 1.86720, LR: 0.0009580, Tokens/sec: 620692.83\n",
      "Step: 303, Training Loss: 1.91881, LR: 0.0009577, Tokens/sec: 621436.32\n",
      "Step: 304, Training Loss: 1.91419, LR: 0.0009574, Tokens/sec: 619944.92\n",
      "Step: 305, Training Loss: 1.89108, LR: 0.0009571, Tokens/sec: 620778.55\n",
      "Step: 306, Training Loss: 1.94528, LR: 0.0009568, Tokens/sec: 619315.40\n",
      "Step: 307, Training Loss: 2.13522, LR: 0.0009565, Tokens/sec: 620447.14\n",
      "Step: 308, Training Loss: 2.15828, LR: 0.0009562, Tokens/sec: 619328.54\n",
      "Step: 309, Training Loss: 1.96531, LR: 0.0009559, Tokens/sec: 619729.15\n",
      "Step: 310, Training Loss: 1.92233, LR: 0.0009556, Tokens/sec: 622027.04\n",
      "Step: 311, Training Loss: 1.93530, LR: 0.0009553, Tokens/sec: 621435.43\n",
      "Step: 312, Training Loss: 1.92258, LR: 0.0009550, Tokens/sec: 618264.32\n",
      "Step: 313, Training Loss: 2.11094, LR: 0.0009547, Tokens/sec: 619156.93\n",
      "Step: 314, Training Loss: 2.05305, LR: 0.0009544, Tokens/sec: 620163.92\n",
      "Step: 315, Training Loss: 1.85844, LR: 0.0009541, Tokens/sec: 620212.52\n",
      "Step: 316, Training Loss: 1.90651, LR: 0.0009538, Tokens/sec: 619741.89\n",
      "Step: 317, Training Loss: 1.87735, LR: 0.0009535, Tokens/sec: 619983.03\n",
      "Step: 318, Training Loss: 2.20949, LR: 0.0009532, Tokens/sec: 619934.97\n",
      "Step: 319, Training Loss: 1.87564, LR: 0.0009529, Tokens/sec: 619777.53\n",
      "Step: 320, Training Loss: 1.95905, LR: 0.0009525, Tokens/sec: 618174.05\n",
      "Step: 321, Training Loss: 1.85407, LR: 0.0009522, Tokens/sec: 619328.72\n",
      "Step: 322, Training Loss: 1.87597, LR: 0.0009519, Tokens/sec: 619434.53\n",
      "Step: 323, Training Loss: 1.86442, LR: 0.0009516, Tokens/sec: 620220.70\n",
      "Step: 324, Training Loss: 1.84174, LR: 0.0009513, Tokens/sec: 620548.49\n",
      "Step: 325, Training Loss: 1.86375, LR: 0.0009510, Tokens/sec: 618960.09\n",
      "Step: 326, Training Loss: 1.78957, LR: 0.0009507, Tokens/sec: 618627.59\n",
      "Step: 327, Training Loss: 1.85642, LR: 0.0009504, Tokens/sec: 619658.96\n",
      "Step: 328, Training Loss: 1.82017, LR: 0.0009500, Tokens/sec: 620309.77\n",
      "Step: 329, Training Loss: 1.78287, LR: 0.0009497, Tokens/sec: 619226.95\n",
      "Step: 330, Training Loss: 1.94035, LR: 0.0009494, Tokens/sec: 619130.54\n",
      "Step: 331, Training Loss: 1.87651, LR: 0.0009491, Tokens/sec: 619569.81\n",
      "Step: 332, Training Loss: 1.89913, LR: 0.0009488, Tokens/sec: 619194.42\n",
      "Step: 333, Training Loss: 1.86321, LR: 0.0009484, Tokens/sec: 619669.12\n",
      "Step: 334, Training Loss: 1.81264, LR: 0.0009481, Tokens/sec: 618715.65\n",
      "Step: 335, Training Loss: 1.89380, LR: 0.0009478, Tokens/sec: 619412.14\n",
      "Step: 336, Training Loss: 1.88003, LR: 0.0009475, Tokens/sec: 619206.44\n",
      "Step: 337, Training Loss: 1.87676, LR: 0.0009471, Tokens/sec: 619547.95\n",
      "Step: 338, Training Loss: 1.81448, LR: 0.0009468, Tokens/sec: 620017.67\n",
      "Step: 339, Training Loss: 1.82089, LR: 0.0009465, Tokens/sec: 618992.19\n",
      "Step: 340, Training Loss: 1.78672, LR: 0.0009461, Tokens/sec: 619559.68\n",
      "Step: 341, Training Loss: 1.88151, LR: 0.0009458, Tokens/sec: 619430.30\n",
      "Step: 342, Training Loss: 1.83941, LR: 0.0009455, Tokens/sec: 619719.53\n",
      "Step: 343, Training Loss: 1.80520, LR: 0.0009452, Tokens/sec: 619889.69\n",
      "Step: 344, Training Loss: 1.79603, LR: 0.0009448, Tokens/sec: 619526.06\n",
      "Step: 345, Training Loss: 1.82371, LR: 0.0009445, Tokens/sec: 619631.94\n",
      "Step: 346, Training Loss: 1.84272, LR: 0.0009442, Tokens/sec: 619548.80\n",
      "Step: 347, Training Loss: 1.80269, LR: 0.0009438, Tokens/sec: 619948.07\n",
      "Step: 348, Training Loss: 1.85321, LR: 0.0009435, Tokens/sec: 618937.20\n",
      "Step: 349, Training Loss: 1.71816, LR: 0.0009431, Tokens/sec: 620397.57\n",
      "Step: 350, Training Loss: 1.77807, LR: 0.0009428, Tokens/sec: 618556.33\n",
      "Step: 351, Training Loss: 1.75270, LR: 0.0009425, Tokens/sec: 618764.14\n",
      "Step: 352, Training Loss: 1.82281, LR: 0.0009421, Tokens/sec: 619274.08\n",
      "Step: 353, Training Loss: 1.81985, LR: 0.0009418, Tokens/sec: 619512.80\n",
      "Step: 354, Training Loss: 1.83025, LR: 0.0009414, Tokens/sec: 621172.27\n",
      "Step: 355, Training Loss: 1.70628, LR: 0.0009411, Tokens/sec: 619185.22\n",
      "Step: 356, Training Loss: 1.77207, LR: 0.0009407, Tokens/sec: 619420.50\n",
      "Step: 357, Training Loss: 1.74444, LR: 0.0009404, Tokens/sec: 619459.35\n",
      "Step: 358, Training Loss: 1.79642, LR: 0.0009401, Tokens/sec: 619031.97\n",
      "Step: 359, Training Loss: 1.74813, LR: 0.0009397, Tokens/sec: 618282.43\n",
      "Step: 360, Training Loss: 1.75018, LR: 0.0009394, Tokens/sec: 619526.09\n",
      "Step: 361, Training Loss: 1.72480, LR: 0.0009390, Tokens/sec: 619863.86\n",
      "Step: 362, Training Loss: 1.71595, LR: 0.0009387, Tokens/sec: 618027.51\n",
      "Step: 363, Training Loss: 1.70490, LR: 0.0009383, Tokens/sec: 617306.17\n",
      "Step: 364, Training Loss: 1.69266, LR: 0.0009380, Tokens/sec: 620075.39\n",
      "Step: 365, Training Loss: 1.76056, LR: 0.0009376, Tokens/sec: 619580.13\n",
      "Step: 366, Training Loss: 1.72737, LR: 0.0009373, Tokens/sec: 620200.04\n",
      "Step: 367, Training Loss: 1.72523, LR: 0.0009369, Tokens/sec: 619034.44\n",
      "Step: 368, Training Loss: 1.66338, LR: 0.0009365, Tokens/sec: 620067.38\n",
      "Step: 369, Training Loss: 1.70363, LR: 0.0009362, Tokens/sec: 619024.13\n",
      "Step: 370, Training Loss: 1.74627, LR: 0.0009358, Tokens/sec: 620787.51\n",
      "Step: 371, Training Loss: 1.70716, LR: 0.0009355, Tokens/sec: 619299.01\n",
      "Step: 372, Training Loss: 1.83153, LR: 0.0009351, Tokens/sec: 618299.37\n",
      "Step: 373, Training Loss: 1.75014, LR: 0.0009348, Tokens/sec: 619330.80\n",
      "Step: 374, Training Loss: 1.72526, LR: 0.0009344, Tokens/sec: 618867.05\n",
      "Step: 375, Training Loss: 1.70080, LR: 0.0009340, Tokens/sec: 620599.14\n",
      "Step: 376, Training Loss: 1.69347, LR: 0.0009337, Tokens/sec: 618171.64\n",
      "Step: 377, Training Loss: 1.71790, LR: 0.0009333, Tokens/sec: 619700.86\n",
      "Step: 378, Training Loss: 1.70626, LR: 0.0009329, Tokens/sec: 619074.31\n",
      "Step: 379, Training Loss: 1.72509, LR: 0.0009326, Tokens/sec: 618446.82\n",
      "Step: 380, Training Loss: 1.70370, LR: 0.0009322, Tokens/sec: 619343.49\n",
      "Step: 381, Training Loss: 1.70101, LR: 0.0009318, Tokens/sec: 620047.15\n",
      "Step: 382, Training Loss: 1.83603, LR: 0.0009315, Tokens/sec: 619027.73\n",
      "Step: 383, Training Loss: 1.67622, LR: 0.0009311, Tokens/sec: 619321.85\n",
      "Step: 384, Training Loss: 1.66148, LR: 0.0009307, Tokens/sec: 619752.81\n",
      "Step: 385, Training Loss: 1.70684, LR: 0.0009304, Tokens/sec: 619466.05\n",
      "Step: 386, Training Loss: 1.66253, LR: 0.0009300, Tokens/sec: 618529.63\n",
      "Step: 387, Training Loss: 1.64642, LR: 0.0009296, Tokens/sec: 619483.16\n",
      "Step: 388, Training Loss: 1.90768, LR: 0.0009292, Tokens/sec: 620212.92\n",
      "Step: 389, Training Loss: 1.65233, LR: 0.0009289, Tokens/sec: 618030.42\n",
      "Step: 390, Training Loss: 1.67265, LR: 0.0009285, Tokens/sec: 619320.51\n",
      "Step: 391, Training Loss: 1.66010, LR: 0.0009281, Tokens/sec: 618285.51\n",
      "Step: 392, Training Loss: 1.81242, LR: 0.0009277, Tokens/sec: 619351.73\n",
      "Step: 393, Training Loss: 1.66638, LR: 0.0009274, Tokens/sec: 620521.61\n",
      "Step: 394, Training Loss: 1.64022, LR: 0.0009270, Tokens/sec: 618915.46\n",
      "Step: 395, Training Loss: 1.70346, LR: 0.0009266, Tokens/sec: 619299.29\n",
      "Step: 396, Training Loss: 1.72395, LR: 0.0009262, Tokens/sec: 617959.35\n",
      "Step: 397, Training Loss: 1.73298, LR: 0.0009258, Tokens/sec: 617909.78\n",
      "Step: 398, Training Loss: 1.79436, LR: 0.0009255, Tokens/sec: 618764.99\n",
      "Step: 399, Training Loss: 1.68954, LR: 0.0009251, Tokens/sec: 620614.37\n",
      "Step: 400, Training Loss: 1.71805, LR: 0.0009247, Tokens/sec: 620156.32\n",
      "Step: 401, Training Loss: 1.72346, LR: 0.0009243, Tokens/sec: 619357.63\n",
      "Step: 402, Training Loss: 1.82216, LR: 0.0009239, Tokens/sec: 618411.16\n",
      "Step: 403, Training Loss: 1.66933, LR: 0.0009235, Tokens/sec: 618882.06\n",
      "Step: 404, Training Loss: 1.64769, LR: 0.0009231, Tokens/sec: 619687.46\n",
      "Step: 405, Training Loss: 1.66515, LR: 0.0009228, Tokens/sec: 618213.97\n",
      "Step: 406, Training Loss: 1.68204, LR: 0.0009224, Tokens/sec: 618480.92\n",
      "Step: 407, Training Loss: 1.72381, LR: 0.0009220, Tokens/sec: 620163.14\n",
      "Step: 408, Training Loss: 1.76311, LR: 0.0009216, Tokens/sec: 619311.66\n",
      "Step: 409, Training Loss: 1.64861, LR: 0.0009212, Tokens/sec: 620040.36\n",
      "Step: 410, Training Loss: 1.67234, LR: 0.0009208, Tokens/sec: 620193.34\n",
      "Step: 411, Training Loss: 1.64426, LR: 0.0009204, Tokens/sec: 618831.60\n",
      "Step: 412, Training Loss: 1.73259, LR: 0.0009200, Tokens/sec: 619390.12\n",
      "Step: 413, Training Loss: 1.62703, LR: 0.0009196, Tokens/sec: 618889.39\n",
      "Step: 414, Training Loss: 1.64197, LR: 0.0009192, Tokens/sec: 619577.72\n",
      "Step: 415, Training Loss: 1.76334, LR: 0.0009188, Tokens/sec: 619190.09\n",
      "Step: 416, Training Loss: 1.62272, LR: 0.0009184, Tokens/sec: 617941.92\n",
      "Step: 417, Training Loss: 1.72450, LR: 0.0009180, Tokens/sec: 619885.02\n",
      "Step: 418, Training Loss: 1.66629, LR: 0.0009176, Tokens/sec: 620000.67\n",
      "Step: 419, Training Loss: 1.59508, LR: 0.0009172, Tokens/sec: 619880.67\n",
      "Step: 420, Training Loss: 1.72827, LR: 0.0009168, Tokens/sec: 617360.75\n",
      "Step: 421, Training Loss: 1.58400, LR: 0.0009164, Tokens/sec: 618866.99\n",
      "Step: 422, Training Loss: 1.61021, LR: 0.0009160, Tokens/sec: 618850.50\n",
      "Step: 423, Training Loss: 1.68238, LR: 0.0009156, Tokens/sec: 618680.65\n",
      "Step: 424, Training Loss: 1.60880, LR: 0.0009152, Tokens/sec: 619278.65\n",
      "Step: 425, Training Loss: 1.64653, LR: 0.0009148, Tokens/sec: 619694.36\n",
      "Step: 426, Training Loss: 1.55706, LR: 0.0009144, Tokens/sec: 619464.32\n",
      "Step: 427, Training Loss: 1.56900, LR: 0.0009140, Tokens/sec: 618494.74\n",
      "Step: 428, Training Loss: 1.64717, LR: 0.0009136, Tokens/sec: 619732.85\n",
      "Step: 429, Training Loss: 1.58421, LR: 0.0009132, Tokens/sec: 618874.81\n",
      "Step: 430, Training Loss: 1.60426, LR: 0.0009127, Tokens/sec: 617378.76\n",
      "Step: 431, Training Loss: 1.67672, LR: 0.0009123, Tokens/sec: 620287.83\n",
      "Step: 432, Training Loss: 1.65711, LR: 0.0009119, Tokens/sec: 620539.49\n",
      "Step: 433, Training Loss: 1.62051, LR: 0.0009115, Tokens/sec: 618652.58\n",
      "Step: 434, Training Loss: 1.61631, LR: 0.0009111, Tokens/sec: 619815.77\n",
      "Step: 435, Training Loss: 1.60881, LR: 0.0009107, Tokens/sec: 618282.89\n",
      "Step: 436, Training Loss: 1.68887, LR: 0.0009103, Tokens/sec: 618874.24\n",
      "Step: 437, Training Loss: 1.58937, LR: 0.0009098, Tokens/sec: 619195.28\n",
      "Step: 438, Training Loss: 1.72717, LR: 0.0009094, Tokens/sec: 619490.48\n",
      "Step: 439, Training Loss: 1.62154, LR: 0.0009090, Tokens/sec: 619290.67\n",
      "Step: 440, Training Loss: 1.64338, LR: 0.0009086, Tokens/sec: 619544.58\n",
      "Step: 441, Training Loss: 1.55706, LR: 0.0009082, Tokens/sec: 619716.02\n",
      "Step: 442, Training Loss: 1.63377, LR: 0.0009077, Tokens/sec: 618251.47\n",
      "Step: 443, Training Loss: 1.59340, LR: 0.0009073, Tokens/sec: 618833.94\n",
      "Step: 444, Training Loss: 1.55749, LR: 0.0009069, Tokens/sec: 620087.16\n",
      "Step: 445, Training Loss: 1.59562, LR: 0.0009065, Tokens/sec: 618895.36\n",
      "Step: 446, Training Loss: 1.73647, LR: 0.0009061, Tokens/sec: 618447.03\n",
      "Step: 447, Training Loss: 1.60518, LR: 0.0009056, Tokens/sec: 618962.35\n",
      "Step: 448, Training Loss: 1.58251, LR: 0.0009052, Tokens/sec: 618607.73\n",
      "Step: 449, Training Loss: 1.58573, LR: 0.0009048, Tokens/sec: 618841.74\n",
      "Step: 450, Training Loss: 1.56698, LR: 0.0009043, Tokens/sec: 619514.12\n",
      "Step: 451, Training Loss: 1.70874, LR: 0.0009039, Tokens/sec: 617853.27\n",
      "Step: 452, Training Loss: 1.79957, LR: 0.0009035, Tokens/sec: 618722.68\n",
      "Step: 453, Training Loss: 1.52510, LR: 0.0009031, Tokens/sec: 619104.16\n",
      "Step: 454, Training Loss: 1.82772, LR: 0.0009026, Tokens/sec: 617060.74\n",
      "Step: 455, Training Loss: 1.52698, LR: 0.0009022, Tokens/sec: 620024.17\n",
      "Step: 456, Training Loss: 1.60255, LR: 0.0009018, Tokens/sec: 618861.45\n",
      "Step: 457, Training Loss: 1.64187, LR: 0.0009013, Tokens/sec: 620175.98\n",
      "Step: 458, Training Loss: 1.60224, LR: 0.0009009, Tokens/sec: 619390.86\n",
      "Step: 459, Training Loss: 1.57804, LR: 0.0009005, Tokens/sec: 619169.43\n",
      "Step: 460, Training Loss: 1.53372, LR: 0.0009000, Tokens/sec: 620267.51\n",
      "Step: 461, Training Loss: 1.55066, LR: 0.0008996, Tokens/sec: 620488.01\n",
      "Step: 462, Training Loss: 1.49764, LR: 0.0008991, Tokens/sec: 618576.46\n",
      "Step: 463, Training Loss: 1.56055, LR: 0.0008987, Tokens/sec: 618774.48\n",
      "Step: 464, Training Loss: 1.56418, LR: 0.0008983, Tokens/sec: 619094.41\n",
      "Step: 465, Training Loss: 1.54950, LR: 0.0008978, Tokens/sec: 618461.44\n",
      "Step: 466, Training Loss: 1.59846, LR: 0.0008974, Tokens/sec: 619488.13\n",
      "Step: 467, Training Loss: 1.53579, LR: 0.0008969, Tokens/sec: 618764.35\n",
      "Step: 468, Training Loss: 1.54998, LR: 0.0008965, Tokens/sec: 618403.70\n",
      "Step: 469, Training Loss: 1.52054, LR: 0.0008961, Tokens/sec: 619325.20\n",
      "Step: 470, Training Loss: 1.50080, LR: 0.0008956, Tokens/sec: 619587.84\n",
      "Step: 471, Training Loss: 1.62244, LR: 0.0008952, Tokens/sec: 618995.97\n",
      "Step: 472, Training Loss: 1.53098, LR: 0.0008947, Tokens/sec: 618800.21\n",
      "Step: 473, Training Loss: 1.55031, LR: 0.0008943, Tokens/sec: 619949.60\n",
      "Step: 474, Training Loss: 1.48536, LR: 0.0008938, Tokens/sec: 619770.73\n",
      "Step: 475, Training Loss: 1.47994, LR: 0.0008934, Tokens/sec: 618306.07\n",
      "Step: 476, Training Loss: 1.51434, LR: 0.0008929, Tokens/sec: 619554.96\n",
      "Step: 477, Training Loss: 1.49869, LR: 0.0008925, Tokens/sec: 619326.44\n",
      "Step: 478, Training Loss: 1.52229, LR: 0.0008920, Tokens/sec: 617940.46\n",
      "Step: 479, Training Loss: 1.45651, LR: 0.0008916, Tokens/sec: 619380.38\n",
      "Step: 480, Training Loss: 1.66667, LR: 0.0008911, Tokens/sec: 619789.77\n",
      "Step: 481, Training Loss: 1.54067, LR: 0.0008907, Tokens/sec: 619218.57\n",
      "Step: 482, Training Loss: 1.56254, LR: 0.0008902, Tokens/sec: 618036.91\n",
      "Step: 483, Training Loss: 1.47648, LR: 0.0008898, Tokens/sec: 619065.87\n",
      "Step: 484, Training Loss: 1.55616, LR: 0.0008893, Tokens/sec: 619022.19\n",
      "Step: 485, Training Loss: 1.48705, LR: 0.0008888, Tokens/sec: 619642.35\n",
      "Step: 486, Training Loss: 1.55704, LR: 0.0008884, Tokens/sec: 619476.26\n",
      "Step: 487, Training Loss: 1.93439, LR: 0.0008879, Tokens/sec: 619517.63\n",
      "Step: 488, Training Loss: 1.51415, LR: 0.0008875, Tokens/sec: 619471.73\n",
      "Step: 489, Training Loss: 1.58320, LR: 0.0008870, Tokens/sec: 618717.28\n",
      "Step: 490, Training Loss: 1.25715, LR: 0.0008865, Tokens/sec: 619880.69\n",
      "Step: 491, Training Loss: 0.82484, LR: 0.0008861, Tokens/sec: 617517.13\n",
      "Step: 492, Training Loss: 1.00644, LR: 0.0008856, Tokens/sec: 619307.31\n",
      "Step: 493, Training Loss: 0.66221, LR: 0.0008852, Tokens/sec: 619910.27\n",
      "Step: 494, Training Loss: 0.53847, LR: 0.0008847, Tokens/sec: 619119.51\n",
      "Step: 495, Training Loss: 0.54665, LR: 0.0008842, Tokens/sec: 619863.24\n",
      "Step: 496, Training Loss: 0.49587, LR: 0.0008838, Tokens/sec: 619830.78\n",
      "Step: 497, Training Loss: 0.35456, LR: 0.0008833, Tokens/sec: 619331.26\n",
      "Step: 498, Training Loss: 0.33363, LR: 0.0008828, Tokens/sec: 618022.84\n",
      "Step: 499, Training Loss: 0.36857, LR: 0.0008824, Tokens/sec: 619821.43\n",
      "Step: 500, Training Loss: 0.38545, LR: 0.0008819, Tokens/sec: 617751.86\n",
      "Computing Eval loss, steps: 21\n",
      "Step: 500, Eval Loss: 2.78918\n",
      "Step: 501, Training Loss: 0.32072, LR: 0.0008814, Tokens/sec: 618186.93\n",
      "Step: 502, Training Loss: 1.17873, LR: 0.0008810, Tokens/sec: 618919.89\n",
      "Step: 503, Training Loss: 0.48431, LR: 0.0008805, Tokens/sec: 619387.03\n",
      "Step: 504, Training Loss: 0.50611, LR: 0.0008800, Tokens/sec: 619505.45\n",
      "Step: 505, Training Loss: 0.43265, LR: 0.0008795, Tokens/sec: 619786.48\n",
      "Step: 506, Training Loss: 0.49628, LR: 0.0008791, Tokens/sec: 619000.75\n",
      "Step: 507, Training Loss: 0.45909, LR: 0.0008786, Tokens/sec: 618408.04\n",
      "Step: 508, Training Loss: 0.54267, LR: 0.0008781, Tokens/sec: 619170.20\n",
      "Step: 509, Training Loss: 0.53895, LR: 0.0008776, Tokens/sec: 618085.47\n",
      "Step: 510, Training Loss: 1.18511, LR: 0.0008772, Tokens/sec: 619159.72\n",
      "Step: 511, Training Loss: 2.37288, LR: 0.0008767, Tokens/sec: 619182.33\n",
      "Step: 512, Training Loss: 2.16804, LR: 0.0008762, Tokens/sec: 618774.23\n",
      "Step: 513, Training Loss: 2.09145, LR: 0.0008757, Tokens/sec: 618981.67\n",
      "Step: 514, Training Loss: 2.04498, LR: 0.0008752, Tokens/sec: 618853.97\n",
      "Step: 515, Training Loss: 1.99064, LR: 0.0008748, Tokens/sec: 619180.51\n",
      "Step: 516, Training Loss: 1.96535, LR: 0.0008743, Tokens/sec: 619076.65\n",
      "Step: 517, Training Loss: 1.84480, LR: 0.0008738, Tokens/sec: 619268.79\n",
      "Step: 518, Training Loss: 1.29436, LR: 0.0008733, Tokens/sec: 618930.88\n",
      "Step: 519, Training Loss: 1.43380, LR: 0.0008728, Tokens/sec: 618564.25\n",
      "Step: 520, Training Loss: 1.52248, LR: 0.0008723, Tokens/sec: 617878.60\n",
      "Step: 521, Training Loss: 0.69032, LR: 0.0008719, Tokens/sec: 619333.77\n",
      "Step: 522, Training Loss: 0.61132, LR: 0.0008714, Tokens/sec: 619030.98\n",
      "Step: 523, Training Loss: 0.45016, LR: 0.0008709, Tokens/sec: 619122.75\n",
      "Step: 524, Training Loss: 1.36502, LR: 0.0008704, Tokens/sec: 619249.35\n",
      "Step: 525, Training Loss: 2.22012, LR: 0.0008699, Tokens/sec: 618355.54\n",
      "Step: 526, Training Loss: 2.05505, LR: 0.0008694, Tokens/sec: 619455.69\n",
      "Step: 527, Training Loss: 1.52814, LR: 0.0008689, Tokens/sec: 618551.31\n",
      "Step: 528, Training Loss: 2.20917, LR: 0.0008684, Tokens/sec: 618901.77\n",
      "Step: 529, Training Loss: 2.09804, LR: 0.0008680, Tokens/sec: 619452.63\n",
      "Step: 530, Training Loss: 2.00650, LR: 0.0008675, Tokens/sec: 620270.11\n",
      "Step: 531, Training Loss: 1.97901, LR: 0.0008670, Tokens/sec: 619533.42\n",
      "Step: 532, Training Loss: 1.94567, LR: 0.0008665, Tokens/sec: 621038.03\n",
      "Step: 533, Training Loss: 2.14541, LR: 0.0008660, Tokens/sec: 619224.88\n",
      "Step: 534, Training Loss: 1.98613, LR: 0.0008655, Tokens/sec: 619928.30\n",
      "Step: 535, Training Loss: 1.87171, LR: 0.0008650, Tokens/sec: 618777.85\n",
      "Step: 536, Training Loss: 1.86988, LR: 0.0008645, Tokens/sec: 617923.03\n",
      "Step: 537, Training Loss: 1.89299, LR: 0.0008640, Tokens/sec: 620293.46\n",
      "Step: 538, Training Loss: 1.83290, LR: 0.0008635, Tokens/sec: 619123.97\n",
      "Step: 539, Training Loss: 1.82929, LR: 0.0008630, Tokens/sec: 620648.82\n",
      "Step: 540, Training Loss: 1.80483, LR: 0.0008625, Tokens/sec: 619593.66\n",
      "Step: 541, Training Loss: 1.77879, LR: 0.0008620, Tokens/sec: 620447.74\n",
      "Step: 542, Training Loss: 1.68563, LR: 0.0008615, Tokens/sec: 619981.09\n",
      "Step: 543, Training Loss: 1.11574, LR: 0.0008610, Tokens/sec: 620020.86\n",
      "Step: 544, Training Loss: 1.33511, LR: 0.0008605, Tokens/sec: 618934.06\n",
      "Step: 545, Training Loss: 1.92412, LR: 0.0008600, Tokens/sec: 619420.09\n",
      "Step: 546, Training Loss: 1.85580, LR: 0.0008595, Tokens/sec: 619855.12\n",
      "Step: 547, Training Loss: 1.81748, LR: 0.0008590, Tokens/sec: 619447.51\n",
      "Step: 548, Training Loss: 1.89838, LR: 0.0008585, Tokens/sec: 618412.82\n",
      "Step: 549, Training Loss: 2.80020, LR: 0.0008580, Tokens/sec: 619497.87\n",
      "Step: 550, Training Loss: 1.83994, LR: 0.0008575, Tokens/sec: 619810.52\n",
      "Step: 551, Training Loss: 1.98477, LR: 0.0008570, Tokens/sec: 620490.70\n",
      "Step: 552, Training Loss: 1.81255, LR: 0.0008564, Tokens/sec: 621285.09\n",
      "Step: 553, Training Loss: 1.74476, LR: 0.0008559, Tokens/sec: 620362.82\n",
      "Step: 554, Training Loss: 1.77914, LR: 0.0008554, Tokens/sec: 619951.62\n",
      "Step: 555, Training Loss: 1.74712, LR: 0.0008549, Tokens/sec: 619502.68\n",
      "Step: 556, Training Loss: 1.81084, LR: 0.0008544, Tokens/sec: 619671.80\n",
      "Step: 557, Training Loss: 2.03974, LR: 0.0008539, Tokens/sec: 619556.16\n",
      "Step: 558, Training Loss: 1.78107, LR: 0.0008534, Tokens/sec: 619808.77\n",
      "Step: 559, Training Loss: 1.78868, LR: 0.0008529, Tokens/sec: 617928.26\n",
      "Step: 560, Training Loss: 1.79841, LR: 0.0008523, Tokens/sec: 620409.79\n",
      "Step: 561, Training Loss: 1.81337, LR: 0.0008518, Tokens/sec: 619841.70\n",
      "Step: 562, Training Loss: 1.77256, LR: 0.0008513, Tokens/sec: 619479.04\n",
      "Step: 563, Training Loss: 1.79239, LR: 0.0008508, Tokens/sec: 619122.42\n",
      "Step: 564, Training Loss: 1.77536, LR: 0.0008503, Tokens/sec: 619301.23\n",
      "Step: 565, Training Loss: 1.73808, LR: 0.0008498, Tokens/sec: 618251.68\n",
      "Step: 566, Training Loss: 1.70411, LR: 0.0008492, Tokens/sec: 619004.62\n",
      "Step: 567, Training Loss: 1.71918, LR: 0.0008487, Tokens/sec: 618338.33\n",
      "Step: 568, Training Loss: 1.74060, LR: 0.0008482, Tokens/sec: 620046.13\n",
      "Step: 569, Training Loss: 1.74070, LR: 0.0008477, Tokens/sec: 618234.84\n",
      "Step: 570, Training Loss: 1.71511, LR: 0.0008472, Tokens/sec: 619192.27\n",
      "Step: 571, Training Loss: 1.73604, LR: 0.0008466, Tokens/sec: 618640.84\n",
      "Step: 572, Training Loss: 1.69248, LR: 0.0008461, Tokens/sec: 619231.08\n",
      "Step: 573, Training Loss: 1.70696, LR: 0.0008456, Tokens/sec: 619984.12\n",
      "Step: 574, Training Loss: 1.70679, LR: 0.0008451, Tokens/sec: 619261.37\n",
      "Step: 575, Training Loss: 1.63206, LR: 0.0008445, Tokens/sec: 618568.24\n",
      "Step: 576, Training Loss: 1.68347, LR: 0.0008440, Tokens/sec: 620776.14\n",
      "Step: 577, Training Loss: 1.65359, LR: 0.0008435, Tokens/sec: 619623.96\n",
      "Step: 578, Training Loss: 1.66632, LR: 0.0008430, Tokens/sec: 618078.37\n",
      "Step: 579, Training Loss: 1.63681, LR: 0.0008424, Tokens/sec: 619059.13\n",
      "Step: 580, Training Loss: 1.65481, LR: 0.0008419, Tokens/sec: 619068.80\n",
      "Step: 581, Training Loss: 1.64565, LR: 0.0008414, Tokens/sec: 619876.50\n",
      "Step: 582, Training Loss: 1.64523, LR: 0.0008408, Tokens/sec: 619103.75\n",
      "Step: 583, Training Loss: 1.61803, LR: 0.0008403, Tokens/sec: 618674.48\n",
      "Step: 584, Training Loss: 1.59757, LR: 0.0008398, Tokens/sec: 619752.59\n",
      "Step: 585, Training Loss: 1.68673, LR: 0.0008393, Tokens/sec: 619202.61\n",
      "Step: 586, Training Loss: 1.66214, LR: 0.0008387, Tokens/sec: 618607.23\n",
      "Step: 587, Training Loss: 1.66265, LR: 0.0008382, Tokens/sec: 618785.26\n",
      "Step: 588, Training Loss: 1.61982, LR: 0.0008377, Tokens/sec: 620739.04\n",
      "Step: 589, Training Loss: 1.63920, LR: 0.0008371, Tokens/sec: 620584.71\n",
      "Step: 590, Training Loss: 1.63303, LR: 0.0008366, Tokens/sec: 619436.77\n",
      "Step: 591, Training Loss: 1.67271, LR: 0.0008360, Tokens/sec: 620598.80\n",
      "Step: 592, Training Loss: 1.60611, LR: 0.0008355, Tokens/sec: 619314.25\n",
      "Step: 593, Training Loss: 1.57636, LR: 0.0008350, Tokens/sec: 617478.83\n",
      "Step: 594, Training Loss: 1.59480, LR: 0.0008344, Tokens/sec: 619186.09\n",
      "Step: 595, Training Loss: 1.49893, LR: 0.0008339, Tokens/sec: 619630.73\n",
      "Step: 596, Training Loss: 1.65231, LR: 0.0008334, Tokens/sec: 620338.03\n",
      "Step: 597, Training Loss: 1.64458, LR: 0.0008328, Tokens/sec: 618788.18\n",
      "Step: 598, Training Loss: 1.64904, LR: 0.0008323, Tokens/sec: 618124.02\n",
      "Step: 599, Training Loss: 1.59813, LR: 0.0008317, Tokens/sec: 619684.50\n",
      "Step: 600, Training Loss: 1.57271, LR: 0.0008312, Tokens/sec: 618227.71\n",
      "Step: 601, Training Loss: 1.60340, LR: 0.0008306, Tokens/sec: 619459.78\n",
      "Step: 602, Training Loss: 1.66384, LR: 0.0008301, Tokens/sec: 619457.93\n",
      "Step: 603, Training Loss: 1.65851, LR: 0.0008296, Tokens/sec: 620344.04\n",
      "Step: 604, Training Loss: 1.67377, LR: 0.0008290, Tokens/sec: 619783.64\n",
      "Step: 605, Training Loss: 1.62478, LR: 0.0008285, Tokens/sec: 618282.50\n",
      "Step: 606, Training Loss: 1.61017, LR: 0.0008279, Tokens/sec: 620034.10\n",
      "Step: 607, Training Loss: 1.52990, LR: 0.0008274, Tokens/sec: 618626.21\n",
      "Step: 608, Training Loss: 1.55689, LR: 0.0008268, Tokens/sec: 618217.18\n",
      "Step: 609, Training Loss: 1.53355, LR: 0.0008263, Tokens/sec: 618152.91\n",
      "Step: 610, Training Loss: 1.73544, LR: 0.0008257, Tokens/sec: 619715.27\n",
      "Step: 611, Training Loss: 1.54331, LR: 0.0008252, Tokens/sec: 618725.19\n",
      "Step: 612, Training Loss: 1.79231, LR: 0.0008246, Tokens/sec: 620450.01\n",
      "Step: 613, Training Loss: 1.56145, LR: 0.0008241, Tokens/sec: 620130.05\n",
      "Step: 614, Training Loss: 1.59603, LR: 0.0008235, Tokens/sec: 619617.59\n",
      "Step: 615, Training Loss: 1.69659, LR: 0.0008230, Tokens/sec: 619835.77\n",
      "Step: 616, Training Loss: 1.61751, LR: 0.0008224, Tokens/sec: 619733.41\n",
      "Step: 617, Training Loss: 1.52107, LR: 0.0008219, Tokens/sec: 619050.39\n",
      "Step: 618, Training Loss: 1.52729, LR: 0.0008213, Tokens/sec: 619353.95\n",
      "Step: 619, Training Loss: 1.40316, LR: 0.0008208, Tokens/sec: 619485.16\n",
      "Step: 620, Training Loss: 1.55846, LR: 0.0008202, Tokens/sec: 619587.50\n",
      "Step: 621, Training Loss: 1.60024, LR: 0.0008196, Tokens/sec: 619650.70\n",
      "Step: 622, Training Loss: 1.54167, LR: 0.0008191, Tokens/sec: 619192.76\n",
      "Step: 623, Training Loss: 1.49326, LR: 0.0008185, Tokens/sec: 619715.84\n",
      "Step: 624, Training Loss: 1.53857, LR: 0.0008180, Tokens/sec: 619763.88\n",
      "Step: 625, Training Loss: 1.53508, LR: 0.0008174, Tokens/sec: 619702.67\n",
      "Step: 626, Training Loss: 1.52980, LR: 0.0008169, Tokens/sec: 618912.71\n",
      "Step: 627, Training Loss: 1.52423, LR: 0.0008163, Tokens/sec: 620572.75\n",
      "Step: 628, Training Loss: 1.53563, LR: 0.0008157, Tokens/sec: 618852.05\n",
      "Step: 629, Training Loss: 1.50644, LR: 0.0008152, Tokens/sec: 620072.23\n",
      "Step: 630, Training Loss: 1.41086, LR: 0.0008146, Tokens/sec: 618579.53\n",
      "Step: 631, Training Loss: 1.41810, LR: 0.0008141, Tokens/sec: 620073.02\n",
      "Step: 632, Training Loss: 1.65129, LR: 0.0008135, Tokens/sec: 619240.89\n",
      "Step: 633, Training Loss: 1.56224, LR: 0.0008129, Tokens/sec: 620340.81\n",
      "Step: 634, Training Loss: 1.55892, LR: 0.0008124, Tokens/sec: 619884.86\n",
      "Step: 635, Training Loss: 1.53914, LR: 0.0008118, Tokens/sec: 620255.08\n",
      "Step: 636, Training Loss: 1.63814, LR: 0.0008112, Tokens/sec: 619373.89\n",
      "Step: 637, Training Loss: 1.52326, LR: 0.0008107, Tokens/sec: 619858.97\n",
      "Step: 638, Training Loss: 1.50843, LR: 0.0008101, Tokens/sec: 618812.42\n",
      "Step: 639, Training Loss: 1.50733, LR: 0.0008095, Tokens/sec: 618326.60\n",
      "Step: 640, Training Loss: 1.56760, LR: 0.0008090, Tokens/sec: 619405.94\n",
      "Step: 641, Training Loss: 1.52523, LR: 0.0008084, Tokens/sec: 618656.92\n",
      "Step: 642, Training Loss: 1.54855, LR: 0.0008078, Tokens/sec: 618916.93\n",
      "Step: 643, Training Loss: 1.57208, LR: 0.0008073, Tokens/sec: 620047.34\n",
      "Step: 644, Training Loss: 1.54625, LR: 0.0008067, Tokens/sec: 619157.90\n",
      "Step: 645, Training Loss: 1.49687, LR: 0.0008061, Tokens/sec: 619237.23\n",
      "Step: 646, Training Loss: 1.39382, LR: 0.0008055, Tokens/sec: 617898.53\n",
      "Step: 647, Training Loss: 1.47677, LR: 0.0008050, Tokens/sec: 618798.48\n",
      "Step: 648, Training Loss: 1.45911, LR: 0.0008044, Tokens/sec: 619120.05\n",
      "Step: 649, Training Loss: 1.62773, LR: 0.0008038, Tokens/sec: 619752.85\n",
      "Step: 650, Training Loss: 1.71149, LR: 0.0008032, Tokens/sec: 619403.13\n",
      "Step: 651, Training Loss: 2.21866, LR: 0.0008027, Tokens/sec: 618622.81\n",
      "Step: 652, Training Loss: 1.55204, LR: 0.0008021, Tokens/sec: 618832.25\n",
      "Step: 653, Training Loss: 1.52998, LR: 0.0008015, Tokens/sec: 619358.50\n",
      "Step: 654, Training Loss: 1.54836, LR: 0.0008009, Tokens/sec: 618829.64\n",
      "Step: 655, Training Loss: 1.56314, LR: 0.0008004, Tokens/sec: 618907.38\n",
      "Step: 656, Training Loss: 1.42938, LR: 0.0007998, Tokens/sec: 618481.53\n",
      "Step: 657, Training Loss: 1.54362, LR: 0.0007992, Tokens/sec: 619296.25\n",
      "Step: 658, Training Loss: 1.52418, LR: 0.0007986, Tokens/sec: 618605.32\n",
      "Step: 659, Training Loss: 1.53421, LR: 0.0007981, Tokens/sec: 620124.65\n",
      "Step: 660, Training Loss: 1.53341, LR: 0.0007975, Tokens/sec: 618833.32\n",
      "Step: 661, Training Loss: 1.53308, LR: 0.0007969, Tokens/sec: 619911.85\n",
      "Step: 662, Training Loss: 1.48836, LR: 0.0007963, Tokens/sec: 617971.49\n",
      "Step: 663, Training Loss: 1.46652, LR: 0.0007957, Tokens/sec: 618587.13\n",
      "Step: 664, Training Loss: 1.60338, LR: 0.0007951, Tokens/sec: 619612.85\n",
      "Step: 665, Training Loss: 1.57082, LR: 0.0007946, Tokens/sec: 619433.14\n",
      "Step: 666, Training Loss: 1.52227, LR: 0.0007940, Tokens/sec: 619671.29\n",
      "Step: 667, Training Loss: 1.56905, LR: 0.0007934, Tokens/sec: 618806.64\n",
      "Step: 668, Training Loss: 1.53342, LR: 0.0007928, Tokens/sec: 618661.81\n",
      "Step: 669, Training Loss: 1.61302, LR: 0.0007922, Tokens/sec: 619438.89\n",
      "Step: 670, Training Loss: 1.59852, LR: 0.0007916, Tokens/sec: 619313.54\n",
      "Step: 671, Training Loss: 1.52777, LR: 0.0007911, Tokens/sec: 620273.47\n",
      "Step: 672, Training Loss: 1.53567, LR: 0.0007905, Tokens/sec: 619401.96\n",
      "Step: 673, Training Loss: 1.52822, LR: 0.0007899, Tokens/sec: 619295.84\n",
      "Step: 674, Training Loss: 1.53757, LR: 0.0007893, Tokens/sec: 617686.06\n",
      "Step: 675, Training Loss: 1.63409, LR: 0.0007887, Tokens/sec: 619280.81\n",
      "Step: 676, Training Loss: 1.45287, LR: 0.0007881, Tokens/sec: 618548.33\n",
      "Step: 677, Training Loss: 1.51501, LR: 0.0007875, Tokens/sec: 620002.11\n",
      "Step: 678, Training Loss: 1.63371, LR: 0.0007869, Tokens/sec: 619149.79\n",
      "Step: 679, Training Loss: 1.53096, LR: 0.0007863, Tokens/sec: 619091.51\n",
      "Step: 680, Training Loss: 1.51309, LR: 0.0007857, Tokens/sec: 620465.35\n",
      "Step: 681, Training Loss: 1.49961, LR: 0.0007852, Tokens/sec: 619206.96\n",
      "Step: 682, Training Loss: 1.46721, LR: 0.0007846, Tokens/sec: 619012.82\n",
      "Step: 683, Training Loss: 1.55092, LR: 0.0007840, Tokens/sec: 620018.74\n",
      "Step: 684, Training Loss: 1.48795, LR: 0.0007834, Tokens/sec: 618854.60\n",
      "Step: 685, Training Loss: 1.39089, LR: 0.0007828, Tokens/sec: 619436.55\n",
      "Step: 686, Training Loss: 1.45414, LR: 0.0007822, Tokens/sec: 619367.78\n",
      "Step: 687, Training Loss: 1.74951, LR: 0.0007816, Tokens/sec: 619107.35\n",
      "Step: 688, Training Loss: 1.99495, LR: 0.0007810, Tokens/sec: 620366.22\n",
      "Step: 689, Training Loss: 1.41594, LR: 0.0007804, Tokens/sec: 619874.74\n",
      "Step: 690, Training Loss: 1.68698, LR: 0.0007798, Tokens/sec: 618327.32\n",
      "Step: 691, Training Loss: 1.48366, LR: 0.0007792, Tokens/sec: 618884.95\n",
      "Step: 692, Training Loss: 1.54904, LR: 0.0007786, Tokens/sec: 619013.94\n",
      "Step: 693, Training Loss: 1.46249, LR: 0.0007780, Tokens/sec: 619251.02\n",
      "Step: 694, Training Loss: 1.40975, LR: 0.0007774, Tokens/sec: 620260.45\n",
      "Step: 695, Training Loss: 1.38286, LR: 0.0007768, Tokens/sec: 618931.96\n",
      "Step: 696, Training Loss: 1.38225, LR: 0.0007762, Tokens/sec: 620542.67\n",
      "Step: 697, Training Loss: 1.22445, LR: 0.0007756, Tokens/sec: 620079.68\n",
      "Step: 698, Training Loss: 1.43350, LR: 0.0007750, Tokens/sec: 619000.97\n",
      "Step: 699, Training Loss: 1.51744, LR: 0.0007744, Tokens/sec: 618837.23\n",
      "Step: 700, Training Loss: 1.51279, LR: 0.0007738, Tokens/sec: 619865.63\n",
      "Step: 701, Training Loss: 1.48879, LR: 0.0007732, Tokens/sec: 618601.76\n",
      "Step: 702, Training Loss: 1.29207, LR: 0.0007726, Tokens/sec: 620301.97\n",
      "Step: 703, Training Loss: 1.03633, LR: 0.0007720, Tokens/sec: 617749.36\n",
      "Step: 704, Training Loss: 1.38645, LR: 0.0007714, Tokens/sec: 619569.94\n",
      "Step: 705, Training Loss: 1.55584, LR: 0.0007708, Tokens/sec: 617266.04\n",
      "Step: 706, Training Loss: 1.55524, LR: 0.0007702, Tokens/sec: 621049.71\n",
      "Step: 707, Training Loss: 1.60458, LR: 0.0007696, Tokens/sec: 619407.84\n",
      "Step: 708, Training Loss: 1.55428, LR: 0.0007690, Tokens/sec: 619165.52\n",
      "Step: 709, Training Loss: 1.55615, LR: 0.0007683, Tokens/sec: 619474.01\n",
      "Step: 710, Training Loss: 1.66394, LR: 0.0007677, Tokens/sec: 620014.49\n",
      "Step: 711, Training Loss: 1.41704, LR: 0.0007671, Tokens/sec: 618563.82\n",
      "Step: 712, Training Loss: 1.44443, LR: 0.0007665, Tokens/sec: 618718.64\n",
      "Step: 713, Training Loss: 1.55499, LR: 0.0007659, Tokens/sec: 619059.22\n",
      "Step: 714, Training Loss: 1.30915, LR: 0.0007653, Tokens/sec: 618717.90\n",
      "Step: 715, Training Loss: 1.49063, LR: 0.0007647, Tokens/sec: 619533.11\n",
      "Step: 716, Training Loss: 1.40259, LR: 0.0007641, Tokens/sec: 618497.87\n",
      "Step: 717, Training Loss: 1.38097, LR: 0.0007635, Tokens/sec: 619445.81\n",
      "Step: 718, Training Loss: 1.10986, LR: 0.0007629, Tokens/sec: 621341.45\n",
      "Step: 719, Training Loss: 1.51347, LR: 0.0007622, Tokens/sec: 618884.21\n",
      "Step: 720, Training Loss: 1.18491, LR: 0.0007616, Tokens/sec: 618929.01\n",
      "Step: 721, Training Loss: 1.58364, LR: 0.0007610, Tokens/sec: 618844.43\n",
      "Step: 722, Training Loss: 1.45518, LR: 0.0007604, Tokens/sec: 620223.99\n",
      "Step: 723, Training Loss: 1.35805, LR: 0.0007598, Tokens/sec: 618816.21\n",
      "Step: 724, Training Loss: 1.33734, LR: 0.0007592, Tokens/sec: 619022.72\n",
      "Step: 725, Training Loss: 1.25333, LR: 0.0007586, Tokens/sec: 617983.82\n",
      "Step: 726, Training Loss: 1.12572, LR: 0.0007579, Tokens/sec: 618232.71\n",
      "Step: 727, Training Loss: 1.08603, LR: 0.0007573, Tokens/sec: 618876.38\n",
      "Step: 728, Training Loss: 1.57545, LR: 0.0007567, Tokens/sec: 618319.99\n",
      "Step: 729, Training Loss: 1.62737, LR: 0.0007561, Tokens/sec: 621251.21\n",
      "Step: 730, Training Loss: 1.53676, LR: 0.0007555, Tokens/sec: 619397.01\n",
      "Step: 731, Training Loss: 1.50613, LR: 0.0007549, Tokens/sec: 618804.24\n",
      "Step: 732, Training Loss: 1.55822, LR: 0.0007542, Tokens/sec: 619142.28\n",
      "Step: 733, Training Loss: 1.50804, LR: 0.0007536, Tokens/sec: 619035.77\n",
      "Step: 734, Training Loss: 1.62058, LR: 0.0007530, Tokens/sec: 619512.26\n",
      "Step: 735, Training Loss: 1.69914, LR: 0.0007524, Tokens/sec: 618459.60\n",
      "Step: 736, Training Loss: 1.62317, LR: 0.0007518, Tokens/sec: 619170.08\n",
      "Step: 737, Training Loss: 1.41244, LR: 0.0007511, Tokens/sec: 620297.24\n",
      "Step: 738, Training Loss: 1.00892, LR: 0.0007505, Tokens/sec: 619815.55\n",
      "Step: 739, Training Loss: 1.24295, LR: 0.0007499, Tokens/sec: 619522.31\n",
      "Step: 740, Training Loss: 1.65949, LR: 0.0007493, Tokens/sec: 619316.91\n",
      "Step: 741, Training Loss: 1.61582, LR: 0.0007486, Tokens/sec: 619280.17\n",
      "Step: 742, Training Loss: 1.55590, LR: 0.0007480, Tokens/sec: 619456.47\n",
      "Step: 743, Training Loss: 1.59730, LR: 0.0007474, Tokens/sec: 618934.92\n",
      "Step: 744, Training Loss: 1.65925, LR: 0.0007468, Tokens/sec: 618410.76\n",
      "Step: 745, Training Loss: 1.59241, LR: 0.0007461, Tokens/sec: 619407.51\n",
      "Step: 746, Training Loss: 1.56878, LR: 0.0007455, Tokens/sec: 619739.26\n",
      "Step: 747, Training Loss: 1.55766, LR: 0.0007449, Tokens/sec: 618796.21\n",
      "Step: 748, Training Loss: 1.59508, LR: 0.0007443, Tokens/sec: 620285.60\n",
      "Step: 749, Training Loss: 1.41444, LR: 0.0007436, Tokens/sec: 618974.12\n",
      "Step: 750, Training Loss: 1.48718, LR: 0.0007430, Tokens/sec: 619340.59\n",
      "Step: 751, Training Loss: 1.54770, LR: 0.0007424, Tokens/sec: 620319.33\n",
      "Step: 752, Training Loss: 1.46085, LR: 0.0007418, Tokens/sec: 617730.09\n",
      "Step: 753, Training Loss: 1.47292, LR: 0.0007411, Tokens/sec: 619346.24\n",
      "Step: 754, Training Loss: 1.47598, LR: 0.0007405, Tokens/sec: 619140.96\n",
      "Step: 755, Training Loss: 1.46471, LR: 0.0007399, Tokens/sec: 619133.82\n",
      "Step: 756, Training Loss: 1.41969, LR: 0.0007392, Tokens/sec: 618277.69\n",
      "Step: 757, Training Loss: 1.43281, LR: 0.0007386, Tokens/sec: 619014.50\n",
      "Step: 758, Training Loss: 1.49318, LR: 0.0007380, Tokens/sec: 619502.50\n",
      "Step: 759, Training Loss: 1.52809, LR: 0.0007373, Tokens/sec: 618196.72\n",
      "Step: 760, Training Loss: 1.51075, LR: 0.0007367, Tokens/sec: 619606.93\n",
      "Step: 761, Training Loss: 1.46242, LR: 0.0007361, Tokens/sec: 618809.86\n",
      "Step: 762, Training Loss: 1.42145, LR: 0.0007354, Tokens/sec: 618665.27\n",
      "Step: 763, Training Loss: 1.39597, LR: 0.0007348, Tokens/sec: 617944.16\n",
      "Step: 764, Training Loss: 1.36202, LR: 0.0007342, Tokens/sec: 618851.77\n",
      "Step: 765, Training Loss: 1.49403, LR: 0.0007335, Tokens/sec: 619293.93\n",
      "Step: 766, Training Loss: 1.55628, LR: 0.0007329, Tokens/sec: 619184.74\n",
      "Step: 767, Training Loss: 1.51095, LR: 0.0007323, Tokens/sec: 619355.03\n",
      "Step: 768, Training Loss: 1.44709, LR: 0.0007316, Tokens/sec: 620242.82\n",
      "Step: 769, Training Loss: 1.43249, LR: 0.0007310, Tokens/sec: 619800.95\n",
      "Step: 770, Training Loss: 1.45864, LR: 0.0007304, Tokens/sec: 617952.36\n",
      "Step: 771, Training Loss: 1.38879, LR: 0.0007297, Tokens/sec: 619496.92\n",
      "Step: 772, Training Loss: 1.45307, LR: 0.0007291, Tokens/sec: 619557.71\n",
      "Step: 773, Training Loss: 1.41912, LR: 0.0007284, Tokens/sec: 619132.85\n",
      "Step: 774, Training Loss: 1.46373, LR: 0.0007278, Tokens/sec: 618336.74\n",
      "Step: 775, Training Loss: 1.44325, LR: 0.0007272, Tokens/sec: 619284.43\n",
      "Step: 776, Training Loss: 1.52385, LR: 0.0007265, Tokens/sec: 618877.38\n",
      "Step: 777, Training Loss: 1.45292, LR: 0.0007259, Tokens/sec: 619580.88\n",
      "Step: 778, Training Loss: 1.38512, LR: 0.0007253, Tokens/sec: 618343.61\n",
      "Step: 779, Training Loss: 1.43789, LR: 0.0007246, Tokens/sec: 620006.81\n",
      "Step: 780, Training Loss: 1.40994, LR: 0.0007240, Tokens/sec: 619114.22\n",
      "Step: 781, Training Loss: 1.47435, LR: 0.0007233, Tokens/sec: 619213.59\n",
      "Step: 782, Training Loss: 1.42985, LR: 0.0007227, Tokens/sec: 619141.71\n",
      "Step: 783, Training Loss: 1.45031, LR: 0.0007220, Tokens/sec: 619269.00\n",
      "Step: 784, Training Loss: 1.45311, LR: 0.0007214, Tokens/sec: 619289.20\n",
      "Step: 785, Training Loss: 1.36670, LR: 0.0007208, Tokens/sec: 619379.94\n",
      "Step: 786, Training Loss: 1.53283, LR: 0.0007201, Tokens/sec: 618843.67\n",
      "Step: 787, Training Loss: 1.41841, LR: 0.0007195, Tokens/sec: 620172.35\n",
      "Step: 788, Training Loss: 1.39090, LR: 0.0007188, Tokens/sec: 618188.65\n",
      "Step: 789, Training Loss: 1.36932, LR: 0.0007182, Tokens/sec: 619047.02\n",
      "Step: 790, Training Loss: 1.49697, LR: 0.0007175, Tokens/sec: 619733.28\n",
      "Step: 791, Training Loss: 1.52497, LR: 0.0007169, Tokens/sec: 620144.85\n",
      "Step: 792, Training Loss: 1.53377, LR: 0.0007163, Tokens/sec: 618959.38\n",
      "Step: 793, Training Loss: 1.47621, LR: 0.0007156, Tokens/sec: 619921.28\n",
      "Step: 794, Training Loss: 1.43615, LR: 0.0007150, Tokens/sec: 618186.92\n",
      "Step: 795, Training Loss: 1.37114, LR: 0.0007143, Tokens/sec: 619667.01\n",
      "Step: 796, Training Loss: 1.46758, LR: 0.0007137, Tokens/sec: 620330.17\n",
      "Step: 797, Training Loss: 1.39942, LR: 0.0007130, Tokens/sec: 620114.24\n",
      "Step: 798, Training Loss: 1.46048, LR: 0.0007124, Tokens/sec: 618210.60\n",
      "Step: 799, Training Loss: 1.37582, LR: 0.0007117, Tokens/sec: 618858.16\n",
      "Step: 800, Training Loss: 1.46883, LR: 0.0007111, Tokens/sec: 620324.65\n",
      "Step: 801, Training Loss: 1.52055, LR: 0.0007104, Tokens/sec: 619860.81\n",
      "Step: 802, Training Loss: 1.51733, LR: 0.0007098, Tokens/sec: 618973.01\n",
      "Step: 803, Training Loss: 1.33469, LR: 0.0007091, Tokens/sec: 619255.89\n",
      "Step: 804, Training Loss: 1.37117, LR: 0.0007085, Tokens/sec: 617920.91\n",
      "Step: 805, Training Loss: 1.45599, LR: 0.0007078, Tokens/sec: 618997.91\n",
      "Step: 806, Training Loss: 1.43836, LR: 0.0007072, Tokens/sec: 618666.49\n",
      "Step: 807, Training Loss: 1.41243, LR: 0.0007065, Tokens/sec: 618481.04\n",
      "Step: 808, Training Loss: 1.39419, LR: 0.0007059, Tokens/sec: 618184.92\n",
      "Step: 809, Training Loss: 1.39496, LR: 0.0007052, Tokens/sec: 619060.10\n",
      "Step: 810, Training Loss: 1.33329, LR: 0.0007046, Tokens/sec: 619552.30\n",
      "Step: 811, Training Loss: 1.38623, LR: 0.0007039, Tokens/sec: 618434.44\n",
      "Step: 812, Training Loss: 1.36467, LR: 0.0007033, Tokens/sec: 619253.98\n",
      "Step: 813, Training Loss: 1.34274, LR: 0.0007026, Tokens/sec: 618371.36\n",
      "Step: 814, Training Loss: 1.38878, LR: 0.0007019, Tokens/sec: 619255.89\n",
      "Step: 815, Training Loss: 1.28879, LR: 0.0007013, Tokens/sec: 618423.95\n",
      "Step: 816, Training Loss: 1.26883, LR: 0.0007006, Tokens/sec: 618224.01\n",
      "Step: 817, Training Loss: 1.26423, LR: 0.0007000, Tokens/sec: 528198.99\n",
      "Step: 818, Training Loss: 1.36203, LR: 0.0006993, Tokens/sec: 612935.04\n",
      "Step: 819, Training Loss: 1.41883, LR: 0.0006987, Tokens/sec: 617148.58\n",
      "Step: 820, Training Loss: 1.28924, LR: 0.0006980, Tokens/sec: 615636.75\n",
      "Step: 821, Training Loss: 1.39739, LR: 0.0006974, Tokens/sec: 616786.44\n",
      "Step: 822, Training Loss: 1.44600, LR: 0.0006967, Tokens/sec: 616131.96\n",
      "Step: 823, Training Loss: 1.48248, LR: 0.0006960, Tokens/sec: 617059.10\n",
      "Step: 824, Training Loss: 1.43787, LR: 0.0006954, Tokens/sec: 616401.32\n",
      "Step: 825, Training Loss: 1.41619, LR: 0.0006947, Tokens/sec: 617535.58\n",
      "Step: 826, Training Loss: 1.39084, LR: 0.0006941, Tokens/sec: 617271.76\n",
      "Step: 827, Training Loss: 1.37686, LR: 0.0006934, Tokens/sec: 616610.73\n",
      "Step: 828, Training Loss: 1.33722, LR: 0.0006928, Tokens/sec: 617907.99\n",
      "Step: 829, Training Loss: 1.38163, LR: 0.0006921, Tokens/sec: 615907.42\n",
      "Step: 830, Training Loss: 1.41520, LR: 0.0006914, Tokens/sec: 617286.47\n",
      "Step: 831, Training Loss: 1.45648, LR: 0.0006908, Tokens/sec: 617455.04\n",
      "Step: 832, Training Loss: 1.39813, LR: 0.0006901, Tokens/sec: 617422.84\n",
      "Step: 833, Training Loss: 1.36275, LR: 0.0006895, Tokens/sec: 616139.25\n",
      "Step: 834, Training Loss: 1.30621, LR: 0.0006888, Tokens/sec: 616940.40\n",
      "Step: 835, Training Loss: 1.29414, LR: 0.0006881, Tokens/sec: 617584.82\n",
      "Step: 836, Training Loss: 1.30828, LR: 0.0006875, Tokens/sec: 616436.60\n",
      "Step: 837, Training Loss: 1.39742, LR: 0.0006868, Tokens/sec: 618897.56\n",
      "Step: 838, Training Loss: 1.39109, LR: 0.0006861, Tokens/sec: 617035.12\n",
      "Step: 839, Training Loss: 1.42326, LR: 0.0006855, Tokens/sec: 616778.42\n",
      "Step: 840, Training Loss: 1.40594, LR: 0.0006848, Tokens/sec: 617444.18\n",
      "Step: 841, Training Loss: 1.34678, LR: 0.0006842, Tokens/sec: 616631.51\n",
      "Step: 842, Training Loss: 1.41639, LR: 0.0006835, Tokens/sec: 617185.79\n",
      "Step: 843, Training Loss: 1.36167, LR: 0.0006828, Tokens/sec: 618349.11\n",
      "Step: 844, Training Loss: 1.31207, LR: 0.0006822, Tokens/sec: 616373.94\n",
      "Step: 845, Training Loss: 1.37762, LR: 0.0006815, Tokens/sec: 617841.81\n",
      "Step: 846, Training Loss: 1.35276, LR: 0.0006808, Tokens/sec: 617277.16\n",
      "Step: 847, Training Loss: 1.43213, LR: 0.0006802, Tokens/sec: 617283.94\n",
      "Step: 848, Training Loss: 1.43092, LR: 0.0006795, Tokens/sec: 616346.22\n",
      "Step: 849, Training Loss: 1.33302, LR: 0.0006788, Tokens/sec: 618358.74\n",
      "Step: 850, Training Loss: 1.31828, LR: 0.0006782, Tokens/sec: 617772.95\n",
      "Step: 851, Training Loss: 1.27365, LR: 0.0006775, Tokens/sec: 616652.29\n",
      "Step: 852, Training Loss: 1.36950, LR: 0.0006768, Tokens/sec: 616587.74\n",
      "Step: 853, Training Loss: 1.31745, LR: 0.0006762, Tokens/sec: 618244.94\n",
      "Step: 854, Training Loss: 1.29955, LR: 0.0006755, Tokens/sec: 616273.44\n",
      "Step: 855, Training Loss: 1.34360, LR: 0.0006748, Tokens/sec: 615946.36\n",
      "Step: 856, Training Loss: 1.32075, LR: 0.0006742, Tokens/sec: 617964.96\n",
      "Step: 857, Training Loss: 1.31885, LR: 0.0006735, Tokens/sec: 617114.27\n",
      "Step: 858, Training Loss: 1.20541, LR: 0.0006728, Tokens/sec: 617337.77\n",
      "Step: 859, Training Loss: 1.24677, LR: 0.0006722, Tokens/sec: 617540.12\n",
      "Step: 860, Training Loss: 1.36680, LR: 0.0006715, Tokens/sec: 617366.52\n",
      "Step: 861, Training Loss: 1.44064, LR: 0.0006708, Tokens/sec: 617842.36\n",
      "Step: 862, Training Loss: 1.33328, LR: 0.0006702, Tokens/sec: 618282.08\n",
      "Step: 863, Training Loss: 1.21049, LR: 0.0006695, Tokens/sec: 617768.53\n",
      "Step: 864, Training Loss: 1.20002, LR: 0.0006688, Tokens/sec: 616958.90\n",
      "Step: 865, Training Loss: 1.24175, LR: 0.0006681, Tokens/sec: 617911.86\n",
      "Step: 866, Training Loss: 1.36451, LR: 0.0006675, Tokens/sec: 617453.37\n",
      "Step: 867, Training Loss: 1.32343, LR: 0.0006668, Tokens/sec: 616464.21\n",
      "Step: 868, Training Loss: 1.36132, LR: 0.0006661, Tokens/sec: 617421.66\n",
      "Step: 869, Training Loss: 1.22592, LR: 0.0006655, Tokens/sec: 617910.62\n",
      "Step: 870, Training Loss: 1.20983, LR: 0.0006648, Tokens/sec: 616484.08\n",
      "Step: 871, Training Loss: 1.33628, LR: 0.0006641, Tokens/sec: 617886.01\n",
      "Step: 872, Training Loss: 1.45350, LR: 0.0006634, Tokens/sec: 615956.19\n",
      "Step: 873, Training Loss: 1.31985, LR: 0.0006628, Tokens/sec: 617316.76\n",
      "Step: 874, Training Loss: 1.35349, LR: 0.0006621, Tokens/sec: 617259.53\n",
      "Step: 875, Training Loss: 1.32917, LR: 0.0006614, Tokens/sec: 616731.03\n",
      "Step: 876, Training Loss: 1.33889, LR: 0.0006608, Tokens/sec: 617878.91\n",
      "Step: 877, Training Loss: 1.41609, LR: 0.0006601, Tokens/sec: 617439.53\n",
      "Step: 878, Training Loss: 1.37499, LR: 0.0006594, Tokens/sec: 616358.55\n",
      "Step: 879, Training Loss: 1.40655, LR: 0.0006587, Tokens/sec: 617092.31\n",
      "Step: 880, Training Loss: 1.43010, LR: 0.0006581, Tokens/sec: 617204.38\n",
      "Step: 881, Training Loss: 1.39460, LR: 0.0006574, Tokens/sec: 616243.80\n",
      "Step: 882, Training Loss: 1.36575, LR: 0.0006567, Tokens/sec: 616902.69\n",
      "Step: 883, Training Loss: 1.36194, LR: 0.0006560, Tokens/sec: 616230.07\n",
      "Step: 884, Training Loss: 1.35293, LR: 0.0006554, Tokens/sec: 617721.15\n",
      "Step: 885, Training Loss: 1.40143, LR: 0.0006547, Tokens/sec: 615778.41\n",
      "Step: 886, Training Loss: 1.55510, LR: 0.0006540, Tokens/sec: 616955.03\n",
      "Step: 887, Training Loss: 1.44797, LR: 0.0006533, Tokens/sec: 616916.07\n",
      "Step: 888, Training Loss: 1.34077, LR: 0.0006526, Tokens/sec: 617482.25\n",
      "Step: 889, Training Loss: 1.43474, LR: 0.0006520, Tokens/sec: 617759.31\n",
      "Step: 890, Training Loss: 1.37456, LR: 0.0006513, Tokens/sec: 616617.31\n",
      "Step: 891, Training Loss: 1.26626, LR: 0.0006506, Tokens/sec: 617292.72\n",
      "Step: 892, Training Loss: 1.25985, LR: 0.0006499, Tokens/sec: 616630.13\n",
      "Step: 893, Training Loss: 1.38507, LR: 0.0006493, Tokens/sec: 616674.05\n",
      "Step: 894, Training Loss: 1.37373, LR: 0.0006486, Tokens/sec: 617421.50\n",
      "Step: 895, Training Loss: 1.33715, LR: 0.0006479, Tokens/sec: 617988.21\n",
      "Step: 896, Training Loss: 1.21678, LR: 0.0006472, Tokens/sec: 617077.40\n",
      "Step: 897, Training Loss: 1.16809, LR: 0.0006465, Tokens/sec: 617152.87\n",
      "Step: 898, Training Loss: 1.28345, LR: 0.0006459, Tokens/sec: 616825.75\n",
      "Step: 899, Training Loss: 1.30324, LR: 0.0006452, Tokens/sec: 616917.72\n",
      "Step: 900, Training Loss: 1.25570, LR: 0.0006445, Tokens/sec: 617101.76\n",
      "Step: 901, Training Loss: 1.31297, LR: 0.0006438, Tokens/sec: 616849.45\n",
      "Step: 902, Training Loss: 1.24471, LR: 0.0006432, Tokens/sec: 617206.78\n",
      "Step: 903, Training Loss: 1.19610, LR: 0.0006425, Tokens/sec: 617306.98\n",
      "Step: 904, Training Loss: 1.27447, LR: 0.0006418, Tokens/sec: 617921.58\n",
      "Step: 905, Training Loss: 1.23934, LR: 0.0006411, Tokens/sec: 617222.05\n",
      "Step: 906, Training Loss: 1.33889, LR: 0.0006404, Tokens/sec: 617612.44\n",
      "Step: 907, Training Loss: 1.21997, LR: 0.0006397, Tokens/sec: 617153.27\n",
      "Step: 908, Training Loss: 1.25478, LR: 0.0006391, Tokens/sec: 617002.31\n",
      "Step: 909, Training Loss: 1.25245, LR: 0.0006384, Tokens/sec: 616512.84\n",
      "Step: 910, Training Loss: 1.12163, LR: 0.0006377, Tokens/sec: 617580.70\n",
      "Step: 911, Training Loss: 1.24922, LR: 0.0006370, Tokens/sec: 617525.81\n",
      "Step: 912, Training Loss: 1.26290, LR: 0.0006363, Tokens/sec: 616369.05\n",
      "Step: 913, Training Loss: 1.19877, LR: 0.0006357, Tokens/sec: 617465.09\n",
      "Step: 914, Training Loss: 1.20417, LR: 0.0006350, Tokens/sec: 616608.99\n",
      "Step: 915, Training Loss: 1.22217, LR: 0.0006343, Tokens/sec: 615606.76\n",
      "Step: 916, Training Loss: 1.18782, LR: 0.0006336, Tokens/sec: 616600.25\n",
      "Step: 917, Training Loss: 1.23281, LR: 0.0006329, Tokens/sec: 617207.54\n",
      "Step: 918, Training Loss: 1.35696, LR: 0.0006322, Tokens/sec: 617921.40\n",
      "Step: 919, Training Loss: 1.34786, LR: 0.0006316, Tokens/sec: 615872.94\n",
      "Step: 920, Training Loss: 1.30875, LR: 0.0006309, Tokens/sec: 616988.01\n",
      "Step: 921, Training Loss: 1.31313, LR: 0.0006302, Tokens/sec: 615991.44\n",
      "Step: 922, Training Loss: 1.32516, LR: 0.0006295, Tokens/sec: 617253.95\n",
      "Step: 923, Training Loss: 1.69651, LR: 0.0006288, Tokens/sec: 616229.44\n",
      "Step: 924, Training Loss: 1.29563, LR: 0.0006281, Tokens/sec: 617392.89\n",
      "Step: 925, Training Loss: 1.26425, LR: 0.0006275, Tokens/sec: 616645.31\n",
      "Step: 926, Training Loss: 1.26377, LR: 0.0006268, Tokens/sec: 618754.15\n",
      "Step: 927, Training Loss: 1.28610, LR: 0.0006261, Tokens/sec: 617584.42\n",
      "Step: 928, Training Loss: 1.25653, LR: 0.0006254, Tokens/sec: 616709.51\n",
      "Step: 929, Training Loss: 1.27505, LR: 0.0006247, Tokens/sec: 617141.37\n",
      "Step: 930, Training Loss: 1.29862, LR: 0.0006240, Tokens/sec: 616397.31\n",
      "Step: 931, Training Loss: 1.24433, LR: 0.0006233, Tokens/sec: 617544.63\n",
      "Step: 932, Training Loss: 1.29728, LR: 0.0006227, Tokens/sec: 615885.30\n",
      "Step: 933, Training Loss: 1.34560, LR: 0.0006220, Tokens/sec: 617548.93\n",
      "Step: 934, Training Loss: 1.26445, LR: 0.0006213, Tokens/sec: 617242.63\n",
      "Step: 935, Training Loss: 1.26239, LR: 0.0006206, Tokens/sec: 616755.27\n",
      "Step: 936, Training Loss: 1.23704, LR: 0.0006199, Tokens/sec: 618130.26\n",
      "Step: 937, Training Loss: 1.04756, LR: 0.0006192, Tokens/sec: 616515.24\n",
      "Step: 938, Training Loss: 1.09463, LR: 0.0006185, Tokens/sec: 617271.97\n",
      "Step: 939, Training Loss: 1.26546, LR: 0.0006179, Tokens/sec: 616931.88\n",
      "Step: 940, Training Loss: 1.07314, LR: 0.0006172, Tokens/sec: 617639.43\n",
      "Step: 941, Training Loss: 1.23559, LR: 0.0006165, Tokens/sec: 618177.26\n",
      "Step: 942, Training Loss: 1.14690, LR: 0.0006158, Tokens/sec: 615796.48\n",
      "Step: 943, Training Loss: 1.23688, LR: 0.0006151, Tokens/sec: 616336.57\n",
      "Step: 944, Training Loss: 1.22776, LR: 0.0006144, Tokens/sec: 615325.37\n",
      "Step: 945, Training Loss: 1.25663, LR: 0.0006137, Tokens/sec: 617776.47\n",
      "Step: 946, Training Loss: 1.21117, LR: 0.0006130, Tokens/sec: 617291.72\n",
      "Step: 947, Training Loss: 1.60848, LR: 0.0006124, Tokens/sec: 616658.64\n",
      "Step: 948, Training Loss: 1.23825, LR: 0.0006117, Tokens/sec: 617481.07\n",
      "Step: 949, Training Loss: 1.20771, LR: 0.0006110, Tokens/sec: 616585.79\n",
      "Step: 950, Training Loss: 1.23787, LR: 0.0006103, Tokens/sec: 617727.59\n",
      "Step: 951, Training Loss: 1.28338, LR: 0.0006096, Tokens/sec: 617084.77\n",
      "Step: 952, Training Loss: 1.30941, LR: 0.0006089, Tokens/sec: 617025.40\n",
      "Step: 953, Training Loss: 1.22587, LR: 0.0006082, Tokens/sec: 616856.51\n",
      "Step: 954, Training Loss: 1.31472, LR: 0.0006075, Tokens/sec: 614231.08\n",
      "Step: 955, Training Loss: 1.27382, LR: 0.0006068, Tokens/sec: 617133.84\n",
      "Step: 956, Training Loss: 1.21935, LR: 0.0006062, Tokens/sec: 617710.49\n",
      "Step: 957, Training Loss: 1.25538, LR: 0.0006055, Tokens/sec: 616613.84\n",
      "Step: 958, Training Loss: 1.22757, LR: 0.0006048, Tokens/sec: 616811.43\n",
      "Step: 959, Training Loss: 1.24068, LR: 0.0006041, Tokens/sec: 617313.27\n",
      "Step: 960, Training Loss: 1.32687, LR: 0.0006034, Tokens/sec: 616598.99\n",
      "Step: 961, Training Loss: 1.26938, LR: 0.0006027, Tokens/sec: 617494.13\n",
      "Step: 962, Training Loss: 1.25911, LR: 0.0006020, Tokens/sec: 615556.14\n",
      "Step: 963, Training Loss: 1.19344, LR: 0.0006013, Tokens/sec: 618511.42\n",
      "Step: 964, Training Loss: 1.32275, LR: 0.0006006, Tokens/sec: 616294.79\n",
      "Step: 965, Training Loss: 1.30398, LR: 0.0005999, Tokens/sec: 616204.52\n",
      "Step: 966, Training Loss: 1.29338, LR: 0.0005992, Tokens/sec: 616508.17\n",
      "Step: 967, Training Loss: 1.22934, LR: 0.0005986, Tokens/sec: 616985.17\n",
      "Step: 968, Training Loss: 1.27094, LR: 0.0005979, Tokens/sec: 617482.57\n",
      "Step: 969, Training Loss: 1.30366, LR: 0.0005972, Tokens/sec: 616548.26\n",
      "Step: 970, Training Loss: 1.22805, LR: 0.0005965, Tokens/sec: 616002.13\n",
      "Step: 971, Training Loss: 1.26471, LR: 0.0005958, Tokens/sec: 617069.70\n",
      "Step: 972, Training Loss: 1.24141, LR: 0.0005951, Tokens/sec: 615939.13\n",
      "Step: 973, Training Loss: 1.32501, LR: 0.0005944, Tokens/sec: 616810.13\n",
      "Step: 974, Training Loss: 1.28548, LR: 0.0005937, Tokens/sec: 617852.97\n",
      "Step: 975, Training Loss: 1.24894, LR: 0.0005930, Tokens/sec: 617585.02\n",
      "Step: 976, Training Loss: 1.23482, LR: 0.0005923, Tokens/sec: 617027.92\n",
      "Step: 977, Training Loss: 1.24774, LR: 0.0005916, Tokens/sec: 616577.74\n",
      "Step: 978, Training Loss: 1.29315, LR: 0.0005910, Tokens/sec: 616683.66\n",
      "Step: 979, Training Loss: 1.28260, LR: 0.0005903, Tokens/sec: 617303.36\n",
      "Step: 980, Training Loss: 1.24987, LR: 0.0005896, Tokens/sec: 617590.34\n",
      "Step: 981, Training Loss: 1.26027, LR: 0.0005889, Tokens/sec: 616713.64\n",
      "Step: 982, Training Loss: 1.27245, LR: 0.0005882, Tokens/sec: 616879.98\n",
      "Step: 983, Training Loss: 1.23581, LR: 0.0005875, Tokens/sec: 616718.53\n",
      "Step: 984, Training Loss: 1.44996, LR: 0.0005868, Tokens/sec: 615859.39\n",
      "Step: 985, Training Loss: 1.41274, LR: 0.0005861, Tokens/sec: 617174.86\n",
      "Step: 986, Training Loss: 1.44200, LR: 0.0005854, Tokens/sec: 616278.54\n",
      "Step: 987, Training Loss: 1.41286, LR: 0.0005847, Tokens/sec: 616750.93\n",
      "Step: 988, Training Loss: 1.30280, LR: 0.0005840, Tokens/sec: 614953.08\n",
      "Step: 989, Training Loss: 1.23950, LR: 0.0005833, Tokens/sec: 616785.78\n",
      "Step: 990, Training Loss: 1.27419, LR: 0.0005826, Tokens/sec: 616456.01\n",
      "Step: 991, Training Loss: 1.29034, LR: 0.0005819, Tokens/sec: 617317.84\n",
      "Step: 992, Training Loss: 1.27802, LR: 0.0005813, Tokens/sec: 618444.21\n",
      "Step: 993, Training Loss: 1.25917, LR: 0.0005806, Tokens/sec: 617293.70\n",
      "Step: 994, Training Loss: 1.27778, LR: 0.0005799, Tokens/sec: 617004.86\n",
      "Step: 995, Training Loss: 1.26713, LR: 0.0005792, Tokens/sec: 616975.61\n",
      "Step: 996, Training Loss: 1.25164, LR: 0.0005785, Tokens/sec: 616683.16\n",
      "Step: 997, Training Loss: 1.25234, LR: 0.0005778, Tokens/sec: 616042.05\n",
      "Step: 998, Training Loss: 1.28284, LR: 0.0005771, Tokens/sec: 617760.37\n",
      "Step: 999, Training Loss: 1.28201, LR: 0.0005764, Tokens/sec: 617254.05\n",
      "Step: 1000, Training Loss: 1.30204, LR: 0.0005757, Tokens/sec: 616421.83\n",
      "Computing Eval loss, steps: 21\n",
      "Step: 1000, Eval Loss: 1.29715\n",
      "Step: 1001, Training Loss: 1.49169, LR: 0.0005750, Tokens/sec: 615851.31\n",
      "Step: 1002, Training Loss: 1.41884, LR: 0.0005743, Tokens/sec: 616993.77\n",
      "Step: 1003, Training Loss: 1.42889, LR: 0.0005736, Tokens/sec: 618430.59\n",
      "Step: 1004, Training Loss: 1.42885, LR: 0.0005729, Tokens/sec: 616467.23\n",
      "Step: 1005, Training Loss: 1.44128, LR: 0.0005722, Tokens/sec: 616908.03\n",
      "Step: 1006, Training Loss: 1.47015, LR: 0.0005715, Tokens/sec: 617240.40\n",
      "Step: 1007, Training Loss: 1.37189, LR: 0.0005708, Tokens/sec: 616933.28\n",
      "Step: 1008, Training Loss: 1.41687, LR: 0.0005701, Tokens/sec: 617953.04\n",
      "Step: 1009, Training Loss: 1.43760, LR: 0.0005695, Tokens/sec: 617867.56\n",
      "Step: 1010, Training Loss: 1.44475, LR: 0.0005688, Tokens/sec: 617694.01\n",
      "Step: 1011, Training Loss: 1.42950, LR: 0.0005681, Tokens/sec: 616267.21\n",
      "Step: 1012, Training Loss: 1.37163, LR: 0.0005674, Tokens/sec: 616680.70\n",
      "Step: 1013, Training Loss: 1.41879, LR: 0.0005667, Tokens/sec: 618376.20\n",
      "Step: 1014, Training Loss: 1.40024, LR: 0.0005660, Tokens/sec: 616700.33\n",
      "Step: 1015, Training Loss: 1.38296, LR: 0.0005653, Tokens/sec: 616191.01\n",
      "Step: 1016, Training Loss: 1.38858, LR: 0.0005646, Tokens/sec: 616494.59\n",
      "Step: 1017, Training Loss: 1.34665, LR: 0.0005639, Tokens/sec: 615495.43\n",
      "Step: 1018, Training Loss: 1.41004, LR: 0.0005632, Tokens/sec: 617178.08\n",
      "Step: 1019, Training Loss: 1.40572, LR: 0.0005625, Tokens/sec: 616240.26\n",
      "Step: 1020, Training Loss: 1.41880, LR: 0.0005618, Tokens/sec: 617068.71\n",
      "Step: 1021, Training Loss: 1.32628, LR: 0.0005611, Tokens/sec: 616641.86\n",
      "Step: 1022, Training Loss: 1.39755, LR: 0.0005604, Tokens/sec: 618630.52\n",
      "Step: 1023, Training Loss: 1.39166, LR: 0.0005597, Tokens/sec: 615466.15\n",
      "Step: 1024, Training Loss: 1.41870, LR: 0.0005590, Tokens/sec: 616748.12\n",
      "Step: 1025, Training Loss: 1.38714, LR: 0.0005583, Tokens/sec: 616181.33\n",
      "Step: 1026, Training Loss: 1.37249, LR: 0.0005576, Tokens/sec: 617652.22\n",
      "Step: 1027, Training Loss: 1.34329, LR: 0.0005570, Tokens/sec: 617684.37\n",
      "Step: 1028, Training Loss: 1.28736, LR: 0.0005563, Tokens/sec: 617711.74\n",
      "Step: 1029, Training Loss: 1.23919, LR: 0.0005556, Tokens/sec: 617664.41\n",
      "Step: 1030, Training Loss: 1.26761, LR: 0.0005549, Tokens/sec: 614661.55\n",
      "Step: 1031, Training Loss: 1.24586, LR: 0.0005542, Tokens/sec: 616211.50\n",
      "Step: 1032, Training Loss: 1.34025, LR: 0.0005535, Tokens/sec: 616037.67\n",
      "Step: 1033, Training Loss: 1.33602, LR: 0.0005528, Tokens/sec: 617425.99\n",
      "Step: 1034, Training Loss: 1.29092, LR: 0.0005521, Tokens/sec: 617631.32\n",
      "Step: 1035, Training Loss: 1.26016, LR: 0.0005514, Tokens/sec: 615559.09\n",
      "Step: 1036, Training Loss: 1.27122, LR: 0.0005507, Tokens/sec: 616366.90\n",
      "Step: 1037, Training Loss: 1.27601, LR: 0.0005500, Tokens/sec: 616075.70\n",
      "Step: 1038, Training Loss: 1.30217, LR: 0.0005493, Tokens/sec: 618041.72\n",
      "Step: 1039, Training Loss: 1.29298, LR: 0.0005486, Tokens/sec: 616330.00\n",
      "Step: 1040, Training Loss: 1.29612, LR: 0.0005479, Tokens/sec: 616798.67\n",
      "Step: 1041, Training Loss: 1.31700, LR: 0.0005472, Tokens/sec: 615931.09\n",
      "Step: 1042, Training Loss: 1.27082, LR: 0.0005465, Tokens/sec: 616295.81\n",
      "Step: 1043, Training Loss: 1.27885, LR: 0.0005458, Tokens/sec: 617843.23\n",
      "Step: 1044, Training Loss: 1.25936, LR: 0.0005451, Tokens/sec: 617450.83\n",
      "Step: 1045, Training Loss: 1.29077, LR: 0.0005444, Tokens/sec: 616891.22\n",
      "Step: 1046, Training Loss: 1.24867, LR: 0.0005437, Tokens/sec: 616560.10\n",
      "Step: 1047, Training Loss: 1.27826, LR: 0.0005430, Tokens/sec: 617527.51\n",
      "Step: 1048, Training Loss: 1.23131, LR: 0.0005424, Tokens/sec: 616861.40\n",
      "Step: 1049, Training Loss: 1.27717, LR: 0.0005417, Tokens/sec: 617233.93\n",
      "Step: 1050, Training Loss: 1.23001, LR: 0.0005410, Tokens/sec: 617252.70\n",
      "Step: 1051, Training Loss: 1.21798, LR: 0.0005403, Tokens/sec: 616576.95\n",
      "Step: 1052, Training Loss: 1.29330, LR: 0.0005396, Tokens/sec: 615761.30\n",
      "Step: 1053, Training Loss: 1.32117, LR: 0.0005389, Tokens/sec: 615538.52\n",
      "Step: 1054, Training Loss: 1.31406, LR: 0.0005382, Tokens/sec: 616952.43\n",
      "Step: 1055, Training Loss: 1.30904, LR: 0.0005375, Tokens/sec: 616959.39\n",
      "Step: 1056, Training Loss: 1.35793, LR: 0.0005368, Tokens/sec: 616495.04\n",
      "Step: 1057, Training Loss: 1.35537, LR: 0.0005361, Tokens/sec: 616169.95\n",
      "Step: 1058, Training Loss: 1.43431, LR: 0.0005354, Tokens/sec: 617543.09\n",
      "Step: 1059, Training Loss: 1.31790, LR: 0.0005347, Tokens/sec: 617023.39\n",
      "Step: 1060, Training Loss: 1.28012, LR: 0.0005340, Tokens/sec: 616564.17\n",
      "Step: 1061, Training Loss: 1.30520, LR: 0.0005333, Tokens/sec: 617412.01\n",
      "Step: 1062, Training Loss: 1.30253, LR: 0.0005326, Tokens/sec: 616877.12\n",
      "Step: 1063, Training Loss: 1.22100, LR: 0.0005319, Tokens/sec: 616933.23\n",
      "Step: 1064, Training Loss: 1.33922, LR: 0.0005312, Tokens/sec: 618108.88\n",
      "Step: 1065, Training Loss: 1.30124, LR: 0.0005305, Tokens/sec: 616840.82\n",
      "Step: 1066, Training Loss: 1.38324, LR: 0.0005299, Tokens/sec: 617191.12\n",
      "Step: 1067, Training Loss: 1.32569, LR: 0.0005292, Tokens/sec: 617170.61\n",
      "Step: 1068, Training Loss: 1.33442, LR: 0.0005285, Tokens/sec: 616949.57\n",
      "Step: 1069, Training Loss: 1.30181, LR: 0.0005278, Tokens/sec: 618200.20\n",
      "Step: 1070, Training Loss: 1.20054, LR: 0.0005271, Tokens/sec: 616712.93\n",
      "Step: 1071, Training Loss: 1.27702, LR: 0.0005264, Tokens/sec: 617842.26\n",
      "Step: 1072, Training Loss: 1.26896, LR: 0.0005257, Tokens/sec: 617763.53\n",
      "Step: 1073, Training Loss: 1.24766, LR: 0.0005250, Tokens/sec: 616931.25\n",
      "Step: 1074, Training Loss: 1.22187, LR: 0.0005243, Tokens/sec: 617499.58\n",
      "Step: 1075, Training Loss: 1.31284, LR: 0.0005236, Tokens/sec: 616777.70\n",
      "Step: 1076, Training Loss: 1.37695, LR: 0.0005229, Tokens/sec: 616538.05\n",
      "Step: 1077, Training Loss: 1.34229, LR: 0.0005222, Tokens/sec: 616739.56\n",
      "Step: 1078, Training Loss: 1.28298, LR: 0.0005215, Tokens/sec: 617377.79\n",
      "Step: 1079, Training Loss: 1.26682, LR: 0.0005208, Tokens/sec: 617865.69\n",
      "Step: 1080, Training Loss: 1.33309, LR: 0.0005201, Tokens/sec: 617846.21\n",
      "Step: 1081, Training Loss: 1.29105, LR: 0.0005194, Tokens/sec: 617601.68\n",
      "Step: 1082, Training Loss: 1.30567, LR: 0.0005187, Tokens/sec: 615991.96\n",
      "Step: 1083, Training Loss: 1.41273, LR: 0.0005181, Tokens/sec: 616813.79\n",
      "Step: 1084, Training Loss: 1.33441, LR: 0.0005174, Tokens/sec: 617835.84\n",
      "Step: 1085, Training Loss: 1.40510, LR: 0.0005167, Tokens/sec: 616156.30\n",
      "Step: 1086, Training Loss: 1.45565, LR: 0.0005160, Tokens/sec: 615875.67\n",
      "Step: 1087, Training Loss: 1.33005, LR: 0.0005153, Tokens/sec: 616418.23\n",
      "Step: 1088, Training Loss: 1.40149, LR: 0.0005146, Tokens/sec: 616857.01\n",
      "Step: 1089, Training Loss: 1.34024, LR: 0.0005139, Tokens/sec: 616729.95\n",
      "Step: 1090, Training Loss: 1.30575, LR: 0.0005132, Tokens/sec: 615364.28\n",
      "Step: 1091, Training Loss: 1.26387, LR: 0.0005125, Tokens/sec: 616625.89\n",
      "Step: 1092, Training Loss: 1.43203, LR: 0.0005118, Tokens/sec: 617326.41\n",
      "Step: 1093, Training Loss: 1.28283, LR: 0.0005111, Tokens/sec: 615710.26\n",
      "Step: 1094, Training Loss: 1.39636, LR: 0.0005104, Tokens/sec: 617002.72\n",
      "Step: 1095, Training Loss: 1.35049, LR: 0.0005097, Tokens/sec: 617964.53\n",
      "Step: 1096, Training Loss: 1.27981, LR: 0.0005090, Tokens/sec: 615934.05\n",
      "Step: 1097, Training Loss: 1.30782, LR: 0.0005084, Tokens/sec: 615939.61\n",
      "Step: 1098, Training Loss: 1.30208, LR: 0.0005077, Tokens/sec: 616698.82\n",
      "Step: 1099, Training Loss: 1.37922, LR: 0.0005070, Tokens/sec: 616455.01\n",
      "Step: 1100, Training Loss: 1.33016, LR: 0.0005063, Tokens/sec: 616557.51\n",
      "Step: 1101, Training Loss: 1.33242, LR: 0.0005056, Tokens/sec: 616492.91\n",
      "Step: 1102, Training Loss: 1.25578, LR: 0.0005049, Tokens/sec: 616337.35\n",
      "Step: 1103, Training Loss: 1.36016, LR: 0.0005042, Tokens/sec: 617596.40\n",
      "Step: 1104, Training Loss: 1.42382, LR: 0.0005035, Tokens/sec: 617946.16\n",
      "Step: 1105, Training Loss: 1.31628, LR: 0.0005028, Tokens/sec: 616973.61\n",
      "Step: 1106, Training Loss: 1.30831, LR: 0.0005021, Tokens/sec: 617534.85\n",
      "Step: 1107, Training Loss: 1.30378, LR: 0.0005014, Tokens/sec: 616899.04\n",
      "Step: 1108, Training Loss: 1.29018, LR: 0.0005008, Tokens/sec: 616886.14\n",
      "Step: 1109, Training Loss: 1.27509, LR: 0.0005001, Tokens/sec: 617221.65\n",
      "Step: 1110, Training Loss: 1.33394, LR: 0.0004994, Tokens/sec: 616590.56\n",
      "Step: 1111, Training Loss: 1.31123, LR: 0.0004987, Tokens/sec: 617150.63\n",
      "Step: 1112, Training Loss: 1.30396, LR: 0.0004980, Tokens/sec: 615320.95\n",
      "Step: 1113, Training Loss: 1.32288, LR: 0.0004973, Tokens/sec: 617371.81\n",
      "Step: 1114, Training Loss: 1.33281, LR: 0.0004966, Tokens/sec: 617524.81\n",
      "Step: 1115, Training Loss: 1.36580, LR: 0.0004959, Tokens/sec: 617205.63\n",
      "Step: 1116, Training Loss: 1.24882, LR: 0.0004952, Tokens/sec: 615314.90\n",
      "Step: 1117, Training Loss: 1.28191, LR: 0.0004945, Tokens/sec: 616824.54\n",
      "Step: 1118, Training Loss: 1.31329, LR: 0.0004938, Tokens/sec: 617845.30\n",
      "Step: 1119, Training Loss: 1.17350, LR: 0.0004932, Tokens/sec: 614158.48\n",
      "Step: 1120, Training Loss: 0.86709, LR: 0.0004925, Tokens/sec: 615408.18\n",
      "Step: 1121, Training Loss: 1.09873, LR: 0.0004918, Tokens/sec: 617287.26\n",
      "Step: 1122, Training Loss: 1.09106, LR: 0.0004911, Tokens/sec: 617530.47\n",
      "Step: 1123, Training Loss: 1.04485, LR: 0.0004904, Tokens/sec: 617078.21\n",
      "Step: 1124, Training Loss: 1.33160, LR: 0.0004897, Tokens/sec: 617582.29\n",
      "Step: 1125, Training Loss: 1.31032, LR: 0.0004890, Tokens/sec: 616554.37\n",
      "Step: 1126, Training Loss: 1.31499, LR: 0.0004883, Tokens/sec: 616439.56\n",
      "Step: 1127, Training Loss: 1.32624, LR: 0.0004876, Tokens/sec: 617711.39\n",
      "Step: 1128, Training Loss: 1.26162, LR: 0.0004870, Tokens/sec: 616872.45\n",
      "Step: 1129, Training Loss: 1.31723, LR: 0.0004863, Tokens/sec: 616968.75\n",
      "Step: 1130, Training Loss: 1.22489, LR: 0.0004856, Tokens/sec: 616308.18\n",
      "Step: 1131, Training Loss: 1.29216, LR: 0.0004849, Tokens/sec: 616455.04\n",
      "Step: 1132, Training Loss: 1.32365, LR: 0.0004842, Tokens/sec: 617280.36\n",
      "Step: 1133, Training Loss: 1.19055, LR: 0.0004835, Tokens/sec: 617209.44\n",
      "Step: 1134, Training Loss: 1.31210, LR: 0.0004828, Tokens/sec: 616186.53\n",
      "Step: 1135, Training Loss: 1.24336, LR: 0.0004821, Tokens/sec: 615906.55\n",
      "Step: 1136, Training Loss: 1.24552, LR: 0.0004815, Tokens/sec: 615765.94\n",
      "Step: 1137, Training Loss: 1.23169, LR: 0.0004808, Tokens/sec: 616276.76\n",
      "Step: 1138, Training Loss: 1.26034, LR: 0.0004801, Tokens/sec: 616825.21\n",
      "Step: 1139, Training Loss: 1.24508, LR: 0.0004794, Tokens/sec: 616062.62\n",
      "Step: 1140, Training Loss: 1.26802, LR: 0.0004787, Tokens/sec: 614056.22\n",
      "Step: 1141, Training Loss: 1.21702, LR: 0.0004780, Tokens/sec: 616289.39\n",
      "Step: 1142, Training Loss: 1.33421, LR: 0.0004773, Tokens/sec: 616565.02\n",
      "Step: 1143, Training Loss: 1.37052, LR: 0.0004767, Tokens/sec: 616405.90\n",
      "Step: 1144, Training Loss: 1.29176, LR: 0.0004760, Tokens/sec: 616434.79\n",
      "Step: 1145, Training Loss: 1.45325, LR: 0.0004753, Tokens/sec: 617454.65\n",
      "Step: 1146, Training Loss: 1.33102, LR: 0.0004746, Tokens/sec: 616618.56\n",
      "Step: 1147, Training Loss: 1.31460, LR: 0.0004739, Tokens/sec: 616389.10\n",
      "Step: 1148, Training Loss: 1.28852, LR: 0.0004732, Tokens/sec: 616253.03\n",
      "Step: 1149, Training Loss: 1.28628, LR: 0.0004725, Tokens/sec: 616513.23\n",
      "Step: 1150, Training Loss: 1.41398, LR: 0.0004719, Tokens/sec: 616769.01\n",
      "Step: 1151, Training Loss: 1.26558, LR: 0.0004712, Tokens/sec: 616689.33\n",
      "Step: 1152, Training Loss: 1.27285, LR: 0.0004705, Tokens/sec: 617451.50\n",
      "Step: 1153, Training Loss: 1.27299, LR: 0.0004698, Tokens/sec: 616274.33\n",
      "Step: 1154, Training Loss: 1.63456, LR: 0.0004691, Tokens/sec: 616309.57\n",
      "Step: 1155, Training Loss: 1.34773, LR: 0.0004684, Tokens/sec: 617769.23\n",
      "Step: 1156, Training Loss: 1.33211, LR: 0.0004678, Tokens/sec: 616287.54\n",
      "Step: 1157, Training Loss: 1.30333, LR: 0.0004671, Tokens/sec: 617201.29\n",
      "Step: 1158, Training Loss: 1.33530, LR: 0.0004664, Tokens/sec: 614118.71\n",
      "Step: 1159, Training Loss: 1.25566, LR: 0.0004657, Tokens/sec: 615921.77\n",
      "Step: 1160, Training Loss: 1.46106, LR: 0.0004650, Tokens/sec: 616900.36\n",
      "Step: 1161, Training Loss: 1.31734, LR: 0.0004643, Tokens/sec: 616726.81\n",
      "Step: 1162, Training Loss: 1.57922, LR: 0.0004637, Tokens/sec: 616116.32\n",
      "Step: 1163, Training Loss: 1.37412, LR: 0.0004630, Tokens/sec: 617343.73\n",
      "Step: 1164, Training Loss: 1.31230, LR: 0.0004623, Tokens/sec: 615983.85\n",
      "Step: 1165, Training Loss: 1.36180, LR: 0.0004616, Tokens/sec: 616473.89\n",
      "Step: 1166, Training Loss: 1.24040, LR: 0.0004609, Tokens/sec: 615663.99\n",
      "Step: 1167, Training Loss: 1.24381, LR: 0.0004603, Tokens/sec: 616079.48\n",
      "Step: 1168, Training Loss: 1.52003, LR: 0.0004596, Tokens/sec: 617461.29\n",
      "Step: 1169, Training Loss: 1.23160, LR: 0.0004589, Tokens/sec: 616711.80\n",
      "Step: 1170, Training Loss: 1.23163, LR: 0.0004582, Tokens/sec: 617571.38\n",
      "Step: 1171, Training Loss: 1.21681, LR: 0.0004575, Tokens/sec: 617465.87\n",
      "Step: 1172, Training Loss: 1.39511, LR: 0.0004568, Tokens/sec: 616992.76\n",
      "Step: 1173, Training Loss: 1.22857, LR: 0.0004562, Tokens/sec: 617841.91\n",
      "Step: 1174, Training Loss: 1.47800, LR: 0.0004555, Tokens/sec: 616045.38\n",
      "Step: 1175, Training Loss: 1.35214, LR: 0.0004548, Tokens/sec: 617020.90\n",
      "Step: 1176, Training Loss: 1.34114, LR: 0.0004541, Tokens/sec: 616465.59\n",
      "Step: 1177, Training Loss: 1.22252, LR: 0.0004535, Tokens/sec: 616042.09\n",
      "Step: 1178, Training Loss: 1.18377, LR: 0.0004528, Tokens/sec: 615344.40\n",
      "Step: 1179, Training Loss: 1.15646, LR: 0.0004521, Tokens/sec: 616614.07\n",
      "Step: 1180, Training Loss: 1.25486, LR: 0.0004514, Tokens/sec: 615910.73\n",
      "Step: 1181, Training Loss: 1.14941, LR: 0.0004507, Tokens/sec: 617061.99\n",
      "Step: 1182, Training Loss: 1.04832, LR: 0.0004501, Tokens/sec: 617372.28\n",
      "Step: 1183, Training Loss: 1.03651, LR: 0.0004494, Tokens/sec: 617167.56\n",
      "Step: 1184, Training Loss: 1.12468, LR: 0.0004487, Tokens/sec: 617304.91\n",
      "Step: 1185, Training Loss: 1.27032, LR: 0.0004480, Tokens/sec: 616127.16\n",
      "Step: 1186, Training Loss: 1.07495, LR: 0.0004474, Tokens/sec: 615319.02\n",
      "Step: 1187, Training Loss: 1.23058, LR: 0.0004467, Tokens/sec: 616083.46\n",
      "Step: 1188, Training Loss: 1.31062, LR: 0.0004460, Tokens/sec: 617444.33\n",
      "Step: 1189, Training Loss: 1.39296, LR: 0.0004453, Tokens/sec: 616592.63\n",
      "Step: 1190, Training Loss: 1.31816, LR: 0.0004446, Tokens/sec: 617099.31\n",
      "Step: 1191, Training Loss: 1.35789, LR: 0.0004440, Tokens/sec: 616033.53\n",
      "Step: 1192, Training Loss: 1.44654, LR: 0.0004433, Tokens/sec: 618173.61\n",
      "Step: 1193, Training Loss: 1.34812, LR: 0.0004426, Tokens/sec: 616542.47\n",
      "Step: 1194, Training Loss: 1.37855, LR: 0.0004419, Tokens/sec: 616850.44\n",
      "Step: 1195, Training Loss: 1.30566, LR: 0.0004413, Tokens/sec: 617826.19\n",
      "Step: 1196, Training Loss: 1.38670, LR: 0.0004406, Tokens/sec: 617377.50\n",
      "Step: 1197, Training Loss: 1.39379, LR: 0.0004399, Tokens/sec: 617608.29\n",
      "Step: 1198, Training Loss: 1.45473, LR: 0.0004392, Tokens/sec: 615271.20\n",
      "Step: 1199, Training Loss: 1.24494, LR: 0.0004386, Tokens/sec: 616046.70\n",
      "Step: 1200, Training Loss: 1.34492, LR: 0.0004379, Tokens/sec: 617486.68\n",
      "Step: 1201, Training Loss: 1.39182, LR: 0.0004372, Tokens/sec: 615285.94\n",
      "Step: 1202, Training Loss: 1.27607, LR: 0.0004366, Tokens/sec: 616264.21\n",
      "Step: 1203, Training Loss: 1.37609, LR: 0.0004359, Tokens/sec: 617591.06\n",
      "Step: 1204, Training Loss: 1.41071, LR: 0.0004352, Tokens/sec: 617233.17\n",
      "Step: 1205, Training Loss: 1.27643, LR: 0.0004345, Tokens/sec: 616297.05\n",
      "Step: 1206, Training Loss: 1.41782, LR: 0.0004339, Tokens/sec: 616040.67\n",
      "Step: 1207, Training Loss: 1.27412, LR: 0.0004332, Tokens/sec: 616510.78\n",
      "Step: 1208, Training Loss: 1.32903, LR: 0.0004325, Tokens/sec: 616474.57\n",
      "Step: 1209, Training Loss: 1.22679, LR: 0.0004319, Tokens/sec: 616440.37\n",
      "Step: 1210, Training Loss: 1.26052, LR: 0.0004312, Tokens/sec: 616225.97\n",
      "Step: 1211, Training Loss: 1.31597, LR: 0.0004305, Tokens/sec: 617208.45\n",
      "Step: 1212, Training Loss: 1.28949, LR: 0.0004298, Tokens/sec: 617960.97\n",
      "Step: 1213, Training Loss: 1.31485, LR: 0.0004292, Tokens/sec: 617269.08\n",
      "Step: 1214, Training Loss: 1.29580, LR: 0.0004285, Tokens/sec: 618310.54\n",
      "Step: 1215, Training Loss: 1.34385, LR: 0.0004278, Tokens/sec: 617823.05\n",
      "Step: 1216, Training Loss: 1.26843, LR: 0.0004272, Tokens/sec: 616662.94\n",
      "Step: 1217, Training Loss: 1.40137, LR: 0.0004265, Tokens/sec: 616137.94\n",
      "Step: 1218, Training Loss: 1.34244, LR: 0.0004258, Tokens/sec: 616552.38\n",
      "Step: 1219, Training Loss: 1.29921, LR: 0.0004252, Tokens/sec: 617669.40\n",
      "Step: 1220, Training Loss: 1.21953, LR: 0.0004245, Tokens/sec: 617583.99\n",
      "Step: 1221, Training Loss: 1.90174, LR: 0.0004238, Tokens/sec: 616845.92\n",
      "Step: 1222, Training Loss: 1.36179, LR: 0.0004232, Tokens/sec: 617802.72\n",
      "Step: 1223, Training Loss: 1.32696, LR: 0.0004225, Tokens/sec: 617232.89\n",
      "Step: 1224, Training Loss: 1.45524, LR: 0.0004218, Tokens/sec: 616535.13\n",
      "Step: 1225, Training Loss: 1.38897, LR: 0.0004212, Tokens/sec: 617985.74\n",
      "Step: 1226, Training Loss: 1.28559, LR: 0.0004205, Tokens/sec: 615059.55\n",
      "Step: 1227, Training Loss: 1.30081, LR: 0.0004198, Tokens/sec: 615333.82\n",
      "Step: 1228, Training Loss: 1.28158, LR: 0.0004192, Tokens/sec: 616418.00\n",
      "Step: 1229, Training Loss: 1.40469, LR: 0.0004185, Tokens/sec: 617136.83\n",
      "Step: 1230, Training Loss: 1.70307, LR: 0.0004178, Tokens/sec: 617261.31\n",
      "Step: 1231, Training Loss: 1.51457, LR: 0.0004172, Tokens/sec: 617330.75\n",
      "Step: 1232, Training Loss: 1.43603, LR: 0.0004165, Tokens/sec: 617533.72\n",
      "Step: 1233, Training Loss: 1.36204, LR: 0.0004158, Tokens/sec: 617838.77\n",
      "Step: 1234, Training Loss: 1.42486, LR: 0.0004152, Tokens/sec: 616072.62\n",
      "Step: 1235, Training Loss: 1.37057, LR: 0.0004145, Tokens/sec: 616650.00\n",
      "Step: 1236, Training Loss: 1.28512, LR: 0.0004139, Tokens/sec: 617506.42\n",
      "Step: 1237, Training Loss: 1.32008, LR: 0.0004132, Tokens/sec: 616978.53\n",
      "Step: 1238, Training Loss: 1.49729, LR: 0.0004125, Tokens/sec: 616427.50\n",
      "Step: 1239, Training Loss: 1.45951, LR: 0.0004119, Tokens/sec: 616677.74\n",
      "Step: 1240, Training Loss: 1.29334, LR: 0.0004112, Tokens/sec: 616783.84\n",
      "Step: 1241, Training Loss: 1.28137, LR: 0.0004105, Tokens/sec: 615717.63\n",
      "Step: 1242, Training Loss: 1.18025, LR: 0.0004099, Tokens/sec: 616785.89\n",
      "Step: 1243, Training Loss: 1.33679, LR: 0.0004092, Tokens/sec: 617858.38\n",
      "Step: 1244, Training Loss: 1.21068, LR: 0.0004086, Tokens/sec: 616969.75\n",
      "Step: 1245, Training Loss: 1.24098, LR: 0.0004079, Tokens/sec: 616818.31\n",
      "Step: 1246, Training Loss: 1.25348, LR: 0.0004072, Tokens/sec: 615787.96\n",
      "Step: 1247, Training Loss: 1.15624, LR: 0.0004066, Tokens/sec: 616645.66\n",
      "Step: 1248, Training Loss: 1.22975, LR: 0.0004059, Tokens/sec: 617465.60\n",
      "Step: 1249, Training Loss: 1.38246, LR: 0.0004053, Tokens/sec: 616301.87\n",
      "Step: 1250, Training Loss: 1.23316, LR: 0.0004046, Tokens/sec: 616472.11\n",
      "Step: 1251, Training Loss: 2.84482, LR: 0.0004040, Tokens/sec: 617254.79\n",
      "Step: 1252, Training Loss: 3.08103, LR: 0.0004033, Tokens/sec: 617731.77\n",
      "Step: 1253, Training Loss: 1.35446, LR: 0.0004026, Tokens/sec: 616668.40\n",
      "Step: 1254, Training Loss: 1.27039, LR: 0.0004020, Tokens/sec: 616454.28\n",
      "Step: 1255, Training Loss: 1.28251, LR: 0.0004013, Tokens/sec: 617248.89\n",
      "Step: 1256, Training Loss: 1.22720, LR: 0.0004007, Tokens/sec: 617242.22\n",
      "Step: 1257, Training Loss: 1.28122, LR: 0.0004000, Tokens/sec: 617549.20\n",
      "Step: 1258, Training Loss: 1.24136, LR: 0.0003994, Tokens/sec: 617159.72\n",
      "Step: 1259, Training Loss: 1.30496, LR: 0.0003987, Tokens/sec: 617548.78\n",
      "Step: 1260, Training Loss: 1.16977, LR: 0.0003981, Tokens/sec: 617286.16\n",
      "Step: 1261, Training Loss: 1.01878, LR: 0.0003974, Tokens/sec: 616554.18\n",
      "Step: 1262, Training Loss: 0.96960, LR: 0.0003967, Tokens/sec: 615646.10\n",
      "Step: 1263, Training Loss: 1.00599, LR: 0.0003961, Tokens/sec: 616610.44\n",
      "Step: 1264, Training Loss: 1.21418, LR: 0.0003954, Tokens/sec: 617500.24\n",
      "Step: 1265, Training Loss: 1.35098, LR: 0.0003948, Tokens/sec: 616329.28\n",
      "Step: 1266, Training Loss: 1.36295, LR: 0.0003941, Tokens/sec: 616647.39\n",
      "Step: 1267, Training Loss: 1.34302, LR: 0.0003935, Tokens/sec: 617262.32\n",
      "Step: 1268, Training Loss: 1.35703, LR: 0.0003928, Tokens/sec: 615500.21\n",
      "Step: 1269, Training Loss: 1.38545, LR: 0.0003922, Tokens/sec: 615838.10\n",
      "Step: 1270, Training Loss: 1.30222, LR: 0.0003915, Tokens/sec: 616432.86\n",
      "Step: 1271, Training Loss: 1.33281, LR: 0.0003909, Tokens/sec: 616372.64\n",
      "Step: 1272, Training Loss: 1.36594, LR: 0.0003902, Tokens/sec: 616328.88\n",
      "Step: 1273, Training Loss: 1.24390, LR: 0.0003896, Tokens/sec: 617055.46\n",
      "Step: 1274, Training Loss: 1.22316, LR: 0.0003889, Tokens/sec: 617165.73\n",
      "Step: 1275, Training Loss: 1.37803, LR: 0.0003883, Tokens/sec: 618162.29\n",
      "Step: 1276, Training Loss: 1.25124, LR: 0.0003876, Tokens/sec: 617204.16\n",
      "Step: 1277, Training Loss: 1.24776, LR: 0.0003870, Tokens/sec: 617285.22\n",
      "Step: 1278, Training Loss: 1.24207, LR: 0.0003863, Tokens/sec: 615861.23\n",
      "Step: 1279, Training Loss: 1.25821, LR: 0.0003857, Tokens/sec: 617310.00\n",
      "Step: 1280, Training Loss: 1.25396, LR: 0.0003850, Tokens/sec: 616856.89\n",
      "Step: 1281, Training Loss: 1.20974, LR: 0.0003844, Tokens/sec: 616686.54\n",
      "Step: 1282, Training Loss: 1.28183, LR: 0.0003837, Tokens/sec: 618090.58\n",
      "Step: 1283, Training Loss: 1.14811, LR: 0.0003831, Tokens/sec: 617377.36\n",
      "Step: 1284, Training Loss: 1.18639, LR: 0.0003825, Tokens/sec: 616561.61\n",
      "Step: 1285, Training Loss: 0.98184, LR: 0.0003818, Tokens/sec: 617184.60\n",
      "Step: 1286, Training Loss: 1.14068, LR: 0.0003812, Tokens/sec: 616588.33\n",
      "Step: 1287, Training Loss: 1.21730, LR: 0.0003805, Tokens/sec: 615721.05\n",
      "Step: 1288, Training Loss: 1.14873, LR: 0.0003799, Tokens/sec: 617086.61\n",
      "Step: 1289, Training Loss: 1.24544, LR: 0.0003792, Tokens/sec: 617483.91\n",
      "Step: 1290, Training Loss: 0.96066, LR: 0.0003786, Tokens/sec: 616535.89\n",
      "Step: 1291, Training Loss: 1.05580, LR: 0.0003780, Tokens/sec: 616371.70\n",
      "Step: 1292, Training Loss: 1.08884, LR: 0.0003773, Tokens/sec: 618020.13\n",
      "Step: 1293, Training Loss: 0.91327, LR: 0.0003767, Tokens/sec: 616646.03\n",
      "Step: 1294, Training Loss: 0.81476, LR: 0.0003760, Tokens/sec: 616718.32\n",
      "Step: 1295, Training Loss: 0.70472, LR: 0.0003754, Tokens/sec: 617077.88\n",
      "Step: 1296, Training Loss: 0.61225, LR: 0.0003747, Tokens/sec: 617378.27\n",
      "Step: 1297, Training Loss: 0.69152, LR: 0.0003741, Tokens/sec: 618412.49\n",
      "Step: 1298, Training Loss: 0.68992, LR: 0.0003735, Tokens/sec: 616240.26\n",
      "Step: 1299, Training Loss: 1.17959, LR: 0.0003728, Tokens/sec: 617656.47\n",
      "Step: 1300, Training Loss: 1.34191, LR: 0.0003722, Tokens/sec: 616820.86\n",
      "Step: 1301, Training Loss: 1.28829, LR: 0.0003716, Tokens/sec: 617561.82\n",
      "Step: 1302, Training Loss: 1.34651, LR: 0.0003709, Tokens/sec: 615978.16\n",
      "Step: 1303, Training Loss: 1.37491, LR: 0.0003703, Tokens/sec: 616805.75\n",
      "Step: 1304, Training Loss: 1.27114, LR: 0.0003696, Tokens/sec: 617012.53\n",
      "Step: 1305, Training Loss: 1.29140, LR: 0.0003690, Tokens/sec: 616551.89\n",
      "Step: 1306, Training Loss: 1.36221, LR: 0.0003684, Tokens/sec: 617239.81\n",
      "Step: 1307, Training Loss: 1.42492, LR: 0.0003677, Tokens/sec: 615982.42\n",
      "Step: 1308, Training Loss: 1.34895, LR: 0.0003671, Tokens/sec: 617807.99\n",
      "Step: 1309, Training Loss: 1.37251, LR: 0.0003665, Tokens/sec: 618117.33\n",
      "Step: 1310, Training Loss: 1.28018, LR: 0.0003658, Tokens/sec: 616975.86\n",
      "Step: 1311, Training Loss: 1.32552, LR: 0.0003652, Tokens/sec: 617541.62\n",
      "Step: 1312, Training Loss: 1.34103, LR: 0.0003646, Tokens/sec: 616931.88\n",
      "Step: 1313, Training Loss: 1.27211, LR: 0.0003639, Tokens/sec: 616600.46\n",
      "Step: 1314, Training Loss: 1.43776, LR: 0.0003633, Tokens/sec: 616661.21\n",
      "Step: 1315, Training Loss: 1.32855, LR: 0.0003627, Tokens/sec: 616944.02\n",
      "Step: 1316, Training Loss: 1.31620, LR: 0.0003620, Tokens/sec: 615401.02\n",
      "Step: 1317, Training Loss: 1.32775, LR: 0.0003614, Tokens/sec: 618389.05\n",
      "Step: 1318, Training Loss: 1.34680, LR: 0.0003608, Tokens/sec: 616520.19\n",
      "Step: 1319, Training Loss: 1.30299, LR: 0.0003601, Tokens/sec: 616463.47\n",
      "Step: 1320, Training Loss: 1.28377, LR: 0.0003595, Tokens/sec: 618147.46\n",
      "Step: 1321, Training Loss: 1.33458, LR: 0.0003589, Tokens/sec: 616319.63\n",
      "Step: 1322, Training Loss: 1.29193, LR: 0.0003582, Tokens/sec: 617091.19\n",
      "Step: 1323, Training Loss: 1.27759, LR: 0.0003576, Tokens/sec: 616700.10\n",
      "Step: 1324, Training Loss: 1.25026, LR: 0.0003570, Tokens/sec: 615215.33\n",
      "Step: 1325, Training Loss: 1.31861, LR: 0.0003564, Tokens/sec: 617311.55\n",
      "Step: 1326, Training Loss: 1.20743, LR: 0.0003557, Tokens/sec: 616949.70\n",
      "Step: 1327, Training Loss: 1.28253, LR: 0.0003551, Tokens/sec: 617049.92\n",
      "Step: 1328, Training Loss: 1.32173, LR: 0.0003545, Tokens/sec: 617857.70\n",
      "Step: 1329, Training Loss: 1.21184, LR: 0.0003539, Tokens/sec: 616210.46\n",
      "Step: 1330, Training Loss: 1.30622, LR: 0.0003532, Tokens/sec: 616639.97\n",
      "Step: 1331, Training Loss: 1.23192, LR: 0.0003526, Tokens/sec: 617051.26\n",
      "Step: 1332, Training Loss: 1.23141, LR: 0.0003520, Tokens/sec: 617238.25\n",
      "Step: 1333, Training Loss: 1.30536, LR: 0.0003514, Tokens/sec: 616724.30\n",
      "Step: 1334, Training Loss: 1.51087, LR: 0.0003507, Tokens/sec: 616297.18\n",
      "Step: 1335, Training Loss: 1.43496, LR: 0.0003501, Tokens/sec: 616599.21\n",
      "Step: 1336, Training Loss: 1.34022, LR: 0.0003495, Tokens/sec: 617159.08\n",
      "Step: 1337, Training Loss: 1.26698, LR: 0.0003489, Tokens/sec: 618351.05\n",
      "Step: 1338, Training Loss: 1.29421, LR: 0.0003482, Tokens/sec: 616262.12\n",
      "Step: 1339, Training Loss: 1.36822, LR: 0.0003476, Tokens/sec: 616805.29\n",
      "Step: 1340, Training Loss: 1.33921, LR: 0.0003470, Tokens/sec: 617481.88\n",
      "Step: 1341, Training Loss: 1.37092, LR: 0.0003464, Tokens/sec: 616861.53\n",
      "Step: 1342, Training Loss: 1.29410, LR: 0.0003458, Tokens/sec: 616762.49\n",
      "Step: 1343, Training Loss: 1.31275, LR: 0.0003451, Tokens/sec: 617998.91\n",
      "Step: 1344, Training Loss: 1.27310, LR: 0.0003445, Tokens/sec: 617227.79\n",
      "Step: 1345, Training Loss: 1.53119, LR: 0.0003439, Tokens/sec: 616514.29\n",
      "Step: 1346, Training Loss: 1.35066, LR: 0.0003433, Tokens/sec: 617104.16\n",
      "Step: 1347, Training Loss: 1.29779, LR: 0.0003427, Tokens/sec: 616228.68\n",
      "Step: 1348, Training Loss: 1.25175, LR: 0.0003421, Tokens/sec: 615150.46\n",
      "Step: 1349, Training Loss: 1.31332, LR: 0.0003414, Tokens/sec: 618046.73\n",
      "Step: 1350, Training Loss: 1.23644, LR: 0.0003408, Tokens/sec: 616513.89\n",
      "Step: 1351, Training Loss: 1.28759, LR: 0.0003402, Tokens/sec: 616217.63\n",
      "Step: 1352, Training Loss: 1.31655, LR: 0.0003396, Tokens/sec: 616740.98\n",
      "Step: 1353, Training Loss: 1.20568, LR: 0.0003390, Tokens/sec: 616713.66\n",
      "Step: 1354, Training Loss: 1.27169, LR: 0.0003384, Tokens/sec: 616523.39\n",
      "Step: 1355, Training Loss: 1.29533, LR: 0.0003378, Tokens/sec: 618012.25\n",
      "Step: 1356, Training Loss: 1.24691, LR: 0.0003371, Tokens/sec: 615620.35\n",
      "Step: 1357, Training Loss: 1.33698, LR: 0.0003365, Tokens/sec: 617350.73\n",
      "Step: 1358, Training Loss: 1.30646, LR: 0.0003359, Tokens/sec: 617229.01\n",
      "Step: 1359, Training Loss: 1.30865, LR: 0.0003353, Tokens/sec: 615784.18\n",
      "Step: 1360, Training Loss: 1.29784, LR: 0.0003347, Tokens/sec: 617397.37\n",
      "Step: 1361, Training Loss: 1.26031, LR: 0.0003341, Tokens/sec: 616363.30\n",
      "Step: 1362, Training Loss: 1.30734, LR: 0.0003335, Tokens/sec: 615930.82\n",
      "Step: 1363, Training Loss: 1.25048, LR: 0.0003329, Tokens/sec: 617716.79\n",
      "Step: 1364, Training Loss: 1.31823, LR: 0.0003323, Tokens/sec: 617658.23\n",
      "Step: 1365, Training Loss: 1.27294, LR: 0.0003317, Tokens/sec: 617855.83\n",
      "Step: 1366, Training Loss: 1.25721, LR: 0.0003310, Tokens/sec: 616941.06\n",
      "Step: 1367, Training Loss: 1.23246, LR: 0.0003304, Tokens/sec: 616017.27\n",
      "Step: 1368, Training Loss: 1.32393, LR: 0.0003298, Tokens/sec: 616542.64\n",
      "Step: 1369, Training Loss: 1.28812, LR: 0.0003292, Tokens/sec: 617990.45\n",
      "Step: 1370, Training Loss: 1.23416, LR: 0.0003286, Tokens/sec: 616318.67\n",
      "Step: 1371, Training Loss: 1.27899, LR: 0.0003280, Tokens/sec: 617754.27\n",
      "Step: 1372, Training Loss: 1.31262, LR: 0.0003274, Tokens/sec: 616247.92\n",
      "Step: 1373, Training Loss: 1.27582, LR: 0.0003268, Tokens/sec: 617374.95\n",
      "Step: 1374, Training Loss: 1.26421, LR: 0.0003262, Tokens/sec: 616622.09\n",
      "Step: 1375, Training Loss: 1.30258, LR: 0.0003256, Tokens/sec: 617824.91\n",
      "Step: 1376, Training Loss: 1.19883, LR: 0.0003250, Tokens/sec: 616151.58\n",
      "Step: 1377, Training Loss: 1.29371, LR: 0.0003244, Tokens/sec: 616878.65\n",
      "Step: 1378, Training Loss: 1.22358, LR: 0.0003238, Tokens/sec: 616232.48\n",
      "Step: 1379, Training Loss: 1.26273, LR: 0.0003232, Tokens/sec: 615541.78\n",
      "Step: 1380, Training Loss: 1.27965, LR: 0.0003226, Tokens/sec: 616704.27\n",
      "Step: 1381, Training Loss: 1.30883, LR: 0.0003220, Tokens/sec: 617537.03\n",
      "Step: 1382, Training Loss: 1.22418, LR: 0.0003214, Tokens/sec: 617259.18\n",
      "Step: 1383, Training Loss: 1.21589, LR: 0.0003208, Tokens/sec: 616900.17\n",
      "Step: 1384, Training Loss: 1.26511, LR: 0.0003202, Tokens/sec: 616244.78\n",
      "Step: 1385, Training Loss: 1.23521, LR: 0.0003196, Tokens/sec: 615484.87\n",
      "Step: 1386, Training Loss: 1.24427, LR: 0.0003190, Tokens/sec: 616740.38\n",
      "Step: 1387, Training Loss: 1.26319, LR: 0.0003184, Tokens/sec: 616683.07\n",
      "Step: 1388, Training Loss: 1.21584, LR: 0.0003178, Tokens/sec: 617293.41\n",
      "Step: 1389, Training Loss: 1.21960, LR: 0.0003172, Tokens/sec: 617464.62\n",
      "Step: 1390, Training Loss: 1.20880, LR: 0.0003166, Tokens/sec: 616747.09\n",
      "Step: 1391, Training Loss: 1.20683, LR: 0.0003160, Tokens/sec: 616902.70\n",
      "Step: 1392, Training Loss: 1.27028, LR: 0.0003154, Tokens/sec: 616487.76\n",
      "Step: 1393, Training Loss: 1.22337, LR: 0.0003148, Tokens/sec: 617465.37\n",
      "Step: 1394, Training Loss: 1.24581, LR: 0.0003143, Tokens/sec: 616688.06\n",
      "Step: 1395, Training Loss: 1.21082, LR: 0.0003137, Tokens/sec: 616043.26\n",
      "Step: 1396, Training Loss: 1.24129, LR: 0.0003131, Tokens/sec: 618261.04\n",
      "Step: 1397, Training Loss: 1.23045, LR: 0.0003125, Tokens/sec: 617391.90\n",
      "Step: 1398, Training Loss: 1.31267, LR: 0.0003119, Tokens/sec: 617001.78\n",
      "Step: 1399, Training Loss: 1.30400, LR: 0.0003113, Tokens/sec: 617637.94\n",
      "Step: 1400, Training Loss: 1.22666, LR: 0.0003107, Tokens/sec: 617002.04\n",
      "Step: 1401, Training Loss: 1.23241, LR: 0.0003101, Tokens/sec: 617509.36\n",
      "Step: 1402, Training Loss: 1.21092, LR: 0.0003095, Tokens/sec: 615359.52\n",
      "Step: 1403, Training Loss: 1.19475, LR: 0.0003089, Tokens/sec: 616338.21\n",
      "Step: 1404, Training Loss: 1.28061, LR: 0.0003084, Tokens/sec: 615832.87\n",
      "Step: 1405, Training Loss: 1.24170, LR: 0.0003078, Tokens/sec: 617038.88\n",
      "Step: 1406, Training Loss: 1.25168, LR: 0.0003072, Tokens/sec: 617178.65\n",
      "Step: 1407, Training Loss: 1.27736, LR: 0.0003066, Tokens/sec: 616330.93\n",
      "Step: 1408, Training Loss: 1.22209, LR: 0.0003060, Tokens/sec: 616471.85\n",
      "Step: 1409, Training Loss: 1.33814, LR: 0.0003054, Tokens/sec: 617021.31\n",
      "Step: 1410, Training Loss: 1.20732, LR: 0.0003049, Tokens/sec: 615693.49\n",
      "Step: 1411, Training Loss: 1.18410, LR: 0.0003043, Tokens/sec: 616301.19\n",
      "Step: 1412, Training Loss: 1.22788, LR: 0.0003037, Tokens/sec: 617336.29\n",
      "Step: 1413, Training Loss: 1.19270, LR: 0.0003031, Tokens/sec: 616702.89\n",
      "Step: 1414, Training Loss: 1.18694, LR: 0.0003025, Tokens/sec: 616161.34\n",
      "Step: 1415, Training Loss: 1.44840, LR: 0.0003019, Tokens/sec: 617103.24\n",
      "Step: 1416, Training Loss: 1.19145, LR: 0.0003014, Tokens/sec: 617112.24\n",
      "Step: 1417, Training Loss: 1.21507, LR: 0.0003008, Tokens/sec: 616902.78\n",
      "Step: 1418, Training Loss: 1.21816, LR: 0.0003002, Tokens/sec: 617658.40\n",
      "Step: 1419, Training Loss: 1.37015, LR: 0.0002996, Tokens/sec: 617888.19\n",
      "Step: 1420, Training Loss: 1.18385, LR: 0.0002991, Tokens/sec: 617206.95\n",
      "Step: 1421, Training Loss: 1.19467, LR: 0.0002985, Tokens/sec: 617379.42\n",
      "Step: 1422, Training Loss: 1.27528, LR: 0.0002979, Tokens/sec: 616427.50\n",
      "Step: 1423, Training Loss: 1.27744, LR: 0.0002973, Tokens/sec: 615890.52\n",
      "Step: 1424, Training Loss: 1.28022, LR: 0.0002968, Tokens/sec: 617337.81\n",
      "Step: 1425, Training Loss: 1.32284, LR: 0.0002962, Tokens/sec: 616590.37\n",
      "Step: 1426, Training Loss: 1.25467, LR: 0.0002956, Tokens/sec: 616854.80\n",
      "Step: 1427, Training Loss: 1.26923, LR: 0.0002950, Tokens/sec: 617385.55\n",
      "Step: 1428, Training Loss: 1.31441, LR: 0.0002945, Tokens/sec: 617163.53\n",
      "Step: 1429, Training Loss: 1.24306, LR: 0.0002939, Tokens/sec: 617614.95\n",
      "Step: 1430, Training Loss: 1.23668, LR: 0.0002933, Tokens/sec: 617584.36\n",
      "Step: 1431, Training Loss: 1.19721, LR: 0.0002927, Tokens/sec: 617006.53\n",
      "Step: 1432, Training Loss: 1.23042, LR: 0.0002922, Tokens/sec: 617228.16\n",
      "Step: 1433, Training Loss: 1.22981, LR: 0.0002916, Tokens/sec: 617027.25\n",
      "Step: 1434, Training Loss: 1.35275, LR: 0.0002910, Tokens/sec: 616573.59\n",
      "Step: 1435, Training Loss: 1.24253, LR: 0.0002905, Tokens/sec: 616903.59\n",
      "Step: 1436, Training Loss: 1.20883, LR: 0.0002899, Tokens/sec: 616536.71\n",
      "Step: 1437, Training Loss: 1.23145, LR: 0.0002893, Tokens/sec: 616977.33\n",
      "Step: 1438, Training Loss: 1.18857, LR: 0.0002888, Tokens/sec: 617049.48\n",
      "Step: 1439, Training Loss: 1.32280, LR: 0.0002882, Tokens/sec: 616105.08\n",
      "Step: 1440, Training Loss: 1.20716, LR: 0.0002876, Tokens/sec: 616858.89\n",
      "Step: 1441, Training Loss: 1.21567, LR: 0.0002871, Tokens/sec: 615605.11\n",
      "Step: 1442, Training Loss: 1.31794, LR: 0.0002865, Tokens/sec: 617239.20\n",
      "Step: 1443, Training Loss: 1.19867, LR: 0.0002859, Tokens/sec: 617500.83\n",
      "Step: 1444, Training Loss: 1.28553, LR: 0.0002854, Tokens/sec: 616902.10\n",
      "Step: 1445, Training Loss: 1.21089, LR: 0.0002848, Tokens/sec: 617237.44\n",
      "Step: 1446, Training Loss: 1.20950, LR: 0.0002843, Tokens/sec: 617393.65\n",
      "Step: 1447, Training Loss: 1.22073, LR: 0.0002837, Tokens/sec: 616906.08\n",
      "Step: 1448, Training Loss: 1.19028, LR: 0.0002831, Tokens/sec: 615882.45\n",
      "Step: 1449, Training Loss: 1.20669, LR: 0.0002826, Tokens/sec: 616002.76\n",
      "Step: 1450, Training Loss: 1.22642, LR: 0.0002820, Tokens/sec: 617794.37\n",
      "Step: 1451, Training Loss: 1.23197, LR: 0.0002815, Tokens/sec: 617802.43\n",
      "Step: 1452, Training Loss: 1.18448, LR: 0.0002809, Tokens/sec: 616323.86\n",
      "Step: 1453, Training Loss: 1.17268, LR: 0.0002804, Tokens/sec: 616545.99\n",
      "Step: 1454, Training Loss: 1.17410, LR: 0.0002798, Tokens/sec: 616320.39\n",
      "Step: 1455, Training Loss: 1.17340, LR: 0.0002792, Tokens/sec: 616750.52\n",
      "Step: 1456, Training Loss: 1.23159, LR: 0.0002787, Tokens/sec: 615515.13\n",
      "Step: 1457, Training Loss: 1.19774, LR: 0.0002781, Tokens/sec: 616996.80\n",
      "Step: 1458, Training Loss: 1.22820, LR: 0.0002776, Tokens/sec: 616913.49\n",
      "Step: 1459, Training Loss: 1.23087, LR: 0.0002770, Tokens/sec: 617930.12\n",
      "Step: 1460, Training Loss: 1.21357, LR: 0.0002765, Tokens/sec: 615101.10\n",
      "Step: 1461, Training Loss: 1.23173, LR: 0.0002759, Tokens/sec: 616650.54\n",
      "Step: 1462, Training Loss: 1.20135, LR: 0.0002754, Tokens/sec: 615401.98\n",
      "Step: 1463, Training Loss: 1.27927, LR: 0.0002748, Tokens/sec: 616497.17\n",
      "Step: 1464, Training Loss: 1.33397, LR: 0.0002743, Tokens/sec: 616570.31\n",
      "Step: 1465, Training Loss: 1.19012, LR: 0.0002737, Tokens/sec: 617658.59\n",
      "Step: 1466, Training Loss: 1.20481, LR: 0.0002732, Tokens/sec: 617595.72\n",
      "Step: 1467, Training Loss: 1.20891, LR: 0.0002726, Tokens/sec: 615702.10\n",
      "Step: 1468, Training Loss: 1.19808, LR: 0.0002721, Tokens/sec: 616689.26\n",
      "Step: 1469, Training Loss: 1.23442, LR: 0.0002715, Tokens/sec: 617592.12\n",
      "Step: 1470, Training Loss: 1.18234, LR: 0.0002710, Tokens/sec: 616684.95\n",
      "Step: 1471, Training Loss: 1.15536, LR: 0.0002704, Tokens/sec: 616475.44\n",
      "Step: 1472, Training Loss: 1.23588, LR: 0.0002699, Tokens/sec: 618357.11\n",
      "Step: 1473, Training Loss: 1.29827, LR: 0.0002694, Tokens/sec: 615757.02\n",
      "Step: 1474, Training Loss: 1.22229, LR: 0.0002688, Tokens/sec: 616128.52\n",
      "Step: 1475, Training Loss: 1.17806, LR: 0.0002683, Tokens/sec: 617308.21\n",
      "Step: 1476, Training Loss: 1.19707, LR: 0.0002677, Tokens/sec: 617010.52\n",
      "Step: 1477, Training Loss: 1.17054, LR: 0.0002672, Tokens/sec: 617333.21\n",
      "Step: 1478, Training Loss: 1.34530, LR: 0.0002666, Tokens/sec: 617365.71\n",
      "Step: 1479, Training Loss: 1.22003, LR: 0.0002661, Tokens/sec: 616645.73\n",
      "Step: 1480, Training Loss: 1.35401, LR: 0.0002656, Tokens/sec: 616184.64\n",
      "Step: 1481, Training Loss: 1.16288, LR: 0.0002650, Tokens/sec: 618306.97\n",
      "Step: 1482, Training Loss: 1.15568, LR: 0.0002645, Tokens/sec: 615618.70\n",
      "Step: 1483, Training Loss: 1.22057, LR: 0.0002640, Tokens/sec: 617722.49\n",
      "Step: 1484, Training Loss: 1.19290, LR: 0.0002634, Tokens/sec: 617918.05\n",
      "Step: 1485, Training Loss: 1.19788, LR: 0.0002629, Tokens/sec: 617004.68\n",
      "Step: 1486, Training Loss: 1.15619, LR: 0.0002623, Tokens/sec: 616150.59\n",
      "Step: 1487, Training Loss: 1.12971, LR: 0.0002618, Tokens/sec: 616985.71\n",
      "Step: 1488, Training Loss: 1.14020, LR: 0.0002613, Tokens/sec: 616879.72\n",
      "Step: 1489, Training Loss: 1.15183, LR: 0.0002607, Tokens/sec: 617669.39\n",
      "Step: 1490, Training Loss: 1.15408, LR: 0.0002602, Tokens/sec: 615606.25\n",
      "Step: 1491, Training Loss: 1.20391, LR: 0.0002597, Tokens/sec: 617353.97\n",
      "Step: 1492, Training Loss: 1.16478, LR: 0.0002592, Tokens/sec: 615008.67\n",
      "Step: 1493, Training Loss: 1.16495, LR: 0.0002586, Tokens/sec: 617136.35\n",
      "Step: 1494, Training Loss: 1.16127, LR: 0.0002581, Tokens/sec: 616828.12\n",
      "Step: 1495, Training Loss: 1.14751, LR: 0.0002576, Tokens/sec: 616465.51\n",
      "Step: 1496, Training Loss: 1.14777, LR: 0.0002570, Tokens/sec: 617136.78\n",
      "Step: 1497, Training Loss: 1.13911, LR: 0.0002565, Tokens/sec: 617298.56\n",
      "Step: 1498, Training Loss: 1.22095, LR: 0.0002560, Tokens/sec: 616962.98\n",
      "Step: 1499, Training Loss: 1.14700, LR: 0.0002555, Tokens/sec: 617150.93\n",
      "Step: 1500, Training Loss: 1.15668, LR: 0.0002549, Tokens/sec: 616941.51\n",
      "Computing Eval loss, steps: 21\n",
      "Step: 1500, Eval Loss: 1.16098\n",
      "Step: 1501, Training Loss: 1.13073, LR: 0.0002544, Tokens/sec: 615557.48\n",
      "Step: 1502, Training Loss: 1.12170, LR: 0.0002539, Tokens/sec: 616706.42\n",
      "Step: 1503, Training Loss: 1.14722, LR: 0.0002534, Tokens/sec: 617139.66\n",
      "Step: 1504, Training Loss: 1.10825, LR: 0.0002528, Tokens/sec: 617844.32\n",
      "Step: 1505, Training Loss: 1.13520, LR: 0.0002523, Tokens/sec: 617278.42\n",
      "Step: 1506, Training Loss: 1.12454, LR: 0.0002518, Tokens/sec: 616558.85\n",
      "Step: 1507, Training Loss: 1.27747, LR: 0.0002513, Tokens/sec: 617061.00\n",
      "Step: 1508, Training Loss: 1.16465, LR: 0.0002508, Tokens/sec: 617972.51\n",
      "Step: 1509, Training Loss: 1.18575, LR: 0.0002502, Tokens/sec: 617160.86\n",
      "Step: 1510, Training Loss: 1.12601, LR: 0.0002497, Tokens/sec: 617281.10\n",
      "Step: 1511, Training Loss: 1.16668, LR: 0.0002492, Tokens/sec: 616673.45\n",
      "Step: 1512, Training Loss: 1.16004, LR: 0.0002487, Tokens/sec: 617193.24\n",
      "Step: 1513, Training Loss: 1.20992, LR: 0.0002482, Tokens/sec: 616719.94\n",
      "Step: 1514, Training Loss: 1.34030, LR: 0.0002477, Tokens/sec: 617302.53\n",
      "Step: 1515, Training Loss: 1.11903, LR: 0.0002471, Tokens/sec: 616262.69\n",
      "Step: 1516, Training Loss: 0.94055, LR: 0.0002466, Tokens/sec: 616128.22\n",
      "Step: 1517, Training Loss: 0.57413, LR: 0.0002461, Tokens/sec: 617153.84\n",
      "Step: 1518, Training Loss: 0.53641, LR: 0.0002456, Tokens/sec: 616724.54\n",
      "Step: 1519, Training Loss: 0.39090, LR: 0.0002451, Tokens/sec: 615728.07\n",
      "Step: 1520, Training Loss: 0.30340, LR: 0.0002446, Tokens/sec: 617173.12\n",
      "Step: 1521, Training Loss: 0.25823, LR: 0.0002441, Tokens/sec: 616581.41\n",
      "Step: 1522, Training Loss: 0.24192, LR: 0.0002436, Tokens/sec: 616022.35\n",
      "Step: 1523, Training Loss: 0.25731, LR: 0.0002430, Tokens/sec: 616595.99\n",
      "Step: 1524, Training Loss: 0.18409, LR: 0.0002425, Tokens/sec: 615960.43\n",
      "Step: 1525, Training Loss: 0.19435, LR: 0.0002420, Tokens/sec: 617722.73\n",
      "Step: 1526, Training Loss: 0.19141, LR: 0.0002415, Tokens/sec: 617129.27\n",
      "Step: 1527, Training Loss: 0.18406, LR: 0.0002410, Tokens/sec: 616431.79\n",
      "Step: 1528, Training Loss: 0.15206, LR: 0.0002405, Tokens/sec: 617018.76\n",
      "Step: 1529, Training Loss: 0.56068, LR: 0.0002400, Tokens/sec: 616600.29\n",
      "Step: 1530, Training Loss: 0.21975, LR: 0.0002395, Tokens/sec: 618018.67\n",
      "Step: 1531, Training Loss: 0.30436, LR: 0.0002390, Tokens/sec: 617041.29\n",
      "Step: 1532, Training Loss: 0.26393, LR: 0.0002385, Tokens/sec: 615762.74\n",
      "Step: 1533, Training Loss: 0.28881, LR: 0.0002380, Tokens/sec: 617298.50\n",
      "Step: 1534, Training Loss: 0.25265, LR: 0.0002375, Tokens/sec: 616785.82\n",
      "Step: 1535, Training Loss: 0.29745, LR: 0.0002370, Tokens/sec: 617298.81\n",
      "Step: 1536, Training Loss: 0.28036, LR: 0.0002365, Tokens/sec: 616445.55\n",
      "Step: 1537, Training Loss: 0.85205, LR: 0.0002360, Tokens/sec: 616386.22\n",
      "Step: 1538, Training Loss: 1.27452, LR: 0.0002355, Tokens/sec: 617814.91\n",
      "Step: 1539, Training Loss: 1.24912, LR: 0.0002350, Tokens/sec: 617190.59\n",
      "Step: 1540, Training Loss: 1.23486, LR: 0.0002345, Tokens/sec: 617235.36\n",
      "Step: 1541, Training Loss: 1.23616, LR: 0.0002340, Tokens/sec: 617097.18\n",
      "Step: 1542, Training Loss: 1.20555, LR: 0.0002335, Tokens/sec: 616335.35\n",
      "Step: 1543, Training Loss: 1.28264, LR: 0.0002330, Tokens/sec: 617777.75\n",
      "Step: 1544, Training Loss: 0.95346, LR: 0.0002325, Tokens/sec: 617627.33\n",
      "Step: 1545, Training Loss: 0.58344, LR: 0.0002320, Tokens/sec: 617693.06\n",
      "Step: 1546, Training Loss: 1.05081, LR: 0.0002316, Tokens/sec: 617355.56\n",
      "Step: 1547, Training Loss: 0.69550, LR: 0.0002311, Tokens/sec: 617125.20\n",
      "Step: 1548, Training Loss: 0.30934, LR: 0.0002306, Tokens/sec: 617565.64\n",
      "Step: 1549, Training Loss: 0.28810, LR: 0.0002301, Tokens/sec: 617013.83\n",
      "Step: 1550, Training Loss: 0.22397, LR: 0.0002296, Tokens/sec: 616630.77\n",
      "Step: 1551, Training Loss: 0.98999, LR: 0.0002291, Tokens/sec: 617569.40\n",
      "Step: 1552, Training Loss: 1.25626, LR: 0.0002286, Tokens/sec: 616110.56\n",
      "Step: 1553, Training Loss: 0.90584, LR: 0.0002281, Tokens/sec: 616880.40\n",
      "Step: 1554, Training Loss: 0.81783, LR: 0.0002277, Tokens/sec: 617150.69\n",
      "Step: 1555, Training Loss: 1.31317, LR: 0.0002272, Tokens/sec: 616436.68\n",
      "Step: 1556, Training Loss: 1.23714, LR: 0.0002267, Tokens/sec: 615829.07\n",
      "Step: 1557, Training Loss: 1.24574, LR: 0.0002262, Tokens/sec: 617457.59\n",
      "Step: 1558, Training Loss: 1.24803, LR: 0.0002257, Tokens/sec: 616018.60\n",
      "Step: 1559, Training Loss: 1.24319, LR: 0.0002252, Tokens/sec: 615128.72\n",
      "Step: 1560, Training Loss: 1.43639, LR: 0.0002248, Tokens/sec: 617701.93\n",
      "Step: 1561, Training Loss: 1.28878, LR: 0.0002243, Tokens/sec: 617860.02\n",
      "Step: 1562, Training Loss: 1.21142, LR: 0.0002238, Tokens/sec: 617336.31\n",
      "Step: 1563, Training Loss: 1.25154, LR: 0.0002233, Tokens/sec: 616642.84\n",
      "Step: 1564, Training Loss: 1.23763, LR: 0.0002228, Tokens/sec: 617316.95\n",
      "Step: 1565, Training Loss: 1.25067, LR: 0.0002224, Tokens/sec: 616572.54\n",
      "Step: 1566, Training Loss: 1.19851, LR: 0.0002219, Tokens/sec: 617024.88\n",
      "Step: 1567, Training Loss: 1.22455, LR: 0.0002214, Tokens/sec: 616755.08\n",
      "Step: 1568, Training Loss: 1.19437, LR: 0.0002209, Tokens/sec: 616158.19\n",
      "Step: 1569, Training Loss: 0.97674, LR: 0.0002205, Tokens/sec: 617572.91\n",
      "Step: 1570, Training Loss: 0.54125, LR: 0.0002200, Tokens/sec: 616917.94\n",
      "Step: 1571, Training Loss: 0.99214, LR: 0.0002195, Tokens/sec: 617506.84\n",
      "Step: 1572, Training Loss: 1.25258, LR: 0.0002190, Tokens/sec: 615787.40\n",
      "Step: 1573, Training Loss: 1.17926, LR: 0.0002186, Tokens/sec: 617746.77\n",
      "Step: 1574, Training Loss: 1.25377, LR: 0.0002181, Tokens/sec: 616253.41\n",
      "Step: 1575, Training Loss: 1.28448, LR: 0.0002176, Tokens/sec: 616105.35\n",
      "Step: 1576, Training Loss: 2.16198, LR: 0.0002172, Tokens/sec: 617562.07\n",
      "Step: 1577, Training Loss: 1.32521, LR: 0.0002167, Tokens/sec: 617193.25\n",
      "Step: 1578, Training Loss: 1.38415, LR: 0.0002162, Tokens/sec: 616244.92\n",
      "Step: 1579, Training Loss: 1.22900, LR: 0.0002158, Tokens/sec: 616556.50\n",
      "Step: 1580, Training Loss: 1.15398, LR: 0.0002153, Tokens/sec: 617317.67\n",
      "Step: 1581, Training Loss: 1.21567, LR: 0.0002148, Tokens/sec: 617371.75\n",
      "Step: 1582, Training Loss: 1.21828, LR: 0.0002144, Tokens/sec: 616811.61\n",
      "Step: 1583, Training Loss: 1.52502, LR: 0.0002139, Tokens/sec: 617026.02\n",
      "Step: 1584, Training Loss: 1.26232, LR: 0.0002135, Tokens/sec: 617588.65\n",
      "Step: 1585, Training Loss: 1.23891, LR: 0.0002130, Tokens/sec: 617392.63\n",
      "Step: 1586, Training Loss: 1.31545, LR: 0.0002125, Tokens/sec: 617216.48\n",
      "Step: 1587, Training Loss: 1.29378, LR: 0.0002121, Tokens/sec: 615249.26\n",
      "Step: 1588, Training Loss: 1.28723, LR: 0.0002116, Tokens/sec: 617032.45\n",
      "Step: 1589, Training Loss: 1.27293, LR: 0.0002112, Tokens/sec: 616670.49\n",
      "Step: 1590, Training Loss: 1.30198, LR: 0.0002107, Tokens/sec: 616021.00\n",
      "Step: 1591, Training Loss: 1.26744, LR: 0.0002102, Tokens/sec: 617015.25\n",
      "Step: 1592, Training Loss: 1.23542, LR: 0.0002098, Tokens/sec: 617959.99\n",
      "Step: 1593, Training Loss: 1.19987, LR: 0.0002093, Tokens/sec: 616819.92\n",
      "Step: 1594, Training Loss: 1.26668, LR: 0.0002089, Tokens/sec: 616984.60\n",
      "Step: 1595, Training Loss: 1.26569, LR: 0.0002084, Tokens/sec: 615845.50\n",
      "Step: 1596, Training Loss: 1.23407, LR: 0.0002080, Tokens/sec: 617608.55\n",
      "Step: 1597, Training Loss: 1.20918, LR: 0.0002075, Tokens/sec: 616587.17\n",
      "Step: 1598, Training Loss: 1.27233, LR: 0.0002071, Tokens/sec: 617206.27\n",
      "Step: 1599, Training Loss: 1.19792, LR: 0.0002066, Tokens/sec: 617054.04\n",
      "Step: 1600, Training Loss: 1.27408, LR: 0.0002062, Tokens/sec: 615941.13\n",
      "Step: 1601, Training Loss: 1.21679, LR: 0.0002057, Tokens/sec: 616810.84\n",
      "Step: 1602, Training Loss: 1.20616, LR: 0.0002053, Tokens/sec: 617400.96\n",
      "Step: 1603, Training Loss: 1.21107, LR: 0.0002048, Tokens/sec: 618070.88\n",
      "Step: 1604, Training Loss: 1.20875, LR: 0.0002044, Tokens/sec: 616896.46\n",
      "Step: 1605, Training Loss: 1.19620, LR: 0.0002039, Tokens/sec: 617976.73\n",
      "Step: 1606, Training Loss: 1.19570, LR: 0.0002035, Tokens/sec: 615939.36\n",
      "Step: 1607, Training Loss: 1.21117, LR: 0.0002031, Tokens/sec: 616873.39\n",
      "Step: 1608, Training Loss: 1.21638, LR: 0.0002026, Tokens/sec: 617262.43\n",
      "Step: 1609, Training Loss: 1.21649, LR: 0.0002022, Tokens/sec: 615762.22\n",
      "Step: 1610, Training Loss: 1.15230, LR: 0.0002017, Tokens/sec: 616796.28\n",
      "Step: 1611, Training Loss: 1.19376, LR: 0.0002013, Tokens/sec: 617397.84\n",
      "Step: 1612, Training Loss: 1.24181, LR: 0.0002009, Tokens/sec: 617401.32\n",
      "Step: 1613, Training Loss: 1.21332, LR: 0.0002004, Tokens/sec: 617952.42\n",
      "Step: 1614, Training Loss: 1.19694, LR: 0.0002000, Tokens/sec: 616940.33\n",
      "Step: 1615, Training Loss: 1.17989, LR: 0.0001995, Tokens/sec: 616147.26\n",
      "Step: 1616, Training Loss: 1.20639, LR: 0.0001991, Tokens/sec: 616655.36\n",
      "Step: 1617, Training Loss: 1.22534, LR: 0.0001987, Tokens/sec: 616519.30\n",
      "Step: 1618, Training Loss: 1.21086, LR: 0.0001982, Tokens/sec: 617398.25\n",
      "Step: 1619, Training Loss: 1.19863, LR: 0.0001978, Tokens/sec: 615918.34\n",
      "Step: 1620, Training Loss: 1.16943, LR: 0.0001974, Tokens/sec: 617463.19\n",
      "Step: 1621, Training Loss: 1.16573, LR: 0.0001969, Tokens/sec: 617918.10\n",
      "Step: 1622, Training Loss: 1.17028, LR: 0.0001965, Tokens/sec: 618293.95\n",
      "Step: 1623, Training Loss: 1.19655, LR: 0.0001961, Tokens/sec: 614795.64\n",
      "Step: 1624, Training Loss: 1.19818, LR: 0.0001957, Tokens/sec: 615527.99\n",
      "Step: 1625, Training Loss: 1.21346, LR: 0.0001952, Tokens/sec: 616595.07\n",
      "Step: 1626, Training Loss: 1.20487, LR: 0.0001948, Tokens/sec: 617970.48\n",
      "Step: 1627, Training Loss: 1.15439, LR: 0.0001944, Tokens/sec: 618714.25\n",
      "Step: 1628, Training Loss: 1.21148, LR: 0.0001939, Tokens/sec: 616463.08\n",
      "Step: 1629, Training Loss: 1.22919, LR: 0.0001935, Tokens/sec: 616218.25\n",
      "Step: 1630, Training Loss: 1.26575, LR: 0.0001931, Tokens/sec: 617839.01\n",
      "Step: 1631, Training Loss: 1.23717, LR: 0.0001927, Tokens/sec: 616935.48\n",
      "Step: 1632, Training Loss: 1.20418, LR: 0.0001923, Tokens/sec: 614551.18\n",
      "Step: 1633, Training Loss: 1.16903, LR: 0.0001918, Tokens/sec: 617759.78\n",
      "Step: 1634, Training Loss: 1.11224, LR: 0.0001914, Tokens/sec: 617487.31\n",
      "Step: 1635, Training Loss: 1.15003, LR: 0.0001910, Tokens/sec: 614763.32\n",
      "Step: 1636, Training Loss: 1.20959, LR: 0.0001906, Tokens/sec: 616328.10\n",
      "Step: 1637, Training Loss: 1.20292, LR: 0.0001902, Tokens/sec: 616256.03\n",
      "Step: 1638, Training Loss: 1.33414, LR: 0.0001897, Tokens/sec: 617000.17\n",
      "Step: 1639, Training Loss: 1.16671, LR: 0.0001893, Tokens/sec: 617205.10\n",
      "Step: 1640, Training Loss: 1.16380, LR: 0.0001889, Tokens/sec: 615534.31\n",
      "Step: 1641, Training Loss: 1.19201, LR: 0.0001885, Tokens/sec: 617093.99\n",
      "Step: 1642, Training Loss: 1.24759, LR: 0.0001881, Tokens/sec: 616529.80\n",
      "Step: 1643, Training Loss: 1.16873, LR: 0.0001877, Tokens/sec: 616324.71\n",
      "Step: 1644, Training Loss: 1.09586, LR: 0.0001873, Tokens/sec: 616467.92\n",
      "Step: 1645, Training Loss: 1.11669, LR: 0.0001868, Tokens/sec: 617718.08\n",
      "Step: 1646, Training Loss: 1.01944, LR: 0.0001864, Tokens/sec: 615760.63\n",
      "Step: 1647, Training Loss: 1.16844, LR: 0.0001860, Tokens/sec: 617687.08\n",
      "Step: 1648, Training Loss: 1.17654, LR: 0.0001856, Tokens/sec: 616413.56\n",
      "Step: 1649, Training Loss: 1.13041, LR: 0.0001852, Tokens/sec: 617429.77\n",
      "Step: 1650, Training Loss: 1.19299, LR: 0.0001848, Tokens/sec: 617790.06\n",
      "Step: 1651, Training Loss: 1.16186, LR: 0.0001844, Tokens/sec: 615489.68\n",
      "Step: 1652, Training Loss: 1.19943, LR: 0.0001840, Tokens/sec: 617170.91\n",
      "Step: 1653, Training Loss: 1.17253, LR: 0.0001836, Tokens/sec: 616682.00\n",
      "Step: 1654, Training Loss: 1.14397, LR: 0.0001832, Tokens/sec: 617240.70\n",
      "Step: 1655, Training Loss: 1.12551, LR: 0.0001828, Tokens/sec: 616150.09\n",
      "Step: 1656, Training Loss: 1.11456, LR: 0.0001824, Tokens/sec: 618213.96\n",
      "Step: 1657, Training Loss: 1.06162, LR: 0.0001820, Tokens/sec: 617495.43\n",
      "Step: 1658, Training Loss: 1.14521, LR: 0.0001816, Tokens/sec: 617613.70\n",
      "Step: 1659, Training Loss: 1.19042, LR: 0.0001812, Tokens/sec: 617964.56\n",
      "Step: 1660, Training Loss: 1.14967, LR: 0.0001808, Tokens/sec: 616844.33\n",
      "Step: 1661, Training Loss: 1.23087, LR: 0.0001804, Tokens/sec: 616431.08\n",
      "Step: 1662, Training Loss: 1.14042, LR: 0.0001800, Tokens/sec: 617247.64\n",
      "Step: 1663, Training Loss: 1.20069, LR: 0.0001796, Tokens/sec: 618134.85\n",
      "Step: 1664, Training Loss: 1.16037, LR: 0.0001792, Tokens/sec: 617043.16\n",
      "Step: 1665, Training Loss: 1.12345, LR: 0.0001788, Tokens/sec: 617903.22\n",
      "Step: 1666, Training Loss: 1.14670, LR: 0.0001784, Tokens/sec: 616553.32\n",
      "Step: 1667, Training Loss: 1.19876, LR: 0.0001780, Tokens/sec: 617337.04\n",
      "Step: 1668, Training Loss: 1.10899, LR: 0.0001776, Tokens/sec: 615907.80\n",
      "Step: 1669, Training Loss: 1.19260, LR: 0.0001772, Tokens/sec: 617498.97\n",
      "Step: 1670, Training Loss: 1.19180, LR: 0.0001769, Tokens/sec: 617211.29\n",
      "Step: 1671, Training Loss: 1.15043, LR: 0.0001765, Tokens/sec: 616178.49\n",
      "Step: 1672, Training Loss: 1.12533, LR: 0.0001761, Tokens/sec: 616610.64\n",
      "Step: 1673, Training Loss: 1.04865, LR: 0.0001757, Tokens/sec: 617716.26\n",
      "Step: 1674, Training Loss: 1.09008, LR: 0.0001753, Tokens/sec: 617425.76\n",
      "Step: 1675, Training Loss: 1.08222, LR: 0.0001749, Tokens/sec: 617580.95\n",
      "Step: 1676, Training Loss: 1.34548, LR: 0.0001745, Tokens/sec: 617647.83\n",
      "Step: 1677, Training Loss: 1.56793, LR: 0.0001742, Tokens/sec: 617189.45\n",
      "Step: 1678, Training Loss: 1.54798, LR: 0.0001738, Tokens/sec: 616383.05\n",
      "Step: 1679, Training Loss: 1.17035, LR: 0.0001734, Tokens/sec: 617236.82\n",
      "Step: 1680, Training Loss: 1.14210, LR: 0.0001730, Tokens/sec: 616288.01\n",
      "Step: 1681, Training Loss: 1.21447, LR: 0.0001726, Tokens/sec: 616636.31\n",
      "Step: 1682, Training Loss: 1.09002, LR: 0.0001723, Tokens/sec: 617552.18\n",
      "Step: 1683, Training Loss: 1.06169, LR: 0.0001719, Tokens/sec: 617260.19\n",
      "Step: 1684, Training Loss: 1.18299, LR: 0.0001715, Tokens/sec: 616273.80\n",
      "Step: 1685, Training Loss: 1.16723, LR: 0.0001711, Tokens/sec: 616638.58\n",
      "Step: 1686, Training Loss: 1.21421, LR: 0.0001708, Tokens/sec: 618583.29\n",
      "Step: 1687, Training Loss: 1.14334, LR: 0.0001704, Tokens/sec: 617786.37\n",
      "Step: 1688, Training Loss: 1.15081, LR: 0.0001700, Tokens/sec: 616910.66\n",
      "Step: 1689, Training Loss: 1.12342, LR: 0.0001696, Tokens/sec: 617583.65\n",
      "Step: 1690, Training Loss: 1.12090, LR: 0.0001693, Tokens/sec: 617774.93\n",
      "Step: 1691, Training Loss: 1.16288, LR: 0.0001689, Tokens/sec: 615807.11\n",
      "Step: 1692, Training Loss: 1.19151, LR: 0.0001685, Tokens/sec: 616493.99\n",
      "Step: 1693, Training Loss: 1.15027, LR: 0.0001682, Tokens/sec: 616641.27\n",
      "Step: 1694, Training Loss: 1.17379, LR: 0.0001678, Tokens/sec: 615824.62\n",
      "Step: 1695, Training Loss: 1.19364, LR: 0.0001674, Tokens/sec: 617698.45\n",
      "Step: 1696, Training Loss: 1.19079, LR: 0.0001671, Tokens/sec: 616984.38\n",
      "Step: 1697, Training Loss: 1.24595, LR: 0.0001667, Tokens/sec: 617058.63\n",
      "Step: 1698, Training Loss: 1.15218, LR: 0.0001663, Tokens/sec: 616961.62\n",
      "Step: 1699, Training Loss: 1.17533, LR: 0.0001660, Tokens/sec: 618266.08\n",
      "Step: 1700, Training Loss: 1.12838, LR: 0.0001656, Tokens/sec: 617002.78\n",
      "Step: 1701, Training Loss: 1.17010, LR: 0.0001652, Tokens/sec: 615316.14\n",
      "Step: 1702, Training Loss: 1.30098, LR: 0.0001649, Tokens/sec: 617071.88\n",
      "Step: 1703, Training Loss: 1.13013, LR: 0.0001645, Tokens/sec: 617270.00\n",
      "Step: 1704, Training Loss: 1.21119, LR: 0.0001642, Tokens/sec: 616837.69\n",
      "Step: 1705, Training Loss: 1.24314, LR: 0.0001638, Tokens/sec: 617337.43\n",
      "Step: 1706, Training Loss: 1.14131, LR: 0.0001635, Tokens/sec: 617004.32\n",
      "Step: 1707, Training Loss: 1.14542, LR: 0.0001631, Tokens/sec: 615889.04\n",
      "Step: 1708, Training Loss: 1.12571, LR: 0.0001627, Tokens/sec: 617709.13\n",
      "Step: 1709, Training Loss: 1.10984, LR: 0.0001624, Tokens/sec: 617028.91\n",
      "Step: 1710, Training Loss: 1.20283, LR: 0.0001620, Tokens/sec: 616762.53\n",
      "Step: 1711, Training Loss: 1.14845, LR: 0.0001617, Tokens/sec: 617461.34\n",
      "Step: 1712, Training Loss: 1.09399, LR: 0.0001613, Tokens/sec: 616404.49\n",
      "Step: 1713, Training Loss: 1.18106, LR: 0.0001610, Tokens/sec: 616597.49\n",
      "Step: 1714, Training Loss: 1.77225, LR: 0.0001606, Tokens/sec: 616931.39\n",
      "Step: 1715, Training Loss: 1.14014, LR: 0.0001603, Tokens/sec: 617118.56\n",
      "Step: 1716, Training Loss: 1.11551, LR: 0.0001599, Tokens/sec: 616387.74\n",
      "Step: 1717, Training Loss: 1.23264, LR: 0.0001596, Tokens/sec: 617203.55\n",
      "Step: 1718, Training Loss: 1.12862, LR: 0.0001593, Tokens/sec: 616695.28\n",
      "Step: 1719, Training Loss: 1.16812, LR: 0.0001589, Tokens/sec: 617467.55\n",
      "Step: 1720, Training Loss: 1.12411, LR: 0.0001586, Tokens/sec: 616433.21\n",
      "Step: 1721, Training Loss: 1.02898, LR: 0.0001582, Tokens/sec: 617305.56\n",
      "Step: 1722, Training Loss: 1.04792, LR: 0.0001579, Tokens/sec: 617445.83\n",
      "Step: 1723, Training Loss: 1.02855, LR: 0.0001575, Tokens/sec: 617090.96\n",
      "Step: 1724, Training Loss: 0.81131, LR: 0.0001572, Tokens/sec: 616645.55\n",
      "Step: 1725, Training Loss: 1.04835, LR: 0.0001569, Tokens/sec: 617733.67\n",
      "Step: 1726, Training Loss: 1.13434, LR: 0.0001565, Tokens/sec: 619256.56\n",
      "Step: 1727, Training Loss: 1.11539, LR: 0.0001562, Tokens/sec: 616284.23\n",
      "Step: 1728, Training Loss: 1.09043, LR: 0.0001558, Tokens/sec: 617653.03\n",
      "Step: 1729, Training Loss: 0.75724, LR: 0.0001555, Tokens/sec: 617129.32\n",
      "Step: 1730, Training Loss: 0.78997, LR: 0.0001552, Tokens/sec: 617691.43\n",
      "Step: 1731, Training Loss: 0.98388, LR: 0.0001548, Tokens/sec: 617144.16\n",
      "Step: 1732, Training Loss: 1.16910, LR: 0.0001545, Tokens/sec: 616984.29\n",
      "Step: 1733, Training Loss: 1.15511, LR: 0.0001542, Tokens/sec: 616684.40\n",
      "Step: 1734, Training Loss: 1.22222, LR: 0.0001539, Tokens/sec: 617311.94\n",
      "Step: 1735, Training Loss: 1.13378, LR: 0.0001535, Tokens/sec: 617922.01\n",
      "Step: 1736, Training Loss: 1.16899, LR: 0.0001532, Tokens/sec: 616560.23\n",
      "Step: 1737, Training Loss: 1.25440, LR: 0.0001529, Tokens/sec: 616611.92\n",
      "Step: 1738, Training Loss: 1.03849, LR: 0.0001525, Tokens/sec: 617208.04\n",
      "Step: 1739, Training Loss: 1.01036, LR: 0.0001522, Tokens/sec: 616216.14\n",
      "Step: 1740, Training Loss: 1.07253, LR: 0.0001519, Tokens/sec: 617705.22\n",
      "Step: 1741, Training Loss: 0.84327, LR: 0.0001516, Tokens/sec: 617439.95\n",
      "Step: 1742, Training Loss: 1.11487, LR: 0.0001512, Tokens/sec: 617559.80\n",
      "Step: 1743, Training Loss: 0.93331, LR: 0.0001509, Tokens/sec: 615615.11\n",
      "Step: 1744, Training Loss: 1.01385, LR: 0.0001506, Tokens/sec: 617025.66\n",
      "Step: 1745, Training Loss: 0.83039, LR: 0.0001503, Tokens/sec: 617300.30\n",
      "Step: 1746, Training Loss: 0.99815, LR: 0.0001500, Tokens/sec: 616404.67\n",
      "Step: 1747, Training Loss: 0.80563, LR: 0.0001496, Tokens/sec: 616312.23\n",
      "Step: 1748, Training Loss: 1.00991, LR: 0.0001493, Tokens/sec: 616900.60\n",
      "Step: 1749, Training Loss: 0.93523, LR: 0.0001490, Tokens/sec: 617972.45\n",
      "Step: 1750, Training Loss: 0.94157, LR: 0.0001487, Tokens/sec: 617212.92\n",
      "Step: 1751, Training Loss: 0.92513, LR: 0.0001484, Tokens/sec: 615890.07\n",
      "Step: 1752, Training Loss: 0.95666, LR: 0.0001481, Tokens/sec: 616353.95\n",
      "Step: 1753, Training Loss: 0.75823, LR: 0.0001478, Tokens/sec: 615706.59\n",
      "Step: 1754, Training Loss: 0.90930, LR: 0.0001475, Tokens/sec: 618361.20\n",
      "Step: 1755, Training Loss: 1.10158, LR: 0.0001471, Tokens/sec: 616863.26\n",
      "Step: 1756, Training Loss: 1.14487, LR: 0.0001468, Tokens/sec: 616853.90\n",
      "Step: 1757, Training Loss: 1.07559, LR: 0.0001465, Tokens/sec: 615899.80\n",
      "Step: 1758, Training Loss: 1.03679, LR: 0.0001462, Tokens/sec: 615359.61\n",
      "Step: 1759, Training Loss: 1.21760, LR: 0.0001459, Tokens/sec: 617382.13\n",
      "Step: 1760, Training Loss: 1.11811, LR: 0.0001456, Tokens/sec: 616720.95\n",
      "Step: 1761, Training Loss: 1.07533, LR: 0.0001453, Tokens/sec: 616841.38\n",
      "Step: 1762, Training Loss: 1.20537, LR: 0.0001450, Tokens/sec: 617295.29\n",
      "Step: 1763, Training Loss: 1.20642, LR: 0.0001447, Tokens/sec: 617554.22\n",
      "Step: 1764, Training Loss: 0.82469, LR: 0.0001444, Tokens/sec: 616487.23\n",
      "Step: 1765, Training Loss: 0.57863, LR: 0.0001441, Tokens/sec: 617054.73\n",
      "Step: 1766, Training Loss: 1.01936, LR: 0.0001438, Tokens/sec: 616628.92\n",
      "Step: 1767, Training Loss: 1.16295, LR: 0.0001435, Tokens/sec: 616154.71\n",
      "Step: 1768, Training Loss: 1.16152, LR: 0.0001432, Tokens/sec: 617081.96\n",
      "Step: 1769, Training Loss: 1.14058, LR: 0.0001429, Tokens/sec: 616861.09\n",
      "Step: 1770, Training Loss: 1.17204, LR: 0.0001426, Tokens/sec: 616587.06\n",
      "Step: 1771, Training Loss: 1.23378, LR: 0.0001423, Tokens/sec: 617086.96\n",
      "Step: 1772, Training Loss: 1.20816, LR: 0.0001420, Tokens/sec: 618650.44\n",
      "Step: 1773, Training Loss: 1.14216, LR: 0.0001417, Tokens/sec: 616355.99\n",
      "Step: 1774, Training Loss: 1.12186, LR: 0.0001414, Tokens/sec: 617858.91\n",
      "Step: 1775, Training Loss: 1.18618, LR: 0.0001411, Tokens/sec: 618713.03\n",
      "Step: 1776, Training Loss: 1.04623, LR: 0.0001408, Tokens/sec: 616231.26\n",
      "Step: 1777, Training Loss: 1.22795, LR: 0.0001406, Tokens/sec: 617435.50\n",
      "Step: 1778, Training Loss: 1.16063, LR: 0.0001403, Tokens/sec: 616415.83\n",
      "Step: 1779, Training Loss: 1.07850, LR: 0.0001400, Tokens/sec: 617288.93\n",
      "Step: 1780, Training Loss: 1.13182, LR: 0.0001397, Tokens/sec: 616981.53\n",
      "Step: 1781, Training Loss: 1.11965, LR: 0.0001394, Tokens/sec: 616739.24\n",
      "Step: 1782, Training Loss: 1.11595, LR: 0.0001391, Tokens/sec: 617434.68\n",
      "Step: 1783, Training Loss: 1.04145, LR: 0.0001388, Tokens/sec: 617374.40\n",
      "Step: 1784, Training Loss: 1.12630, LR: 0.0001386, Tokens/sec: 616607.54\n",
      "Step: 1785, Training Loss: 1.12187, LR: 0.0001383, Tokens/sec: 616288.95\n",
      "Step: 1786, Training Loss: 1.19876, LR: 0.0001380, Tokens/sec: 616451.99\n",
      "Step: 1787, Training Loss: 1.17625, LR: 0.0001377, Tokens/sec: 617270.70\n",
      "Step: 1788, Training Loss: 1.11615, LR: 0.0001374, Tokens/sec: 616275.45\n",
      "Step: 1789, Training Loss: 1.06946, LR: 0.0001372, Tokens/sec: 617283.59\n",
      "Step: 1790, Training Loss: 1.08169, LR: 0.0001369, Tokens/sec: 618236.50\n",
      "Step: 1791, Training Loss: 1.05615, LR: 0.0001366, Tokens/sec: 616095.48\n",
      "Step: 1792, Training Loss: 1.18117, LR: 0.0001363, Tokens/sec: 615947.26\n",
      "Step: 1793, Training Loss: 1.17240, LR: 0.0001361, Tokens/sec: 616284.98\n",
      "Step: 1794, Training Loss: 1.13935, LR: 0.0001358, Tokens/sec: 617414.92\n",
      "Step: 1795, Training Loss: 1.13007, LR: 0.0001355, Tokens/sec: 615735.12\n",
      "Step: 1796, Training Loss: 1.10923, LR: 0.0001353, Tokens/sec: 616051.84\n",
      "Step: 1797, Training Loss: 1.11985, LR: 0.0001350, Tokens/sec: 616215.74\n",
      "Step: 1798, Training Loss: 1.06724, LR: 0.0001347, Tokens/sec: 617017.40\n",
      "Step: 1799, Training Loss: 1.14354, LR: 0.0001345, Tokens/sec: 616509.55\n",
      "Step: 1800, Training Loss: 1.11449, LR: 0.0001342, Tokens/sec: 618160.88\n",
      "Step: 1801, Training Loss: 1.11186, LR: 0.0001339, Tokens/sec: 616370.41\n",
      "Step: 1802, Training Loss: 1.12827, LR: 0.0001337, Tokens/sec: 616582.49\n",
      "Step: 1803, Training Loss: 1.20605, LR: 0.0001334, Tokens/sec: 616000.61\n",
      "Step: 1804, Training Loss: 1.10499, LR: 0.0001331, Tokens/sec: 617218.04\n",
      "Step: 1805, Training Loss: 1.07008, LR: 0.0001329, Tokens/sec: 617053.71\n",
      "Step: 1806, Training Loss: 1.13437, LR: 0.0001326, Tokens/sec: 617864.15\n",
      "Step: 1807, Training Loss: 1.09856, LR: 0.0001324, Tokens/sec: 618425.85\n",
      "Step: 1808, Training Loss: 1.13311, LR: 0.0001321, Tokens/sec: 617180.95\n",
      "Step: 1809, Training Loss: 1.09560, LR: 0.0001318, Tokens/sec: 617101.75\n",
      "Step: 1810, Training Loss: 1.16924, LR: 0.0001316, Tokens/sec: 616049.92\n",
      "Step: 1811, Training Loss: 1.09907, LR: 0.0001313, Tokens/sec: 616601.58\n",
      "Step: 1812, Training Loss: 1.08893, LR: 0.0001311, Tokens/sec: 616137.18\n",
      "Step: 1813, Training Loss: 1.10055, LR: 0.0001308, Tokens/sec: 617385.61\n",
      "Step: 1814, Training Loss: 1.07078, LR: 0.0001306, Tokens/sec: 615224.47\n",
      "Step: 1815, Training Loss: 1.06704, LR: 0.0001303, Tokens/sec: 618548.23\n",
      "Step: 1816, Training Loss: 1.05685, LR: 0.0001301, Tokens/sec: 616400.54\n",
      "Step: 1817, Training Loss: 1.21804, LR: 0.0001298, Tokens/sec: 616939.22\n",
      "Step: 1818, Training Loss: 1.10771, LR: 0.0001296, Tokens/sec: 617275.51\n",
      "Step: 1819, Training Loss: 1.20366, LR: 0.0001293, Tokens/sec: 617200.64\n",
      "Step: 1820, Training Loss: 1.17094, LR: 0.0001291, Tokens/sec: 617332.00\n",
      "Step: 1821, Training Loss: 1.12309, LR: 0.0001288, Tokens/sec: 617501.94\n",
      "Step: 1822, Training Loss: 1.06467, LR: 0.0001286, Tokens/sec: 615309.00\n",
      "Step: 1823, Training Loss: 1.11719, LR: 0.0001283, Tokens/sec: 618328.65\n",
      "Step: 1824, Training Loss: 1.09073, LR: 0.0001281, Tokens/sec: 616730.11\n",
      "Step: 1825, Training Loss: 1.09865, LR: 0.0001279, Tokens/sec: 617136.34\n",
      "Step: 1826, Training Loss: 1.07436, LR: 0.0001276, Tokens/sec: 615675.82\n",
      "Step: 1827, Training Loss: 1.11758, LR: 0.0001274, Tokens/sec: 615385.55\n",
      "Step: 1828, Training Loss: 1.25118, LR: 0.0001271, Tokens/sec: 616910.19\n",
      "Step: 1829, Training Loss: 1.09965, LR: 0.0001269, Tokens/sec: 616940.08\n",
      "Step: 1830, Training Loss: 1.06110, LR: 0.0001267, Tokens/sec: 614634.52\n",
      "Step: 1831, Training Loss: 1.06637, LR: 0.0001264, Tokens/sec: 616843.05\n",
      "Step: 1832, Training Loss: 1.15021, LR: 0.0001262, Tokens/sec: 617940.23\n",
      "Step: 1833, Training Loss: 1.12116, LR: 0.0001260, Tokens/sec: 617344.74\n",
      "Step: 1834, Training Loss: 1.10430, LR: 0.0001257, Tokens/sec: 617617.15\n",
      "Step: 1835, Training Loss: 1.12463, LR: 0.0001255, Tokens/sec: 616191.81\n",
      "Step: 1836, Training Loss: 1.05454, LR: 0.0001253, Tokens/sec: 616017.42\n",
      "Step: 1837, Training Loss: 1.05886, LR: 0.0001250, Tokens/sec: 615845.46\n",
      "Step: 1838, Training Loss: 1.09244, LR: 0.0001248, Tokens/sec: 616143.66\n",
      "Step: 1839, Training Loss: 1.06789, LR: 0.0001246, Tokens/sec: 616639.97\n",
      "Step: 1840, Training Loss: 1.06522, LR: 0.0001244, Tokens/sec: 616331.40\n",
      "Step: 1841, Training Loss: 1.07562, LR: 0.0001241, Tokens/sec: 616551.35\n",
      "Step: 1842, Training Loss: 0.97293, LR: 0.0001239, Tokens/sec: 616635.50\n",
      "Step: 1843, Training Loss: 0.98713, LR: 0.0001237, Tokens/sec: 616397.13\n",
      "Step: 1844, Training Loss: 0.96338, LR: 0.0001235, Tokens/sec: 616805.56\n",
      "Step: 1845, Training Loss: 1.08793, LR: 0.0001232, Tokens/sec: 616248.65\n",
      "Step: 1846, Training Loss: 1.11580, LR: 0.0001230, Tokens/sec: 617763.53\n",
      "Step: 1847, Training Loss: 0.84112, LR: 0.0001228, Tokens/sec: 616432.28\n",
      "Step: 1848, Training Loss: 1.12751, LR: 0.0001226, Tokens/sec: 615193.68\n",
      "Step: 1849, Training Loss: 1.12302, LR: 0.0001224, Tokens/sec: 616252.65\n",
      "Step: 1850, Training Loss: 1.15185, LR: 0.0001222, Tokens/sec: 615115.23\n",
      "Step: 1851, Training Loss: 1.12414, LR: 0.0001219, Tokens/sec: 615329.85\n",
      "Step: 1852, Training Loss: 1.08401, LR: 0.0001217, Tokens/sec: 616815.24\n",
      "Step: 1853, Training Loss: 1.07108, LR: 0.0001215, Tokens/sec: 616636.28\n",
      "Step: 1854, Training Loss: 1.09216, LR: 0.0001213, Tokens/sec: 614931.94\n",
      "Step: 1855, Training Loss: 1.05892, LR: 0.0001211, Tokens/sec: 617644.99\n",
      "Step: 1856, Training Loss: 1.09080, LR: 0.0001209, Tokens/sec: 616702.96\n",
      "Step: 1857, Training Loss: 1.10559, LR: 0.0001207, Tokens/sec: 616737.40\n",
      "Step: 1858, Training Loss: 1.15279, LR: 0.0001205, Tokens/sec: 617896.13\n",
      "Step: 1859, Training Loss: 1.12944, LR: 0.0001203, Tokens/sec: 616609.10\n",
      "Step: 1860, Training Loss: 1.08261, LR: 0.0001201, Tokens/sec: 616420.30\n",
      "Step: 1861, Training Loss: 1.05219, LR: 0.0001198, Tokens/sec: 615001.89\n",
      "Step: 1862, Training Loss: 1.05077, LR: 0.0001196, Tokens/sec: 617239.27\n",
      "Step: 1863, Training Loss: 1.05291, LR: 0.0001194, Tokens/sec: 616108.62\n",
      "Step: 1864, Training Loss: 1.08049, LR: 0.0001192, Tokens/sec: 542238.63\n",
      "Step: 1865, Training Loss: 1.12684, LR: 0.0001190, Tokens/sec: 610549.66\n",
      "Step: 1866, Training Loss: 1.07925, LR: 0.0001188, Tokens/sec: 616395.11\n",
      "Step: 1867, Training Loss: 1.07826, LR: 0.0001186, Tokens/sec: 617352.03\n",
      "Step: 1868, Training Loss: 1.05205, LR: 0.0001184, Tokens/sec: 615431.57\n",
      "Step: 1869, Training Loss: 1.05832, LR: 0.0001182, Tokens/sec: 615677.30\n",
      "Step: 1870, Training Loss: 1.07280, LR: 0.0001181, Tokens/sec: 618668.28\n",
      "Step: 1871, Training Loss: 0.99211, LR: 0.0001179, Tokens/sec: 618059.68\n",
      "Step: 1872, Training Loss: 1.05992, LR: 0.0001177, Tokens/sec: 616715.76\n",
      "Step: 1873, Training Loss: 1.08981, LR: 0.0001175, Tokens/sec: 618281.80\n",
      "Step: 1874, Training Loss: 1.15951, LR: 0.0001173, Tokens/sec: 615847.88\n",
      "Step: 1875, Training Loss: 1.04582, LR: 0.0001171, Tokens/sec: 616707.54\n",
      "Step: 1876, Training Loss: 0.99700, LR: 0.0001169, Tokens/sec: 617164.49\n",
      "Step: 1877, Training Loss: 1.00111, LR: 0.0001167, Tokens/sec: 616048.67\n",
      "Step: 1878, Training Loss: 0.99541, LR: 0.0001165, Tokens/sec: 616749.06\n",
      "Step: 1879, Training Loss: 1.08086, LR: 0.0001163, Tokens/sec: 617764.73\n",
      "Step: 1880, Training Loss: 1.04981, LR: 0.0001162, Tokens/sec: 618223.78\n",
      "Step: 1881, Training Loss: 1.02552, LR: 0.0001160, Tokens/sec: 616401.70\n",
      "Step: 1882, Training Loss: 1.04970, LR: 0.0001158, Tokens/sec: 616692.58\n",
      "Step: 1883, Training Loss: 1.09312, LR: 0.0001156, Tokens/sec: 617384.38\n",
      "Step: 1884, Training Loss: 1.04069, LR: 0.0001154, Tokens/sec: 618181.60\n",
      "Step: 1885, Training Loss: 0.97442, LR: 0.0001152, Tokens/sec: 616346.16\n",
      "Step: 1886, Training Loss: 1.01989, LR: 0.0001151, Tokens/sec: 616038.69\n",
      "Step: 1887, Training Loss: 1.10605, LR: 0.0001149, Tokens/sec: 616418.63\n",
      "Step: 1888, Training Loss: 1.15451, LR: 0.0001147, Tokens/sec: 616580.07\n",
      "Step: 1889, Training Loss: 1.03802, LR: 0.0001145, Tokens/sec: 617098.71\n",
      "Step: 1890, Training Loss: 0.95791, LR: 0.0001144, Tokens/sec: 616658.98\n",
      "Step: 1891, Training Loss: 0.92073, LR: 0.0001142, Tokens/sec: 618006.77\n",
      "Step: 1892, Training Loss: 1.01174, LR: 0.0001140, Tokens/sec: 616560.06\n",
      "Step: 1893, Training Loss: 1.09086, LR: 0.0001138, Tokens/sec: 618252.06\n",
      "Step: 1894, Training Loss: 1.02724, LR: 0.0001137, Tokens/sec: 616134.00\n",
      "Step: 1895, Training Loss: 1.08199, LR: 0.0001135, Tokens/sec: 618650.90\n",
      "Step: 1896, Training Loss: 0.93696, LR: 0.0001133, Tokens/sec: 617162.15\n",
      "Step: 1897, Training Loss: 0.99113, LR: 0.0001132, Tokens/sec: 617117.74\n",
      "Step: 1898, Training Loss: 1.03054, LR: 0.0001130, Tokens/sec: 616578.16\n",
      "Step: 1899, Training Loss: 1.21809, LR: 0.0001128, Tokens/sec: 616307.37\n",
      "Step: 1900, Training Loss: 1.06455, LR: 0.0001127, Tokens/sec: 616699.80\n",
      "Step: 1901, Training Loss: 1.07208, LR: 0.0001125, Tokens/sec: 616613.00\n",
      "Step: 1902, Training Loss: 1.09157, LR: 0.0001123, Tokens/sec: 615222.47\n",
      "Step: 1903, Training Loss: 1.11378, LR: 0.0001122, Tokens/sec: 617399.23\n",
      "Step: 1904, Training Loss: 1.08883, LR: 0.0001120, Tokens/sec: 617619.96\n",
      "Step: 1905, Training Loss: 1.08078, LR: 0.0001119, Tokens/sec: 617288.23\n",
      "Step: 1906, Training Loss: 1.16213, LR: 0.0001117, Tokens/sec: 616732.03\n",
      "Step: 1907, Training Loss: 1.11264, LR: 0.0001115, Tokens/sec: 617614.54\n",
      "Step: 1908, Training Loss: 1.09125, LR: 0.0001114, Tokens/sec: 617535.62\n",
      "Step: 1909, Training Loss: 1.06448, LR: 0.0001112, Tokens/sec: 617485.50\n",
      "Step: 1910, Training Loss: 1.08099, LR: 0.0001111, Tokens/sec: 617268.10\n",
      "Step: 1911, Training Loss: 1.09540, LR: 0.0001109, Tokens/sec: 616229.50\n",
      "Step: 1912, Training Loss: 1.15031, LR: 0.0001108, Tokens/sec: 616505.58\n",
      "Step: 1913, Training Loss: 1.31609, LR: 0.0001106, Tokens/sec: 617334.83\n",
      "Step: 1914, Training Loss: 1.07897, LR: 0.0001105, Tokens/sec: 617043.27\n",
      "Step: 1915, Training Loss: 1.11623, LR: 0.0001103, Tokens/sec: 616429.09\n",
      "Step: 1916, Training Loss: 1.08142, LR: 0.0001102, Tokens/sec: 617154.27\n",
      "Step: 1917, Training Loss: 1.06373, LR: 0.0001100, Tokens/sec: 616571.01\n",
      "Step: 1918, Training Loss: 1.01112, LR: 0.0001099, Tokens/sec: 617094.65\n",
      "Step: 1919, Training Loss: 0.97405, LR: 0.0001097, Tokens/sec: 616426.47\n",
      "Step: 1920, Training Loss: 1.13417, LR: 0.0001096, Tokens/sec: 617618.77\n",
      "Step: 1921, Training Loss: 1.10144, LR: 0.0001095, Tokens/sec: 615800.47\n",
      "Step: 1922, Training Loss: 0.96769, LR: 0.0001093, Tokens/sec: 615836.59\n",
      "Step: 1923, Training Loss: 0.94371, LR: 0.0001092, Tokens/sec: 617229.86\n",
      "Step: 1924, Training Loss: 0.86545, LR: 0.0001090, Tokens/sec: 616637.49\n",
      "Step: 1925, Training Loss: 0.96459, LR: 0.0001089, Tokens/sec: 616671.42\n",
      "Step: 1926, Training Loss: 1.03544, LR: 0.0001088, Tokens/sec: 618572.21\n",
      "Step: 1927, Training Loss: 1.01461, LR: 0.0001086, Tokens/sec: 615882.10\n",
      "Step: 1928, Training Loss: 0.98789, LR: 0.0001085, Tokens/sec: 616733.26\n",
      "Step: 1929, Training Loss: 0.97575, LR: 0.0001084, Tokens/sec: 616658.75\n",
      "Step: 1930, Training Loss: 0.92081, LR: 0.0001082, Tokens/sec: 617290.55\n",
      "Step: 1931, Training Loss: 1.03463, LR: 0.0001081, Tokens/sec: 615205.99\n",
      "Step: 1932, Training Loss: 0.98692, LR: 0.0001080, Tokens/sec: 616940.81\n",
      "Step: 1933, Training Loss: 1.01759, LR: 0.0001078, Tokens/sec: 617769.85\n",
      "Step: 1934, Training Loss: 1.02679, LR: 0.0001077, Tokens/sec: 617344.81\n",
      "Step: 1935, Training Loss: 0.96684, LR: 0.0001076, Tokens/sec: 617448.42\n",
      "Step: 1936, Training Loss: 0.98418, LR: 0.0001075, Tokens/sec: 617294.93\n",
      "Step: 1937, Training Loss: 0.85654, LR: 0.0001073, Tokens/sec: 615759.92\n",
      "Step: 1938, Training Loss: 1.02692, LR: 0.0001072, Tokens/sec: 614555.65\n",
      "Step: 1939, Training Loss: 1.00576, LR: 0.0001071, Tokens/sec: 617069.96\n",
      "Step: 1940, Training Loss: 0.92581, LR: 0.0001070, Tokens/sec: 618881.36\n",
      "Step: 1941, Training Loss: 0.99607, LR: 0.0001068, Tokens/sec: 616602.48\n",
      "Step: 1942, Training Loss: 0.95317, LR: 0.0001067, Tokens/sec: 616817.06\n",
      "Step: 1943, Training Loss: 0.94974, LR: 0.0001066, Tokens/sec: 616117.98\n",
      "Step: 1944, Training Loss: 0.99925, LR: 0.0001065, Tokens/sec: 616530.71\n",
      "Step: 1945, Training Loss: 1.05453, LR: 0.0001064, Tokens/sec: 616940.67\n",
      "Step: 1946, Training Loss: 1.10234, LR: 0.0001062, Tokens/sec: 616382.64\n",
      "Step: 1947, Training Loss: 1.04657, LR: 0.0001061, Tokens/sec: 617910.63\n",
      "Step: 1948, Training Loss: 1.05996, LR: 0.0001060, Tokens/sec: 617056.76\n",
      "Step: 1949, Training Loss: 1.07180, LR: 0.0001059, Tokens/sec: 617896.18\n",
      "Step: 1950, Training Loss: 1.40732, LR: 0.0001058, Tokens/sec: 616698.25\n",
      "Step: 1951, Training Loss: 1.02111, LR: 0.0001057, Tokens/sec: 615493.72\n",
      "Step: 1952, Training Loss: 0.99654, LR: 0.0001056, Tokens/sec: 616977.82\n",
      "Step: 1953, Training Loss: 1.02333, LR: 0.0001055, Tokens/sec: 615760.15\n",
      "Step: 1954, Training Loss: 1.02135, LR: 0.0001054, Tokens/sec: 615122.32\n",
      "Step: 1955, Training Loss: 1.01594, LR: 0.0001053, Tokens/sec: 616368.40\n",
      "Step: 1956, Training Loss: 1.00123, LR: 0.0001051, Tokens/sec: 616634.60\n",
      "Step: 1957, Training Loss: 1.00699, LR: 0.0001050, Tokens/sec: 617198.51\n",
      "Step: 1958, Training Loss: 0.99870, LR: 0.0001049, Tokens/sec: 617446.18\n",
      "Step: 1959, Training Loss: 1.07460, LR: 0.0001048, Tokens/sec: 616426.64\n",
      "Step: 1960, Training Loss: 1.00780, LR: 0.0001047, Tokens/sec: 616428.20\n",
      "Step: 1961, Training Loss: 1.00494, LR: 0.0001046, Tokens/sec: 617302.76\n",
      "Step: 1962, Training Loss: 1.02303, LR: 0.0001045, Tokens/sec: 617159.37\n",
      "Step: 1963, Training Loss: 0.97838, LR: 0.0001044, Tokens/sec: 615525.10\n",
      "Step: 1964, Training Loss: 0.78737, LR: 0.0001043, Tokens/sec: 617290.63\n",
      "Step: 1965, Training Loss: 0.95122, LR: 0.0001042, Tokens/sec: 616474.33\n",
      "Step: 1966, Training Loss: 0.99364, LR: 0.0001042, Tokens/sec: 616577.09\n",
      "Step: 1967, Training Loss: 0.85432, LR: 0.0001041, Tokens/sec: 617503.07\n",
      "Step: 1968, Training Loss: 0.94703, LR: 0.0001040, Tokens/sec: 617254.13\n",
      "Step: 1969, Training Loss: 0.96499, LR: 0.0001039, Tokens/sec: 616317.99\n",
      "Step: 1970, Training Loss: 0.98356, LR: 0.0001038, Tokens/sec: 617086.04\n",
      "Step: 1971, Training Loss: 1.00479, LR: 0.0001037, Tokens/sec: 617305.60\n",
      "Step: 1972, Training Loss: 1.01770, LR: 0.0001036, Tokens/sec: 616479.53\n",
      "Step: 1973, Training Loss: 1.00242, LR: 0.0001035, Tokens/sec: 616600.06\n",
      "Step: 1974, Training Loss: 1.30176, LR: 0.0001034, Tokens/sec: 617292.95\n",
      "Step: 1975, Training Loss: 0.94650, LR: 0.0001033, Tokens/sec: 617813.04\n",
      "Step: 1976, Training Loss: 1.00593, LR: 0.0001033, Tokens/sec: 615157.60\n",
      "Step: 1977, Training Loss: 1.02397, LR: 0.0001032, Tokens/sec: 614194.35\n",
      "Step: 1978, Training Loss: 1.03460, LR: 0.0001031, Tokens/sec: 616364.26\n",
      "Step: 1979, Training Loss: 1.06126, LR: 0.0001030, Tokens/sec: 616827.84\n",
      "Step: 1980, Training Loss: 0.96665, LR: 0.0001029, Tokens/sec: 616185.88\n",
      "Step: 1981, Training Loss: 1.07997, LR: 0.0001029, Tokens/sec: 616463.45\n",
      "Step: 1982, Training Loss: 1.04847, LR: 0.0001028, Tokens/sec: 617187.93\n",
      "Step: 1983, Training Loss: 0.95466, LR: 0.0001027, Tokens/sec: 616270.82\n",
      "Step: 1984, Training Loss: 1.03166, LR: 0.0001026, Tokens/sec: 617416.40\n",
      "Step: 1985, Training Loss: 0.99347, LR: 0.0001026, Tokens/sec: 617333.38\n",
      "Step: 1986, Training Loss: 1.02359, LR: 0.0001025, Tokens/sec: 617431.51\n",
      "Step: 1987, Training Loss: 1.01178, LR: 0.0001024, Tokens/sec: 617358.40\n",
      "Step: 1988, Training Loss: 1.03546, LR: 0.0001023, Tokens/sec: 616942.14\n",
      "Step: 1989, Training Loss: 1.03584, LR: 0.0001023, Tokens/sec: 617032.20\n",
      "Step: 1990, Training Loss: 0.97636, LR: 0.0001022, Tokens/sec: 616172.77\n",
      "Step: 1991, Training Loss: 1.04362, LR: 0.0001021, Tokens/sec: 616208.87\n",
      "Step: 1992, Training Loss: 1.07034, LR: 0.0001021, Tokens/sec: 615894.98\n",
      "Step: 1993, Training Loss: 1.02159, LR: 0.0001020, Tokens/sec: 616831.18\n",
      "Step: 1994, Training Loss: 1.01508, LR: 0.0001019, Tokens/sec: 616642.82\n",
      "Step: 1995, Training Loss: 1.00380, LR: 0.0001019, Tokens/sec: 616503.52\n",
      "Step: 1996, Training Loss: 1.05792, LR: 0.0001018, Tokens/sec: 617170.04\n",
      "Step: 1997, Training Loss: 0.98044, LR: 0.0001017, Tokens/sec: 617650.78\n",
      "Step: 1998, Training Loss: 1.01149, LR: 0.0001017, Tokens/sec: 617550.65\n",
      "Step: 1999, Training Loss: 1.03072, LR: 0.0001016, Tokens/sec: 617203.81\n",
      "Step: 2000, Training Loss: 1.06922, LR: 0.0001016, Tokens/sec: 616057.20\n",
      "Computing Eval loss, steps: 21\n",
      "Step: 2000, Eval Loss: 1.05985\n",
      "Step: 2001, Training Loss: 1.04988, LR: 0.0001015, Tokens/sec: 614354.77\n",
      "Step: 2002, Training Loss: 0.99203, LR: 0.0001015, Tokens/sec: 617580.04\n",
      "Step: 2003, Training Loss: 0.99796, LR: 0.0001014, Tokens/sec: 617929.42\n",
      "Step: 2004, Training Loss: 0.97307, LR: 0.0001013, Tokens/sec: 616362.49\n",
      "Step: 2005, Training Loss: 1.07458, LR: 0.0001013, Tokens/sec: 616347.66\n",
      "Step: 2006, Training Loss: 1.02098, LR: 0.0001012, Tokens/sec: 617310.61\n",
      "Step: 2007, Training Loss: 1.01915, LR: 0.0001012, Tokens/sec: 616872.01\n",
      "Step: 2008, Training Loss: 1.04290, LR: 0.0001011, Tokens/sec: 617395.90\n",
      "Step: 2009, Training Loss: 1.04021, LR: 0.0001011, Tokens/sec: 617755.47\n",
      "Step: 2010, Training Loss: 1.02084, LR: 0.0001010, Tokens/sec: 616141.89\n",
      "Step: 2011, Training Loss: 1.22520, LR: 0.0001010, Tokens/sec: 617399.32\n",
      "Step: 2012, Training Loss: 1.17140, LR: 0.0001009, Tokens/sec: 618352.65\n",
      "Step: 2013, Training Loss: 1.13613, LR: 0.0001009, Tokens/sec: 617569.87\n",
      "Step: 2014, Training Loss: 1.09413, LR: 0.0001009, Tokens/sec: 616707.53\n",
      "Step: 2015, Training Loss: 1.01205, LR: 0.0001008, Tokens/sec: 616721.27\n",
      "Step: 2016, Training Loss: 1.04063, LR: 0.0001008, Tokens/sec: 615560.70\n",
      "Step: 2017, Training Loss: 1.05945, LR: 0.0001007, Tokens/sec: 617196.61\n",
      "Step: 2018, Training Loss: 1.06251, LR: 0.0001007, Tokens/sec: 616647.62\n",
      "Step: 2019, Training Loss: 1.03306, LR: 0.0001007, Tokens/sec: 615433.65\n",
      "Step: 2020, Training Loss: 1.05440, LR: 0.0001006, Tokens/sec: 618262.72\n",
      "Step: 2021, Training Loss: 1.00060, LR: 0.0001006, Tokens/sec: 617667.94\n",
      "Step: 2022, Training Loss: 1.04874, LR: 0.0001005, Tokens/sec: 616683.31\n",
      "Step: 2023, Training Loss: 1.05054, LR: 0.0001005, Tokens/sec: 617243.79\n",
      "Step: 2024, Training Loss: 1.02012, LR: 0.0001005, Tokens/sec: 615552.49\n",
      "Step: 2025, Training Loss: 1.08507, LR: 0.0001005, Tokens/sec: 617005.02\n",
      "Step: 2026, Training Loss: 1.02395, LR: 0.0001004, Tokens/sec: 615988.47\n",
      "Step: 2027, Training Loss: 1.09861, LR: 0.0001004, Tokens/sec: 618566.98\n",
      "Step: 2028, Training Loss: 1.04887, LR: 0.0001004, Tokens/sec: 618135.57\n",
      "Step: 2029, Training Loss: 1.09932, LR: 0.0001003, Tokens/sec: 615846.10\n",
      "Step: 2030, Training Loss: 1.07172, LR: 0.0001003, Tokens/sec: 616998.84\n",
      "Step: 2031, Training Loss: 1.05000, LR: 0.0001003, Tokens/sec: 616924.98\n",
      "Step: 2032, Training Loss: 1.00982, LR: 0.0001003, Tokens/sec: 616630.87\n",
      "Step: 2033, Training Loss: 1.07501, LR: 0.0001002, Tokens/sec: 616596.43\n",
      "Step: 2034, Training Loss: 1.02973, LR: 0.0001002, Tokens/sec: 615289.49\n",
      "Step: 2035, Training Loss: 1.04523, LR: 0.0001002, Tokens/sec: 616590.87\n",
      "Step: 2036, Training Loss: 1.05708, LR: 0.0001002, Tokens/sec: 616294.49\n",
      "Step: 2037, Training Loss: 1.05764, LR: 0.0001002, Tokens/sec: 617166.50\n",
      "Step: 2038, Training Loss: 0.97162, LR: 0.0001001, Tokens/sec: 616336.92\n",
      "Step: 2039, Training Loss: 1.01930, LR: 0.0001001, Tokens/sec: 617629.00\n",
      "Step: 2040, Training Loss: 1.01413, LR: 0.0001001, Tokens/sec: 617939.38\n",
      "Step: 2041, Training Loss: 0.99459, LR: 0.0001001, Tokens/sec: 616833.09\n",
      "Step: 2042, Training Loss: 1.00527, LR: 0.0001001, Tokens/sec: 616433.42\n",
      "Step: 2043, Training Loss: 1.04962, LR: 0.0001001, Tokens/sec: 617765.41\n",
      "Step: 2044, Training Loss: 1.01045, LR: 0.0001001, Tokens/sec: 616263.05\n",
      "Step: 2045, Training Loss: 1.04520, LR: 0.0001000, Tokens/sec: 617688.79\n",
      "Step: 2046, Training Loss: 1.11754, LR: 0.0001000, Tokens/sec: 618352.19\n",
      "Step: 2047, Training Loss: 1.20871, LR: 0.0001000, Tokens/sec: 615816.27\n",
      "Step: 2048, Training Loss: 1.02681, LR: 0.0001000, Tokens/sec: 617252.20\n",
      "Step: 2049, Training Loss: 1.03855, LR: 0.0001000, Tokens/sec: 616288.20\n",
      "Step: 2050, Training Loss: 1.02391, LR: 0.0001000, Tokens/sec: 616904.75\n",
      "Step: 2051, Training Loss: 1.04656, LR: 0.0001000, Tokens/sec: 615703.30\n",
      "Step: 2052, Training Loss: 1.08956, LR: 0.0001000, Tokens/sec: 616355.55\n",
      "Step: 2053, Training Loss: 1.12233, LR: 0.0001000, Tokens/sec: 618061.65\n"
     ]
    }
   ],
   "source": [
    "trainer.train(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b5596eda083de0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:22:17.417599499Z",
     "start_time": "2024-12-16T06:27:52.251011Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      \n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer([\"  \"], return_tensors=\"pt\")['input_ids'].to(trainer.device)\n",
    "idx = model.generate(input_ids, temperature=0.25, top_k=50, max_new_tokens=256)\n",
    "print(tokenizer.batch_decode(idx)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78ff348-a198-4d2d-bdc9-322660f96e02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
