{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:22:48.329931Z",
     "start_time": "2024-12-16T09:22:47.413234Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from model import ModelConfig, LlamaModel\n",
    "from train import TrainerConfig, FileDataLoader, Trainer\n",
    "\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f28fa23c987e72b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:22:48.333961Z",
     "start_time": "2024-12-16T09:22:48.332382Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer_id = \"HuggingFaceTB/SmolLM2-135M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bb4e51aa142abee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:22:48.533405Z",
     "start_time": "2024-12-16T09:22:48.376114Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cde027092af8291e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:22:48.540534Z",
     "start_time": "2024-12-16T09:22:48.538895Z"
    }
   },
   "outputs": [],
   "source": [
    "model_config = ModelConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=576,\n",
    "    d_head=64,\n",
    "    d_mlp_proj=1536,\n",
    "    n_layers=30,\n",
    "    n_kv_heads=3,\n",
    "    n_attn_heads=9,\n",
    "    rms_norm_eps=1e-5,\n",
    "    initializer_range=0.041666666666666664,\n",
    "    rope_theta=100000.0,\n",
    "    padding_idx=tokenizer.pad_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0897594b27eb59f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:22:48.582151Z",
     "start_time": "2024-12-16T09:22:48.580277Z"
    }
   },
   "outputs": [],
   "source": [
    "train_config = TrainerConfig(\n",
    "    per_device_train_batch_size=8,\n",
    "    max_seq_len=2048,\n",
    "    num_epochs=1,\n",
    "    eval_interval_steps=100,\n",
    "    learning_rate=1e-3,\n",
    "    grad_clip_norm=1.0,\n",
    "    tokens_folder=\"wiki_hindi_tok\",\n",
    "    max_steps=2000,\n",
    "    log_dir=\"runs/hindi_wiki\",\n",
    "    warmup_ratio=0.1,\n",
    "    val_size=0.005\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6504e357e2012d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:22:50.546015Z",
     "start_time": "2024-12-16T09:22:48.624998Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens                   | 270,491,648\n",
      "Num Trainable Params           | 162,826,560\n",
      "Train device                   | cuda, NVIDIA GeForce RTX 3090, N=1\n",
      "Training precision             | torch.bfloat16\n",
      "Flash Attention                | True\n",
      "torch.compile()                | True\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LlamaModel(model_config)\n",
    "dataloader = FileDataLoader(train_config, tokenizer)\n",
    "trainer = Trainer(train_config, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c853027a7a843745",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-16T09:22:50.552519Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training steps                 | 2,000 \n",
      "Step: 0, Training Loss: 11.31632, LR: 0.0000050, Tokens/sec: 1472.411248717527\n",
      "Computing Eval loss, steps: 83\n",
      "Step: 0, Eval Loss: 11.09461\n",
      "Step: 1, Training Loss: 11.04354, LR: 0.0000100, Tokens/sec: 1631.558609134983\n",
      "Step: 2, Training Loss: 10.77904, LR: 0.0000150, Tokens/sec: 84052.02716088382\n",
      "Step: 3, Training Loss: 10.29739, LR: 0.0000200, Tokens/sec: 86054.25690366127\n",
      "Step: 4, Training Loss: 9.77577, LR: 0.0000250, Tokens/sec: 81848.7191781952\n",
      "Step: 5, Training Loss: 9.60134, LR: 0.0000300, Tokens/sec: 83818.23877841422\n",
      "Step: 6, Training Loss: 8.84350, LR: 0.0000350, Tokens/sec: 83332.67424299945\n",
      "Step: 7, Training Loss: 8.33997, LR: 0.0000400, Tokens/sec: 80517.70288147315\n",
      "Step: 8, Training Loss: 7.92904, LR: 0.0000450, Tokens/sec: 83517.34097057937\n",
      "Step: 9, Training Loss: 7.53227, LR: 0.0000500, Tokens/sec: 84051.03498362956\n",
      "Step: 10, Training Loss: 7.28492, LR: 0.0000550, Tokens/sec: 81848.68605997278\n",
      "Step: 11, Training Loss: 7.09386, LR: 0.0000600, Tokens/sec: 82821.74033417132\n",
      "Step: 12, Training Loss: 6.91376, LR: 0.0000650, Tokens/sec: 82036.13814926284\n",
      "Step: 13, Training Loss: 6.53116, LR: 0.0000700, Tokens/sec: 83069.04845939003\n",
      "Step: 14, Training Loss: 6.30418, LR: 0.0000750, Tokens/sec: 84418.62977969527\n",
      "Step: 15, Training Loss: 6.25254, LR: 0.0000800, Tokens/sec: 81577.60167560022\n",
      "Step: 16, Training Loss: 6.06675, LR: 0.0000850, Tokens/sec: 83703.87607817283\n",
      "Step: 17, Training Loss: 5.93341, LR: 0.0000900, Tokens/sec: 84336.10674851468\n",
      "Step: 18, Training Loss: 5.73329, LR: 0.0000950, Tokens/sec: 81837.01440082984\n",
      "Step: 19, Training Loss: 5.66078, LR: 0.0001000, Tokens/sec: 83491.73738280399\n",
      "Step: 20, Training Loss: 5.69312, LR: 0.0001050, Tokens/sec: 85485.75621532621\n",
      "Step: 21, Training Loss: 5.39262, LR: 0.0001100, Tokens/sec: 82407.7728434661\n",
      "Step: 22, Training Loss: 5.48422, LR: 0.0001150, Tokens/sec: 84332.3908689992\n",
      "Step: 23, Training Loss: 4.98179, LR: 0.0001200, Tokens/sec: 84880.71136201486\n",
      "Step: 24, Training Loss: 5.43185, LR: 0.0001250, Tokens/sec: 84166.07351103923\n",
      "Step: 25, Training Loss: 4.85023, LR: 0.0001300, Tokens/sec: 83518.71226706261\n",
      "Step: 26, Training Loss: 4.60819, LR: 0.0001350, Tokens/sec: 81939.64994540722\n"
     ]
    }
   ],
   "source": [
    "trainer.train(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b5596eda083de0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:22:17.417599499Z",
     "start_time": "2024-12-16T06:27:52.251011Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "आज की चर्चा मिलता है। इसके प्रमुख प्रभावित होने के लिए प्रति प्राप्त होते हैं। इसका प्राचीन प्रकाशित होता है। इसके अतिरिक्त प्रसारण में ही प्रतिभाग्य प्राप्त होता है। इसके प्रभावित हैं। इस प्रकार के अनुसार अपने प्रतिपत्ति में ही है। विश्वास म\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer([\"आज की चर्चा\"], return_tensors=\"pt\")['input_ids'].to(trainer.device)\n",
    "idx = model.generate(input_ids, temperature=0.25, top_k=50, max_new_tokens=256)\n",
    "print(tokenizer.batch_decode(idx)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3bcc343073d67a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
