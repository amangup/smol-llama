{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:22:48.329931Z",
     "start_time": "2024-12-16T09:22:47.413234Z"
    }
   },
   "outputs": [],
   "source": [
    "from model import ModelConfig, LlamaModel\n",
    "from train import TrainerConfig, FileDataLoader, Trainer\n",
    "\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f28fa23c987e72b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:22:48.333961Z",
     "start_time": "2024-12-16T09:22:48.332382Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer_id = \"HuggingFaceTB/SmolLM-360M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bb4e51aa142abee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:22:48.533405Z",
     "start_time": "2024-12-16T09:22:48.376114Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b38fa5fd-ff52-4feb-b870-133de2909c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61db1d95-147f-4511-af23-f80fa052c441",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tokenizer([\"Aman\", \"You are a doofus\"], return_tensors='pt', padding=\"longest\", padding_side='left')['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe947da0-6eea-424b-90ec-5d83b0c99d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Aman', 'You are a doofus']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(x, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cde027092af8291e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:22:48.540534Z",
     "start_time": "2024-12-16T09:22:48.538895Z"
    }
   },
   "outputs": [],
   "source": [
    "model_config = ModelConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=960,\n",
    "    d_head=64,\n",
    "    d_mlp_proj=2560,\n",
    "    n_layers=32,\n",
    "    n_kv_heads=5,\n",
    "    n_attn_heads=15,\n",
    "    rms_norm_eps=1e-5,\n",
    "    initializer_range=0.02,\n",
    "    rope_theta=100000.0,\n",
    "    padding_idx=tokenizer.pad_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0897594b27eb59f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:22:48.582151Z",
     "start_time": "2024-12-16T09:22:48.580277Z"
    }
   },
   "outputs": [],
   "source": [
    "train_config = TrainerConfig(\n",
    "    per_device_train_batch_size=32,\n",
    "    grad_accumulation_steps=4,\n",
    "    max_seq_len=2048,\n",
    "    num_epochs=2,\n",
    "    eval_interval_steps=500,\n",
    "    learning_rate=1e-3,\n",
    "    grad_clip_norm=1.0,\n",
    "    tokens_folder=\"wiki_hindi_tok/\",\n",
    "    log_dir=\"runs/hindi_wiki\",\n",
    "    warmup_ratio=0.01,\n",
    "    val_size=0.005,\n",
    "    checkpoint_save_interval=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6504e357e2012d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:22:50.546015Z",
     "start_time": "2024-12-16T09:22:48.624998Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens                   | 270,505,984\n",
      "Shard range rank:0             | (0,131423)\n",
      "Num Trainable Params           | 409,007,040\n",
      "Train device                   | cuda, NVIDIA H200, N=4\n",
      "Training precision             | torch.bfloat16\n",
      "Flash Attention                | True\n",
      "torch.compile()                | True\n",
      "DistributedDataParallel        | False\n",
      "Batch size                     | 262,144\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LlamaModel(model_config)\n",
    "dataloader = FileDataLoader(train_config, tokenizer)\n",
    "trainer = Trainer(train_config, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c853027a7a843745",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-16T09:22:50.552519Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training steps                 | 2,054 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:674.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:674.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Training Loss: 11.00659, LR: 0.0000500, Tokens/sec: 32287.57\n",
      "Step: 1, Training Loss: 9.38478, LR: 0.0000975, Tokens/sec: 630610.14\n",
      "Step: 2, Training Loss: 8.16895, LR: 0.0001450, Tokens/sec: 624619.20\n",
      "Step: 3, Training Loss: 7.52400, LR: 0.0001925, Tokens/sec: 624507.36\n",
      "Computing Eval loss, steps: 21\n",
      "Step: 3, Eval Loss: 7.51354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:674.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 4, Training Loss: 7.45392, LR: 0.0002400, Tokens/sec: 628849.92\n",
      "Step: 5, Training Loss: 7.14768, LR: 0.0002875, Tokens/sec: 625545.73\n",
      "Step: 6, Training Loss: 6.66279, LR: 0.0003350, Tokens/sec: 626470.59\n",
      "Step: 7, Training Loss: 6.22066, LR: 0.0003825, Tokens/sec: 624861.38\n",
      "Step: 8, Training Loss: 5.75518, LR: 0.0004300, Tokens/sec: 626854.53\n",
      "Step: 9, Training Loss: 5.30798, LR: 0.0004775, Tokens/sec: 627772.67\n",
      "Step: 10, Training Loss: 4.84102, LR: 0.0005250, Tokens/sec: 623923.06\n",
      "Step: 11, Training Loss: 4.45114, LR: 0.0005725, Tokens/sec: 625755.42\n",
      "Step: 12, Training Loss: 4.16633, LR: 0.0006200, Tokens/sec: 624366.42\n",
      "Step: 13, Training Loss: 3.99765, LR: 0.0006675, Tokens/sec: 625337.98\n",
      "Step: 14, Training Loss: 3.84446, LR: 0.0007150, Tokens/sec: 625368.88\n",
      "Step: 15, Training Loss: 3.78879, LR: 0.0007625, Tokens/sec: 623409.40\n",
      "Step: 16, Training Loss: 3.74289, LR: 0.0008100, Tokens/sec: 624871.62\n",
      "Step: 17, Training Loss: 3.74924, LR: 0.0008575, Tokens/sec: 623160.72\n",
      "Step: 18, Training Loss: 3.72142, LR: 0.0009050, Tokens/sec: 622146.39\n",
      "Step: 19, Training Loss: 3.63705, LR: 0.0009525, Tokens/sec: 622134.58\n",
      "Step: 20, Training Loss: 3.51977, LR: 0.0010000, Tokens/sec: 622928.65\n",
      "Step: 21, Training Loss: 3.34742, LR: 0.0010000, Tokens/sec: 621043.78\n",
      "Step: 22, Training Loss: 3.16780, LR: 0.0010000, Tokens/sec: 619866.11\n",
      "Step: 23, Training Loss: 3.07595, LR: 0.0010000, Tokens/sec: 619954.04\n",
      "Step: 24, Training Loss: 3.18943, LR: 0.0010000, Tokens/sec: 619674.80\n",
      "Step: 25, Training Loss: 3.17113, LR: 0.0010000, Tokens/sec: 619888.92\n",
      "Step: 26, Training Loss: 3.11950, LR: 0.0010000, Tokens/sec: 619754.45\n",
      "Step: 27, Training Loss: 3.04345, LR: 0.0010000, Tokens/sec: 621449.74\n",
      "Step: 28, Training Loss: 3.08270, LR: 0.0010000, Tokens/sec: 620739.50\n",
      "Step: 29, Training Loss: 3.00264, LR: 0.0010000, Tokens/sec: 621737.30\n",
      "Step: 30, Training Loss: 2.95601, LR: 0.0009999, Tokens/sec: 621201.59\n",
      "Step: 31, Training Loss: 2.99528, LR: 0.0009999, Tokens/sec: 621568.67\n",
      "Step: 32, Training Loss: 3.07787, LR: 0.0009999, Tokens/sec: 619701.38\n",
      "Step: 33, Training Loss: 3.27965, LR: 0.0009999, Tokens/sec: 621740.63\n",
      "Step: 34, Training Loss: 3.13455, LR: 0.0009999, Tokens/sec: 622603.21\n",
      "Step: 35, Training Loss: 2.94368, LR: 0.0009999, Tokens/sec: 621038.15\n",
      "Step: 36, Training Loss: 2.91291, LR: 0.0009999, Tokens/sec: 621249.04\n",
      "Step: 37, Training Loss: 2.95120, LR: 0.0009998, Tokens/sec: 621939.80\n",
      "Step: 38, Training Loss: 2.89412, LR: 0.0009998, Tokens/sec: 620986.54\n",
      "Step: 39, Training Loss: 3.00849, LR: 0.0009998, Tokens/sec: 622976.67\n",
      "Step: 40, Training Loss: 2.92996, LR: 0.0009998, Tokens/sec: 622705.43\n",
      "Step: 41, Training Loss: 2.97413, LR: 0.0009998, Tokens/sec: 622118.34\n",
      "Step: 42, Training Loss: 2.89430, LR: 0.0009997, Tokens/sec: 620386.00\n",
      "Step: 43, Training Loss: 2.82928, LR: 0.0009997, Tokens/sec: 620765.51\n",
      "Step: 44, Training Loss: 2.84111, LR: 0.0009997, Tokens/sec: 621873.23\n",
      "Step: 45, Training Loss: 2.80170, LR: 0.0009997, Tokens/sec: 620852.22\n",
      "Step: 46, Training Loss: 2.78735, LR: 0.0009996, Tokens/sec: 621039.70\n",
      "Step: 47, Training Loss: 2.82023, LR: 0.0009996, Tokens/sec: 621768.33\n",
      "Step: 48, Training Loss: 2.80662, LR: 0.0009996, Tokens/sec: 621568.09\n",
      "Step: 49, Training Loss: 2.91735, LR: 0.0009995, Tokens/sec: 621394.29\n",
      "Step: 50, Training Loss: 2.86722, LR: 0.0009995, Tokens/sec: 623206.66\n",
      "Step: 51, Training Loss: 2.83748, LR: 0.0009995, Tokens/sec: 620885.45\n",
      "Step: 52, Training Loss: 2.81532, LR: 0.0009995, Tokens/sec: 621842.77\n",
      "Step: 53, Training Loss: 2.79322, LR: 0.0009994, Tokens/sec: 621370.30\n",
      "Step: 54, Training Loss: 2.77307, LR: 0.0009994, Tokens/sec: 621170.06\n",
      "Step: 55, Training Loss: 2.83475, LR: 0.0009993, Tokens/sec: 620660.53\n",
      "Step: 56, Training Loss: 2.88888, LR: 0.0009993, Tokens/sec: 621225.14\n",
      "Step: 57, Training Loss: 2.76162, LR: 0.0009993, Tokens/sec: 621890.21\n",
      "Step: 58, Training Loss: 2.78256, LR: 0.0009992, Tokens/sec: 621047.99\n",
      "Step: 59, Training Loss: 2.92796, LR: 0.0009992, Tokens/sec: 622339.86\n",
      "Step: 60, Training Loss: 2.75493, LR: 0.0009991, Tokens/sec: 619326.30\n",
      "Step: 61, Training Loss: 2.71047, LR: 0.0009991, Tokens/sec: 618787.73\n",
      "Step: 62, Training Loss: 2.71839, LR: 0.0009991, Tokens/sec: 620356.27\n",
      "Step: 63, Training Loss: 2.80829, LR: 0.0009990, Tokens/sec: 621302.67\n",
      "Step: 64, Training Loss: 2.67558, LR: 0.0009990, Tokens/sec: 620576.82\n",
      "Step: 65, Training Loss: 2.74260, LR: 0.0009989, Tokens/sec: 620290.08\n",
      "Step: 66, Training Loss: 2.69818, LR: 0.0009989, Tokens/sec: 620626.85\n",
      "Step: 67, Training Loss: 2.62654, LR: 0.0009988, Tokens/sec: 620654.14\n",
      "Step: 68, Training Loss: 2.68005, LR: 0.0009988, Tokens/sec: 621493.28\n",
      "Step: 69, Training Loss: 2.62776, LR: 0.0009987, Tokens/sec: 620536.20\n",
      "Step: 70, Training Loss: 2.61420, LR: 0.0009987, Tokens/sec: 621105.94\n",
      "Step: 71, Training Loss: 2.62674, LR: 0.0009986, Tokens/sec: 620621.08\n",
      "Step: 72, Training Loss: 2.70077, LR: 0.0009985, Tokens/sec: 620139.44\n",
      "Step: 73, Training Loss: 2.62299, LR: 0.0009985, Tokens/sec: 621396.37\n",
      "Step: 74, Training Loss: 2.64133, LR: 0.0009984, Tokens/sec: 620341.69\n",
      "Step: 75, Training Loss: 2.61273, LR: 0.0009984, Tokens/sec: 621488.12\n",
      "Step: 76, Training Loss: 2.65084, LR: 0.0009983, Tokens/sec: 620889.07\n",
      "Step: 77, Training Loss: 2.67811, LR: 0.0009983, Tokens/sec: 621326.25\n",
      "Step: 78, Training Loss: 2.64334, LR: 0.0009982, Tokens/sec: 622190.36\n",
      "Step: 79, Training Loss: 2.64728, LR: 0.0009981, Tokens/sec: 621600.18\n",
      "Step: 80, Training Loss: 2.64052, LR: 0.0009981, Tokens/sec: 620998.03\n",
      "Step: 81, Training Loss: 2.58258, LR: 0.0009980, Tokens/sec: 622518.83\n",
      "Step: 82, Training Loss: 2.59274, LR: 0.0009979, Tokens/sec: 620431.59\n",
      "Step: 83, Training Loss: 2.58145, LR: 0.0009979, Tokens/sec: 619931.05\n",
      "Step: 84, Training Loss: 2.62200, LR: 0.0009978, Tokens/sec: 619717.66\n",
      "Step: 85, Training Loss: 2.62300, LR: 0.0009977, Tokens/sec: 619165.28\n",
      "Step: 86, Training Loss: 2.61610, LR: 0.0009977, Tokens/sec: 621396.23\n",
      "Step: 87, Training Loss: 2.62842, LR: 0.0009976, Tokens/sec: 621015.40\n",
      "Step: 88, Training Loss: 2.64671, LR: 0.0009975, Tokens/sec: 621130.53\n",
      "Step: 89, Training Loss: 2.58859, LR: 0.0009974, Tokens/sec: 620927.09\n",
      "Step: 90, Training Loss: 2.57990, LR: 0.0009974, Tokens/sec: 621010.71\n",
      "Step: 91, Training Loss: 2.57977, LR: 0.0009973, Tokens/sec: 620233.99\n",
      "Step: 92, Training Loss: 2.70013, LR: 0.0009972, Tokens/sec: 620649.27\n",
      "Step: 93, Training Loss: 2.63478, LR: 0.0009971, Tokens/sec: 620681.49\n",
      "Step: 94, Training Loss: 2.63403, LR: 0.0009971, Tokens/sec: 620669.96\n",
      "Step: 95, Training Loss: 2.53452, LR: 0.0009970, Tokens/sec: 620396.62\n",
      "Step: 96, Training Loss: 2.60787, LR: 0.0009969, Tokens/sec: 621431.49\n",
      "Step: 97, Training Loss: 2.70233, LR: 0.0009968, Tokens/sec: 618945.40\n",
      "Step: 98, Training Loss: 2.63310, LR: 0.0009967, Tokens/sec: 619088.20\n",
      "Step: 99, Training Loss: 2.60672, LR: 0.0009967, Tokens/sec: 620573.23\n",
      "Step: 100, Training Loss: 2.60230, LR: 0.0009966, Tokens/sec: 620702.40\n",
      "Step: 101, Training Loss: 2.56495, LR: 0.0009965, Tokens/sec: 621152.49\n",
      "Step: 102, Training Loss: 2.55428, LR: 0.0009964, Tokens/sec: 621524.90\n",
      "Step: 103, Training Loss: 2.55799, LR: 0.0009963, Tokens/sec: 620994.30\n",
      "Step: 104, Training Loss: 2.53253, LR: 0.0009962, Tokens/sec: 621202.43\n",
      "Step: 105, Training Loss: 2.53581, LR: 0.0009961, Tokens/sec: 620545.61\n",
      "Step: 106, Training Loss: 2.51533, LR: 0.0009960, Tokens/sec: 621660.20\n",
      "Step: 107, Training Loss: 2.56205, LR: 0.0009959, Tokens/sec: 621209.22\n",
      "Step: 108, Training Loss: 2.47845, LR: 0.0009958, Tokens/sec: 620655.01\n",
      "Step: 109, Training Loss: 2.45626, LR: 0.0009958, Tokens/sec: 621497.07\n",
      "Step: 110, Training Loss: 2.46686, LR: 0.0009957, Tokens/sec: 621372.47\n",
      "Step: 111, Training Loss: 2.49604, LR: 0.0009956, Tokens/sec: 621426.55\n",
      "Step: 112, Training Loss: 2.47351, LR: 0.0009955, Tokens/sec: 622240.94\n",
      "Step: 113, Training Loss: 2.46810, LR: 0.0009954, Tokens/sec: 620461.30\n",
      "Step: 114, Training Loss: 2.43167, LR: 0.0009953, Tokens/sec: 621566.23\n",
      "Step: 115, Training Loss: 2.43021, LR: 0.0009952, Tokens/sec: 621585.98\n",
      "Step: 116, Training Loss: 2.39423, LR: 0.0009951, Tokens/sec: 621427.99\n",
      "Step: 117, Training Loss: 2.42551, LR: 0.0009950, Tokens/sec: 622764.26\n",
      "Step: 118, Training Loss: 2.48852, LR: 0.0009949, Tokens/sec: 620826.43\n",
      "Step: 119, Training Loss: 2.46607, LR: 0.0009947, Tokens/sec: 621632.11\n",
      "Step: 120, Training Loss: 2.45080, LR: 0.0009946, Tokens/sec: 621827.08\n",
      "Step: 121, Training Loss: 2.46581, LR: 0.0009945, Tokens/sec: 621085.83\n",
      "Step: 122, Training Loss: 2.39205, LR: 0.0009944, Tokens/sec: 622133.10\n",
      "Step: 123, Training Loss: 2.46082, LR: 0.0009943, Tokens/sec: 621020.99\n",
      "Step: 124, Training Loss: 2.43686, LR: 0.0009942, Tokens/sec: 619870.68\n",
      "Step: 125, Training Loss: 2.35393, LR: 0.0009941, Tokens/sec: 621641.69\n",
      "Step: 126, Training Loss: 2.42632, LR: 0.0009940, Tokens/sec: 620315.31\n",
      "Step: 127, Training Loss: 2.79748, LR: 0.0009939, Tokens/sec: 618393.75\n",
      "Step: 128, Training Loss: 2.49339, LR: 0.0009938, Tokens/sec: 620889.73\n",
      "Step: 129, Training Loss: 2.44884, LR: 0.0009936, Tokens/sec: 620071.57\n",
      "Step: 130, Training Loss: 2.40266, LR: 0.0009935, Tokens/sec: 619210.44\n",
      "Step: 131, Training Loss: 2.38343, LR: 0.0009934, Tokens/sec: 622965.56\n",
      "Step: 132, Training Loss: 2.37374, LR: 0.0009933, Tokens/sec: 620564.62\n",
      "Step: 133, Training Loss: 2.54952, LR: 0.0009932, Tokens/sec: 620716.36\n",
      "Step: 134, Training Loss: 2.38083, LR: 0.0009930, Tokens/sec: 621106.58\n",
      "Step: 135, Training Loss: 2.63675, LR: 0.0009929, Tokens/sec: 620696.25\n",
      "Step: 136, Training Loss: 2.47443, LR: 0.0009928, Tokens/sec: 620788.10\n",
      "Step: 137, Training Loss: 2.38871, LR: 0.0009927, Tokens/sec: 621217.03\n",
      "Step: 138, Training Loss: 2.29538, LR: 0.0009925, Tokens/sec: 619585.60\n",
      "Step: 139, Training Loss: 2.43848, LR: 0.0009924, Tokens/sec: 620072.26\n",
      "Step: 140, Training Loss: 2.29596, LR: 0.0009923, Tokens/sec: 621091.19\n",
      "Step: 141, Training Loss: 2.49421, LR: 0.0009922, Tokens/sec: 621679.31\n",
      "Step: 142, Training Loss: 2.29142, LR: 0.0009920, Tokens/sec: 619459.34\n",
      "Step: 143, Training Loss: 2.26570, LR: 0.0009919, Tokens/sec: 619836.51\n",
      "Step: 144, Training Loss: 2.24205, LR: 0.0009918, Tokens/sec: 619845.63\n",
      "Step: 145, Training Loss: 2.32763, LR: 0.0009916, Tokens/sec: 619611.63\n",
      "Step: 146, Training Loss: 2.26433, LR: 0.0009915, Tokens/sec: 618755.84\n",
      "Step: 147, Training Loss: 2.50167, LR: 0.0009914, Tokens/sec: 618905.38\n",
      "Step: 148, Training Loss: 2.37648, LR: 0.0009912, Tokens/sec: 620917.89\n",
      "Step: 149, Training Loss: 2.43041, LR: 0.0009911, Tokens/sec: 618641.80\n",
      "Step: 150, Training Loss: 2.25297, LR: 0.0009910, Tokens/sec: 621008.05\n",
      "Step: 151, Training Loss: 2.23521, LR: 0.0009908, Tokens/sec: 619822.58\n",
      "Step: 152, Training Loss: 2.17254, LR: 0.0009907, Tokens/sec: 619437.91\n",
      "Step: 153, Training Loss: 2.26627, LR: 0.0009905, Tokens/sec: 620470.20\n",
      "Step: 154, Training Loss: 2.25150, LR: 0.0009904, Tokens/sec: 618880.58\n",
      "Step: 155, Training Loss: 2.04167, LR: 0.0009903, Tokens/sec: 620229.67\n",
      "Step: 156, Training Loss: 2.11375, LR: 0.0009901, Tokens/sec: 619502.83\n",
      "Step: 157, Training Loss: 2.10291, LR: 0.0009900, Tokens/sec: 619036.07\n",
      "Step: 158, Training Loss: 2.19086, LR: 0.0009898, Tokens/sec: 619520.93\n",
      "Step: 159, Training Loss: 2.03224, LR: 0.0009897, Tokens/sec: 621583.47\n",
      "Step: 160, Training Loss: 2.19280, LR: 0.0009895, Tokens/sec: 621073.93\n",
      "Step: 161, Training Loss: 2.32846, LR: 0.0009894, Tokens/sec: 620757.73\n",
      "Step: 162, Training Loss: 2.29791, LR: 0.0009892, Tokens/sec: 619979.40\n",
      "Step: 163, Training Loss: 2.27493, LR: 0.0009891, Tokens/sec: 621162.54\n",
      "Step: 164, Training Loss: 2.23718, LR: 0.0009889, Tokens/sec: 618903.40\n",
      "Step: 165, Training Loss: 2.37507, LR: 0.0009888, Tokens/sec: 619535.14\n",
      "Step: 166, Training Loss: 2.21018, LR: 0.0009886, Tokens/sec: 620275.00\n",
      "Step: 167, Training Loss: 2.25456, LR: 0.0009885, Tokens/sec: 620001.78\n",
      "Step: 168, Training Loss: 2.21139, LR: 0.0009883, Tokens/sec: 621223.97\n",
      "Step: 169, Training Loss: 2.20491, LR: 0.0009881, Tokens/sec: 619026.94\n",
      "Step: 170, Training Loss: 2.25604, LR: 0.0009880, Tokens/sec: 620505.02\n",
      "Step: 171, Training Loss: 2.36479, LR: 0.0009878, Tokens/sec: 620340.81\n",
      "Step: 172, Training Loss: 2.12457, LR: 0.0009877, Tokens/sec: 620401.67\n",
      "Step: 173, Training Loss: 2.17302, LR: 0.0009875, Tokens/sec: 621040.15\n",
      "Step: 174, Training Loss: 2.27116, LR: 0.0009873, Tokens/sec: 619486.70\n",
      "Step: 175, Training Loss: 2.16541, LR: 0.0009872, Tokens/sec: 619747.44\n",
      "Step: 176, Training Loss: 2.20742, LR: 0.0009870, Tokens/sec: 621001.32\n",
      "Step: 177, Training Loss: 2.18590, LR: 0.0009868, Tokens/sec: 619302.44\n",
      "Step: 178, Training Loss: 2.14460, LR: 0.0009867, Tokens/sec: 620173.97\n",
      "Step: 179, Training Loss: 2.23150, LR: 0.0009865, Tokens/sec: 619154.23\n",
      "Step: 180, Training Loss: 2.06051, LR: 0.0009863, Tokens/sec: 620491.34\n",
      "Step: 181, Training Loss: 2.06597, LR: 0.0009862, Tokens/sec: 619312.18\n",
      "Step: 182, Training Loss: 2.10760, LR: 0.0009860, Tokens/sec: 620563.96\n",
      "Step: 183, Training Loss: 2.08959, LR: 0.0009858, Tokens/sec: 619760.53\n",
      "Step: 184, Training Loss: 2.12086, LR: 0.0009856, Tokens/sec: 620593.09\n",
      "Step: 185, Training Loss: 2.11841, LR: 0.0009855, Tokens/sec: 619963.75\n",
      "Step: 186, Training Loss: 2.09483, LR: 0.0009853, Tokens/sec: 620720.51\n",
      "Step: 187, Training Loss: 2.09442, LR: 0.0009851, Tokens/sec: 619851.85\n",
      "Step: 188, Training Loss: 2.13177, LR: 0.0009849, Tokens/sec: 621588.33\n",
      "Step: 189, Training Loss: 2.02219, LR: 0.0009848, Tokens/sec: 620039.63\n",
      "Step: 190, Training Loss: 2.20843, LR: 0.0009846, Tokens/sec: 620520.72\n",
      "Step: 191, Training Loss: 2.14017, LR: 0.0009844, Tokens/sec: 619893.39\n",
      "Step: 192, Training Loss: 2.09662, LR: 0.0009842, Tokens/sec: 620575.33\n",
      "Step: 193, Training Loss: 2.07056, LR: 0.0009840, Tokens/sec: 619584.23\n",
      "Step: 194, Training Loss: 2.69810, LR: 0.0009838, Tokens/sec: 619409.05\n",
      "Step: 195, Training Loss: 2.10169, LR: 0.0009837, Tokens/sec: 620210.16\n",
      "Step: 196, Training Loss: 2.17753, LR: 0.0009835, Tokens/sec: 622222.26\n",
      "Step: 197, Training Loss: 2.23630, LR: 0.0009833, Tokens/sec: 619645.92\n",
      "Step: 198, Training Loss: 2.06165, LR: 0.0009831, Tokens/sec: 620036.58\n",
      "Step: 199, Training Loss: 2.08360, LR: 0.0009829, Tokens/sec: 620331.80\n",
      "Step: 200, Training Loss: 2.07598, LR: 0.0009827, Tokens/sec: 619870.70\n",
      "Step: 201, Training Loss: 2.13431, LR: 0.0009825, Tokens/sec: 619876.63\n",
      "Step: 202, Training Loss: 2.05484, LR: 0.0009823, Tokens/sec: 620139.29\n",
      "Step: 203, Training Loss: 2.35687, LR: 0.0009821, Tokens/sec: 621068.90\n",
      "Step: 204, Training Loss: 2.22470, LR: 0.0009819, Tokens/sec: 618557.58\n",
      "Step: 205, Training Loss: 2.21814, LR: 0.0009818, Tokens/sec: 620627.79\n",
      "Step: 206, Training Loss: 2.12673, LR: 0.0009816, Tokens/sec: 621062.69\n",
      "Step: 207, Training Loss: 2.14165, LR: 0.0009814, Tokens/sec: 619282.81\n",
      "Step: 208, Training Loss: 2.36362, LR: 0.0009812, Tokens/sec: 619781.57\n",
      "Step: 209, Training Loss: 2.12753, LR: 0.0009810, Tokens/sec: 620358.06\n",
      "Step: 210, Training Loss: 2.14132, LR: 0.0009808, Tokens/sec: 620324.99\n",
      "Step: 211, Training Loss: 2.32222, LR: 0.0009806, Tokens/sec: 619183.48\n",
      "Step: 212, Training Loss: 2.23668, LR: 0.0009804, Tokens/sec: 618263.51\n",
      "Step: 213, Training Loss: 2.16900, LR: 0.0009802, Tokens/sec: 619730.50\n",
      "Step: 214, Training Loss: 2.06372, LR: 0.0009799, Tokens/sec: 620641.87\n",
      "Step: 215, Training Loss: 1.98356, LR: 0.0009797, Tokens/sec: 619995.68\n",
      "Step: 216, Training Loss: 2.02601, LR: 0.0009795, Tokens/sec: 620725.11\n",
      "Step: 217, Training Loss: 2.15241, LR: 0.0009793, Tokens/sec: 619642.81\n",
      "Step: 218, Training Loss: 2.06725, LR: 0.0009791, Tokens/sec: 617952.62\n",
      "Step: 219, Training Loss: 2.14306, LR: 0.0009789, Tokens/sec: 618466.55\n",
      "Step: 220, Training Loss: 1.99847, LR: 0.0009787, Tokens/sec: 620515.44\n",
      "Step: 221, Training Loss: 2.00176, LR: 0.0009785, Tokens/sec: 619749.68\n",
      "Step: 222, Training Loss: 2.13411, LR: 0.0009783, Tokens/sec: 620387.16\n",
      "Step: 223, Training Loss: 2.04305, LR: 0.0009781, Tokens/sec: 620338.04\n",
      "Step: 224, Training Loss: 2.18114, LR: 0.0009778, Tokens/sec: 618738.69\n",
      "Step: 225, Training Loss: 5.57159, LR: 0.0009776, Tokens/sec: 620994.04\n",
      "Step: 226, Training Loss: 2.10014, LR: 0.0009774, Tokens/sec: 619612.57\n",
      "Step: 227, Training Loss: 2.06986, LR: 0.0009772, Tokens/sec: 619770.29\n",
      "Step: 228, Training Loss: 2.02262, LR: 0.0009770, Tokens/sec: 619209.66\n",
      "Step: 229, Training Loss: 1.98680, LR: 0.0009768, Tokens/sec: 618544.45\n",
      "Step: 230, Training Loss: 2.02221, LR: 0.0009765, Tokens/sec: 618760.32\n",
      "Step: 231, Training Loss: 2.00338, LR: 0.0009763, Tokens/sec: 619276.59\n",
      "Step: 232, Training Loss: 2.13383, LR: 0.0009761, Tokens/sec: 619271.89\n",
      "Step: 233, Training Loss: 2.02981, LR: 0.0009759, Tokens/sec: 618938.10\n",
      "Step: 234, Training Loss: 1.86996, LR: 0.0009756, Tokens/sec: 619997.73\n",
      "Step: 235, Training Loss: 1.81260, LR: 0.0009754, Tokens/sec: 619102.35\n",
      "Step: 236, Training Loss: 1.89398, LR: 0.0009752, Tokens/sec: 617390.31\n",
      "Step: 237, Training Loss: 1.78801, LR: 0.0009750, Tokens/sec: 619336.92\n",
      "Step: 238, Training Loss: 2.19302, LR: 0.0009747, Tokens/sec: 619259.98\n",
      "Step: 239, Training Loss: 2.16104, LR: 0.0009745, Tokens/sec: 619745.32\n",
      "Step: 240, Training Loss: 2.09968, LR: 0.0009743, Tokens/sec: 619367.70\n",
      "Step: 241, Training Loss: 2.11671, LR: 0.0009740, Tokens/sec: 621264.45\n",
      "Step: 242, Training Loss: 2.10294, LR: 0.0009738, Tokens/sec: 619941.70\n",
      "Step: 243, Training Loss: 2.09287, LR: 0.0009736, Tokens/sec: 618961.13\n",
      "Step: 244, Training Loss: 2.08503, LR: 0.0009733, Tokens/sec: 619645.60\n",
      "Step: 245, Training Loss: 2.09068, LR: 0.0009731, Tokens/sec: 619141.37\n",
      "Step: 246, Training Loss: 2.04806, LR: 0.0009729, Tokens/sec: 620028.33\n",
      "Step: 247, Training Loss: 1.93469, LR: 0.0009726, Tokens/sec: 619719.37\n",
      "Step: 248, Training Loss: 2.16605, LR: 0.0009724, Tokens/sec: 618015.82\n",
      "Step: 249, Training Loss: 1.94477, LR: 0.0009721, Tokens/sec: 620058.80\n",
      "Step: 250, Training Loss: 1.98091, LR: 0.0009719, Tokens/sec: 620425.27\n",
      "Step: 251, Training Loss: 1.95067, LR: 0.0009717, Tokens/sec: 617880.66\n",
      "Step: 252, Training Loss: 1.99325, LR: 0.0009714, Tokens/sec: 620840.46\n",
      "Step: 253, Training Loss: 2.02870, LR: 0.0009712, Tokens/sec: 619725.77\n",
      "Step: 254, Training Loss: 1.95223, LR: 0.0009709, Tokens/sec: 619103.34\n",
      "Step: 255, Training Loss: 2.05523, LR: 0.0009707, Tokens/sec: 619181.60\n",
      "Step: 256, Training Loss: 1.93665, LR: 0.0009704, Tokens/sec: 619517.19\n",
      "Step: 257, Training Loss: 2.03862, LR: 0.0009702, Tokens/sec: 619684.82\n",
      "Step: 258, Training Loss: 1.88961, LR: 0.0009699, Tokens/sec: 619242.86\n",
      "Step: 259, Training Loss: 1.83685, LR: 0.0009697, Tokens/sec: 619554.85\n",
      "Step: 260, Training Loss: 2.02971, LR: 0.0009694, Tokens/sec: 619084.42\n",
      "Step: 261, Training Loss: 2.00766, LR: 0.0009692, Tokens/sec: 619500.00\n",
      "Step: 262, Training Loss: 2.08514, LR: 0.0009689, Tokens/sec: 620866.96\n",
      "Step: 263, Training Loss: 1.99690, LR: 0.0009687, Tokens/sec: 619175.23\n",
      "Step: 264, Training Loss: 1.93331, LR: 0.0009684, Tokens/sec: 619963.32\n",
      "Step: 265, Training Loss: 1.74824, LR: 0.0009682, Tokens/sec: 619579.44\n",
      "Step: 266, Training Loss: 1.60367, LR: 0.0009679, Tokens/sec: 618858.71\n",
      "Step: 267, Training Loss: 1.62794, LR: 0.0009676, Tokens/sec: 618707.50\n",
      "Step: 268, Training Loss: 1.24378, LR: 0.0009674, Tokens/sec: 619554.68\n",
      "Step: 269, Training Loss: 0.85486, LR: 0.0009671, Tokens/sec: 619497.98\n",
      "Step: 270, Training Loss: 1.30246, LR: 0.0009669, Tokens/sec: 620118.96\n",
      "Step: 271, Training Loss: 1.25192, LR: 0.0009666, Tokens/sec: 619795.35\n",
      "Step: 272, Training Loss: 1.87256, LR: 0.0009663, Tokens/sec: 620852.40\n",
      "Step: 273, Training Loss: 2.16195, LR: 0.0009661, Tokens/sec: 620523.51\n",
      "Step: 274, Training Loss: 2.13727, LR: 0.0009658, Tokens/sec: 619946.21\n",
      "Step: 275, Training Loss: 2.10268, LR: 0.0009655, Tokens/sec: 619911.24\n",
      "Step: 276, Training Loss: 2.20300, LR: 0.0009653, Tokens/sec: 619413.40\n",
      "Step: 277, Training Loss: 2.08679, LR: 0.0009650, Tokens/sec: 619445.37\n",
      "Step: 278, Training Loss: 2.03479, LR: 0.0009647, Tokens/sec: 620164.54\n",
      "Step: 279, Training Loss: 2.08011, LR: 0.0009645, Tokens/sec: 619620.64\n",
      "Step: 280, Training Loss: 2.09747, LR: 0.0009642, Tokens/sec: 619592.25\n",
      "Step: 281, Training Loss: 2.13442, LR: 0.0009639, Tokens/sec: 620496.88\n",
      "Step: 282, Training Loss: 2.08610, LR: 0.0009637, Tokens/sec: 620040.33\n",
      "Step: 283, Training Loss: 2.15572, LR: 0.0009634, Tokens/sec: 621727.01\n",
      "Step: 284, Training Loss: 2.03926, LR: 0.0009631, Tokens/sec: 621071.97\n",
      "Step: 285, Training Loss: 2.04498, LR: 0.0009628, Tokens/sec: 620475.18\n",
      "Step: 286, Training Loss: 2.01850, LR: 0.0009626, Tokens/sec: 620262.73\n",
      "Step: 287, Training Loss: 2.14982, LR: 0.0009623, Tokens/sec: 620676.80\n",
      "Step: 288, Training Loss: 2.02482, LR: 0.0009620, Tokens/sec: 619395.05\n",
      "Step: 289, Training Loss: 1.96683, LR: 0.0009617, Tokens/sec: 619585.78\n",
      "Step: 290, Training Loss: 2.07159, LR: 0.0009614, Tokens/sec: 618783.93\n",
      "Step: 291, Training Loss: 2.00248, LR: 0.0009612, Tokens/sec: 620003.98\n",
      "Step: 292, Training Loss: 2.01326, LR: 0.0009609, Tokens/sec: 619532.17\n",
      "Step: 293, Training Loss: 1.94874, LR: 0.0009606, Tokens/sec: 618998.37\n",
      "Step: 294, Training Loss: 1.97922, LR: 0.0009603, Tokens/sec: 620141.99\n",
      "Step: 295, Training Loss: 1.98491, LR: 0.0009600, Tokens/sec: 620030.85\n",
      "Step: 296, Training Loss: 1.93936, LR: 0.0009597, Tokens/sec: 619311.15\n",
      "Step: 297, Training Loss: 1.93869, LR: 0.0009594, Tokens/sec: 620373.31\n",
      "Step: 298, Training Loss: 1.97995, LR: 0.0009592, Tokens/sec: 619025.29\n",
      "Step: 299, Training Loss: 1.91818, LR: 0.0009589, Tokens/sec: 620249.63\n",
      "Step: 300, Training Loss: 1.94215, LR: 0.0009586, Tokens/sec: 619787.72\n",
      "Step: 301, Training Loss: 1.96357, LR: 0.0009583, Tokens/sec: 619536.12\n",
      "Step: 302, Training Loss: 1.86720, LR: 0.0009580, Tokens/sec: 620692.83\n",
      "Step: 303, Training Loss: 1.91881, LR: 0.0009577, Tokens/sec: 621436.32\n",
      "Step: 304, Training Loss: 1.91419, LR: 0.0009574, Tokens/sec: 619944.92\n",
      "Step: 305, Training Loss: 1.89108, LR: 0.0009571, Tokens/sec: 620778.55\n",
      "Step: 306, Training Loss: 1.94528, LR: 0.0009568, Tokens/sec: 619315.40\n",
      "Step: 307, Training Loss: 2.13522, LR: 0.0009565, Tokens/sec: 620447.14\n",
      "Step: 308, Training Loss: 2.15828, LR: 0.0009562, Tokens/sec: 619328.54\n",
      "Step: 309, Training Loss: 1.96531, LR: 0.0009559, Tokens/sec: 619729.15\n",
      "Step: 310, Training Loss: 1.92233, LR: 0.0009556, Tokens/sec: 622027.04\n",
      "Step: 311, Training Loss: 1.93530, LR: 0.0009553, Tokens/sec: 621435.43\n",
      "Step: 312, Training Loss: 1.92258, LR: 0.0009550, Tokens/sec: 618264.32\n",
      "Step: 313, Training Loss: 2.11094, LR: 0.0009547, Tokens/sec: 619156.93\n",
      "Step: 314, Training Loss: 2.05305, LR: 0.0009544, Tokens/sec: 620163.92\n",
      "Step: 315, Training Loss: 1.85844, LR: 0.0009541, Tokens/sec: 620212.52\n",
      "Step: 316, Training Loss: 1.90651, LR: 0.0009538, Tokens/sec: 619741.89\n",
      "Step: 317, Training Loss: 1.87735, LR: 0.0009535, Tokens/sec: 619983.03\n",
      "Step: 318, Training Loss: 2.20949, LR: 0.0009532, Tokens/sec: 619934.97\n",
      "Step: 319, Training Loss: 1.87564, LR: 0.0009529, Tokens/sec: 619777.53\n",
      "Step: 320, Training Loss: 1.95905, LR: 0.0009525, Tokens/sec: 618174.05\n",
      "Step: 321, Training Loss: 1.85407, LR: 0.0009522, Tokens/sec: 619328.72\n",
      "Step: 322, Training Loss: 1.87597, LR: 0.0009519, Tokens/sec: 619434.53\n",
      "Step: 323, Training Loss: 1.86442, LR: 0.0009516, Tokens/sec: 620220.70\n",
      "Step: 324, Training Loss: 1.84174, LR: 0.0009513, Tokens/sec: 620548.49\n",
      "Step: 325, Training Loss: 1.86375, LR: 0.0009510, Tokens/sec: 618960.09\n",
      "Step: 326, Training Loss: 1.78957, LR: 0.0009507, Tokens/sec: 618627.59\n",
      "Step: 327, Training Loss: 1.85642, LR: 0.0009504, Tokens/sec: 619658.96\n",
      "Step: 328, Training Loss: 1.82017, LR: 0.0009500, Tokens/sec: 620309.77\n",
      "Step: 329, Training Loss: 1.78287, LR: 0.0009497, Tokens/sec: 619226.95\n",
      "Step: 330, Training Loss: 1.94035, LR: 0.0009494, Tokens/sec: 619130.54\n",
      "Step: 331, Training Loss: 1.87651, LR: 0.0009491, Tokens/sec: 619569.81\n",
      "Step: 332, Training Loss: 1.89913, LR: 0.0009488, Tokens/sec: 619194.42\n",
      "Step: 333, Training Loss: 1.86321, LR: 0.0009484, Tokens/sec: 619669.12\n",
      "Step: 334, Training Loss: 1.81264, LR: 0.0009481, Tokens/sec: 618715.65\n",
      "Step: 335, Training Loss: 1.89380, LR: 0.0009478, Tokens/sec: 619412.14\n",
      "Step: 336, Training Loss: 1.88003, LR: 0.0009475, Tokens/sec: 619206.44\n",
      "Step: 337, Training Loss: 1.87676, LR: 0.0009471, Tokens/sec: 619547.95\n",
      "Step: 338, Training Loss: 1.81448, LR: 0.0009468, Tokens/sec: 620017.67\n",
      "Step: 339, Training Loss: 1.82089, LR: 0.0009465, Tokens/sec: 618992.19\n",
      "Step: 340, Training Loss: 1.78672, LR: 0.0009461, Tokens/sec: 619559.68\n",
      "Step: 341, Training Loss: 1.88151, LR: 0.0009458, Tokens/sec: 619430.30\n",
      "Step: 342, Training Loss: 1.83941, LR: 0.0009455, Tokens/sec: 619719.53\n",
      "Step: 343, Training Loss: 1.80520, LR: 0.0009452, Tokens/sec: 619889.69\n",
      "Step: 344, Training Loss: 1.79603, LR: 0.0009448, Tokens/sec: 619526.06\n",
      "Step: 345, Training Loss: 1.82371, LR: 0.0009445, Tokens/sec: 619631.94\n",
      "Step: 346, Training Loss: 1.84272, LR: 0.0009442, Tokens/sec: 619548.80\n",
      "Step: 347, Training Loss: 1.80269, LR: 0.0009438, Tokens/sec: 619948.07\n",
      "Step: 348, Training Loss: 1.85321, LR: 0.0009435, Tokens/sec: 618937.20\n",
      "Step: 349, Training Loss: 1.71816, LR: 0.0009431, Tokens/sec: 620397.57\n",
      "Step: 350, Training Loss: 1.77807, LR: 0.0009428, Tokens/sec: 618556.33\n",
      "Step: 351, Training Loss: 1.75270, LR: 0.0009425, Tokens/sec: 618764.14\n",
      "Step: 352, Training Loss: 1.82281, LR: 0.0009421, Tokens/sec: 619274.08\n",
      "Step: 353, Training Loss: 1.81985, LR: 0.0009418, Tokens/sec: 619512.80\n",
      "Step: 354, Training Loss: 1.83025, LR: 0.0009414, Tokens/sec: 621172.27\n",
      "Step: 355, Training Loss: 1.70628, LR: 0.0009411, Tokens/sec: 619185.22\n",
      "Step: 356, Training Loss: 1.77207, LR: 0.0009407, Tokens/sec: 619420.50\n",
      "Step: 357, Training Loss: 1.74444, LR: 0.0009404, Tokens/sec: 619459.35\n",
      "Step: 358, Training Loss: 1.79642, LR: 0.0009401, Tokens/sec: 619031.97\n",
      "Step: 359, Training Loss: 1.74813, LR: 0.0009397, Tokens/sec: 618282.43\n",
      "Step: 360, Training Loss: 1.75018, LR: 0.0009394, Tokens/sec: 619526.09\n",
      "Step: 361, Training Loss: 1.72480, LR: 0.0009390, Tokens/sec: 619863.86\n",
      "Step: 362, Training Loss: 1.71595, LR: 0.0009387, Tokens/sec: 618027.51\n",
      "Step: 363, Training Loss: 1.70490, LR: 0.0009383, Tokens/sec: 617306.17\n",
      "Step: 364, Training Loss: 1.69266, LR: 0.0009380, Tokens/sec: 620075.39\n",
      "Step: 365, Training Loss: 1.76056, LR: 0.0009376, Tokens/sec: 619580.13\n",
      "Step: 366, Training Loss: 1.72737, LR: 0.0009373, Tokens/sec: 620200.04\n",
      "Step: 367, Training Loss: 1.72523, LR: 0.0009369, Tokens/sec: 619034.44\n",
      "Step: 368, Training Loss: 1.66338, LR: 0.0009365, Tokens/sec: 620067.38\n",
      "Step: 369, Training Loss: 1.70363, LR: 0.0009362, Tokens/sec: 619024.13\n",
      "Step: 370, Training Loss: 1.74627, LR: 0.0009358, Tokens/sec: 620787.51\n",
      "Step: 371, Training Loss: 1.70716, LR: 0.0009355, Tokens/sec: 619299.01\n",
      "Step: 372, Training Loss: 1.83153, LR: 0.0009351, Tokens/sec: 618299.37\n",
      "Step: 373, Training Loss: 1.75014, LR: 0.0009348, Tokens/sec: 619330.80\n",
      "Step: 374, Training Loss: 1.72526, LR: 0.0009344, Tokens/sec: 618867.05\n",
      "Step: 375, Training Loss: 1.70080, LR: 0.0009340, Tokens/sec: 620599.14\n",
      "Step: 376, Training Loss: 1.69347, LR: 0.0009337, Tokens/sec: 618171.64\n",
      "Step: 377, Training Loss: 1.71790, LR: 0.0009333, Tokens/sec: 619700.86\n",
      "Step: 378, Training Loss: 1.70626, LR: 0.0009329, Tokens/sec: 619074.31\n",
      "Step: 379, Training Loss: 1.72509, LR: 0.0009326, Tokens/sec: 618446.82\n",
      "Step: 380, Training Loss: 1.70370, LR: 0.0009322, Tokens/sec: 619343.49\n",
      "Step: 381, Training Loss: 1.70101, LR: 0.0009318, Tokens/sec: 620047.15\n",
      "Step: 382, Training Loss: 1.83603, LR: 0.0009315, Tokens/sec: 619027.73\n",
      "Step: 383, Training Loss: 1.67622, LR: 0.0009311, Tokens/sec: 619321.85\n",
      "Step: 384, Training Loss: 1.66148, LR: 0.0009307, Tokens/sec: 619752.81\n",
      "Step: 385, Training Loss: 1.70684, LR: 0.0009304, Tokens/sec: 619466.05\n",
      "Step: 386, Training Loss: 1.66253, LR: 0.0009300, Tokens/sec: 618529.63\n",
      "Step: 387, Training Loss: 1.64642, LR: 0.0009296, Tokens/sec: 619483.16\n",
      "Step: 388, Training Loss: 1.90768, LR: 0.0009292, Tokens/sec: 620212.92\n",
      "Step: 389, Training Loss: 1.65233, LR: 0.0009289, Tokens/sec: 618030.42\n",
      "Step: 390, Training Loss: 1.67265, LR: 0.0009285, Tokens/sec: 619320.51\n",
      "Step: 391, Training Loss: 1.66010, LR: 0.0009281, Tokens/sec: 618285.51\n",
      "Step: 392, Training Loss: 1.81242, LR: 0.0009277, Tokens/sec: 619351.73\n",
      "Step: 393, Training Loss: 1.66638, LR: 0.0009274, Tokens/sec: 620521.61\n",
      "Step: 394, Training Loss: 1.64022, LR: 0.0009270, Tokens/sec: 618915.46\n",
      "Step: 395, Training Loss: 1.70346, LR: 0.0009266, Tokens/sec: 619299.29\n",
      "Step: 396, Training Loss: 1.72395, LR: 0.0009262, Tokens/sec: 617959.35\n",
      "Step: 397, Training Loss: 1.73298, LR: 0.0009258, Tokens/sec: 617909.78\n",
      "Step: 398, Training Loss: 1.79436, LR: 0.0009255, Tokens/sec: 618764.99\n",
      "Step: 399, Training Loss: 1.68954, LR: 0.0009251, Tokens/sec: 620614.37\n",
      "Step: 400, Training Loss: 1.71805, LR: 0.0009247, Tokens/sec: 620156.32\n",
      "Step: 401, Training Loss: 1.72346, LR: 0.0009243, Tokens/sec: 619357.63\n",
      "Step: 402, Training Loss: 1.82216, LR: 0.0009239, Tokens/sec: 618411.16\n",
      "Step: 403, Training Loss: 1.66933, LR: 0.0009235, Tokens/sec: 618882.06\n",
      "Step: 404, Training Loss: 1.64769, LR: 0.0009231, Tokens/sec: 619687.46\n",
      "Step: 405, Training Loss: 1.66515, LR: 0.0009228, Tokens/sec: 618213.97\n",
      "Step: 406, Training Loss: 1.68204, LR: 0.0009224, Tokens/sec: 618480.92\n",
      "Step: 407, Training Loss: 1.72381, LR: 0.0009220, Tokens/sec: 620163.14\n",
      "Step: 408, Training Loss: 1.76311, LR: 0.0009216, Tokens/sec: 619311.66\n",
      "Step: 409, Training Loss: 1.64861, LR: 0.0009212, Tokens/sec: 620040.36\n",
      "Step: 410, Training Loss: 1.67234, LR: 0.0009208, Tokens/sec: 620193.34\n",
      "Step: 411, Training Loss: 1.64426, LR: 0.0009204, Tokens/sec: 618831.60\n",
      "Step: 412, Training Loss: 1.73259, LR: 0.0009200, Tokens/sec: 619390.12\n",
      "Step: 413, Training Loss: 1.62703, LR: 0.0009196, Tokens/sec: 618889.39\n",
      "Step: 414, Training Loss: 1.64197, LR: 0.0009192, Tokens/sec: 619577.72\n",
      "Step: 415, Training Loss: 1.76334, LR: 0.0009188, Tokens/sec: 619190.09\n",
      "Step: 416, Training Loss: 1.62272, LR: 0.0009184, Tokens/sec: 617941.92\n",
      "Step: 417, Training Loss: 1.72450, LR: 0.0009180, Tokens/sec: 619885.02\n",
      "Step: 418, Training Loss: 1.66629, LR: 0.0009176, Tokens/sec: 620000.67\n",
      "Step: 419, Training Loss: 1.59508, LR: 0.0009172, Tokens/sec: 619880.67\n",
      "Step: 420, Training Loss: 1.72827, LR: 0.0009168, Tokens/sec: 617360.75\n",
      "Step: 421, Training Loss: 1.58400, LR: 0.0009164, Tokens/sec: 618866.99\n",
      "Step: 422, Training Loss: 1.61021, LR: 0.0009160, Tokens/sec: 618850.50\n",
      "Step: 423, Training Loss: 1.68238, LR: 0.0009156, Tokens/sec: 618680.65\n",
      "Step: 424, Training Loss: 1.60880, LR: 0.0009152, Tokens/sec: 619278.65\n",
      "Step: 425, Training Loss: 1.64653, LR: 0.0009148, Tokens/sec: 619694.36\n",
      "Step: 426, Training Loss: 1.55706, LR: 0.0009144, Tokens/sec: 619464.32\n",
      "Step: 427, Training Loss: 1.56900, LR: 0.0009140, Tokens/sec: 618494.74\n",
      "Step: 428, Training Loss: 1.64717, LR: 0.0009136, Tokens/sec: 619732.85\n",
      "Step: 429, Training Loss: 1.58421, LR: 0.0009132, Tokens/sec: 618874.81\n",
      "Step: 430, Training Loss: 1.60426, LR: 0.0009127, Tokens/sec: 617378.76\n",
      "Step: 431, Training Loss: 1.67672, LR: 0.0009123, Tokens/sec: 620287.83\n",
      "Step: 432, Training Loss: 1.65711, LR: 0.0009119, Tokens/sec: 620539.49\n",
      "Step: 433, Training Loss: 1.62051, LR: 0.0009115, Tokens/sec: 618652.58\n",
      "Step: 434, Training Loss: 1.61631, LR: 0.0009111, Tokens/sec: 619815.77\n",
      "Step: 435, Training Loss: 1.60881, LR: 0.0009107, Tokens/sec: 618282.89\n",
      "Step: 436, Training Loss: 1.68887, LR: 0.0009103, Tokens/sec: 618874.24\n",
      "Step: 437, Training Loss: 1.58937, LR: 0.0009098, Tokens/sec: 619195.28\n",
      "Step: 438, Training Loss: 1.72717, LR: 0.0009094, Tokens/sec: 619490.48\n",
      "Step: 439, Training Loss: 1.62154, LR: 0.0009090, Tokens/sec: 619290.67\n",
      "Step: 440, Training Loss: 1.64338, LR: 0.0009086, Tokens/sec: 619544.58\n",
      "Step: 441, Training Loss: 1.55706, LR: 0.0009082, Tokens/sec: 619716.02\n",
      "Step: 442, Training Loss: 1.63377, LR: 0.0009077, Tokens/sec: 618251.47\n",
      "Step: 443, Training Loss: 1.59340, LR: 0.0009073, Tokens/sec: 618833.94\n",
      "Step: 444, Training Loss: 1.55749, LR: 0.0009069, Tokens/sec: 620087.16\n",
      "Step: 445, Training Loss: 1.59562, LR: 0.0009065, Tokens/sec: 618895.36\n",
      "Step: 446, Training Loss: 1.73647, LR: 0.0009061, Tokens/sec: 618447.03\n",
      "Step: 447, Training Loss: 1.60518, LR: 0.0009056, Tokens/sec: 618962.35\n",
      "Step: 448, Training Loss: 1.58251, LR: 0.0009052, Tokens/sec: 618607.73\n",
      "Step: 449, Training Loss: 1.58573, LR: 0.0009048, Tokens/sec: 618841.74\n",
      "Step: 450, Training Loss: 1.56698, LR: 0.0009043, Tokens/sec: 619514.12\n",
      "Step: 451, Training Loss: 1.70874, LR: 0.0009039, Tokens/sec: 617853.27\n",
      "Step: 452, Training Loss: 1.79957, LR: 0.0009035, Tokens/sec: 618722.68\n",
      "Step: 453, Training Loss: 1.52510, LR: 0.0009031, Tokens/sec: 619104.16\n",
      "Step: 454, Training Loss: 1.82772, LR: 0.0009026, Tokens/sec: 617060.74\n",
      "Step: 455, Training Loss: 1.52698, LR: 0.0009022, Tokens/sec: 620024.17\n",
      "Step: 456, Training Loss: 1.60255, LR: 0.0009018, Tokens/sec: 618861.45\n",
      "Step: 457, Training Loss: 1.64187, LR: 0.0009013, Tokens/sec: 620175.98\n",
      "Step: 458, Training Loss: 1.60224, LR: 0.0009009, Tokens/sec: 619390.86\n",
      "Step: 459, Training Loss: 1.57804, LR: 0.0009005, Tokens/sec: 619169.43\n",
      "Step: 460, Training Loss: 1.53372, LR: 0.0009000, Tokens/sec: 620267.51\n",
      "Step: 461, Training Loss: 1.55066, LR: 0.0008996, Tokens/sec: 620488.01\n",
      "Step: 462, Training Loss: 1.49764, LR: 0.0008991, Tokens/sec: 618576.46\n",
      "Step: 463, Training Loss: 1.56055, LR: 0.0008987, Tokens/sec: 618774.48\n",
      "Step: 464, Training Loss: 1.56418, LR: 0.0008983, Tokens/sec: 619094.41\n",
      "Step: 465, Training Loss: 1.54950, LR: 0.0008978, Tokens/sec: 618461.44\n",
      "Step: 466, Training Loss: 1.59846, LR: 0.0008974, Tokens/sec: 619488.13\n",
      "Step: 467, Training Loss: 1.53579, LR: 0.0008969, Tokens/sec: 618764.35\n",
      "Step: 468, Training Loss: 1.54998, LR: 0.0008965, Tokens/sec: 618403.70\n",
      "Step: 469, Training Loss: 1.52054, LR: 0.0008961, Tokens/sec: 619325.20\n",
      "Step: 470, Training Loss: 1.50080, LR: 0.0008956, Tokens/sec: 619587.84\n",
      "Step: 471, Training Loss: 1.62244, LR: 0.0008952, Tokens/sec: 618995.97\n",
      "Step: 472, Training Loss: 1.53098, LR: 0.0008947, Tokens/sec: 618800.21\n",
      "Step: 473, Training Loss: 1.55031, LR: 0.0008943, Tokens/sec: 619949.60\n",
      "Step: 474, Training Loss: 1.48536, LR: 0.0008938, Tokens/sec: 619770.73\n",
      "Step: 475, Training Loss: 1.47994, LR: 0.0008934, Tokens/sec: 618306.07\n",
      "Step: 476, Training Loss: 1.51434, LR: 0.0008929, Tokens/sec: 619554.96\n",
      "Step: 477, Training Loss: 1.49869, LR: 0.0008925, Tokens/sec: 619326.44\n",
      "Step: 478, Training Loss: 1.52229, LR: 0.0008920, Tokens/sec: 617940.46\n",
      "Step: 479, Training Loss: 1.45651, LR: 0.0008916, Tokens/sec: 619380.38\n",
      "Step: 480, Training Loss: 1.66667, LR: 0.0008911, Tokens/sec: 619789.77\n",
      "Step: 481, Training Loss: 1.54067, LR: 0.0008907, Tokens/sec: 619218.57\n",
      "Step: 482, Training Loss: 1.56254, LR: 0.0008902, Tokens/sec: 618036.91\n",
      "Step: 483, Training Loss: 1.47648, LR: 0.0008898, Tokens/sec: 619065.87\n",
      "Step: 484, Training Loss: 1.55616, LR: 0.0008893, Tokens/sec: 619022.19\n",
      "Step: 485, Training Loss: 1.48705, LR: 0.0008888, Tokens/sec: 619642.35\n",
      "Step: 486, Training Loss: 1.55704, LR: 0.0008884, Tokens/sec: 619476.26\n",
      "Step: 487, Training Loss: 1.93439, LR: 0.0008879, Tokens/sec: 619517.63\n",
      "Step: 488, Training Loss: 1.51415, LR: 0.0008875, Tokens/sec: 619471.73\n",
      "Step: 489, Training Loss: 1.58320, LR: 0.0008870, Tokens/sec: 618717.28\n",
      "Step: 490, Training Loss: 1.25715, LR: 0.0008865, Tokens/sec: 619880.69\n",
      "Step: 491, Training Loss: 0.82484, LR: 0.0008861, Tokens/sec: 617517.13\n",
      "Step: 492, Training Loss: 1.00644, LR: 0.0008856, Tokens/sec: 619307.31\n",
      "Step: 493, Training Loss: 0.66221, LR: 0.0008852, Tokens/sec: 619910.27\n",
      "Step: 494, Training Loss: 0.53847, LR: 0.0008847, Tokens/sec: 619119.51\n",
      "Step: 495, Training Loss: 0.54665, LR: 0.0008842, Tokens/sec: 619863.24\n",
      "Step: 496, Training Loss: 0.49587, LR: 0.0008838, Tokens/sec: 619830.78\n",
      "Step: 497, Training Loss: 0.35456, LR: 0.0008833, Tokens/sec: 619331.26\n",
      "Step: 498, Training Loss: 0.33363, LR: 0.0008828, Tokens/sec: 618022.84\n",
      "Step: 499, Training Loss: 0.36857, LR: 0.0008824, Tokens/sec: 619821.43\n",
      "Step: 500, Training Loss: 0.38545, LR: 0.0008819, Tokens/sec: 617751.86\n",
      "Computing Eval loss, steps: 21\n",
      "Step: 500, Eval Loss: 2.78918\n",
      "Step: 501, Training Loss: 0.32072, LR: 0.0008814, Tokens/sec: 618186.93\n",
      "Step: 502, Training Loss: 1.17873, LR: 0.0008810, Tokens/sec: 618919.89\n",
      "Step: 503, Training Loss: 0.48431, LR: 0.0008805, Tokens/sec: 619387.03\n",
      "Step: 504, Training Loss: 0.50611, LR: 0.0008800, Tokens/sec: 619505.45\n",
      "Step: 505, Training Loss: 0.43265, LR: 0.0008795, Tokens/sec: 619786.48\n",
      "Step: 506, Training Loss: 0.49628, LR: 0.0008791, Tokens/sec: 619000.75\n",
      "Step: 507, Training Loss: 0.45909, LR: 0.0008786, Tokens/sec: 618408.04\n",
      "Step: 508, Training Loss: 0.54267, LR: 0.0008781, Tokens/sec: 619170.20\n",
      "Step: 509, Training Loss: 0.53895, LR: 0.0008776, Tokens/sec: 618085.47\n",
      "Step: 510, Training Loss: 1.18511, LR: 0.0008772, Tokens/sec: 619159.72\n",
      "Step: 511, Training Loss: 2.37288, LR: 0.0008767, Tokens/sec: 619182.33\n",
      "Step: 512, Training Loss: 2.16804, LR: 0.0008762, Tokens/sec: 618774.23\n",
      "Step: 513, Training Loss: 2.09145, LR: 0.0008757, Tokens/sec: 618981.67\n",
      "Step: 514, Training Loss: 2.04498, LR: 0.0008752, Tokens/sec: 618853.97\n",
      "Step: 515, Training Loss: 1.99064, LR: 0.0008748, Tokens/sec: 619180.51\n",
      "Step: 516, Training Loss: 1.96535, LR: 0.0008743, Tokens/sec: 619076.65\n",
      "Step: 517, Training Loss: 1.84480, LR: 0.0008738, Tokens/sec: 619268.79\n",
      "Step: 518, Training Loss: 1.29436, LR: 0.0008733, Tokens/sec: 618930.88\n",
      "Step: 519, Training Loss: 1.43380, LR: 0.0008728, Tokens/sec: 618564.25\n",
      "Step: 520, Training Loss: 1.52248, LR: 0.0008723, Tokens/sec: 617878.60\n",
      "Step: 521, Training Loss: 0.69032, LR: 0.0008719, Tokens/sec: 619333.77\n",
      "Step: 522, Training Loss: 0.61132, LR: 0.0008714, Tokens/sec: 619030.98\n",
      "Step: 523, Training Loss: 0.45016, LR: 0.0008709, Tokens/sec: 619122.75\n",
      "Step: 524, Training Loss: 1.36502, LR: 0.0008704, Tokens/sec: 619249.35\n",
      "Step: 525, Training Loss: 2.22012, LR: 0.0008699, Tokens/sec: 618355.54\n",
      "Step: 526, Training Loss: 2.05505, LR: 0.0008694, Tokens/sec: 619455.69\n",
      "Step: 527, Training Loss: 1.52814, LR: 0.0008689, Tokens/sec: 618551.31\n",
      "Step: 528, Training Loss: 2.20917, LR: 0.0008684, Tokens/sec: 618901.77\n",
      "Step: 529, Training Loss: 2.09804, LR: 0.0008680, Tokens/sec: 619452.63\n",
      "Step: 530, Training Loss: 2.00650, LR: 0.0008675, Tokens/sec: 620270.11\n",
      "Step: 531, Training Loss: 1.97901, LR: 0.0008670, Tokens/sec: 619533.42\n",
      "Step: 532, Training Loss: 1.94567, LR: 0.0008665, Tokens/sec: 621038.03\n",
      "Step: 533, Training Loss: 2.14541, LR: 0.0008660, Tokens/sec: 619224.88\n",
      "Step: 534, Training Loss: 1.98613, LR: 0.0008655, Tokens/sec: 619928.30\n",
      "Step: 535, Training Loss: 1.87171, LR: 0.0008650, Tokens/sec: 618777.85\n",
      "Step: 536, Training Loss: 1.86988, LR: 0.0008645, Tokens/sec: 617923.03\n",
      "Step: 537, Training Loss: 1.89299, LR: 0.0008640, Tokens/sec: 620293.46\n",
      "Step: 538, Training Loss: 1.83290, LR: 0.0008635, Tokens/sec: 619123.97\n",
      "Step: 539, Training Loss: 1.82929, LR: 0.0008630, Tokens/sec: 620648.82\n",
      "Step: 540, Training Loss: 1.80483, LR: 0.0008625, Tokens/sec: 619593.66\n",
      "Step: 541, Training Loss: 1.77879, LR: 0.0008620, Tokens/sec: 620447.74\n",
      "Step: 542, Training Loss: 1.68563, LR: 0.0008615, Tokens/sec: 619981.09\n",
      "Step: 543, Training Loss: 1.11574, LR: 0.0008610, Tokens/sec: 620020.86\n",
      "Step: 544, Training Loss: 1.33511, LR: 0.0008605, Tokens/sec: 618934.06\n",
      "Step: 545, Training Loss: 1.92412, LR: 0.0008600, Tokens/sec: 619420.09\n",
      "Step: 546, Training Loss: 1.85580, LR: 0.0008595, Tokens/sec: 619855.12\n",
      "Step: 547, Training Loss: 1.81748, LR: 0.0008590, Tokens/sec: 619447.51\n",
      "Step: 548, Training Loss: 1.89838, LR: 0.0008585, Tokens/sec: 618412.82\n",
      "Step: 549, Training Loss: 2.80020, LR: 0.0008580, Tokens/sec: 619497.87\n",
      "Step: 550, Training Loss: 1.83994, LR: 0.0008575, Tokens/sec: 619810.52\n",
      "Step: 551, Training Loss: 1.98477, LR: 0.0008570, Tokens/sec: 620490.70\n",
      "Step: 552, Training Loss: 1.81255, LR: 0.0008564, Tokens/sec: 621285.09\n",
      "Step: 553, Training Loss: 1.74476, LR: 0.0008559, Tokens/sec: 620362.82\n",
      "Step: 554, Training Loss: 1.77914, LR: 0.0008554, Tokens/sec: 619951.62\n",
      "Step: 555, Training Loss: 1.74712, LR: 0.0008549, Tokens/sec: 619502.68\n",
      "Step: 556, Training Loss: 1.81084, LR: 0.0008544, Tokens/sec: 619671.80\n",
      "Step: 557, Training Loss: 2.03974, LR: 0.0008539, Tokens/sec: 619556.16\n",
      "Step: 558, Training Loss: 1.78107, LR: 0.0008534, Tokens/sec: 619808.77\n",
      "Step: 559, Training Loss: 1.78868, LR: 0.0008529, Tokens/sec: 617928.26\n",
      "Step: 560, Training Loss: 1.79841, LR: 0.0008523, Tokens/sec: 620409.79\n",
      "Step: 561, Training Loss: 1.81337, LR: 0.0008518, Tokens/sec: 619841.70\n",
      "Step: 562, Training Loss: 1.77256, LR: 0.0008513, Tokens/sec: 619479.04\n",
      "Step: 563, Training Loss: 1.79239, LR: 0.0008508, Tokens/sec: 619122.42\n",
      "Step: 564, Training Loss: 1.77536, LR: 0.0008503, Tokens/sec: 619301.23\n",
      "Step: 565, Training Loss: 1.73808, LR: 0.0008498, Tokens/sec: 618251.68\n",
      "Step: 566, Training Loss: 1.70411, LR: 0.0008492, Tokens/sec: 619004.62\n",
      "Step: 567, Training Loss: 1.71918, LR: 0.0008487, Tokens/sec: 618338.33\n",
      "Step: 568, Training Loss: 1.74060, LR: 0.0008482, Tokens/sec: 620046.13\n",
      "Step: 569, Training Loss: 1.74070, LR: 0.0008477, Tokens/sec: 618234.84\n",
      "Step: 570, Training Loss: 1.71511, LR: 0.0008472, Tokens/sec: 619192.27\n",
      "Step: 571, Training Loss: 1.73604, LR: 0.0008466, Tokens/sec: 618640.84\n",
      "Step: 572, Training Loss: 1.69248, LR: 0.0008461, Tokens/sec: 619231.08\n",
      "Step: 573, Training Loss: 1.70696, LR: 0.0008456, Tokens/sec: 619984.12\n",
      "Step: 574, Training Loss: 1.70679, LR: 0.0008451, Tokens/sec: 619261.37\n",
      "Step: 575, Training Loss: 1.63206, LR: 0.0008445, Tokens/sec: 618568.24\n",
      "Step: 576, Training Loss: 1.68347, LR: 0.0008440, Tokens/sec: 620776.14\n",
      "Step: 577, Training Loss: 1.65359, LR: 0.0008435, Tokens/sec: 619623.96\n",
      "Step: 578, Training Loss: 1.66632, LR: 0.0008430, Tokens/sec: 618078.37\n",
      "Step: 579, Training Loss: 1.63681, LR: 0.0008424, Tokens/sec: 619059.13\n",
      "Step: 580, Training Loss: 1.65481, LR: 0.0008419, Tokens/sec: 619068.80\n",
      "Step: 581, Training Loss: 1.64565, LR: 0.0008414, Tokens/sec: 619876.50\n",
      "Step: 582, Training Loss: 1.64523, LR: 0.0008408, Tokens/sec: 619103.75\n",
      "Step: 583, Training Loss: 1.61803, LR: 0.0008403, Tokens/sec: 618674.48\n",
      "Step: 584, Training Loss: 1.59757, LR: 0.0008398, Tokens/sec: 619752.59\n",
      "Step: 585, Training Loss: 1.68673, LR: 0.0008393, Tokens/sec: 619202.61\n",
      "Step: 586, Training Loss: 1.66214, LR: 0.0008387, Tokens/sec: 618607.23\n",
      "Step: 587, Training Loss: 1.66265, LR: 0.0008382, Tokens/sec: 618785.26\n",
      "Step: 588, Training Loss: 1.61982, LR: 0.0008377, Tokens/sec: 620739.04\n",
      "Step: 589, Training Loss: 1.63920, LR: 0.0008371, Tokens/sec: 620584.71\n",
      "Step: 590, Training Loss: 1.63303, LR: 0.0008366, Tokens/sec: 619436.77\n",
      "Step: 591, Training Loss: 1.67271, LR: 0.0008360, Tokens/sec: 620598.80\n",
      "Step: 592, Training Loss: 1.60611, LR: 0.0008355, Tokens/sec: 619314.25\n",
      "Step: 593, Training Loss: 1.57636, LR: 0.0008350, Tokens/sec: 617478.83\n",
      "Step: 594, Training Loss: 1.59480, LR: 0.0008344, Tokens/sec: 619186.09\n",
      "Step: 595, Training Loss: 1.49893, LR: 0.0008339, Tokens/sec: 619630.73\n",
      "Step: 596, Training Loss: 1.65231, LR: 0.0008334, Tokens/sec: 620338.03\n",
      "Step: 597, Training Loss: 1.64458, LR: 0.0008328, Tokens/sec: 618788.18\n",
      "Step: 598, Training Loss: 1.64904, LR: 0.0008323, Tokens/sec: 618124.02\n",
      "Step: 599, Training Loss: 1.59813, LR: 0.0008317, Tokens/sec: 619684.50\n",
      "Step: 600, Training Loss: 1.57271, LR: 0.0008312, Tokens/sec: 618227.71\n",
      "Step: 601, Training Loss: 1.60340, LR: 0.0008306, Tokens/sec: 619459.78\n",
      "Step: 602, Training Loss: 1.66384, LR: 0.0008301, Tokens/sec: 619457.93\n",
      "Step: 603, Training Loss: 1.65851, LR: 0.0008296, Tokens/sec: 620344.04\n",
      "Step: 604, Training Loss: 1.67377, LR: 0.0008290, Tokens/sec: 619783.64\n",
      "Step: 605, Training Loss: 1.62478, LR: 0.0008285, Tokens/sec: 618282.50\n",
      "Step: 606, Training Loss: 1.61017, LR: 0.0008279, Tokens/sec: 620034.10\n",
      "Step: 607, Training Loss: 1.52990, LR: 0.0008274, Tokens/sec: 618626.21\n",
      "Step: 608, Training Loss: 1.55689, LR: 0.0008268, Tokens/sec: 618217.18\n",
      "Step: 609, Training Loss: 1.53355, LR: 0.0008263, Tokens/sec: 618152.91\n",
      "Step: 610, Training Loss: 1.73544, LR: 0.0008257, Tokens/sec: 619715.27\n",
      "Step: 611, Training Loss: 1.54331, LR: 0.0008252, Tokens/sec: 618725.19\n",
      "Step: 612, Training Loss: 1.79231, LR: 0.0008246, Tokens/sec: 620450.01\n",
      "Step: 613, Training Loss: 1.56145, LR: 0.0008241, Tokens/sec: 620130.05\n",
      "Step: 614, Training Loss: 1.59603, LR: 0.0008235, Tokens/sec: 619617.59\n",
      "Step: 615, Training Loss: 1.69659, LR: 0.0008230, Tokens/sec: 619835.77\n",
      "Step: 616, Training Loss: 1.61751, LR: 0.0008224, Tokens/sec: 619733.41\n",
      "Step: 617, Training Loss: 1.52107, LR: 0.0008219, Tokens/sec: 619050.39\n",
      "Step: 618, Training Loss: 1.52729, LR: 0.0008213, Tokens/sec: 619353.95\n",
      "Step: 619, Training Loss: 1.40316, LR: 0.0008208, Tokens/sec: 619485.16\n",
      "Step: 620, Training Loss: 1.55846, LR: 0.0008202, Tokens/sec: 619587.50\n",
      "Step: 621, Training Loss: 1.60024, LR: 0.0008196, Tokens/sec: 619650.70\n",
      "Step: 622, Training Loss: 1.54167, LR: 0.0008191, Tokens/sec: 619192.76\n",
      "Step: 623, Training Loss: 1.49326, LR: 0.0008185, Tokens/sec: 619715.84\n",
      "Step: 624, Training Loss: 1.53857, LR: 0.0008180, Tokens/sec: 619763.88\n",
      "Step: 625, Training Loss: 1.53508, LR: 0.0008174, Tokens/sec: 619702.67\n",
      "Step: 626, Training Loss: 1.52980, LR: 0.0008169, Tokens/sec: 618912.71\n",
      "Step: 627, Training Loss: 1.52423, LR: 0.0008163, Tokens/sec: 620572.75\n",
      "Step: 628, Training Loss: 1.53563, LR: 0.0008157, Tokens/sec: 618852.05\n",
      "Step: 629, Training Loss: 1.50644, LR: 0.0008152, Tokens/sec: 620072.23\n",
      "Step: 630, Training Loss: 1.41086, LR: 0.0008146, Tokens/sec: 618579.53\n",
      "Step: 631, Training Loss: 1.41810, LR: 0.0008141, Tokens/sec: 620073.02\n",
      "Step: 632, Training Loss: 1.65129, LR: 0.0008135, Tokens/sec: 619240.89\n",
      "Step: 633, Training Loss: 1.56224, LR: 0.0008129, Tokens/sec: 620340.81\n",
      "Step: 634, Training Loss: 1.55892, LR: 0.0008124, Tokens/sec: 619884.86\n",
      "Step: 635, Training Loss: 1.53914, LR: 0.0008118, Tokens/sec: 620255.08\n",
      "Step: 636, Training Loss: 1.63814, LR: 0.0008112, Tokens/sec: 619373.89\n",
      "Step: 637, Training Loss: 1.52326, LR: 0.0008107, Tokens/sec: 619858.97\n",
      "Step: 638, Training Loss: 1.50843, LR: 0.0008101, Tokens/sec: 618812.42\n",
      "Step: 639, Training Loss: 1.50733, LR: 0.0008095, Tokens/sec: 618326.60\n",
      "Step: 640, Training Loss: 1.56760, LR: 0.0008090, Tokens/sec: 619405.94\n",
      "Step: 641, Training Loss: 1.52523, LR: 0.0008084, Tokens/sec: 618656.92\n",
      "Step: 642, Training Loss: 1.54855, LR: 0.0008078, Tokens/sec: 618916.93\n",
      "Step: 643, Training Loss: 1.57208, LR: 0.0008073, Tokens/sec: 620047.34\n",
      "Step: 644, Training Loss: 1.54625, LR: 0.0008067, Tokens/sec: 619157.90\n",
      "Step: 645, Training Loss: 1.49687, LR: 0.0008061, Tokens/sec: 619237.23\n",
      "Step: 646, Training Loss: 1.39382, LR: 0.0008055, Tokens/sec: 617898.53\n",
      "Step: 647, Training Loss: 1.47677, LR: 0.0008050, Tokens/sec: 618798.48\n",
      "Step: 648, Training Loss: 1.45911, LR: 0.0008044, Tokens/sec: 619120.05\n",
      "Step: 649, Training Loss: 1.62773, LR: 0.0008038, Tokens/sec: 619752.85\n",
      "Step: 650, Training Loss: 1.71149, LR: 0.0008032, Tokens/sec: 619403.13\n",
      "Step: 651, Training Loss: 2.21866, LR: 0.0008027, Tokens/sec: 618622.81\n",
      "Step: 652, Training Loss: 1.55204, LR: 0.0008021, Tokens/sec: 618832.25\n",
      "Step: 653, Training Loss: 1.52998, LR: 0.0008015, Tokens/sec: 619358.50\n",
      "Step: 654, Training Loss: 1.54836, LR: 0.0008009, Tokens/sec: 618829.64\n",
      "Step: 655, Training Loss: 1.56314, LR: 0.0008004, Tokens/sec: 618907.38\n",
      "Step: 656, Training Loss: 1.42938, LR: 0.0007998, Tokens/sec: 618481.53\n",
      "Step: 657, Training Loss: 1.54362, LR: 0.0007992, Tokens/sec: 619296.25\n",
      "Step: 658, Training Loss: 1.52418, LR: 0.0007986, Tokens/sec: 618605.32\n",
      "Step: 659, Training Loss: 1.53421, LR: 0.0007981, Tokens/sec: 620124.65\n",
      "Step: 660, Training Loss: 1.53341, LR: 0.0007975, Tokens/sec: 618833.32\n",
      "Step: 661, Training Loss: 1.53308, LR: 0.0007969, Tokens/sec: 619911.85\n",
      "Step: 662, Training Loss: 1.48836, LR: 0.0007963, Tokens/sec: 617971.49\n",
      "Step: 663, Training Loss: 1.46652, LR: 0.0007957, Tokens/sec: 618587.13\n",
      "Step: 664, Training Loss: 1.60338, LR: 0.0007951, Tokens/sec: 619612.85\n",
      "Step: 665, Training Loss: 1.57082, LR: 0.0007946, Tokens/sec: 619433.14\n",
      "Step: 666, Training Loss: 1.52227, LR: 0.0007940, Tokens/sec: 619671.29\n",
      "Step: 667, Training Loss: 1.56905, LR: 0.0007934, Tokens/sec: 618806.64\n",
      "Step: 668, Training Loss: 1.53342, LR: 0.0007928, Tokens/sec: 618661.81\n",
      "Step: 669, Training Loss: 1.61302, LR: 0.0007922, Tokens/sec: 619438.89\n",
      "Step: 670, Training Loss: 1.59852, LR: 0.0007916, Tokens/sec: 619313.54\n",
      "Step: 671, Training Loss: 1.52777, LR: 0.0007911, Tokens/sec: 620273.47\n",
      "Step: 672, Training Loss: 1.53567, LR: 0.0007905, Tokens/sec: 619401.96\n",
      "Step: 673, Training Loss: 1.52822, LR: 0.0007899, Tokens/sec: 619295.84\n",
      "Step: 674, Training Loss: 1.53757, LR: 0.0007893, Tokens/sec: 617686.06\n",
      "Step: 675, Training Loss: 1.63409, LR: 0.0007887, Tokens/sec: 619280.81\n",
      "Step: 676, Training Loss: 1.45287, LR: 0.0007881, Tokens/sec: 618548.33\n",
      "Step: 677, Training Loss: 1.51501, LR: 0.0007875, Tokens/sec: 620002.11\n",
      "Step: 678, Training Loss: 1.63371, LR: 0.0007869, Tokens/sec: 619149.79\n",
      "Step: 679, Training Loss: 1.53096, LR: 0.0007863, Tokens/sec: 619091.51\n",
      "Step: 680, Training Loss: 1.51309, LR: 0.0007857, Tokens/sec: 620465.35\n",
      "Step: 681, Training Loss: 1.49961, LR: 0.0007852, Tokens/sec: 619206.96\n",
      "Step: 682, Training Loss: 1.46721, LR: 0.0007846, Tokens/sec: 619012.82\n",
      "Step: 683, Training Loss: 1.55092, LR: 0.0007840, Tokens/sec: 620018.74\n",
      "Step: 684, Training Loss: 1.48795, LR: 0.0007834, Tokens/sec: 618854.60\n",
      "Step: 685, Training Loss: 1.39089, LR: 0.0007828, Tokens/sec: 619436.55\n",
      "Step: 686, Training Loss: 1.45414, LR: 0.0007822, Tokens/sec: 619367.78\n",
      "Step: 687, Training Loss: 1.74951, LR: 0.0007816, Tokens/sec: 619107.35\n",
      "Step: 688, Training Loss: 1.99495, LR: 0.0007810, Tokens/sec: 620366.22\n",
      "Step: 689, Training Loss: 1.41594, LR: 0.0007804, Tokens/sec: 619874.74\n",
      "Step: 690, Training Loss: 1.68698, LR: 0.0007798, Tokens/sec: 618327.32\n",
      "Step: 691, Training Loss: 1.48366, LR: 0.0007792, Tokens/sec: 618884.95\n",
      "Step: 692, Training Loss: 1.54904, LR: 0.0007786, Tokens/sec: 619013.94\n",
      "Step: 693, Training Loss: 1.46249, LR: 0.0007780, Tokens/sec: 619251.02\n",
      "Step: 694, Training Loss: 1.40975, LR: 0.0007774, Tokens/sec: 620260.45\n",
      "Step: 695, Training Loss: 1.38286, LR: 0.0007768, Tokens/sec: 618931.96\n",
      "Step: 696, Training Loss: 1.38225, LR: 0.0007762, Tokens/sec: 620542.67\n",
      "Step: 697, Training Loss: 1.22445, LR: 0.0007756, Tokens/sec: 620079.68\n",
      "Step: 698, Training Loss: 1.43350, LR: 0.0007750, Tokens/sec: 619000.97\n",
      "Step: 699, Training Loss: 1.51744, LR: 0.0007744, Tokens/sec: 618837.23\n",
      "Step: 700, Training Loss: 1.51279, LR: 0.0007738, Tokens/sec: 619865.63\n",
      "Step: 701, Training Loss: 1.48879, LR: 0.0007732, Tokens/sec: 618601.76\n",
      "Step: 702, Training Loss: 1.29207, LR: 0.0007726, Tokens/sec: 620301.97\n",
      "Step: 703, Training Loss: 1.03633, LR: 0.0007720, Tokens/sec: 617749.36\n",
      "Step: 704, Training Loss: 1.38645, LR: 0.0007714, Tokens/sec: 619569.94\n",
      "Step: 705, Training Loss: 1.55584, LR: 0.0007708, Tokens/sec: 617266.04\n",
      "Step: 706, Training Loss: 1.55524, LR: 0.0007702, Tokens/sec: 621049.71\n",
      "Step: 707, Training Loss: 1.60458, LR: 0.0007696, Tokens/sec: 619407.84\n",
      "Step: 708, Training Loss: 1.55428, LR: 0.0007690, Tokens/sec: 619165.52\n",
      "Step: 709, Training Loss: 1.55615, LR: 0.0007683, Tokens/sec: 619474.01\n",
      "Step: 710, Training Loss: 1.66394, LR: 0.0007677, Tokens/sec: 620014.49\n",
      "Step: 711, Training Loss: 1.41704, LR: 0.0007671, Tokens/sec: 618563.82\n",
      "Step: 712, Training Loss: 1.44443, LR: 0.0007665, Tokens/sec: 618718.64\n",
      "Step: 713, Training Loss: 1.55499, LR: 0.0007659, Tokens/sec: 619059.22\n",
      "Step: 714, Training Loss: 1.30915, LR: 0.0007653, Tokens/sec: 618717.90\n",
      "Step: 715, Training Loss: 1.49063, LR: 0.0007647, Tokens/sec: 619533.11\n",
      "Step: 716, Training Loss: 1.40259, LR: 0.0007641, Tokens/sec: 618497.87\n",
      "Step: 717, Training Loss: 1.38097, LR: 0.0007635, Tokens/sec: 619445.81\n",
      "Step: 718, Training Loss: 1.10986, LR: 0.0007629, Tokens/sec: 621341.45\n",
      "Step: 719, Training Loss: 1.51347, LR: 0.0007622, Tokens/sec: 618884.21\n",
      "Step: 720, Training Loss: 1.18491, LR: 0.0007616, Tokens/sec: 618929.01\n",
      "Step: 721, Training Loss: 1.58364, LR: 0.0007610, Tokens/sec: 618844.43\n",
      "Step: 722, Training Loss: 1.45518, LR: 0.0007604, Tokens/sec: 620223.99\n",
      "Step: 723, Training Loss: 1.35805, LR: 0.0007598, Tokens/sec: 618816.21\n",
      "Step: 724, Training Loss: 1.33734, LR: 0.0007592, Tokens/sec: 619022.72\n",
      "Step: 725, Training Loss: 1.25333, LR: 0.0007586, Tokens/sec: 617983.82\n",
      "Step: 726, Training Loss: 1.12572, LR: 0.0007579, Tokens/sec: 618232.71\n",
      "Step: 727, Training Loss: 1.08603, LR: 0.0007573, Tokens/sec: 618876.38\n",
      "Step: 728, Training Loss: 1.57545, LR: 0.0007567, Tokens/sec: 618319.99\n",
      "Step: 729, Training Loss: 1.62737, LR: 0.0007561, Tokens/sec: 621251.21\n",
      "Step: 730, Training Loss: 1.53676, LR: 0.0007555, Tokens/sec: 619397.01\n",
      "Step: 731, Training Loss: 1.50613, LR: 0.0007549, Tokens/sec: 618804.24\n",
      "Step: 732, Training Loss: 1.55822, LR: 0.0007542, Tokens/sec: 619142.28\n",
      "Step: 733, Training Loss: 1.50804, LR: 0.0007536, Tokens/sec: 619035.77\n",
      "Step: 734, Training Loss: 1.62058, LR: 0.0007530, Tokens/sec: 619512.26\n",
      "Step: 735, Training Loss: 1.69914, LR: 0.0007524, Tokens/sec: 618459.60\n",
      "Step: 736, Training Loss: 1.62317, LR: 0.0007518, Tokens/sec: 619170.08\n",
      "Step: 737, Training Loss: 1.41244, LR: 0.0007511, Tokens/sec: 620297.24\n",
      "Step: 738, Training Loss: 1.00892, LR: 0.0007505, Tokens/sec: 619815.55\n",
      "Step: 739, Training Loss: 1.24295, LR: 0.0007499, Tokens/sec: 619522.31\n",
      "Step: 740, Training Loss: 1.65949, LR: 0.0007493, Tokens/sec: 619316.91\n",
      "Step: 741, Training Loss: 1.61582, LR: 0.0007486, Tokens/sec: 619280.17\n",
      "Step: 742, Training Loss: 1.55590, LR: 0.0007480, Tokens/sec: 619456.47\n",
      "Step: 743, Training Loss: 1.59730, LR: 0.0007474, Tokens/sec: 618934.92\n",
      "Step: 744, Training Loss: 1.65925, LR: 0.0007468, Tokens/sec: 618410.76\n",
      "Step: 745, Training Loss: 1.59241, LR: 0.0007461, Tokens/sec: 619407.51\n",
      "Step: 746, Training Loss: 1.56878, LR: 0.0007455, Tokens/sec: 619739.26\n",
      "Step: 747, Training Loss: 1.55766, LR: 0.0007449, Tokens/sec: 618796.21\n",
      "Step: 748, Training Loss: 1.59508, LR: 0.0007443, Tokens/sec: 620285.60\n",
      "Step: 749, Training Loss: 1.41444, LR: 0.0007436, Tokens/sec: 618974.12\n",
      "Step: 750, Training Loss: 1.48718, LR: 0.0007430, Tokens/sec: 619340.59\n",
      "Step: 751, Training Loss: 1.54770, LR: 0.0007424, Tokens/sec: 620319.33\n",
      "Step: 752, Training Loss: 1.46085, LR: 0.0007418, Tokens/sec: 617730.09\n",
      "Step: 753, Training Loss: 1.47292, LR: 0.0007411, Tokens/sec: 619346.24\n",
      "Step: 754, Training Loss: 1.47598, LR: 0.0007405, Tokens/sec: 619140.96\n",
      "Step: 755, Training Loss: 1.46471, LR: 0.0007399, Tokens/sec: 619133.82\n",
      "Step: 756, Training Loss: 1.41969, LR: 0.0007392, Tokens/sec: 618277.69\n",
      "Step: 757, Training Loss: 1.43281, LR: 0.0007386, Tokens/sec: 619014.50\n",
      "Step: 758, Training Loss: 1.49318, LR: 0.0007380, Tokens/sec: 619502.50\n",
      "Step: 759, Training Loss: 1.52809, LR: 0.0007373, Tokens/sec: 618196.72\n",
      "Step: 760, Training Loss: 1.51075, LR: 0.0007367, Tokens/sec: 619606.93\n",
      "Step: 761, Training Loss: 1.46242, LR: 0.0007361, Tokens/sec: 618809.86\n",
      "Step: 762, Training Loss: 1.42145, LR: 0.0007354, Tokens/sec: 618665.27\n",
      "Step: 763, Training Loss: 1.39597, LR: 0.0007348, Tokens/sec: 617944.16\n",
      "Step: 764, Training Loss: 1.36202, LR: 0.0007342, Tokens/sec: 618851.77\n",
      "Step: 765, Training Loss: 1.49403, LR: 0.0007335, Tokens/sec: 619293.93\n",
      "Step: 766, Training Loss: 1.55628, LR: 0.0007329, Tokens/sec: 619184.74\n",
      "Step: 767, Training Loss: 1.51095, LR: 0.0007323, Tokens/sec: 619355.03\n",
      "Step: 768, Training Loss: 1.44709, LR: 0.0007316, Tokens/sec: 620242.82\n",
      "Step: 769, Training Loss: 1.43249, LR: 0.0007310, Tokens/sec: 619800.95\n",
      "Step: 770, Training Loss: 1.45864, LR: 0.0007304, Tokens/sec: 617952.36\n",
      "Step: 771, Training Loss: 1.38879, LR: 0.0007297, Tokens/sec: 619496.92\n",
      "Step: 772, Training Loss: 1.45307, LR: 0.0007291, Tokens/sec: 619557.71\n",
      "Step: 773, Training Loss: 1.41912, LR: 0.0007284, Tokens/sec: 619132.85\n",
      "Step: 774, Training Loss: 1.46373, LR: 0.0007278, Tokens/sec: 618336.74\n",
      "Step: 775, Training Loss: 1.44325, LR: 0.0007272, Tokens/sec: 619284.43\n",
      "Step: 776, Training Loss: 1.52385, LR: 0.0007265, Tokens/sec: 618877.38\n",
      "Step: 777, Training Loss: 1.45292, LR: 0.0007259, Tokens/sec: 619580.88\n",
      "Step: 778, Training Loss: 1.38512, LR: 0.0007253, Tokens/sec: 618343.61\n",
      "Step: 779, Training Loss: 1.43789, LR: 0.0007246, Tokens/sec: 620006.81\n",
      "Step: 780, Training Loss: 1.40994, LR: 0.0007240, Tokens/sec: 619114.22\n",
      "Step: 781, Training Loss: 1.47435, LR: 0.0007233, Tokens/sec: 619213.59\n",
      "Step: 782, Training Loss: 1.42985, LR: 0.0007227, Tokens/sec: 619141.71\n",
      "Step: 783, Training Loss: 1.45031, LR: 0.0007220, Tokens/sec: 619269.00\n",
      "Step: 784, Training Loss: 1.45311, LR: 0.0007214, Tokens/sec: 619289.20\n",
      "Step: 785, Training Loss: 1.36670, LR: 0.0007208, Tokens/sec: 619379.94\n",
      "Step: 786, Training Loss: 1.53283, LR: 0.0007201, Tokens/sec: 618843.67\n",
      "Step: 787, Training Loss: 1.41841, LR: 0.0007195, Tokens/sec: 620172.35\n",
      "Step: 788, Training Loss: 1.39090, LR: 0.0007188, Tokens/sec: 618188.65\n",
      "Step: 789, Training Loss: 1.36932, LR: 0.0007182, Tokens/sec: 619047.02\n",
      "Step: 790, Training Loss: 1.49697, LR: 0.0007175, Tokens/sec: 619733.28\n",
      "Step: 791, Training Loss: 1.52497, LR: 0.0007169, Tokens/sec: 620144.85\n",
      "Step: 792, Training Loss: 1.53377, LR: 0.0007163, Tokens/sec: 618959.38\n",
      "Step: 793, Training Loss: 1.47621, LR: 0.0007156, Tokens/sec: 619921.28\n",
      "Step: 794, Training Loss: 1.43615, LR: 0.0007150, Tokens/sec: 618186.92\n",
      "Step: 795, Training Loss: 1.37114, LR: 0.0007143, Tokens/sec: 619667.01\n",
      "Step: 796, Training Loss: 1.46758, LR: 0.0007137, Tokens/sec: 620330.17\n",
      "Step: 797, Training Loss: 1.39942, LR: 0.0007130, Tokens/sec: 620114.24\n",
      "Step: 798, Training Loss: 1.46048, LR: 0.0007124, Tokens/sec: 618210.60\n",
      "Step: 799, Training Loss: 1.37582, LR: 0.0007117, Tokens/sec: 618858.16\n",
      "Step: 800, Training Loss: 1.46883, LR: 0.0007111, Tokens/sec: 620324.65\n",
      "Step: 801, Training Loss: 1.52055, LR: 0.0007104, Tokens/sec: 619860.81\n",
      "Step: 802, Training Loss: 1.51733, LR: 0.0007098, Tokens/sec: 618973.01\n",
      "Step: 803, Training Loss: 1.33469, LR: 0.0007091, Tokens/sec: 619255.89\n",
      "Step: 804, Training Loss: 1.37117, LR: 0.0007085, Tokens/sec: 617920.91\n",
      "Step: 805, Training Loss: 1.45599, LR: 0.0007078, Tokens/sec: 618997.91\n",
      "Step: 806, Training Loss: 1.43836, LR: 0.0007072, Tokens/sec: 618666.49\n",
      "Step: 807, Training Loss: 1.41243, LR: 0.0007065, Tokens/sec: 618481.04\n",
      "Step: 808, Training Loss: 1.39419, LR: 0.0007059, Tokens/sec: 618184.92\n",
      "Step: 809, Training Loss: 1.39496, LR: 0.0007052, Tokens/sec: 619060.10\n",
      "Step: 810, Training Loss: 1.33329, LR: 0.0007046, Tokens/sec: 619552.30\n",
      "Step: 811, Training Loss: 1.38623, LR: 0.0007039, Tokens/sec: 618434.44\n",
      "Step: 812, Training Loss: 1.36467, LR: 0.0007033, Tokens/sec: 619253.98\n",
      "Step: 813, Training Loss: 1.34274, LR: 0.0007026, Tokens/sec: 618371.36\n",
      "Step: 814, Training Loss: 1.38878, LR: 0.0007019, Tokens/sec: 619255.89\n",
      "Step: 815, Training Loss: 1.28879, LR: 0.0007013, Tokens/sec: 618423.95\n",
      "Step: 816, Training Loss: 1.26883, LR: 0.0007006, Tokens/sec: 618224.01\n",
      "Step: 817, Training Loss: 1.26423, LR: 0.0007000, Tokens/sec: 528198.99\n",
      "Step: 818, Training Loss: 1.36203, LR: 0.0006993, Tokens/sec: 612935.04\n",
      "Step: 819, Training Loss: 1.41883, LR: 0.0006987, Tokens/sec: 617148.58\n",
      "Step: 820, Training Loss: 1.28924, LR: 0.0006980, Tokens/sec: 615636.75\n",
      "Step: 821, Training Loss: 1.39739, LR: 0.0006974, Tokens/sec: 616786.44\n",
      "Step: 822, Training Loss: 1.44600, LR: 0.0006967, Tokens/sec: 616131.96\n",
      "Step: 823, Training Loss: 1.48248, LR: 0.0006960, Tokens/sec: 617059.10\n",
      "Step: 824, Training Loss: 1.43787, LR: 0.0006954, Tokens/sec: 616401.32\n",
      "Step: 825, Training Loss: 1.41619, LR: 0.0006947, Tokens/sec: 617535.58\n",
      "Step: 826, Training Loss: 1.39084, LR: 0.0006941, Tokens/sec: 617271.76\n",
      "Step: 827, Training Loss: 1.37686, LR: 0.0006934, Tokens/sec: 616610.73\n",
      "Step: 828, Training Loss: 1.33722, LR: 0.0006928, Tokens/sec: 617907.99\n",
      "Step: 829, Training Loss: 1.38163, LR: 0.0006921, Tokens/sec: 615907.42\n",
      "Step: 830, Training Loss: 1.41520, LR: 0.0006914, Tokens/sec: 617286.47\n",
      "Step: 831, Training Loss: 1.45648, LR: 0.0006908, Tokens/sec: 617455.04\n",
      "Step: 832, Training Loss: 1.39813, LR: 0.0006901, Tokens/sec: 617422.84\n",
      "Step: 833, Training Loss: 1.36275, LR: 0.0006895, Tokens/sec: 616139.25\n",
      "Step: 834, Training Loss: 1.30621, LR: 0.0006888, Tokens/sec: 616940.40\n",
      "Step: 835, Training Loss: 1.29414, LR: 0.0006881, Tokens/sec: 617584.82\n",
      "Step: 836, Training Loss: 1.30828, LR: 0.0006875, Tokens/sec: 616436.60\n",
      "Step: 837, Training Loss: 1.39742, LR: 0.0006868, Tokens/sec: 618897.56\n",
      "Step: 838, Training Loss: 1.39109, LR: 0.0006861, Tokens/sec: 617035.12\n",
      "Step: 839, Training Loss: 1.42326, LR: 0.0006855, Tokens/sec: 616778.42\n",
      "Step: 840, Training Loss: 1.40594, LR: 0.0006848, Tokens/sec: 617444.18\n",
      "Step: 841, Training Loss: 1.34678, LR: 0.0006842, Tokens/sec: 616631.51\n",
      "Step: 842, Training Loss: 1.41639, LR: 0.0006835, Tokens/sec: 617185.79\n",
      "Step: 843, Training Loss: 1.36167, LR: 0.0006828, Tokens/sec: 618349.11\n",
      "Step: 844, Training Loss: 1.31207, LR: 0.0006822, Tokens/sec: 616373.94\n",
      "Step: 845, Training Loss: 1.37762, LR: 0.0006815, Tokens/sec: 617841.81\n",
      "Step: 846, Training Loss: 1.35276, LR: 0.0006808, Tokens/sec: 617277.16\n",
      "Step: 847, Training Loss: 1.43213, LR: 0.0006802, Tokens/sec: 617283.94\n",
      "Step: 848, Training Loss: 1.43092, LR: 0.0006795, Tokens/sec: 616346.22\n",
      "Step: 849, Training Loss: 1.33302, LR: 0.0006788, Tokens/sec: 618358.74\n",
      "Step: 850, Training Loss: 1.31828, LR: 0.0006782, Tokens/sec: 617772.95\n",
      "Step: 851, Training Loss: 1.27365, LR: 0.0006775, Tokens/sec: 616652.29\n",
      "Step: 852, Training Loss: 1.36950, LR: 0.0006768, Tokens/sec: 616587.74\n",
      "Step: 853, Training Loss: 1.31745, LR: 0.0006762, Tokens/sec: 618244.94\n",
      "Step: 854, Training Loss: 1.29955, LR: 0.0006755, Tokens/sec: 616273.44\n",
      "Step: 855, Training Loss: 1.34360, LR: 0.0006748, Tokens/sec: 615946.36\n",
      "Step: 856, Training Loss: 1.32075, LR: 0.0006742, Tokens/sec: 617964.96\n",
      "Step: 857, Training Loss: 1.31885, LR: 0.0006735, Tokens/sec: 617114.27\n",
      "Step: 858, Training Loss: 1.20541, LR: 0.0006728, Tokens/sec: 617337.77\n",
      "Step: 859, Training Loss: 1.24677, LR: 0.0006722, Tokens/sec: 617540.12\n",
      "Step: 860, Training Loss: 1.36680, LR: 0.0006715, Tokens/sec: 617366.52\n",
      "Step: 861, Training Loss: 1.44064, LR: 0.0006708, Tokens/sec: 617842.36\n",
      "Step: 862, Training Loss: 1.33328, LR: 0.0006702, Tokens/sec: 618282.08\n",
      "Step: 863, Training Loss: 1.21049, LR: 0.0006695, Tokens/sec: 617768.53\n",
      "Step: 864, Training Loss: 1.20002, LR: 0.0006688, Tokens/sec: 616958.90\n",
      "Step: 865, Training Loss: 1.24175, LR: 0.0006681, Tokens/sec: 617911.86\n",
      "Step: 866, Training Loss: 1.36451, LR: 0.0006675, Tokens/sec: 617453.37\n",
      "Step: 867, Training Loss: 1.32343, LR: 0.0006668, Tokens/sec: 616464.21\n",
      "Step: 868, Training Loss: 1.36132, LR: 0.0006661, Tokens/sec: 617421.66\n",
      "Step: 869, Training Loss: 1.22592, LR: 0.0006655, Tokens/sec: 617910.62\n",
      "Step: 870, Training Loss: 1.20983, LR: 0.0006648, Tokens/sec: 616484.08\n",
      "Step: 871, Training Loss: 1.33628, LR: 0.0006641, Tokens/sec: 617886.01\n",
      "Step: 872, Training Loss: 1.45350, LR: 0.0006634, Tokens/sec: 615956.19\n",
      "Step: 873, Training Loss: 1.31985, LR: 0.0006628, Tokens/sec: 617316.76\n",
      "Step: 874, Training Loss: 1.35349, LR: 0.0006621, Tokens/sec: 617259.53\n",
      "Step: 875, Training Loss: 1.32917, LR: 0.0006614, Tokens/sec: 616731.03\n",
      "Step: 876, Training Loss: 1.33889, LR: 0.0006608, Tokens/sec: 617878.91\n",
      "Step: 877, Training Loss: 1.41609, LR: 0.0006601, Tokens/sec: 617439.53\n",
      "Step: 878, Training Loss: 1.37499, LR: 0.0006594, Tokens/sec: 616358.55\n",
      "Step: 879, Training Loss: 1.40655, LR: 0.0006587, Tokens/sec: 617092.31\n",
      "Step: 880, Training Loss: 1.43010, LR: 0.0006581, Tokens/sec: 617204.38\n",
      "Step: 881, Training Loss: 1.39460, LR: 0.0006574, Tokens/sec: 616243.80\n",
      "Step: 882, Training Loss: 1.36575, LR: 0.0006567, Tokens/sec: 616902.69\n",
      "Step: 883, Training Loss: 1.36194, LR: 0.0006560, Tokens/sec: 616230.07\n",
      "Step: 884, Training Loss: 1.35293, LR: 0.0006554, Tokens/sec: 617721.15\n",
      "Step: 885, Training Loss: 1.40143, LR: 0.0006547, Tokens/sec: 615778.41\n",
      "Step: 886, Training Loss: 1.55510, LR: 0.0006540, Tokens/sec: 616955.03\n",
      "Step: 887, Training Loss: 1.44797, LR: 0.0006533, Tokens/sec: 616916.07\n",
      "Step: 888, Training Loss: 1.34077, LR: 0.0006526, Tokens/sec: 617482.25\n",
      "Step: 889, Training Loss: 1.43474, LR: 0.0006520, Tokens/sec: 617759.31\n",
      "Step: 890, Training Loss: 1.37456, LR: 0.0006513, Tokens/sec: 616617.31\n",
      "Step: 891, Training Loss: 1.26626, LR: 0.0006506, Tokens/sec: 617292.72\n",
      "Step: 892, Training Loss: 1.25985, LR: 0.0006499, Tokens/sec: 616630.13\n",
      "Step: 893, Training Loss: 1.38507, LR: 0.0006493, Tokens/sec: 616674.05\n",
      "Step: 894, Training Loss: 1.37373, LR: 0.0006486, Tokens/sec: 617421.50\n",
      "Step: 895, Training Loss: 1.33715, LR: 0.0006479, Tokens/sec: 617988.21\n",
      "Step: 896, Training Loss: 1.21678, LR: 0.0006472, Tokens/sec: 617077.40\n",
      "Step: 897, Training Loss: 1.16809, LR: 0.0006465, Tokens/sec: 617152.87\n",
      "Step: 898, Training Loss: 1.28345, LR: 0.0006459, Tokens/sec: 616825.75\n",
      "Step: 899, Training Loss: 1.30324, LR: 0.0006452, Tokens/sec: 616917.72\n",
      "Step: 900, Training Loss: 1.25570, LR: 0.0006445, Tokens/sec: 617101.76\n",
      "Step: 901, Training Loss: 1.31297, LR: 0.0006438, Tokens/sec: 616849.45\n",
      "Step: 902, Training Loss: 1.24471, LR: 0.0006432, Tokens/sec: 617206.78\n",
      "Step: 903, Training Loss: 1.19610, LR: 0.0006425, Tokens/sec: 617306.98\n",
      "Step: 904, Training Loss: 1.27447, LR: 0.0006418, Tokens/sec: 617921.58\n",
      "Step: 905, Training Loss: 1.23934, LR: 0.0006411, Tokens/sec: 617222.05\n",
      "Step: 906, Training Loss: 1.33889, LR: 0.0006404, Tokens/sec: 617612.44\n",
      "Step: 907, Training Loss: 1.21997, LR: 0.0006397, Tokens/sec: 617153.27\n",
      "Step: 908, Training Loss: 1.25478, LR: 0.0006391, Tokens/sec: 617002.31\n",
      "Step: 909, Training Loss: 1.25245, LR: 0.0006384, Tokens/sec: 616512.84\n",
      "Step: 910, Training Loss: 1.12163, LR: 0.0006377, Tokens/sec: 617580.70\n",
      "Step: 911, Training Loss: 1.24922, LR: 0.0006370, Tokens/sec: 617525.81\n",
      "Step: 912, Training Loss: 1.26290, LR: 0.0006363, Tokens/sec: 616369.05\n",
      "Step: 913, Training Loss: 1.19877, LR: 0.0006357, Tokens/sec: 617465.09\n",
      "Step: 914, Training Loss: 1.20417, LR: 0.0006350, Tokens/sec: 616608.99\n",
      "Step: 915, Training Loss: 1.22217, LR: 0.0006343, Tokens/sec: 615606.76\n",
      "Step: 916, Training Loss: 1.18782, LR: 0.0006336, Tokens/sec: 616600.25\n",
      "Step: 917, Training Loss: 1.23281, LR: 0.0006329, Tokens/sec: 617207.54\n",
      "Step: 918, Training Loss: 1.35696, LR: 0.0006322, Tokens/sec: 617921.40\n",
      "Step: 919, Training Loss: 1.34786, LR: 0.0006316, Tokens/sec: 615872.94\n",
      "Step: 920, Training Loss: 1.30875, LR: 0.0006309, Tokens/sec: 616988.01\n",
      "Step: 921, Training Loss: 1.31313, LR: 0.0006302, Tokens/sec: 615991.44\n",
      "Step: 922, Training Loss: 1.32516, LR: 0.0006295, Tokens/sec: 617253.95\n",
      "Step: 923, Training Loss: 1.69651, LR: 0.0006288, Tokens/sec: 616229.44\n",
      "Step: 924, Training Loss: 1.29563, LR: 0.0006281, Tokens/sec: 617392.89\n",
      "Step: 925, Training Loss: 1.26425, LR: 0.0006275, Tokens/sec: 616645.31\n",
      "Step: 926, Training Loss: 1.26377, LR: 0.0006268, Tokens/sec: 618754.15\n",
      "Step: 927, Training Loss: 1.28610, LR: 0.0006261, Tokens/sec: 617584.42\n",
      "Step: 928, Training Loss: 1.25653, LR: 0.0006254, Tokens/sec: 616709.51\n",
      "Step: 929, Training Loss: 1.27505, LR: 0.0006247, Tokens/sec: 617141.37\n",
      "Step: 930, Training Loss: 1.29862, LR: 0.0006240, Tokens/sec: 616397.31\n",
      "Step: 931, Training Loss: 1.24433, LR: 0.0006233, Tokens/sec: 617544.63\n",
      "Step: 932, Training Loss: 1.29728, LR: 0.0006227, Tokens/sec: 615885.30\n",
      "Step: 933, Training Loss: 1.34560, LR: 0.0006220, Tokens/sec: 617548.93\n",
      "Step: 934, Training Loss: 1.26445, LR: 0.0006213, Tokens/sec: 617242.63\n",
      "Step: 935, Training Loss: 1.26239, LR: 0.0006206, Tokens/sec: 616755.27\n",
      "Step: 936, Training Loss: 1.23704, LR: 0.0006199, Tokens/sec: 618130.26\n",
      "Step: 937, Training Loss: 1.04756, LR: 0.0006192, Tokens/sec: 616515.24\n",
      "Step: 938, Training Loss: 1.09463, LR: 0.0006185, Tokens/sec: 617271.97\n",
      "Step: 939, Training Loss: 1.26546, LR: 0.0006179, Tokens/sec: 616931.88\n",
      "Step: 940, Training Loss: 1.07314, LR: 0.0006172, Tokens/sec: 617639.43\n",
      "Step: 941, Training Loss: 1.23559, LR: 0.0006165, Tokens/sec: 618177.26\n",
      "Step: 942, Training Loss: 1.14690, LR: 0.0006158, Tokens/sec: 615796.48\n",
      "Step: 943, Training Loss: 1.23688, LR: 0.0006151, Tokens/sec: 616336.57\n",
      "Step: 944, Training Loss: 1.22776, LR: 0.0006144, Tokens/sec: 615325.37\n",
      "Step: 945, Training Loss: 1.25663, LR: 0.0006137, Tokens/sec: 617776.47\n",
      "Step: 946, Training Loss: 1.21117, LR: 0.0006130, Tokens/sec: 617291.72\n",
      "Step: 947, Training Loss: 1.60848, LR: 0.0006124, Tokens/sec: 616658.64\n",
      "Step: 948, Training Loss: 1.23825, LR: 0.0006117, Tokens/sec: 617481.07\n",
      "Step: 949, Training Loss: 1.20771, LR: 0.0006110, Tokens/sec: 616585.79\n",
      "Step: 950, Training Loss: 1.23787, LR: 0.0006103, Tokens/sec: 617727.59\n",
      "Step: 951, Training Loss: 1.28338, LR: 0.0006096, Tokens/sec: 617084.77\n",
      "Step: 952, Training Loss: 1.30941, LR: 0.0006089, Tokens/sec: 617025.40\n",
      "Step: 953, Training Loss: 1.22587, LR: 0.0006082, Tokens/sec: 616856.51\n",
      "Step: 954, Training Loss: 1.31472, LR: 0.0006075, Tokens/sec: 614231.08\n",
      "Step: 955, Training Loss: 1.27382, LR: 0.0006068, Tokens/sec: 617133.84\n",
      "Step: 956, Training Loss: 1.21935, LR: 0.0006062, Tokens/sec: 617710.49\n",
      "Step: 957, Training Loss: 1.25538, LR: 0.0006055, Tokens/sec: 616613.84\n",
      "Step: 958, Training Loss: 1.22757, LR: 0.0006048, Tokens/sec: 616811.43\n",
      "Step: 959, Training Loss: 1.24068, LR: 0.0006041, Tokens/sec: 617313.27\n",
      "Step: 960, Training Loss: 1.32687, LR: 0.0006034, Tokens/sec: 616598.99\n",
      "Step: 961, Training Loss: 1.26938, LR: 0.0006027, Tokens/sec: 617494.13\n",
      "Step: 962, Training Loss: 1.25911, LR: 0.0006020, Tokens/sec: 615556.14\n",
      "Step: 963, Training Loss: 1.19344, LR: 0.0006013, Tokens/sec: 618511.42\n",
      "Step: 964, Training Loss: 1.32275, LR: 0.0006006, Tokens/sec: 616294.79\n",
      "Step: 965, Training Loss: 1.30398, LR: 0.0005999, Tokens/sec: 616204.52\n",
      "Step: 966, Training Loss: 1.29338, LR: 0.0005992, Tokens/sec: 616508.17\n",
      "Step: 967, Training Loss: 1.22934, LR: 0.0005986, Tokens/sec: 616985.17\n",
      "Step: 968, Training Loss: 1.27094, LR: 0.0005979, Tokens/sec: 617482.57\n",
      "Step: 969, Training Loss: 1.30366, LR: 0.0005972, Tokens/sec: 616548.26\n",
      "Step: 970, Training Loss: 1.22805, LR: 0.0005965, Tokens/sec: 616002.13\n",
      "Step: 971, Training Loss: 1.26471, LR: 0.0005958, Tokens/sec: 617069.70\n",
      "Step: 972, Training Loss: 1.24141, LR: 0.0005951, Tokens/sec: 615939.13\n",
      "Step: 973, Training Loss: 1.32501, LR: 0.0005944, Tokens/sec: 616810.13\n",
      "Step: 974, Training Loss: 1.28548, LR: 0.0005937, Tokens/sec: 617852.97\n",
      "Step: 975, Training Loss: 1.24894, LR: 0.0005930, Tokens/sec: 617585.02\n",
      "Step: 976, Training Loss: 1.23482, LR: 0.0005923, Tokens/sec: 617027.92\n",
      "Step: 977, Training Loss: 1.24774, LR: 0.0005916, Tokens/sec: 616577.74\n",
      "Step: 978, Training Loss: 1.29315, LR: 0.0005910, Tokens/sec: 616683.66\n",
      "Step: 979, Training Loss: 1.28260, LR: 0.0005903, Tokens/sec: 617303.36\n",
      "Step: 980, Training Loss: 1.24987, LR: 0.0005896, Tokens/sec: 617590.34\n",
      "Step: 981, Training Loss: 1.26027, LR: 0.0005889, Tokens/sec: 616713.64\n",
      "Step: 982, Training Loss: 1.27245, LR: 0.0005882, Tokens/sec: 616879.98\n",
      "Step: 983, Training Loss: 1.23581, LR: 0.0005875, Tokens/sec: 616718.53\n",
      "Step: 984, Training Loss: 1.44996, LR: 0.0005868, Tokens/sec: 615859.39\n",
      "Step: 985, Training Loss: 1.41274, LR: 0.0005861, Tokens/sec: 617174.86\n",
      "Step: 986, Training Loss: 1.44200, LR: 0.0005854, Tokens/sec: 616278.54\n",
      "Step: 987, Training Loss: 1.41286, LR: 0.0005847, Tokens/sec: 616750.93\n",
      "Step: 988, Training Loss: 1.30280, LR: 0.0005840, Tokens/sec: 614953.08\n",
      "Step: 989, Training Loss: 1.23950, LR: 0.0005833, Tokens/sec: 616785.78\n",
      "Step: 990, Training Loss: 1.27419, LR: 0.0005826, Tokens/sec: 616456.01\n",
      "Step: 991, Training Loss: 1.29034, LR: 0.0005819, Tokens/sec: 617317.84\n",
      "Step: 992, Training Loss: 1.27802, LR: 0.0005813, Tokens/sec: 618444.21\n",
      "Step: 993, Training Loss: 1.25917, LR: 0.0005806, Tokens/sec: 617293.70\n",
      "Step: 994, Training Loss: 1.27778, LR: 0.0005799, Tokens/sec: 617004.86\n",
      "Step: 995, Training Loss: 1.26713, LR: 0.0005792, Tokens/sec: 616975.61\n",
      "Step: 996, Training Loss: 1.25164, LR: 0.0005785, Tokens/sec: 616683.16\n",
      "Step: 997, Training Loss: 1.25234, LR: 0.0005778, Tokens/sec: 616042.05\n",
      "Step: 998, Training Loss: 1.28284, LR: 0.0005771, Tokens/sec: 617760.37\n",
      "Step: 999, Training Loss: 1.28201, LR: 0.0005764, Tokens/sec: 617254.05\n",
      "Step: 1000, Training Loss: 1.30204, LR: 0.0005757, Tokens/sec: 616421.83\n",
      "Computing Eval loss, steps: 21\n",
      "Step: 1000, Eval Loss: 1.29715\n",
      "Step: 1001, Training Loss: 1.49169, LR: 0.0005750, Tokens/sec: 615851.31\n",
      "Step: 1002, Training Loss: 1.41884, LR: 0.0005743, Tokens/sec: 616993.77\n",
      "Step: 1003, Training Loss: 1.42889, LR: 0.0005736, Tokens/sec: 618430.59\n",
      "Step: 1004, Training Loss: 1.42885, LR: 0.0005729, Tokens/sec: 616467.23\n",
      "Step: 1005, Training Loss: 1.44128, LR: 0.0005722, Tokens/sec: 616908.03\n",
      "Step: 1006, Training Loss: 1.47015, LR: 0.0005715, Tokens/sec: 617240.40\n",
      "Step: 1007, Training Loss: 1.37189, LR: 0.0005708, Tokens/sec: 616933.28\n",
      "Step: 1008, Training Loss: 1.41687, LR: 0.0005701, Tokens/sec: 617953.04\n",
      "Step: 1009, Training Loss: 1.43760, LR: 0.0005695, Tokens/sec: 617867.56\n",
      "Step: 1010, Training Loss: 1.44475, LR: 0.0005688, Tokens/sec: 617694.01\n",
      "Step: 1011, Training Loss: 1.42950, LR: 0.0005681, Tokens/sec: 616267.21\n",
      "Step: 1012, Training Loss: 1.37163, LR: 0.0005674, Tokens/sec: 616680.70\n",
      "Step: 1013, Training Loss: 1.41879, LR: 0.0005667, Tokens/sec: 618376.20\n",
      "Step: 1014, Training Loss: 1.40024, LR: 0.0005660, Tokens/sec: 616700.33\n",
      "Step: 1015, Training Loss: 1.38296, LR: 0.0005653, Tokens/sec: 616191.01\n",
      "Step: 1016, Training Loss: 1.38858, LR: 0.0005646, Tokens/sec: 616494.59\n",
      "Step: 1017, Training Loss: 1.34665, LR: 0.0005639, Tokens/sec: 615495.43\n",
      "Step: 1018, Training Loss: 1.41004, LR: 0.0005632, Tokens/sec: 617178.08\n",
      "Step: 1019, Training Loss: 1.40572, LR: 0.0005625, Tokens/sec: 616240.26\n",
      "Step: 1020, Training Loss: 1.41880, LR: 0.0005618, Tokens/sec: 617068.71\n",
      "Step: 1021, Training Loss: 1.32628, LR: 0.0005611, Tokens/sec: 616641.86\n",
      "Step: 1022, Training Loss: 1.39755, LR: 0.0005604, Tokens/sec: 618630.52\n",
      "Step: 1023, Training Loss: 1.39166, LR: 0.0005597, Tokens/sec: 615466.15\n",
      "Step: 1024, Training Loss: 1.41870, LR: 0.0005590, Tokens/sec: 616748.12\n",
      "Step: 1025, Training Loss: 1.38714, LR: 0.0005583, Tokens/sec: 616181.33\n",
      "Step: 1026, Training Loss: 1.37249, LR: 0.0005576, Tokens/sec: 617652.22\n",
      "Step: 1027, Training Loss: 1.34329, LR: 0.0005570, Tokens/sec: 617684.37\n",
      "Step: 1028, Training Loss: 1.28736, LR: 0.0005563, Tokens/sec: 617711.74\n",
      "Step: 1029, Training Loss: 1.23919, LR: 0.0005556, Tokens/sec: 617664.41\n",
      "Step: 1030, Training Loss: 1.26761, LR: 0.0005549, Tokens/sec: 614661.55\n",
      "Step: 1031, Training Loss: 1.24586, LR: 0.0005542, Tokens/sec: 616211.50\n",
      "Step: 1032, Training Loss: 1.34025, LR: 0.0005535, Tokens/sec: 616037.67\n",
      "Step: 1033, Training Loss: 1.33602, LR: 0.0005528, Tokens/sec: 617425.99\n",
      "Step: 1034, Training Loss: 1.29092, LR: 0.0005521, Tokens/sec: 617631.32\n",
      "Step: 1035, Training Loss: 1.26016, LR: 0.0005514, Tokens/sec: 615559.09\n",
      "Step: 1036, Training Loss: 1.27122, LR: 0.0005507, Tokens/sec: 616366.90\n",
      "Step: 1037, Training Loss: 1.27601, LR: 0.0005500, Tokens/sec: 616075.70\n",
      "Step: 1038, Training Loss: 1.30217, LR: 0.0005493, Tokens/sec: 618041.72\n",
      "Step: 1039, Training Loss: 1.29298, LR: 0.0005486, Tokens/sec: 616330.00\n",
      "Step: 1040, Training Loss: 1.29612, LR: 0.0005479, Tokens/sec: 616798.67\n",
      "Step: 1041, Training Loss: 1.31700, LR: 0.0005472, Tokens/sec: 615931.09\n",
      "Step: 1042, Training Loss: 1.27082, LR: 0.0005465, Tokens/sec: 616295.81\n",
      "Step: 1043, Training Loss: 1.27885, LR: 0.0005458, Tokens/sec: 617843.23\n",
      "Step: 1044, Training Loss: 1.25936, LR: 0.0005451, Tokens/sec: 617450.83\n",
      "Step: 1045, Training Loss: 1.29077, LR: 0.0005444, Tokens/sec: 616891.22\n",
      "Step: 1046, Training Loss: 1.24867, LR: 0.0005437, Tokens/sec: 616560.10\n",
      "Step: 1047, Training Loss: 1.27826, LR: 0.0005430, Tokens/sec: 617527.51\n",
      "Step: 1048, Training Loss: 1.23131, LR: 0.0005424, Tokens/sec: 616861.40\n",
      "Step: 1049, Training Loss: 1.27717, LR: 0.0005417, Tokens/sec: 617233.93\n",
      "Step: 1050, Training Loss: 1.23001, LR: 0.0005410, Tokens/sec: 617252.70\n",
      "Step: 1051, Training Loss: 1.21798, LR: 0.0005403, Tokens/sec: 616576.95\n",
      "Step: 1052, Training Loss: 1.29330, LR: 0.0005396, Tokens/sec: 615761.30\n",
      "Step: 1053, Training Loss: 1.32117, LR: 0.0005389, Tokens/sec: 615538.52\n",
      "Step: 1054, Training Loss: 1.31406, LR: 0.0005382, Tokens/sec: 616952.43\n",
      "Step: 1055, Training Loss: 1.30904, LR: 0.0005375, Tokens/sec: 616959.39\n",
      "Step: 1056, Training Loss: 1.35793, LR: 0.0005368, Tokens/sec: 616495.04\n",
      "Step: 1057, Training Loss: 1.35537, LR: 0.0005361, Tokens/sec: 616169.95\n",
      "Step: 1058, Training Loss: 1.43431, LR: 0.0005354, Tokens/sec: 617543.09\n",
      "Step: 1059, Training Loss: 1.31790, LR: 0.0005347, Tokens/sec: 617023.39\n",
      "Step: 1060, Training Loss: 1.28012, LR: 0.0005340, Tokens/sec: 616564.17\n",
      "Step: 1061, Training Loss: 1.30520, LR: 0.0005333, Tokens/sec: 617412.01\n",
      "Step: 1062, Training Loss: 1.30253, LR: 0.0005326, Tokens/sec: 616877.12\n",
      "Step: 1063, Training Loss: 1.22100, LR: 0.0005319, Tokens/sec: 616933.23\n",
      "Step: 1064, Training Loss: 1.33922, LR: 0.0005312, Tokens/sec: 618108.88\n",
      "Step: 1065, Training Loss: 1.30124, LR: 0.0005305, Tokens/sec: 616840.82\n",
      "Step: 1066, Training Loss: 1.38324, LR: 0.0005299, Tokens/sec: 617191.12\n",
      "Step: 1067, Training Loss: 1.32569, LR: 0.0005292, Tokens/sec: 617170.61\n",
      "Step: 1068, Training Loss: 1.33442, LR: 0.0005285, Tokens/sec: 616949.57\n",
      "Step: 1069, Training Loss: 1.30181, LR: 0.0005278, Tokens/sec: 618200.20\n",
      "Step: 1070, Training Loss: 1.20054, LR: 0.0005271, Tokens/sec: 616712.93\n",
      "Step: 1071, Training Loss: 1.27702, LR: 0.0005264, Tokens/sec: 617842.26\n",
      "Step: 1072, Training Loss: 1.26896, LR: 0.0005257, Tokens/sec: 617763.53\n",
      "Step: 1073, Training Loss: 1.24766, LR: 0.0005250, Tokens/sec: 616931.25\n",
      "Step: 1074, Training Loss: 1.22187, LR: 0.0005243, Tokens/sec: 617499.58\n",
      "Step: 1075, Training Loss: 1.31284, LR: 0.0005236, Tokens/sec: 616777.70\n",
      "Step: 1076, Training Loss: 1.37695, LR: 0.0005229, Tokens/sec: 616538.05\n",
      "Step: 1077, Training Loss: 1.34229, LR: 0.0005222, Tokens/sec: 616739.56\n",
      "Step: 1078, Training Loss: 1.28298, LR: 0.0005215, Tokens/sec: 617377.79\n",
      "Step: 1079, Training Loss: 1.26682, LR: 0.0005208, Tokens/sec: 617865.69\n",
      "Step: 1080, Training Loss: 1.33309, LR: 0.0005201, Tokens/sec: 617846.21\n",
      "Step: 1081, Training Loss: 1.29105, LR: 0.0005194, Tokens/sec: 617601.68\n",
      "Step: 1082, Training Loss: 1.30567, LR: 0.0005187, Tokens/sec: 615991.96\n",
      "Step: 1083, Training Loss: 1.41273, LR: 0.0005181, Tokens/sec: 616813.79\n",
      "Step: 1084, Training Loss: 1.33441, LR: 0.0005174, Tokens/sec: 617835.84\n",
      "Step: 1085, Training Loss: 1.40510, LR: 0.0005167, Tokens/sec: 616156.30\n",
      "Step: 1086, Training Loss: 1.45565, LR: 0.0005160, Tokens/sec: 615875.67\n",
      "Step: 1087, Training Loss: 1.33005, LR: 0.0005153, Tokens/sec: 616418.23\n",
      "Step: 1088, Training Loss: 1.40149, LR: 0.0005146, Tokens/sec: 616857.01\n",
      "Step: 1089, Training Loss: 1.34024, LR: 0.0005139, Tokens/sec: 616729.95\n",
      "Step: 1090, Training Loss: 1.30575, LR: 0.0005132, Tokens/sec: 615364.28\n",
      "Step: 1091, Training Loss: 1.26387, LR: 0.0005125, Tokens/sec: 616625.89\n",
      "Step: 1092, Training Loss: 1.43203, LR: 0.0005118, Tokens/sec: 617326.41\n",
      "Step: 1093, Training Loss: 1.28283, LR: 0.0005111, Tokens/sec: 615710.26\n",
      "Step: 1094, Training Loss: 1.39636, LR: 0.0005104, Tokens/sec: 617002.72\n",
      "Step: 1095, Training Loss: 1.35049, LR: 0.0005097, Tokens/sec: 617964.53\n",
      "Step: 1096, Training Loss: 1.27981, LR: 0.0005090, Tokens/sec: 615934.05\n",
      "Step: 1097, Training Loss: 1.30782, LR: 0.0005084, Tokens/sec: 615939.61\n",
      "Step: 1098, Training Loss: 1.30208, LR: 0.0005077, Tokens/sec: 616698.82\n",
      "Step: 1099, Training Loss: 1.37922, LR: 0.0005070, Tokens/sec: 616455.01\n",
      "Step: 1100, Training Loss: 1.33016, LR: 0.0005063, Tokens/sec: 616557.51\n",
      "Step: 1101, Training Loss: 1.33242, LR: 0.0005056, Tokens/sec: 616492.91\n",
      "Step: 1102, Training Loss: 1.25578, LR: 0.0005049, Tokens/sec: 616337.35\n",
      "Step: 1103, Training Loss: 1.36016, LR: 0.0005042, Tokens/sec: 617596.40\n",
      "Step: 1104, Training Loss: 1.42382, LR: 0.0005035, Tokens/sec: 617946.16\n",
      "Step: 1105, Training Loss: 1.31628, LR: 0.0005028, Tokens/sec: 616973.61\n",
      "Step: 1106, Training Loss: 1.30831, LR: 0.0005021, Tokens/sec: 617534.85\n",
      "Step: 1107, Training Loss: 1.30378, LR: 0.0005014, Tokens/sec: 616899.04\n",
      "Step: 1108, Training Loss: 1.29018, LR: 0.0005008, Tokens/sec: 616886.14\n",
      "Step: 1109, Training Loss: 1.27509, LR: 0.0005001, Tokens/sec: 617221.65\n",
      "Step: 1110, Training Loss: 1.33394, LR: 0.0004994, Tokens/sec: 616590.56\n",
      "Step: 1111, Training Loss: 1.31123, LR: 0.0004987, Tokens/sec: 617150.63\n",
      "Step: 1112, Training Loss: 1.30396, LR: 0.0004980, Tokens/sec: 615320.95\n",
      "Step: 1113, Training Loss: 1.32288, LR: 0.0004973, Tokens/sec: 617371.81\n",
      "Step: 1114, Training Loss: 1.33281, LR: 0.0004966, Tokens/sec: 617524.81\n",
      "Step: 1115, Training Loss: 1.36580, LR: 0.0004959, Tokens/sec: 617205.63\n",
      "Step: 1116, Training Loss: 1.24882, LR: 0.0004952, Tokens/sec: 615314.90\n",
      "Step: 1117, Training Loss: 1.28191, LR: 0.0004945, Tokens/sec: 616824.54\n",
      "Step: 1118, Training Loss: 1.31329, LR: 0.0004938, Tokens/sec: 617845.30\n",
      "Step: 1119, Training Loss: 1.17350, LR: 0.0004932, Tokens/sec: 614158.48\n",
      "Step: 1120, Training Loss: 0.86709, LR: 0.0004925, Tokens/sec: 615408.18\n",
      "Step: 1121, Training Loss: 1.09873, LR: 0.0004918, Tokens/sec: 617287.26\n",
      "Step: 1122, Training Loss: 1.09106, LR: 0.0004911, Tokens/sec: 617530.47\n",
      "Step: 1123, Training Loss: 1.04485, LR: 0.0004904, Tokens/sec: 617078.21\n",
      "Step: 1124, Training Loss: 1.33160, LR: 0.0004897, Tokens/sec: 617582.29\n",
      "Step: 1125, Training Loss: 1.31032, LR: 0.0004890, Tokens/sec: 616554.37\n",
      "Step: 1126, Training Loss: 1.31499, LR: 0.0004883, Tokens/sec: 616439.56\n",
      "Step: 1127, Training Loss: 1.32624, LR: 0.0004876, Tokens/sec: 617711.39\n",
      "Step: 1128, Training Loss: 1.26162, LR: 0.0004870, Tokens/sec: 616872.45\n",
      "Step: 1129, Training Loss: 1.31723, LR: 0.0004863, Tokens/sec: 616968.75\n",
      "Step: 1130, Training Loss: 1.22489, LR: 0.0004856, Tokens/sec: 616308.18\n",
      "Step: 1131, Training Loss: 1.29216, LR: 0.0004849, Tokens/sec: 616455.04\n",
      "Step: 1132, Training Loss: 1.32365, LR: 0.0004842, Tokens/sec: 617280.36\n",
      "Step: 1133, Training Loss: 1.19055, LR: 0.0004835, Tokens/sec: 617209.44\n",
      "Step: 1134, Training Loss: 1.31210, LR: 0.0004828, Tokens/sec: 616186.53\n",
      "Step: 1135, Training Loss: 1.24336, LR: 0.0004821, Tokens/sec: 615906.55\n",
      "Step: 1136, Training Loss: 1.24552, LR: 0.0004815, Tokens/sec: 615765.94\n",
      "Step: 1137, Training Loss: 1.23169, LR: 0.0004808, Tokens/sec: 616276.76\n",
      "Step: 1138, Training Loss: 1.26034, LR: 0.0004801, Tokens/sec: 616825.21\n",
      "Step: 1139, Training Loss: 1.24508, LR: 0.0004794, Tokens/sec: 616062.62\n",
      "Step: 1140, Training Loss: 1.26802, LR: 0.0004787, Tokens/sec: 614056.22\n",
      "Step: 1141, Training Loss: 1.21702, LR: 0.0004780, Tokens/sec: 616289.39\n",
      "Step: 1142, Training Loss: 1.33421, LR: 0.0004773, Tokens/sec: 616565.02\n",
      "Step: 1143, Training Loss: 1.37052, LR: 0.0004767, Tokens/sec: 616405.90\n",
      "Step: 1144, Training Loss: 1.29176, LR: 0.0004760, Tokens/sec: 616434.79\n",
      "Step: 1145, Training Loss: 1.45325, LR: 0.0004753, Tokens/sec: 617454.65\n",
      "Step: 1146, Training Loss: 1.33102, LR: 0.0004746, Tokens/sec: 616618.56\n",
      "Step: 1147, Training Loss: 1.31460, LR: 0.0004739, Tokens/sec: 616389.10\n",
      "Step: 1148, Training Loss: 1.28852, LR: 0.0004732, Tokens/sec: 616253.03\n",
      "Step: 1149, Training Loss: 1.28628, LR: 0.0004725, Tokens/sec: 616513.23\n",
      "Step: 1150, Training Loss: 1.41398, LR: 0.0004719, Tokens/sec: 616769.01\n",
      "Step: 1151, Training Loss: 1.26558, LR: 0.0004712, Tokens/sec: 616689.33\n",
      "Step: 1152, Training Loss: 1.27285, LR: 0.0004705, Tokens/sec: 617451.50\n",
      "Step: 1153, Training Loss: 1.27299, LR: 0.0004698, Tokens/sec: 616274.33\n",
      "Step: 1154, Training Loss: 1.63456, LR: 0.0004691, Tokens/sec: 616309.57\n",
      "Step: 1155, Training Loss: 1.34773, LR: 0.0004684, Tokens/sec: 617769.23\n",
      "Step: 1156, Training Loss: 1.33211, LR: 0.0004678, Tokens/sec: 616287.54\n",
      "Step: 1157, Training Loss: 1.30333, LR: 0.0004671, Tokens/sec: 617201.29\n",
      "Step: 1158, Training Loss: 1.33530, LR: 0.0004664, Tokens/sec: 614118.71\n",
      "Step: 1159, Training Loss: 1.25566, LR: 0.0004657, Tokens/sec: 615921.77\n",
      "Step: 1160, Training Loss: 1.46106, LR: 0.0004650, Tokens/sec: 616900.36\n",
      "Step: 1161, Training Loss: 1.31734, LR: 0.0004643, Tokens/sec: 616726.81\n",
      "Step: 1162, Training Loss: 1.57922, LR: 0.0004637, Tokens/sec: 616116.32\n",
      "Step: 1163, Training Loss: 1.37412, LR: 0.0004630, Tokens/sec: 617343.73\n",
      "Step: 1164, Training Loss: 1.31230, LR: 0.0004623, Tokens/sec: 615983.85\n",
      "Step: 1165, Training Loss: 1.36180, LR: 0.0004616, Tokens/sec: 616473.89\n",
      "Step: 1166, Training Loss: 1.24040, LR: 0.0004609, Tokens/sec: 615663.99\n",
      "Step: 1167, Training Loss: 1.24381, LR: 0.0004603, Tokens/sec: 616079.48\n",
      "Step: 1168, Training Loss: 1.52003, LR: 0.0004596, Tokens/sec: 617461.29\n",
      "Step: 1169, Training Loss: 1.23160, LR: 0.0004589, Tokens/sec: 616711.80\n",
      "Step: 1170, Training Loss: 1.23163, LR: 0.0004582, Tokens/sec: 617571.38\n",
      "Step: 1171, Training Loss: 1.21681, LR: 0.0004575, Tokens/sec: 617465.87\n",
      "Step: 1172, Training Loss: 1.39511, LR: 0.0004568, Tokens/sec: 616992.76\n",
      "Step: 1173, Training Loss: 1.22857, LR: 0.0004562, Tokens/sec: 617841.91\n",
      "Step: 1174, Training Loss: 1.47800, LR: 0.0004555, Tokens/sec: 616045.38\n",
      "Step: 1175, Training Loss: 1.35214, LR: 0.0004548, Tokens/sec: 617020.90\n",
      "Step: 1176, Training Loss: 1.34114, LR: 0.0004541, Tokens/sec: 616465.59\n",
      "Step: 1177, Training Loss: 1.22252, LR: 0.0004535, Tokens/sec: 616042.09\n",
      "Step: 1178, Training Loss: 1.18377, LR: 0.0004528, Tokens/sec: 615344.40\n",
      "Step: 1179, Training Loss: 1.15646, LR: 0.0004521, Tokens/sec: 616614.07\n",
      "Step: 1180, Training Loss: 1.25486, LR: 0.0004514, Tokens/sec: 615910.73\n",
      "Step: 1181, Training Loss: 1.14941, LR: 0.0004507, Tokens/sec: 617061.99\n",
      "Step: 1182, Training Loss: 1.04832, LR: 0.0004501, Tokens/sec: 617372.28\n",
      "Step: 1183, Training Loss: 1.03651, LR: 0.0004494, Tokens/sec: 617167.56\n",
      "Step: 1184, Training Loss: 1.12468, LR: 0.0004487, Tokens/sec: 617304.91\n",
      "Step: 1185, Training Loss: 1.27032, LR: 0.0004480, Tokens/sec: 616127.16\n",
      "Step: 1186, Training Loss: 1.07495, LR: 0.0004474, Tokens/sec: 615319.02\n",
      "Step: 1187, Training Loss: 1.23058, LR: 0.0004467, Tokens/sec: 616083.46\n",
      "Step: 1188, Training Loss: 1.31062, LR: 0.0004460, Tokens/sec: 617444.33\n",
      "Step: 1189, Training Loss: 1.39296, LR: 0.0004453, Tokens/sec: 616592.63\n",
      "Step: 1190, Training Loss: 1.31816, LR: 0.0004446, Tokens/sec: 617099.31\n",
      "Step: 1191, Training Loss: 1.35789, LR: 0.0004440, Tokens/sec: 616033.53\n",
      "Step: 1192, Training Loss: 1.44654, LR: 0.0004433, Tokens/sec: 618173.61\n",
      "Step: 1193, Training Loss: 1.34812, LR: 0.0004426, Tokens/sec: 616542.47\n",
      "Step: 1194, Training Loss: 1.37855, LR: 0.0004419, Tokens/sec: 616850.44\n",
      "Step: 1195, Training Loss: 1.30566, LR: 0.0004413, Tokens/sec: 617826.19\n",
      "Step: 1196, Training Loss: 1.38670, LR: 0.0004406, Tokens/sec: 617377.50\n",
      "Step: 1197, Training Loss: 1.39379, LR: 0.0004399, Tokens/sec: 617608.29\n",
      "Step: 1198, Training Loss: 1.45473, LR: 0.0004392, Tokens/sec: 615271.20\n",
      "Step: 1199, Training Loss: 1.24494, LR: 0.0004386, Tokens/sec: 616046.70\n",
      "Step: 1200, Training Loss: 1.34492, LR: 0.0004379, Tokens/sec: 617486.68\n",
      "Step: 1201, Training Loss: 1.39182, LR: 0.0004372, Tokens/sec: 615285.94\n",
      "Step: 1202, Training Loss: 1.27607, LR: 0.0004366, Tokens/sec: 616264.21\n",
      "Step: 1203, Training Loss: 1.37609, LR: 0.0004359, Tokens/sec: 617591.06\n",
      "Step: 1204, Training Loss: 1.41071, LR: 0.0004352, Tokens/sec: 617233.17\n",
      "Step: 1205, Training Loss: 1.27643, LR: 0.0004345, Tokens/sec: 616297.05\n",
      "Step: 1206, Training Loss: 1.41782, LR: 0.0004339, Tokens/sec: 616040.67\n",
      "Step: 1207, Training Loss: 1.27412, LR: 0.0004332, Tokens/sec: 616510.78\n",
      "Step: 1208, Training Loss: 1.32903, LR: 0.0004325, Tokens/sec: 616474.57\n",
      "Step: 1209, Training Loss: 1.22679, LR: 0.0004319, Tokens/sec: 616440.37\n",
      "Step: 1210, Training Loss: 1.26052, LR: 0.0004312, Tokens/sec: 616225.97\n",
      "Step: 1211, Training Loss: 1.31597, LR: 0.0004305, Tokens/sec: 617208.45\n",
      "Step: 1212, Training Loss: 1.28949, LR: 0.0004298, Tokens/sec: 617960.97\n",
      "Step: 1213, Training Loss: 1.31485, LR: 0.0004292, Tokens/sec: 617269.08\n",
      "Step: 1214, Training Loss: 1.29580, LR: 0.0004285, Tokens/sec: 618310.54\n",
      "Step: 1215, Training Loss: 1.34385, LR: 0.0004278, Tokens/sec: 617823.05\n",
      "Step: 1216, Training Loss: 1.26843, LR: 0.0004272, Tokens/sec: 616662.94\n",
      "Step: 1217, Training Loss: 1.40137, LR: 0.0004265, Tokens/sec: 616137.94\n",
      "Step: 1218, Training Loss: 1.34244, LR: 0.0004258, Tokens/sec: 616552.38\n",
      "Step: 1219, Training Loss: 1.29921, LR: 0.0004252, Tokens/sec: 617669.40\n",
      "Step: 1220, Training Loss: 1.21953, LR: 0.0004245, Tokens/sec: 617583.99\n",
      "Step: 1221, Training Loss: 1.90174, LR: 0.0004238, Tokens/sec: 616845.92\n",
      "Step: 1222, Training Loss: 1.36179, LR: 0.0004232, Tokens/sec: 617802.72\n",
      "Step: 1223, Training Loss: 1.32696, LR: 0.0004225, Tokens/sec: 617232.89\n",
      "Step: 1224, Training Loss: 1.45524, LR: 0.0004218, Tokens/sec: 616535.13\n",
      "Step: 1225, Training Loss: 1.38897, LR: 0.0004212, Tokens/sec: 617985.74\n",
      "Step: 1226, Training Loss: 1.28559, LR: 0.0004205, Tokens/sec: 615059.55\n",
      "Step: 1227, Training Loss: 1.30081, LR: 0.0004198, Tokens/sec: 615333.82\n",
      "Step: 1228, Training Loss: 1.28158, LR: 0.0004192, Tokens/sec: 616418.00\n",
      "Step: 1229, Training Loss: 1.40469, LR: 0.0004185, Tokens/sec: 617136.83\n",
      "Step: 1230, Training Loss: 1.70307, LR: 0.0004178, Tokens/sec: 617261.31\n",
      "Step: 1231, Training Loss: 1.51457, LR: 0.0004172, Tokens/sec: 617330.75\n",
      "Step: 1232, Training Loss: 1.43603, LR: 0.0004165, Tokens/sec: 617533.72\n",
      "Step: 1233, Training Loss: 1.36204, LR: 0.0004158, Tokens/sec: 617838.77\n",
      "Step: 1234, Training Loss: 1.42486, LR: 0.0004152, Tokens/sec: 616072.62\n",
      "Step: 1235, Training Loss: 1.37057, LR: 0.0004145, Tokens/sec: 616650.00\n",
      "Step: 1236, Training Loss: 1.28512, LR: 0.0004139, Tokens/sec: 617506.42\n",
      "Step: 1237, Training Loss: 1.32008, LR: 0.0004132, Tokens/sec: 616978.53\n",
      "Step: 1238, Training Loss: 1.49729, LR: 0.0004125, Tokens/sec: 616427.50\n",
      "Step: 1239, Training Loss: 1.45951, LR: 0.0004119, Tokens/sec: 616677.74\n",
      "Step: 1240, Training Loss: 1.29334, LR: 0.0004112, Tokens/sec: 616783.84\n",
      "Step: 1241, Training Loss: 1.28137, LR: 0.0004105, Tokens/sec: 615717.63\n",
      "Step: 1242, Training Loss: 1.18025, LR: 0.0004099, Tokens/sec: 616785.89\n",
      "Step: 1243, Training Loss: 1.33679, LR: 0.0004092, Tokens/sec: 617858.38\n",
      "Step: 1244, Training Loss: 1.21068, LR: 0.0004086, Tokens/sec: 616969.75\n",
      "Step: 1245, Training Loss: 1.24098, LR: 0.0004079, Tokens/sec: 616818.31\n",
      "Step: 1246, Training Loss: 1.25348, LR: 0.0004072, Tokens/sec: 615787.96\n",
      "Step: 1247, Training Loss: 1.15624, LR: 0.0004066, Tokens/sec: 616645.66\n",
      "Step: 1248, Training Loss: 1.22975, LR: 0.0004059, Tokens/sec: 617465.60\n",
      "Step: 1249, Training Loss: 1.38246, LR: 0.0004053, Tokens/sec: 616301.87\n",
      "Step: 1250, Training Loss: 1.23316, LR: 0.0004046, Tokens/sec: 616472.11\n",
      "Step: 1251, Training Loss: 2.84482, LR: 0.0004040, Tokens/sec: 617254.79\n",
      "Step: 1252, Training Loss: 3.08103, LR: 0.0004033, Tokens/sec: 617731.77\n",
      "Step: 1253, Training Loss: 1.35446, LR: 0.0004026, Tokens/sec: 616668.40\n",
      "Step: 1254, Training Loss: 1.27039, LR: 0.0004020, Tokens/sec: 616454.28\n",
      "Step: 1255, Training Loss: 1.28251, LR: 0.0004013, Tokens/sec: 617248.89\n",
      "Step: 1256, Training Loss: 1.22720, LR: 0.0004007, Tokens/sec: 617242.22\n",
      "Step: 1257, Training Loss: 1.28122, LR: 0.0004000, Tokens/sec: 617549.20\n",
      "Step: 1258, Training Loss: 1.24136, LR: 0.0003994, Tokens/sec: 617159.72\n",
      "Step: 1259, Training Loss: 1.30496, LR: 0.0003987, Tokens/sec: 617548.78\n",
      "Step: 1260, Training Loss: 1.16977, LR: 0.0003981, Tokens/sec: 617286.16\n",
      "Step: 1261, Training Loss: 1.01878, LR: 0.0003974, Tokens/sec: 616554.18\n",
      "Step: 1262, Training Loss: 0.96960, LR: 0.0003967, Tokens/sec: 615646.10\n",
      "Step: 1263, Training Loss: 1.00599, LR: 0.0003961, Tokens/sec: 616610.44\n",
      "Step: 1264, Training Loss: 1.21418, LR: 0.0003954, Tokens/sec: 617500.24\n",
      "Step: 1265, Training Loss: 1.35098, LR: 0.0003948, Tokens/sec: 616329.28\n",
      "Step: 1266, Training Loss: 1.36295, LR: 0.0003941, Tokens/sec: 616647.39\n",
      "Step: 1267, Training Loss: 1.34302, LR: 0.0003935, Tokens/sec: 617262.32\n",
      "Step: 1268, Training Loss: 1.35703, LR: 0.0003928, Tokens/sec: 615500.21\n",
      "Step: 1269, Training Loss: 1.38545, LR: 0.0003922, Tokens/sec: 615838.10\n",
      "Step: 1270, Training Loss: 1.30222, LR: 0.0003915, Tokens/sec: 616432.86\n",
      "Step: 1271, Training Loss: 1.33281, LR: 0.0003909, Tokens/sec: 616372.64\n",
      "Step: 1272, Training Loss: 1.36594, LR: 0.0003902, Tokens/sec: 616328.88\n",
      "Step: 1273, Training Loss: 1.24390, LR: 0.0003896, Tokens/sec: 617055.46\n",
      "Step: 1274, Training Loss: 1.22316, LR: 0.0003889, Tokens/sec: 617165.73\n",
      "Step: 1275, Training Loss: 1.37803, LR: 0.0003883, Tokens/sec: 618162.29\n",
      "Step: 1276, Training Loss: 1.25124, LR: 0.0003876, Tokens/sec: 617204.16\n",
      "Step: 1277, Training Loss: 1.24776, LR: 0.0003870, Tokens/sec: 617285.22\n",
      "Step: 1278, Training Loss: 1.24207, LR: 0.0003863, Tokens/sec: 615861.23\n",
      "Step: 1279, Training Loss: 1.25821, LR: 0.0003857, Tokens/sec: 617310.00\n",
      "Step: 1280, Training Loss: 1.25396, LR: 0.0003850, Tokens/sec: 616856.89\n",
      "Step: 1281, Training Loss: 1.20974, LR: 0.0003844, Tokens/sec: 616686.54\n",
      "Step: 1282, Training Loss: 1.28183, LR: 0.0003837, Tokens/sec: 618090.58\n",
      "Step: 1283, Training Loss: 1.14811, LR: 0.0003831, Tokens/sec: 617377.36\n",
      "Step: 1284, Training Loss: 1.18639, LR: 0.0003825, Tokens/sec: 616561.61\n",
      "Step: 1285, Training Loss: 0.98184, LR: 0.0003818, Tokens/sec: 617184.60\n",
      "Step: 1286, Training Loss: 1.14068, LR: 0.0003812, Tokens/sec: 616588.33\n",
      "Step: 1287, Training Loss: 1.21730, LR: 0.0003805, Tokens/sec: 615721.05\n",
      "Step: 1288, Training Loss: 1.14873, LR: 0.0003799, Tokens/sec: 617086.61\n",
      "Step: 1289, Training Loss: 1.24544, LR: 0.0003792, Tokens/sec: 617483.91\n",
      "Step: 1290, Training Loss: 0.96066, LR: 0.0003786, Tokens/sec: 616535.89\n",
      "Step: 1291, Training Loss: 1.05580, LR: 0.0003780, Tokens/sec: 616371.70\n",
      "Step: 1292, Training Loss: 1.08884, LR: 0.0003773, Tokens/sec: 618020.13\n",
      "Step: 1293, Training Loss: 0.91327, LR: 0.0003767, Tokens/sec: 616646.03\n",
      "Step: 1294, Training Loss: 0.81476, LR: 0.0003760, Tokens/sec: 616718.32\n",
      "Step: 1295, Training Loss: 0.70472, LR: 0.0003754, Tokens/sec: 617077.88\n",
      "Step: 1296, Training Loss: 0.61225, LR: 0.0003747, Tokens/sec: 617378.27\n",
      "Step: 1297, Training Loss: 0.69152, LR: 0.0003741, Tokens/sec: 618412.49\n",
      "Step: 1298, Training Loss: 0.68992, LR: 0.0003735, Tokens/sec: 616240.26\n",
      "Step: 1299, Training Loss: 1.17959, LR: 0.0003728, Tokens/sec: 617656.47\n",
      "Step: 1300, Training Loss: 1.34191, LR: 0.0003722, Tokens/sec: 616820.86\n",
      "Step: 1301, Training Loss: 1.28829, LR: 0.0003716, Tokens/sec: 617561.82\n",
      "Step: 1302, Training Loss: 1.34651, LR: 0.0003709, Tokens/sec: 615978.16\n",
      "Step: 1303, Training Loss: 1.37491, LR: 0.0003703, Tokens/sec: 616805.75\n",
      "Step: 1304, Training Loss: 1.27114, LR: 0.0003696, Tokens/sec: 617012.53\n",
      "Step: 1305, Training Loss: 1.29140, LR: 0.0003690, Tokens/sec: 616551.89\n",
      "Step: 1306, Training Loss: 1.36221, LR: 0.0003684, Tokens/sec: 617239.81\n",
      "Step: 1307, Training Loss: 1.42492, LR: 0.0003677, Tokens/sec: 615982.42\n",
      "Step: 1308, Training Loss: 1.34895, LR: 0.0003671, Tokens/sec: 617807.99\n",
      "Step: 1309, Training Loss: 1.37251, LR: 0.0003665, Tokens/sec: 618117.33\n",
      "Step: 1310, Training Loss: 1.28018, LR: 0.0003658, Tokens/sec: 616975.86\n",
      "Step: 1311, Training Loss: 1.32552, LR: 0.0003652, Tokens/sec: 617541.62\n",
      "Step: 1312, Training Loss: 1.34103, LR: 0.0003646, Tokens/sec: 616931.88\n",
      "Step: 1313, Training Loss: 1.27211, LR: 0.0003639, Tokens/sec: 616600.46\n",
      "Step: 1314, Training Loss: 1.43776, LR: 0.0003633, Tokens/sec: 616661.21\n",
      "Step: 1315, Training Loss: 1.32855, LR: 0.0003627, Tokens/sec: 616944.02\n",
      "Step: 1316, Training Loss: 1.31620, LR: 0.0003620, Tokens/sec: 615401.02\n",
      "Step: 1317, Training Loss: 1.32775, LR: 0.0003614, Tokens/sec: 618389.05\n",
      "Step: 1318, Training Loss: 1.34680, LR: 0.0003608, Tokens/sec: 616520.19\n",
      "Step: 1319, Training Loss: 1.30299, LR: 0.0003601, Tokens/sec: 616463.47\n",
      "Step: 1320, Training Loss: 1.28377, LR: 0.0003595, Tokens/sec: 618147.46\n",
      "Step: 1321, Training Loss: 1.33458, LR: 0.0003589, Tokens/sec: 616319.63\n",
      "Step: 1322, Training Loss: 1.29193, LR: 0.0003582, Tokens/sec: 617091.19\n",
      "Step: 1323, Training Loss: 1.27759, LR: 0.0003576, Tokens/sec: 616700.10\n",
      "Step: 1324, Training Loss: 1.25026, LR: 0.0003570, Tokens/sec: 615215.33\n",
      "Step: 1325, Training Loss: 1.31861, LR: 0.0003564, Tokens/sec: 617311.55\n",
      "Step: 1326, Training Loss: 1.20743, LR: 0.0003557, Tokens/sec: 616949.70\n",
      "Step: 1327, Training Loss: 1.28253, LR: 0.0003551, Tokens/sec: 617049.92\n",
      "Step: 1328, Training Loss: 1.32173, LR: 0.0003545, Tokens/sec: 617857.70\n",
      "Step: 1329, Training Loss: 1.21184, LR: 0.0003539, Tokens/sec: 616210.46\n",
      "Step: 1330, Training Loss: 1.30622, LR: 0.0003532, Tokens/sec: 616639.97\n",
      "Step: 1331, Training Loss: 1.23192, LR: 0.0003526, Tokens/sec: 617051.26\n",
      "Step: 1332, Training Loss: 1.23141, LR: 0.0003520, Tokens/sec: 617238.25\n",
      "Step: 1333, Training Loss: 1.30536, LR: 0.0003514, Tokens/sec: 616724.30\n",
      "Step: 1334, Training Loss: 1.51087, LR: 0.0003507, Tokens/sec: 616297.18\n",
      "Step: 1335, Training Loss: 1.43496, LR: 0.0003501, Tokens/sec: 616599.21\n",
      "Step: 1336, Training Loss: 1.34022, LR: 0.0003495, Tokens/sec: 617159.08\n",
      "Step: 1337, Training Loss: 1.26698, LR: 0.0003489, Tokens/sec: 618351.05\n",
      "Step: 1338, Training Loss: 1.29421, LR: 0.0003482, Tokens/sec: 616262.12\n",
      "Step: 1339, Training Loss: 1.36822, LR: 0.0003476, Tokens/sec: 616805.29\n",
      "Step: 1340, Training Loss: 1.33921, LR: 0.0003470, Tokens/sec: 617481.88\n",
      "Step: 1341, Training Loss: 1.37092, LR: 0.0003464, Tokens/sec: 616861.53\n",
      "Step: 1342, Training Loss: 1.29410, LR: 0.0003458, Tokens/sec: 616762.49\n",
      "Step: 1343, Training Loss: 1.31275, LR: 0.0003451, Tokens/sec: 617998.91\n",
      "Step: 1344, Training Loss: 1.27310, LR: 0.0003445, Tokens/sec: 617227.79\n",
      "Step: 1345, Training Loss: 1.53119, LR: 0.0003439, Tokens/sec: 616514.29\n",
      "Step: 1346, Training Loss: 1.35066, LR: 0.0003433, Tokens/sec: 617104.16\n",
      "Step: 1347, Training Loss: 1.29779, LR: 0.0003427, Tokens/sec: 616228.68\n",
      "Step: 1348, Training Loss: 1.25175, LR: 0.0003421, Tokens/sec: 615150.46\n",
      "Step: 1349, Training Loss: 1.31332, LR: 0.0003414, Tokens/sec: 618046.73\n",
      "Step: 1350, Training Loss: 1.23644, LR: 0.0003408, Tokens/sec: 616513.89\n",
      "Step: 1351, Training Loss: 1.28759, LR: 0.0003402, Tokens/sec: 616217.63\n",
      "Step: 1352, Training Loss: 1.31655, LR: 0.0003396, Tokens/sec: 616740.98\n",
      "Step: 1353, Training Loss: 1.20568, LR: 0.0003390, Tokens/sec: 616713.66\n",
      "Step: 1354, Training Loss: 1.27169, LR: 0.0003384, Tokens/sec: 616523.39\n",
      "Step: 1355, Training Loss: 1.29533, LR: 0.0003378, Tokens/sec: 618012.25\n",
      "Step: 1356, Training Loss: 1.24691, LR: 0.0003371, Tokens/sec: 615620.35\n",
      "Step: 1357, Training Loss: 1.33698, LR: 0.0003365, Tokens/sec: 617350.73\n",
      "Step: 1358, Training Loss: 1.30646, LR: 0.0003359, Tokens/sec: 617229.01\n",
      "Step: 1359, Training Loss: 1.30865, LR: 0.0003353, Tokens/sec: 615784.18\n",
      "Step: 1360, Training Loss: 1.29784, LR: 0.0003347, Tokens/sec: 617397.37\n",
      "Step: 1361, Training Loss: 1.26031, LR: 0.0003341, Tokens/sec: 616363.30\n",
      "Step: 1362, Training Loss: 1.30734, LR: 0.0003335, Tokens/sec: 615930.82\n",
      "Step: 1363, Training Loss: 1.25048, LR: 0.0003329, Tokens/sec: 617716.79\n",
      "Step: 1364, Training Loss: 1.31823, LR: 0.0003323, Tokens/sec: 617658.23\n",
      "Step: 1365, Training Loss: 1.27294, LR: 0.0003317, Tokens/sec: 617855.83\n",
      "Step: 1366, Training Loss: 1.25721, LR: 0.0003310, Tokens/sec: 616941.06\n",
      "Step: 1367, Training Loss: 1.23246, LR: 0.0003304, Tokens/sec: 616017.27\n",
      "Step: 1368, Training Loss: 1.32393, LR: 0.0003298, Tokens/sec: 616542.64\n",
      "Step: 1369, Training Loss: 1.28812, LR: 0.0003292, Tokens/sec: 617990.45\n",
      "Step: 1370, Training Loss: 1.23416, LR: 0.0003286, Tokens/sec: 616318.67\n",
      "Step: 1371, Training Loss: 1.27899, LR: 0.0003280, Tokens/sec: 617754.27\n",
      "Step: 1372, Training Loss: 1.31262, LR: 0.0003274, Tokens/sec: 616247.92\n",
      "Step: 1373, Training Loss: 1.27582, LR: 0.0003268, Tokens/sec: 617374.95\n",
      "Step: 1374, Training Loss: 1.26421, LR: 0.0003262, Tokens/sec: 616622.09\n",
      "Step: 1375, Training Loss: 1.30258, LR: 0.0003256, Tokens/sec: 617824.91\n",
      "Step: 1376, Training Loss: 1.19883, LR: 0.0003250, Tokens/sec: 616151.58\n",
      "Step: 1377, Training Loss: 1.29371, LR: 0.0003244, Tokens/sec: 616878.65\n",
      "Step: 1378, Training Loss: 1.22358, LR: 0.0003238, Tokens/sec: 616232.48\n",
      "Step: 1379, Training Loss: 1.26273, LR: 0.0003232, Tokens/sec: 615541.78\n",
      "Step: 1380, Training Loss: 1.27965, LR: 0.0003226, Tokens/sec: 616704.27\n",
      "Step: 1381, Training Loss: 1.30883, LR: 0.0003220, Tokens/sec: 617537.03\n",
      "Step: 1382, Training Loss: 1.22418, LR: 0.0003214, Tokens/sec: 617259.18\n",
      "Step: 1383, Training Loss: 1.21589, LR: 0.0003208, Tokens/sec: 616900.17\n",
      "Step: 1384, Training Loss: 1.26511, LR: 0.0003202, Tokens/sec: 616244.78\n",
      "Step: 1385, Training Loss: 1.23521, LR: 0.0003196, Tokens/sec: 615484.87\n",
      "Step: 1386, Training Loss: 1.24427, LR: 0.0003190, Tokens/sec: 616740.38\n",
      "Step: 1387, Training Loss: 1.26319, LR: 0.0003184, Tokens/sec: 616683.07\n",
      "Step: 1388, Training Loss: 1.21584, LR: 0.0003178, Tokens/sec: 617293.41\n",
      "Step: 1389, Training Loss: 1.21960, LR: 0.0003172, Tokens/sec: 617464.62\n",
      "Step: 1390, Training Loss: 1.20880, LR: 0.0003166, Tokens/sec: 616747.09\n",
      "Step: 1391, Training Loss: 1.20683, LR: 0.0003160, Tokens/sec: 616902.70\n",
      "Step: 1392, Training Loss: 1.27028, LR: 0.0003154, Tokens/sec: 616487.76\n",
      "Step: 1393, Training Loss: 1.22337, LR: 0.0003148, Tokens/sec: 617465.37\n",
      "Step: 1394, Training Loss: 1.24581, LR: 0.0003143, Tokens/sec: 616688.06\n",
      "Step: 1395, Training Loss: 1.21082, LR: 0.0003137, Tokens/sec: 616043.26\n",
      "Step: 1396, Training Loss: 1.24129, LR: 0.0003131, Tokens/sec: 618261.04\n",
      "Step: 1397, Training Loss: 1.23045, LR: 0.0003125, Tokens/sec: 617391.90\n",
      "Step: 1398, Training Loss: 1.31267, LR: 0.0003119, Tokens/sec: 617001.78\n",
      "Step: 1399, Training Loss: 1.30400, LR: 0.0003113, Tokens/sec: 617637.94\n",
      "Step: 1400, Training Loss: 1.22666, LR: 0.0003107, Tokens/sec: 617002.04\n",
      "Step: 1401, Training Loss: 1.23241, LR: 0.0003101, Tokens/sec: 617509.36\n",
      "Step: 1402, Training Loss: 1.21092, LR: 0.0003095, Tokens/sec: 615359.52\n",
      "Step: 1403, Training Loss: 1.19475, LR: 0.0003089, Tokens/sec: 616338.21\n",
      "Step: 1404, Training Loss: 1.28061, LR: 0.0003084, Tokens/sec: 615832.87\n",
      "Step: 1405, Training Loss: 1.24170, LR: 0.0003078, Tokens/sec: 617038.88\n",
      "Step: 1406, Training Loss: 1.25168, LR: 0.0003072, Tokens/sec: 617178.65\n",
      "Step: 1407, Training Loss: 1.27736, LR: 0.0003066, Tokens/sec: 616330.93\n",
      "Step: 1408, Training Loss: 1.22209, LR: 0.0003060, Tokens/sec: 616471.85\n",
      "Step: 1409, Training Loss: 1.33814, LR: 0.0003054, Tokens/sec: 617021.31\n",
      "Step: 1410, Training Loss: 1.20732, LR: 0.0003049, Tokens/sec: 615693.49\n",
      "Step: 1411, Training Loss: 1.18410, LR: 0.0003043, Tokens/sec: 616301.19\n",
      "Step: 1412, Training Loss: 1.22788, LR: 0.0003037, Tokens/sec: 617336.29\n",
      "Step: 1413, Training Loss: 1.19270, LR: 0.0003031, Tokens/sec: 616702.89\n",
      "Step: 1414, Training Loss: 1.18694, LR: 0.0003025, Tokens/sec: 616161.34\n",
      "Step: 1415, Training Loss: 1.44840, LR: 0.0003019, Tokens/sec: 617103.24\n",
      "Step: 1416, Training Loss: 1.19145, LR: 0.0003014, Tokens/sec: 617112.24\n",
      "Step: 1417, Training Loss: 1.21507, LR: 0.0003008, Tokens/sec: 616902.78\n",
      "Step: 1418, Training Loss: 1.21816, LR: 0.0003002, Tokens/sec: 617658.40\n",
      "Step: 1419, Training Loss: 1.37015, LR: 0.0002996, Tokens/sec: 617888.19\n",
      "Step: 1420, Training Loss: 1.18385, LR: 0.0002991, Tokens/sec: 617206.95\n",
      "Step: 1421, Training Loss: 1.19467, LR: 0.0002985, Tokens/sec: 617379.42\n",
      "Step: 1422, Training Loss: 1.27528, LR: 0.0002979, Tokens/sec: 616427.50\n",
      "Step: 1423, Training Loss: 1.27744, LR: 0.0002973, Tokens/sec: 615890.52\n",
      "Step: 1424, Training Loss: 1.28022, LR: 0.0002968, Tokens/sec: 617337.81\n",
      "Step: 1425, Training Loss: 1.32284, LR: 0.0002962, Tokens/sec: 616590.37\n",
      "Step: 1426, Training Loss: 1.25467, LR: 0.0002956, Tokens/sec: 616854.80\n",
      "Step: 1427, Training Loss: 1.26923, LR: 0.0002950, Tokens/sec: 617385.55\n",
      "Step: 1428, Training Loss: 1.31441, LR: 0.0002945, Tokens/sec: 617163.53\n",
      "Step: 1429, Training Loss: 1.24306, LR: 0.0002939, Tokens/sec: 617614.95\n",
      "Step: 1430, Training Loss: 1.23668, LR: 0.0002933, Tokens/sec: 617584.36\n",
      "Step: 1431, Training Loss: 1.19721, LR: 0.0002927, Tokens/sec: 617006.53\n",
      "Step: 1432, Training Loss: 1.23042, LR: 0.0002922, Tokens/sec: 617228.16\n",
      "Step: 1433, Training Loss: 1.22981, LR: 0.0002916, Tokens/sec: 617027.25\n",
      "Step: 1434, Training Loss: 1.35275, LR: 0.0002910, Tokens/sec: 616573.59\n",
      "Step: 1435, Training Loss: 1.24253, LR: 0.0002905, Tokens/sec: 616903.59\n",
      "Step: 1436, Training Loss: 1.20883, LR: 0.0002899, Tokens/sec: 616536.71\n",
      "Step: 1437, Training Loss: 1.23145, LR: 0.0002893, Tokens/sec: 616977.33\n",
      "Step: 1438, Training Loss: 1.18857, LR: 0.0002888, Tokens/sec: 617049.48\n",
      "Step: 1439, Training Loss: 1.32280, LR: 0.0002882, Tokens/sec: 616105.08\n",
      "Step: 1440, Training Loss: 1.20716, LR: 0.0002876, Tokens/sec: 616858.89\n",
      "Step: 1441, Training Loss: 1.21567, LR: 0.0002871, Tokens/sec: 615605.11\n",
      "Step: 1442, Training Loss: 1.31794, LR: 0.0002865, Tokens/sec: 617239.20\n",
      "Step: 1443, Training Loss: 1.19867, LR: 0.0002859, Tokens/sec: 617500.83\n",
      "Step: 1444, Training Loss: 1.28553, LR: 0.0002854, Tokens/sec: 616902.10\n",
      "Step: 1445, Training Loss: 1.21089, LR: 0.0002848, Tokens/sec: 617237.44\n",
      "Step: 1446, Training Loss: 1.20950, LR: 0.0002843, Tokens/sec: 617393.65\n",
      "Step: 1447, Training Loss: 1.22073, LR: 0.0002837, Tokens/sec: 616906.08\n",
      "Step: 1448, Training Loss: 1.19028, LR: 0.0002831, Tokens/sec: 615882.45\n",
      "Step: 1449, Training Loss: 1.20669, LR: 0.0002826, Tokens/sec: 616002.76\n",
      "Step: 1450, Training Loss: 1.22642, LR: 0.0002820, Tokens/sec: 617794.37\n",
      "Step: 1451, Training Loss: 1.23197, LR: 0.0002815, Tokens/sec: 617802.43\n",
      "Step: 1452, Training Loss: 1.18448, LR: 0.0002809, Tokens/sec: 616323.86\n",
      "Step: 1453, Training Loss: 1.17268, LR: 0.0002804, Tokens/sec: 616545.99\n",
      "Step: 1454, Training Loss: 1.17410, LR: 0.0002798, Tokens/sec: 616320.39\n",
      "Step: 1455, Training Loss: 1.17340, LR: 0.0002792, Tokens/sec: 616750.52\n",
      "Step: 1456, Training Loss: 1.23159, LR: 0.0002787, Tokens/sec: 615515.13\n",
      "Step: 1457, Training Loss: 1.19774, LR: 0.0002781, Tokens/sec: 616996.80\n",
      "Step: 1458, Training Loss: 1.22820, LR: 0.0002776, Tokens/sec: 616913.49\n",
      "Step: 1459, Training Loss: 1.23087, LR: 0.0002770, Tokens/sec: 617930.12\n",
      "Step: 1460, Training Loss: 1.21357, LR: 0.0002765, Tokens/sec: 615101.10\n",
      "Step: 1461, Training Loss: 1.23173, LR: 0.0002759, Tokens/sec: 616650.54\n",
      "Step: 1462, Training Loss: 1.20135, LR: 0.0002754, Tokens/sec: 615401.98\n",
      "Step: 1463, Training Loss: 1.27927, LR: 0.0002748, Tokens/sec: 616497.17\n",
      "Step: 1464, Training Loss: 1.33397, LR: 0.0002743, Tokens/sec: 616570.31\n",
      "Step: 1465, Training Loss: 1.19012, LR: 0.0002737, Tokens/sec: 617658.59\n",
      "Step: 1466, Training Loss: 1.20481, LR: 0.0002732, Tokens/sec: 617595.72\n",
      "Step: 1467, Training Loss: 1.20891, LR: 0.0002726, Tokens/sec: 615702.10\n",
      "Step: 1468, Training Loss: 1.19808, LR: 0.0002721, Tokens/sec: 616689.26\n",
      "Step: 1469, Training Loss: 1.23442, LR: 0.0002715, Tokens/sec: 617592.12\n",
      "Step: 1470, Training Loss: 1.18234, LR: 0.0002710, Tokens/sec: 616684.95\n",
      "Step: 1471, Training Loss: 1.15536, LR: 0.0002704, Tokens/sec: 616475.44\n",
      "Step: 1472, Training Loss: 1.23588, LR: 0.0002699, Tokens/sec: 618357.11\n",
      "Step: 1473, Training Loss: 1.29827, LR: 0.0002694, Tokens/sec: 615757.02\n",
      "Step: 1474, Training Loss: 1.22229, LR: 0.0002688, Tokens/sec: 616128.52\n",
      "Step: 1475, Training Loss: 1.17806, LR: 0.0002683, Tokens/sec: 617308.21\n",
      "Step: 1476, Training Loss: 1.19707, LR: 0.0002677, Tokens/sec: 617010.52\n",
      "Step: 1477, Training Loss: 1.17054, LR: 0.0002672, Tokens/sec: 617333.21\n",
      "Step: 1478, Training Loss: 1.34530, LR: 0.0002666, Tokens/sec: 617365.71\n",
      "Step: 1479, Training Loss: 1.22003, LR: 0.0002661, Tokens/sec: 616645.73\n",
      "Step: 1480, Training Loss: 1.35401, LR: 0.0002656, Tokens/sec: 616184.64\n",
      "Step: 1481, Training Loss: 1.16288, LR: 0.0002650, Tokens/sec: 618306.97\n",
      "Step: 1482, Training Loss: 1.15568, LR: 0.0002645, Tokens/sec: 615618.70\n",
      "Step: 1483, Training Loss: 1.22057, LR: 0.0002640, Tokens/sec: 617722.49\n",
      "Step: 1484, Training Loss: 1.19290, LR: 0.0002634, Tokens/sec: 617918.05\n",
      "Step: 1485, Training Loss: 1.19788, LR: 0.0002629, Tokens/sec: 617004.68\n",
      "Step: 1486, Training Loss: 1.15619, LR: 0.0002623, Tokens/sec: 616150.59\n",
      "Step: 1487, Training Loss: 1.12971, LR: 0.0002618, Tokens/sec: 616985.71\n",
      "Step: 1488, Training Loss: 1.14020, LR: 0.0002613, Tokens/sec: 616879.72\n",
      "Step: 1489, Training Loss: 1.15183, LR: 0.0002607, Tokens/sec: 617669.39\n",
      "Step: 1490, Training Loss: 1.15408, LR: 0.0002602, Tokens/sec: 615606.25\n",
      "Step: 1491, Training Loss: 1.20391, LR: 0.0002597, Tokens/sec: 617353.97\n",
      "Step: 1492, Training Loss: 1.16478, LR: 0.0002592, Tokens/sec: 615008.67\n",
      "Step: 1493, Training Loss: 1.16495, LR: 0.0002586, Tokens/sec: 617136.35\n",
      "Step: 1494, Training Loss: 1.16127, LR: 0.0002581, Tokens/sec: 616828.12\n",
      "Step: 1495, Training Loss: 1.14751, LR: 0.0002576, Tokens/sec: 616465.51\n",
      "Step: 1496, Training Loss: 1.14777, LR: 0.0002570, Tokens/sec: 617136.78\n",
      "Step: 1497, Training Loss: 1.13911, LR: 0.0002565, Tokens/sec: 617298.56\n",
      "Step: 1498, Training Loss: 1.22095, LR: 0.0002560, Tokens/sec: 616962.98\n",
      "Step: 1499, Training Loss: 1.14700, LR: 0.0002555, Tokens/sec: 617150.93\n",
      "Step: 1500, Training Loss: 1.15668, LR: 0.0002549, Tokens/sec: 616941.51\n",
      "Computing Eval loss, steps: 21\n",
      "Step: 1500, Eval Loss: 1.16098\n",
      "Step: 1501, Training Loss: 1.13073, LR: 0.0002544, Tokens/sec: 615557.48\n",
      "Step: 1502, Training Loss: 1.12170, LR: 0.0002539, Tokens/sec: 616706.42\n",
      "Step: 1503, Training Loss: 1.14722, LR: 0.0002534, Tokens/sec: 617139.66\n",
      "Step: 1504, Training Loss: 1.10825, LR: 0.0002528, Tokens/sec: 617844.32\n",
      "Step: 1505, Training Loss: 1.13520, LR: 0.0002523, Tokens/sec: 617278.42\n",
      "Step: 1506, Training Loss: 1.12454, LR: 0.0002518, Tokens/sec: 616558.85\n",
      "Step: 1507, Training Loss: 1.27747, LR: 0.0002513, Tokens/sec: 617061.00\n",
      "Step: 1508, Training Loss: 1.16465, LR: 0.0002508, Tokens/sec: 617972.51\n",
      "Step: 1509, Training Loss: 1.18575, LR: 0.0002502, Tokens/sec: 617160.86\n",
      "Step: 1510, Training Loss: 1.12601, LR: 0.0002497, Tokens/sec: 617281.10\n",
      "Step: 1511, Training Loss: 1.16668, LR: 0.0002492, Tokens/sec: 616673.45\n",
      "Step: 1512, Training Loss: 1.16004, LR: 0.0002487, Tokens/sec: 617193.24\n",
      "Step: 1513, Training Loss: 1.20992, LR: 0.0002482, Tokens/sec: 616719.94\n",
      "Step: 1514, Training Loss: 1.34030, LR: 0.0002477, Tokens/sec: 617302.53\n",
      "Step: 1515, Training Loss: 1.11903, LR: 0.0002471, Tokens/sec: 616262.69\n",
      "Step: 1516, Training Loss: 0.94055, LR: 0.0002466, Tokens/sec: 616128.22\n",
      "Step: 1517, Training Loss: 0.57413, LR: 0.0002461, Tokens/sec: 617153.84\n",
      "Step: 1518, Training Loss: 0.53641, LR: 0.0002456, Tokens/sec: 616724.54\n",
      "Step: 1519, Training Loss: 0.39090, LR: 0.0002451, Tokens/sec: 615728.07\n",
      "Step: 1520, Training Loss: 0.30340, LR: 0.0002446, Tokens/sec: 617173.12\n",
      "Step: 1521, Training Loss: 0.25823, LR: 0.0002441, Tokens/sec: 616581.41\n",
      "Step: 1522, Training Loss: 0.24192, LR: 0.0002436, Tokens/sec: 616022.35\n",
      "Step: 1523, Training Loss: 0.25731, LR: 0.0002430, Tokens/sec: 616595.99\n",
      "Step: 1524, Training Loss: 0.18409, LR: 0.0002425, Tokens/sec: 615960.43\n",
      "Step: 1525, Training Loss: 0.19435, LR: 0.0002420, Tokens/sec: 617722.73\n",
      "Step: 1526, Training Loss: 0.19141, LR: 0.0002415, Tokens/sec: 617129.27\n",
      "Step: 1527, Training Loss: 0.18406, LR: 0.0002410, Tokens/sec: 616431.79\n",
      "Step: 1528, Training Loss: 0.15206, LR: 0.0002405, Tokens/sec: 617018.76\n",
      "Step: 1529, Training Loss: 0.56068, LR: 0.0002400, Tokens/sec: 616600.29\n",
      "Step: 1530, Training Loss: 0.21975, LR: 0.0002395, Tokens/sec: 618018.67\n",
      "Step: 1531, Training Loss: 0.30436, LR: 0.0002390, Tokens/sec: 617041.29\n",
      "Step: 1532, Training Loss: 0.26393, LR: 0.0002385, Tokens/sec: 615762.74\n",
      "Step: 1533, Training Loss: 0.28881, LR: 0.0002380, Tokens/sec: 617298.50\n",
      "Step: 1534, Training Loss: 0.25265, LR: 0.0002375, Tokens/sec: 616785.82\n",
      "Step: 1535, Training Loss: 0.29745, LR: 0.0002370, Tokens/sec: 617298.81\n",
      "Step: 1536, Training Loss: 0.28036, LR: 0.0002365, Tokens/sec: 616445.55\n",
      "Step: 1537, Training Loss: 0.85205, LR: 0.0002360, Tokens/sec: 616386.22\n",
      "Step: 1538, Training Loss: 1.27452, LR: 0.0002355, Tokens/sec: 617814.91\n",
      "Step: 1539, Training Loss: 1.24912, LR: 0.0002350, Tokens/sec: 617190.59\n",
      "Step: 1540, Training Loss: 1.23486, LR: 0.0002345, Tokens/sec: 617235.36\n",
      "Step: 1541, Training Loss: 1.23616, LR: 0.0002340, Tokens/sec: 617097.18\n",
      "Step: 1542, Training Loss: 1.20555, LR: 0.0002335, Tokens/sec: 616335.35\n",
      "Step: 1543, Training Loss: 1.28264, LR: 0.0002330, Tokens/sec: 617777.75\n",
      "Step: 1544, Training Loss: 0.95346, LR: 0.0002325, Tokens/sec: 617627.33\n",
      "Step: 1545, Training Loss: 0.58344, LR: 0.0002320, Tokens/sec: 617693.06\n",
      "Step: 1546, Training Loss: 1.05081, LR: 0.0002316, Tokens/sec: 617355.56\n",
      "Step: 1547, Training Loss: 0.69550, LR: 0.0002311, Tokens/sec: 617125.20\n",
      "Step: 1548, Training Loss: 0.30934, LR: 0.0002306, Tokens/sec: 617565.64\n",
      "Step: 1549, Training Loss: 0.28810, LR: 0.0002301, Tokens/sec: 617013.83\n",
      "Step: 1550, Training Loss: 0.22397, LR: 0.0002296, Tokens/sec: 616630.77\n",
      "Step: 1551, Training Loss: 0.98999, LR: 0.0002291, Tokens/sec: 617569.40\n",
      "Step: 1552, Training Loss: 1.25626, LR: 0.0002286, Tokens/sec: 616110.56\n",
      "Step: 1553, Training Loss: 0.90584, LR: 0.0002281, Tokens/sec: 616880.40\n",
      "Step: 1554, Training Loss: 0.81783, LR: 0.0002277, Tokens/sec: 617150.69\n",
      "Step: 1555, Training Loss: 1.31317, LR: 0.0002272, Tokens/sec: 616436.68\n",
      "Step: 1556, Training Loss: 1.23714, LR: 0.0002267, Tokens/sec: 615829.07\n",
      "Step: 1557, Training Loss: 1.24574, LR: 0.0002262, Tokens/sec: 617457.59\n",
      "Step: 1558, Training Loss: 1.24803, LR: 0.0002257, Tokens/sec: 616018.60\n",
      "Step: 1559, Training Loss: 1.24319, LR: 0.0002252, Tokens/sec: 615128.72\n",
      "Step: 1560, Training Loss: 1.43639, LR: 0.0002248, Tokens/sec: 617701.93\n",
      "Step: 1561, Training Loss: 1.28878, LR: 0.0002243, Tokens/sec: 617860.02\n",
      "Step: 1562, Training Loss: 1.21142, LR: 0.0002238, Tokens/sec: 617336.31\n",
      "Step: 1563, Training Loss: 1.25154, LR: 0.0002233, Tokens/sec: 616642.84\n",
      "Step: 1564, Training Loss: 1.23763, LR: 0.0002228, Tokens/sec: 617316.95\n",
      "Step: 1565, Training Loss: 1.25067, LR: 0.0002224, Tokens/sec: 616572.54\n",
      "Step: 1566, Training Loss: 1.19851, LR: 0.0002219, Tokens/sec: 617024.88\n",
      "Step: 1567, Training Loss: 1.22455, LR: 0.0002214, Tokens/sec: 616755.08\n",
      "Step: 1568, Training Loss: 1.19437, LR: 0.0002209, Tokens/sec: 616158.19\n",
      "Step: 1569, Training Loss: 0.97674, LR: 0.0002205, Tokens/sec: 617572.91\n",
      "Step: 1570, Training Loss: 0.54125, LR: 0.0002200, Tokens/sec: 616917.94\n",
      "Step: 1571, Training Loss: 0.99214, LR: 0.0002195, Tokens/sec: 617506.84\n",
      "Step: 1572, Training Loss: 1.25258, LR: 0.0002190, Tokens/sec: 615787.40\n",
      "Step: 1573, Training Loss: 1.17926, LR: 0.0002186, Tokens/sec: 617746.77\n",
      "Step: 1574, Training Loss: 1.25377, LR: 0.0002181, Tokens/sec: 616253.41\n",
      "Step: 1575, Training Loss: 1.28448, LR: 0.0002176, Tokens/sec: 616105.35\n",
      "Step: 1576, Training Loss: 2.16198, LR: 0.0002172, Tokens/sec: 617562.07\n",
      "Step: 1577, Training Loss: 1.32521, LR: 0.0002167, Tokens/sec: 617193.25\n",
      "Step: 1578, Training Loss: 1.38415, LR: 0.0002162, Tokens/sec: 616244.92\n",
      "Step: 1579, Training Loss: 1.22900, LR: 0.0002158, Tokens/sec: 616556.50\n",
      "Step: 1580, Training Loss: 1.15398, LR: 0.0002153, Tokens/sec: 617317.67\n",
      "Step: 1581, Training Loss: 1.21567, LR: 0.0002148, Tokens/sec: 617371.75\n",
      "Step: 1582, Training Loss: 1.21828, LR: 0.0002144, Tokens/sec: 616811.61\n",
      "Step: 1583, Training Loss: 1.52502, LR: 0.0002139, Tokens/sec: 617026.02\n",
      "Step: 1584, Training Loss: 1.26232, LR: 0.0002135, Tokens/sec: 617588.65\n",
      "Step: 1585, Training Loss: 1.23891, LR: 0.0002130, Tokens/sec: 617392.63\n",
      "Step: 1586, Training Loss: 1.31545, LR: 0.0002125, Tokens/sec: 617216.48\n",
      "Step: 1587, Training Loss: 1.29378, LR: 0.0002121, Tokens/sec: 615249.26\n",
      "Step: 1588, Training Loss: 1.28723, LR: 0.0002116, Tokens/sec: 617032.45\n",
      "Step: 1589, Training Loss: 1.27293, LR: 0.0002112, Tokens/sec: 616670.49\n",
      "Step: 1590, Training Loss: 1.30198, LR: 0.0002107, Tokens/sec: 616021.00\n",
      "Step: 1591, Training Loss: 1.26744, LR: 0.0002102, Tokens/sec: 617015.25\n",
      "Step: 1592, Training Loss: 1.23542, LR: 0.0002098, Tokens/sec: 617959.99\n",
      "Step: 1593, Training Loss: 1.19987, LR: 0.0002093, Tokens/sec: 616819.92\n",
      "Step: 1594, Training Loss: 1.26668, LR: 0.0002089, Tokens/sec: 616984.60\n",
      "Step: 1595, Training Loss: 1.26569, LR: 0.0002084, Tokens/sec: 615845.50\n",
      "Step: 1596, Training Loss: 1.23407, LR: 0.0002080, Tokens/sec: 617608.55\n",
      "Step: 1597, Training Loss: 1.20918, LR: 0.0002075, Tokens/sec: 616587.17\n",
      "Step: 1598, Training Loss: 1.27233, LR: 0.0002071, Tokens/sec: 617206.27\n",
      "Step: 1599, Training Loss: 1.19792, LR: 0.0002066, Tokens/sec: 617054.04\n",
      "Step: 1600, Training Loss: 1.27408, LR: 0.0002062, Tokens/sec: 615941.13\n",
      "Step: 1601, Training Loss: 1.21679, LR: 0.0002057, Tokens/sec: 616810.84\n",
      "Step: 1602, Training Loss: 1.20616, LR: 0.0002053, Tokens/sec: 617400.96\n",
      "Step: 1603, Training Loss: 1.21107, LR: 0.0002048, Tokens/sec: 618070.88\n",
      "Step: 1604, Training Loss: 1.20875, LR: 0.0002044, Tokens/sec: 616896.46\n",
      "Step: 1605, Training Loss: 1.19620, LR: 0.0002039, Tokens/sec: 617976.73\n",
      "Step: 1606, Training Loss: 1.19570, LR: 0.0002035, Tokens/sec: 615939.36\n",
      "Step: 1607, Training Loss: 1.21117, LR: 0.0002031, Tokens/sec: 616873.39\n",
      "Step: 1608, Training Loss: 1.21638, LR: 0.0002026, Tokens/sec: 617262.43\n",
      "Step: 1609, Training Loss: 1.21649, LR: 0.0002022, Tokens/sec: 615762.22\n",
      "Step: 1610, Training Loss: 1.15230, LR: 0.0002017, Tokens/sec: 616796.28\n",
      "Step: 1611, Training Loss: 1.19376, LR: 0.0002013, Tokens/sec: 617397.84\n",
      "Step: 1612, Training Loss: 1.24181, LR: 0.0002009, Tokens/sec: 617401.32\n",
      "Step: 1613, Training Loss: 1.21332, LR: 0.0002004, Tokens/sec: 617952.42\n",
      "Step: 1614, Training Loss: 1.19694, LR: 0.0002000, Tokens/sec: 616940.33\n",
      "Step: 1615, Training Loss: 1.17989, LR: 0.0001995, Tokens/sec: 616147.26\n",
      "Step: 1616, Training Loss: 1.20639, LR: 0.0001991, Tokens/sec: 616655.36\n",
      "Step: 1617, Training Loss: 1.22534, LR: 0.0001987, Tokens/sec: 616519.30\n",
      "Step: 1618, Training Loss: 1.21086, LR: 0.0001982, Tokens/sec: 617398.25\n",
      "Step: 1619, Training Loss: 1.19863, LR: 0.0001978, Tokens/sec: 615918.34\n",
      "Step: 1620, Training Loss: 1.16943, LR: 0.0001974, Tokens/sec: 617463.19\n",
      "Step: 1621, Training Loss: 1.16573, LR: 0.0001969, Tokens/sec: 617918.10\n",
      "Step: 1622, Training Loss: 1.17028, LR: 0.0001965, Tokens/sec: 618293.95\n",
      "Step: 1623, Training Loss: 1.19655, LR: 0.0001961, Tokens/sec: 614795.64\n",
      "Step: 1624, Training Loss: 1.19818, LR: 0.0001957, Tokens/sec: 615527.99\n",
      "Step: 1625, Training Loss: 1.21346, LR: 0.0001952, Tokens/sec: 616595.07\n",
      "Step: 1626, Training Loss: 1.20487, LR: 0.0001948, Tokens/sec: 617970.48\n",
      "Step: 1627, Training Loss: 1.15439, LR: 0.0001944, Tokens/sec: 618714.25\n",
      "Step: 1628, Training Loss: 1.21148, LR: 0.0001939, Tokens/sec: 616463.08\n",
      "Step: 1629, Training Loss: 1.22919, LR: 0.0001935, Tokens/sec: 616218.25\n",
      "Step: 1630, Training Loss: 1.26575, LR: 0.0001931, Tokens/sec: 617839.01\n",
      "Step: 1631, Training Loss: 1.23717, LR: 0.0001927, Tokens/sec: 616935.48\n",
      "Step: 1632, Training Loss: 1.20418, LR: 0.0001923, Tokens/sec: 614551.18\n",
      "Step: 1633, Training Loss: 1.16903, LR: 0.0001918, Tokens/sec: 617759.78\n",
      "Step: 1634, Training Loss: 1.11224, LR: 0.0001914, Tokens/sec: 617487.31\n",
      "Step: 1635, Training Loss: 1.15003, LR: 0.0001910, Tokens/sec: 614763.32\n",
      "Step: 1636, Training Loss: 1.20959, LR: 0.0001906, Tokens/sec: 616328.10\n",
      "Step: 1637, Training Loss: 1.20292, LR: 0.0001902, Tokens/sec: 616256.03\n",
      "Step: 1638, Training Loss: 1.33414, LR: 0.0001897, Tokens/sec: 617000.17\n",
      "Step: 1639, Training Loss: 1.16671, LR: 0.0001893, Tokens/sec: 617205.10\n",
      "Step: 1640, Training Loss: 1.16380, LR: 0.0001889, Tokens/sec: 615534.31\n",
      "Step: 1641, Training Loss: 1.19201, LR: 0.0001885, Tokens/sec: 617093.99\n",
      "Step: 1642, Training Loss: 1.24759, LR: 0.0001881, Tokens/sec: 616529.80\n",
      "Step: 1643, Training Loss: 1.16873, LR: 0.0001877, Tokens/sec: 616324.71\n",
      "Step: 1644, Training Loss: 1.09586, LR: 0.0001873, Tokens/sec: 616467.92\n",
      "Step: 1645, Training Loss: 1.11669, LR: 0.0001868, Tokens/sec: 617718.08\n",
      "Step: 1646, Training Loss: 1.01944, LR: 0.0001864, Tokens/sec: 615760.63\n",
      "Step: 1647, Training Loss: 1.16844, LR: 0.0001860, Tokens/sec: 617687.08\n",
      "Step: 1648, Training Loss: 1.17654, LR: 0.0001856, Tokens/sec: 616413.56\n",
      "Step: 1649, Training Loss: 1.13041, LR: 0.0001852, Tokens/sec: 617429.77\n",
      "Step: 1650, Training Loss: 1.19299, LR: 0.0001848, Tokens/sec: 617790.06\n",
      "Step: 1651, Training Loss: 1.16186, LR: 0.0001844, Tokens/sec: 615489.68\n",
      "Step: 1652, Training Loss: 1.19943, LR: 0.0001840, Tokens/sec: 617170.91\n",
      "Step: 1653, Training Loss: 1.17253, LR: 0.0001836, Tokens/sec: 616682.00\n",
      "Step: 1654, Training Loss: 1.14397, LR: 0.0001832, Tokens/sec: 617240.70\n",
      "Step: 1655, Training Loss: 1.12551, LR: 0.0001828, Tokens/sec: 616150.09\n",
      "Step: 1656, Training Loss: 1.11456, LR: 0.0001824, Tokens/sec: 618213.96\n",
      "Step: 1657, Training Loss: 1.06162, LR: 0.0001820, Tokens/sec: 617495.43\n",
      "Step: 1658, Training Loss: 1.14521, LR: 0.0001816, Tokens/sec: 617613.70\n",
      "Step: 1659, Training Loss: 1.19042, LR: 0.0001812, Tokens/sec: 617964.56\n",
      "Step: 1660, Training Loss: 1.14967, LR: 0.0001808, Tokens/sec: 616844.33\n",
      "Step: 1661, Training Loss: 1.23087, LR: 0.0001804, Tokens/sec: 616431.08\n",
      "Step: 1662, Training Loss: 1.14042, LR: 0.0001800, Tokens/sec: 617247.64\n",
      "Step: 1663, Training Loss: 1.20069, LR: 0.0001796, Tokens/sec: 618134.85\n",
      "Step: 1664, Training Loss: 1.16037, LR: 0.0001792, Tokens/sec: 617043.16\n",
      "Step: 1665, Training Loss: 1.12345, LR: 0.0001788, Tokens/sec: 617903.22\n",
      "Step: 1666, Training Loss: 1.14670, LR: 0.0001784, Tokens/sec: 616553.32\n",
      "Step: 1667, Training Loss: 1.19876, LR: 0.0001780, Tokens/sec: 617337.04\n",
      "Step: 1668, Training Loss: 1.10899, LR: 0.0001776, Tokens/sec: 615907.80\n",
      "Step: 1669, Training Loss: 1.19260, LR: 0.0001772, Tokens/sec: 617498.97\n",
      "Step: 1670, Training Loss: 1.19180, LR: 0.0001769, Tokens/sec: 617211.29\n",
      "Step: 1671, Training Loss: 1.15043, LR: 0.0001765, Tokens/sec: 616178.49\n",
      "Step: 1672, Training Loss: 1.12533, LR: 0.0001761, Tokens/sec: 616610.64\n",
      "Step: 1673, Training Loss: 1.04865, LR: 0.0001757, Tokens/sec: 617716.26\n",
      "Step: 1674, Training Loss: 1.09008, LR: 0.0001753, Tokens/sec: 617425.76\n",
      "Step: 1675, Training Loss: 1.08222, LR: 0.0001749, Tokens/sec: 617580.95\n",
      "Step: 1676, Training Loss: 1.34548, LR: 0.0001745, Tokens/sec: 617647.83\n",
      "Step: 1677, Training Loss: 1.56793, LR: 0.0001742, Tokens/sec: 617189.45\n",
      "Step: 1678, Training Loss: 1.54798, LR: 0.0001738, Tokens/sec: 616383.05\n",
      "Step: 1679, Training Loss: 1.17035, LR: 0.0001734, Tokens/sec: 617236.82\n",
      "Step: 1680, Training Loss: 1.14210, LR: 0.0001730, Tokens/sec: 616288.01\n",
      "Step: 1681, Training Loss: 1.21447, LR: 0.0001726, Tokens/sec: 616636.31\n",
      "Step: 1682, Training Loss: 1.09002, LR: 0.0001723, Tokens/sec: 617552.18\n",
      "Step: 1683, Training Loss: 1.06169, LR: 0.0001719, Tokens/sec: 617260.19\n",
      "Step: 1684, Training Loss: 1.18299, LR: 0.0001715, Tokens/sec: 616273.80\n",
      "Step: 1685, Training Loss: 1.16723, LR: 0.0001711, Tokens/sec: 616638.58\n",
      "Step: 1686, Training Loss: 1.21421, LR: 0.0001708, Tokens/sec: 618583.29\n",
      "Step: 1687, Training Loss: 1.14334, LR: 0.0001704, Tokens/sec: 617786.37\n",
      "Step: 1688, Training Loss: 1.15081, LR: 0.0001700, Tokens/sec: 616910.66\n",
      "Step: 1689, Training Loss: 1.12342, LR: 0.0001696, Tokens/sec: 617583.65\n",
      "Step: 1690, Training Loss: 1.12090, LR: 0.0001693, Tokens/sec: 617774.93\n",
      "Step: 1691, Training Loss: 1.16288, LR: 0.0001689, Tokens/sec: 615807.11\n",
      "Step: 1692, Training Loss: 1.19151, LR: 0.0001685, Tokens/sec: 616493.99\n",
      "Step: 1693, Training Loss: 1.15027, LR: 0.0001682, Tokens/sec: 616641.27\n",
      "Step: 1694, Training Loss: 1.17379, LR: 0.0001678, Tokens/sec: 615824.62\n",
      "Step: 1695, Training Loss: 1.19364, LR: 0.0001674, Tokens/sec: 617698.45\n",
      "Step: 1696, Training Loss: 1.19079, LR: 0.0001671, Tokens/sec: 616984.38\n",
      "Step: 1697, Training Loss: 1.24595, LR: 0.0001667, Tokens/sec: 617058.63\n",
      "Step: 1698, Training Loss: 1.15218, LR: 0.0001663, Tokens/sec: 616961.62\n",
      "Step: 1699, Training Loss: 1.17533, LR: 0.0001660, Tokens/sec: 618266.08\n",
      "Step: 1700, Training Loss: 1.12838, LR: 0.0001656, Tokens/sec: 617002.78\n",
      "Step: 1701, Training Loss: 1.17010, LR: 0.0001652, Tokens/sec: 615316.14\n",
      "Step: 1702, Training Loss: 1.30098, LR: 0.0001649, Tokens/sec: 617071.88\n",
      "Step: 1703, Training Loss: 1.13013, LR: 0.0001645, Tokens/sec: 617270.00\n",
      "Step: 1704, Training Loss: 1.21119, LR: 0.0001642, Tokens/sec: 616837.69\n",
      "Step: 1705, Training Loss: 1.24314, LR: 0.0001638, Tokens/sec: 617337.43\n",
      "Step: 1706, Training Loss: 1.14131, LR: 0.0001635, Tokens/sec: 617004.32\n",
      "Step: 1707, Training Loss: 1.14542, LR: 0.0001631, Tokens/sec: 615889.04\n",
      "Step: 1708, Training Loss: 1.12571, LR: 0.0001627, Tokens/sec: 617709.13\n",
      "Step: 1709, Training Loss: 1.10984, LR: 0.0001624, Tokens/sec: 617028.91\n",
      "Step: 1710, Training Loss: 1.20283, LR: 0.0001620, Tokens/sec: 616762.53\n",
      "Step: 1711, Training Loss: 1.14845, LR: 0.0001617, Tokens/sec: 617461.34\n",
      "Step: 1712, Training Loss: 1.09399, LR: 0.0001613, Tokens/sec: 616404.49\n",
      "Step: 1713, Training Loss: 1.18106, LR: 0.0001610, Tokens/sec: 616597.49\n",
      "Step: 1714, Training Loss: 1.77225, LR: 0.0001606, Tokens/sec: 616931.39\n",
      "Step: 1715, Training Loss: 1.14014, LR: 0.0001603, Tokens/sec: 617118.56\n",
      "Step: 1716, Training Loss: 1.11551, LR: 0.0001599, Tokens/sec: 616387.74\n",
      "Step: 1717, Training Loss: 1.23264, LR: 0.0001596, Tokens/sec: 617203.55\n",
      "Step: 1718, Training Loss: 1.12862, LR: 0.0001593, Tokens/sec: 616695.28\n",
      "Step: 1719, Training Loss: 1.16812, LR: 0.0001589, Tokens/sec: 617467.55\n",
      "Step: 1720, Training Loss: 1.12411, LR: 0.0001586, Tokens/sec: 616433.21\n",
      "Step: 1721, Training Loss: 1.02898, LR: 0.0001582, Tokens/sec: 617305.56\n",
      "Step: 1722, Training Loss: 1.04792, LR: 0.0001579, Tokens/sec: 617445.83\n",
      "Step: 1723, Training Loss: 1.02855, LR: 0.0001575, Tokens/sec: 617090.96\n",
      "Step: 1724, Training Loss: 0.81131, LR: 0.0001572, Tokens/sec: 616645.55\n",
      "Step: 1725, Training Loss: 1.04835, LR: 0.0001569, Tokens/sec: 617733.67\n",
      "Step: 1726, Training Loss: 1.13434, LR: 0.0001565, Tokens/sec: 619256.56\n",
      "Step: 1727, Training Loss: 1.11539, LR: 0.0001562, Tokens/sec: 616284.23\n",
      "Step: 1728, Training Loss: 1.09043, LR: 0.0001558, Tokens/sec: 617653.03\n",
      "Step: 1729, Training Loss: 0.75724, LR: 0.0001555, Tokens/sec: 617129.32\n",
      "Step: 1730, Training Loss: 0.78997, LR: 0.0001552, Tokens/sec: 617691.43\n",
      "Step: 1731, Training Loss: 0.98388, LR: 0.0001548, Tokens/sec: 617144.16\n",
      "Step: 1732, Training Loss: 1.16910, LR: 0.0001545, Tokens/sec: 616984.29\n",
      "Step: 1733, Training Loss: 1.15511, LR: 0.0001542, Tokens/sec: 616684.40\n",
      "Step: 1734, Training Loss: 1.22222, LR: 0.0001539, Tokens/sec: 617311.94\n",
      "Step: 1735, Training Loss: 1.13378, LR: 0.0001535, Tokens/sec: 617922.01\n",
      "Step: 1736, Training Loss: 1.16899, LR: 0.0001532, Tokens/sec: 616560.23\n",
      "Step: 1737, Training Loss: 1.25440, LR: 0.0001529, Tokens/sec: 616611.92\n",
      "Step: 1738, Training Loss: 1.03849, LR: 0.0001525, Tokens/sec: 617208.04\n",
      "Step: 1739, Training Loss: 1.01036, LR: 0.0001522, Tokens/sec: 616216.14\n",
      "Step: 1740, Training Loss: 1.07253, LR: 0.0001519, Tokens/sec: 617705.22\n",
      "Step: 1741, Training Loss: 0.84327, LR: 0.0001516, Tokens/sec: 617439.95\n",
      "Step: 1742, Training Loss: 1.11487, LR: 0.0001512, Tokens/sec: 617559.80\n",
      "Step: 1743, Training Loss: 0.93331, LR: 0.0001509, Tokens/sec: 615615.11\n",
      "Step: 1744, Training Loss: 1.01385, LR: 0.0001506, Tokens/sec: 617025.66\n",
      "Step: 1745, Training Loss: 0.83039, LR: 0.0001503, Tokens/sec: 617300.30\n",
      "Step: 1746, Training Loss: 0.99815, LR: 0.0001500, Tokens/sec: 616404.67\n",
      "Step: 1747, Training Loss: 0.80563, LR: 0.0001496, Tokens/sec: 616312.23\n",
      "Step: 1748, Training Loss: 1.00991, LR: 0.0001493, Tokens/sec: 616900.60\n",
      "Step: 1749, Training Loss: 0.93523, LR: 0.0001490, Tokens/sec: 617972.45\n",
      "Step: 1750, Training Loss: 0.94157, LR: 0.0001487, Tokens/sec: 617212.92\n",
      "Step: 1751, Training Loss: 0.92513, LR: 0.0001484, Tokens/sec: 615890.07\n",
      "Step: 1752, Training Loss: 0.95666, LR: 0.0001481, Tokens/sec: 616353.95\n",
      "Step: 1753, Training Loss: 0.75823, LR: 0.0001478, Tokens/sec: 615706.59\n",
      "Step: 1754, Training Loss: 0.90930, LR: 0.0001475, Tokens/sec: 618361.20\n",
      "Step: 1755, Training Loss: 1.10158, LR: 0.0001471, Tokens/sec: 616863.26\n",
      "Step: 1756, Training Loss: 1.14487, LR: 0.0001468, Tokens/sec: 616853.90\n",
      "Step: 1757, Training Loss: 1.07559, LR: 0.0001465, Tokens/sec: 615899.80\n",
      "Step: 1758, Training Loss: 1.03679, LR: 0.0001462, Tokens/sec: 615359.61\n",
      "Step: 1759, Training Loss: 1.21760, LR: 0.0001459, Tokens/sec: 617382.13\n",
      "Step: 1760, Training Loss: 1.11811, LR: 0.0001456, Tokens/sec: 616720.95\n",
      "Step: 1761, Training Loss: 1.07533, LR: 0.0001453, Tokens/sec: 616841.38\n",
      "Step: 1762, Training Loss: 1.20537, LR: 0.0001450, Tokens/sec: 617295.29\n",
      "Step: 1763, Training Loss: 1.20642, LR: 0.0001447, Tokens/sec: 617554.22\n",
      "Step: 1764, Training Loss: 0.82469, LR: 0.0001444, Tokens/sec: 616487.23\n",
      "Step: 1765, Training Loss: 0.57863, LR: 0.0001441, Tokens/sec: 617054.73\n",
      "Step: 1766, Training Loss: 1.01936, LR: 0.0001438, Tokens/sec: 616628.92\n",
      "Step: 1767, Training Loss: 1.16295, LR: 0.0001435, Tokens/sec: 616154.71\n",
      "Step: 1768, Training Loss: 1.16152, LR: 0.0001432, Tokens/sec: 617081.96\n",
      "Step: 1769, Training Loss: 1.14058, LR: 0.0001429, Tokens/sec: 616861.09\n",
      "Step: 1770, Training Loss: 1.17204, LR: 0.0001426, Tokens/sec: 616587.06\n",
      "Step: 1771, Training Loss: 1.23378, LR: 0.0001423, Tokens/sec: 617086.96\n",
      "Step: 1772, Training Loss: 1.20816, LR: 0.0001420, Tokens/sec: 618650.44\n",
      "Step: 1773, Training Loss: 1.14216, LR: 0.0001417, Tokens/sec: 616355.99\n",
      "Step: 1774, Training Loss: 1.12186, LR: 0.0001414, Tokens/sec: 617858.91\n",
      "Step: 1775, Training Loss: 1.18618, LR: 0.0001411, Tokens/sec: 618713.03\n",
      "Step: 1776, Training Loss: 1.04623, LR: 0.0001408, Tokens/sec: 616231.26\n",
      "Step: 1777, Training Loss: 1.22795, LR: 0.0001406, Tokens/sec: 617435.50\n",
      "Step: 1778, Training Loss: 1.16063, LR: 0.0001403, Tokens/sec: 616415.83\n",
      "Step: 1779, Training Loss: 1.07850, LR: 0.0001400, Tokens/sec: 617288.93\n",
      "Step: 1780, Training Loss: 1.13182, LR: 0.0001397, Tokens/sec: 616981.53\n",
      "Step: 1781, Training Loss: 1.11965, LR: 0.0001394, Tokens/sec: 616739.24\n",
      "Step: 1782, Training Loss: 1.11595, LR: 0.0001391, Tokens/sec: 617434.68\n",
      "Step: 1783, Training Loss: 1.04145, LR: 0.0001388, Tokens/sec: 617374.40\n",
      "Step: 1784, Training Loss: 1.12630, LR: 0.0001386, Tokens/sec: 616607.54\n",
      "Step: 1785, Training Loss: 1.12187, LR: 0.0001383, Tokens/sec: 616288.95\n",
      "Step: 1786, Training Loss: 1.19876, LR: 0.0001380, Tokens/sec: 616451.99\n",
      "Step: 1787, Training Loss: 1.17625, LR: 0.0001377, Tokens/sec: 617270.70\n",
      "Step: 1788, Training Loss: 1.11615, LR: 0.0001374, Tokens/sec: 616275.45\n",
      "Step: 1789, Training Loss: 1.06946, LR: 0.0001372, Tokens/sec: 617283.59\n",
      "Step: 1790, Training Loss: 1.08169, LR: 0.0001369, Tokens/sec: 618236.50\n",
      "Step: 1791, Training Loss: 1.05615, LR: 0.0001366, Tokens/sec: 616095.48\n",
      "Step: 1792, Training Loss: 1.18117, LR: 0.0001363, Tokens/sec: 615947.26\n",
      "Step: 1793, Training Loss: 1.17240, LR: 0.0001361, Tokens/sec: 616284.98\n",
      "Step: 1794, Training Loss: 1.13935, LR: 0.0001358, Tokens/sec: 617414.92\n",
      "Step: 1795, Training Loss: 1.13007, LR: 0.0001355, Tokens/sec: 615735.12\n",
      "Step: 1796, Training Loss: 1.10923, LR: 0.0001353, Tokens/sec: 616051.84\n",
      "Step: 1797, Training Loss: 1.11985, LR: 0.0001350, Tokens/sec: 616215.74\n",
      "Step: 1798, Training Loss: 1.06724, LR: 0.0001347, Tokens/sec: 617017.40\n",
      "Step: 1799, Training Loss: 1.14354, LR: 0.0001345, Tokens/sec: 616509.55\n",
      "Step: 1800, Training Loss: 1.11449, LR: 0.0001342, Tokens/sec: 618160.88\n",
      "Step: 1801, Training Loss: 1.11186, LR: 0.0001339, Tokens/sec: 616370.41\n",
      "Step: 1802, Training Loss: 1.12827, LR: 0.0001337, Tokens/sec: 616582.49\n",
      "Step: 1803, Training Loss: 1.20605, LR: 0.0001334, Tokens/sec: 616000.61\n",
      "Step: 1804, Training Loss: 1.10499, LR: 0.0001331, Tokens/sec: 617218.04\n",
      "Step: 1805, Training Loss: 1.07008, LR: 0.0001329, Tokens/sec: 617053.71\n",
      "Step: 1806, Training Loss: 1.13437, LR: 0.0001326, Tokens/sec: 617864.15\n",
      "Step: 1807, Training Loss: 1.09856, LR: 0.0001324, Tokens/sec: 618425.85\n",
      "Step: 1808, Training Loss: 1.13311, LR: 0.0001321, Tokens/sec: 617180.95\n",
      "Step: 1809, Training Loss: 1.09560, LR: 0.0001318, Tokens/sec: 617101.75\n",
      "Step: 1810, Training Loss: 1.16924, LR: 0.0001316, Tokens/sec: 616049.92\n",
      "Step: 1811, Training Loss: 1.09907, LR: 0.0001313, Tokens/sec: 616601.58\n",
      "Step: 1812, Training Loss: 1.08893, LR: 0.0001311, Tokens/sec: 616137.18\n",
      "Step: 1813, Training Loss: 1.10055, LR: 0.0001308, Tokens/sec: 617385.61\n",
      "Step: 1814, Training Loss: 1.07078, LR: 0.0001306, Tokens/sec: 615224.47\n",
      "Step: 1815, Training Loss: 1.06704, LR: 0.0001303, Tokens/sec: 618548.23\n",
      "Step: 1816, Training Loss: 1.05685, LR: 0.0001301, Tokens/sec: 616400.54\n",
      "Step: 1817, Training Loss: 1.21804, LR: 0.0001298, Tokens/sec: 616939.22\n",
      "Step: 1818, Training Loss: 1.10771, LR: 0.0001296, Tokens/sec: 617275.51\n",
      "Step: 1819, Training Loss: 1.20366, LR: 0.0001293, Tokens/sec: 617200.64\n",
      "Step: 1820, Training Loss: 1.17094, LR: 0.0001291, Tokens/sec: 617332.00\n",
      "Step: 1821, Training Loss: 1.12309, LR: 0.0001288, Tokens/sec: 617501.94\n",
      "Step: 1822, Training Loss: 1.06467, LR: 0.0001286, Tokens/sec: 615309.00\n",
      "Step: 1823, Training Loss: 1.11719, LR: 0.0001283, Tokens/sec: 618328.65\n",
      "Step: 1824, Training Loss: 1.09073, LR: 0.0001281, Tokens/sec: 616730.11\n",
      "Step: 1825, Training Loss: 1.09865, LR: 0.0001279, Tokens/sec: 617136.34\n",
      "Step: 1826, Training Loss: 1.07436, LR: 0.0001276, Tokens/sec: 615675.82\n",
      "Step: 1827, Training Loss: 1.11758, LR: 0.0001274, Tokens/sec: 615385.55\n",
      "Step: 1828, Training Loss: 1.25118, LR: 0.0001271, Tokens/sec: 616910.19\n",
      "Step: 1829, Training Loss: 1.09965, LR: 0.0001269, Tokens/sec: 616940.08\n",
      "Step: 1830, Training Loss: 1.06110, LR: 0.0001267, Tokens/sec: 614634.52\n",
      "Step: 1831, Training Loss: 1.06637, LR: 0.0001264, Tokens/sec: 616843.05\n",
      "Step: 1832, Training Loss: 1.15021, LR: 0.0001262, Tokens/sec: 617940.23\n",
      "Step: 1833, Training Loss: 1.12116, LR: 0.0001260, Tokens/sec: 617344.74\n",
      "Step: 1834, Training Loss: 1.10430, LR: 0.0001257, Tokens/sec: 617617.15\n",
      "Step: 1835, Training Loss: 1.12463, LR: 0.0001255, Tokens/sec: 616191.81\n",
      "Step: 1836, Training Loss: 1.05454, LR: 0.0001253, Tokens/sec: 616017.42\n",
      "Step: 1837, Training Loss: 1.05886, LR: 0.0001250, Tokens/sec: 615845.46\n",
      "Step: 1838, Training Loss: 1.09244, LR: 0.0001248, Tokens/sec: 616143.66\n",
      "Step: 1839, Training Loss: 1.06789, LR: 0.0001246, Tokens/sec: 616639.97\n",
      "Step: 1840, Training Loss: 1.06522, LR: 0.0001244, Tokens/sec: 616331.40\n",
      "Step: 1841, Training Loss: 1.07562, LR: 0.0001241, Tokens/sec: 616551.35\n",
      "Step: 1842, Training Loss: 0.97293, LR: 0.0001239, Tokens/sec: 616635.50\n",
      "Step: 1843, Training Loss: 0.98713, LR: 0.0001237, Tokens/sec: 616397.13\n",
      "Step: 1844, Training Loss: 0.96338, LR: 0.0001235, Tokens/sec: 616805.56\n",
      "Step: 1845, Training Loss: 1.08793, LR: 0.0001232, Tokens/sec: 616248.65\n",
      "Step: 1846, Training Loss: 1.11580, LR: 0.0001230, Tokens/sec: 617763.53\n",
      "Step: 1847, Training Loss: 0.84112, LR: 0.0001228, Tokens/sec: 616432.28\n",
      "Step: 1848, Training Loss: 1.12751, LR: 0.0001226, Tokens/sec: 615193.68\n",
      "Step: 1849, Training Loss: 1.12302, LR: 0.0001224, Tokens/sec: 616252.65\n",
      "Step: 1850, Training Loss: 1.15185, LR: 0.0001222, Tokens/sec: 615115.23\n",
      "Step: 1851, Training Loss: 1.12414, LR: 0.0001219, Tokens/sec: 615329.85\n",
      "Step: 1852, Training Loss: 1.08401, LR: 0.0001217, Tokens/sec: 616815.24\n",
      "Step: 1853, Training Loss: 1.07108, LR: 0.0001215, Tokens/sec: 616636.28\n",
      "Step: 1854, Training Loss: 1.09216, LR: 0.0001213, Tokens/sec: 614931.94\n",
      "Step: 1855, Training Loss: 1.05892, LR: 0.0001211, Tokens/sec: 617644.99\n",
      "Step: 1856, Training Loss: 1.09080, LR: 0.0001209, Tokens/sec: 616702.96\n",
      "Step: 1857, Training Loss: 1.10559, LR: 0.0001207, Tokens/sec: 616737.40\n",
      "Step: 1858, Training Loss: 1.15279, LR: 0.0001205, Tokens/sec: 617896.13\n",
      "Step: 1859, Training Loss: 1.12944, LR: 0.0001203, Tokens/sec: 616609.10\n",
      "Step: 1860, Training Loss: 1.08261, LR: 0.0001201, Tokens/sec: 616420.30\n",
      "Step: 1861, Training Loss: 1.05219, LR: 0.0001198, Tokens/sec: 615001.89\n",
      "Step: 1862, Training Loss: 1.05077, LR: 0.0001196, Tokens/sec: 617239.27\n",
      "Step: 1863, Training Loss: 1.05291, LR: 0.0001194, Tokens/sec: 616108.62\n",
      "Step: 1864, Training Loss: 1.08049, LR: 0.0001192, Tokens/sec: 542238.63\n",
      "Step: 1865, Training Loss: 1.12684, LR: 0.0001190, Tokens/sec: 610549.66\n",
      "Step: 1866, Training Loss: 1.07925, LR: 0.0001188, Tokens/sec: 616395.11\n",
      "Step: 1867, Training Loss: 1.07826, LR: 0.0001186, Tokens/sec: 617352.03\n",
      "Step: 1868, Training Loss: 1.05205, LR: 0.0001184, Tokens/sec: 615431.57\n",
      "Step: 1869, Training Loss: 1.05832, LR: 0.0001182, Tokens/sec: 615677.30\n",
      "Step: 1870, Training Loss: 1.07280, LR: 0.0001181, Tokens/sec: 618668.28\n",
      "Step: 1871, Training Loss: 0.99211, LR: 0.0001179, Tokens/sec: 618059.68\n",
      "Step: 1872, Training Loss: 1.05992, LR: 0.0001177, Tokens/sec: 616715.76\n",
      "Step: 1873, Training Loss: 1.08981, LR: 0.0001175, Tokens/sec: 618281.80\n",
      "Step: 1874, Training Loss: 1.15951, LR: 0.0001173, Tokens/sec: 615847.88\n",
      "Step: 1875, Training Loss: 1.04582, LR: 0.0001171, Tokens/sec: 616707.54\n",
      "Step: 1876, Training Loss: 0.99700, LR: 0.0001169, Tokens/sec: 617164.49\n",
      "Step: 1877, Training Loss: 1.00111, LR: 0.0001167, Tokens/sec: 616048.67\n",
      "Step: 1878, Training Loss: 0.99541, LR: 0.0001165, Tokens/sec: 616749.06\n",
      "Step: 1879, Training Loss: 1.08086, LR: 0.0001163, Tokens/sec: 617764.73\n",
      "Step: 1880, Training Loss: 1.04981, LR: 0.0001162, Tokens/sec: 618223.78\n",
      "Step: 1881, Training Loss: 1.02552, LR: 0.0001160, Tokens/sec: 616401.70\n",
      "Step: 1882, Training Loss: 1.04970, LR: 0.0001158, Tokens/sec: 616692.58\n",
      "Step: 1883, Training Loss: 1.09312, LR: 0.0001156, Tokens/sec: 617384.38\n",
      "Step: 1884, Training Loss: 1.04069, LR: 0.0001154, Tokens/sec: 618181.60\n",
      "Step: 1885, Training Loss: 0.97442, LR: 0.0001152, Tokens/sec: 616346.16\n",
      "Step: 1886, Training Loss: 1.01989, LR: 0.0001151, Tokens/sec: 616038.69\n",
      "Step: 1887, Training Loss: 1.10605, LR: 0.0001149, Tokens/sec: 616418.63\n",
      "Step: 1888, Training Loss: 1.15451, LR: 0.0001147, Tokens/sec: 616580.07\n",
      "Step: 1889, Training Loss: 1.03802, LR: 0.0001145, Tokens/sec: 617098.71\n",
      "Step: 1890, Training Loss: 0.95791, LR: 0.0001144, Tokens/sec: 616658.98\n",
      "Step: 1891, Training Loss: 0.92073, LR: 0.0001142, Tokens/sec: 618006.77\n",
      "Step: 1892, Training Loss: 1.01174, LR: 0.0001140, Tokens/sec: 616560.06\n",
      "Step: 1893, Training Loss: 1.09086, LR: 0.0001138, Tokens/sec: 618252.06\n",
      "Step: 1894, Training Loss: 1.02724, LR: 0.0001137, Tokens/sec: 616134.00\n",
      "Step: 1895, Training Loss: 1.08199, LR: 0.0001135, Tokens/sec: 618650.90\n",
      "Step: 1896, Training Loss: 0.93696, LR: 0.0001133, Tokens/sec: 617162.15\n",
      "Step: 1897, Training Loss: 0.99113, LR: 0.0001132, Tokens/sec: 617117.74\n",
      "Step: 1898, Training Loss: 1.03054, LR: 0.0001130, Tokens/sec: 616578.16\n",
      "Step: 1899, Training Loss: 1.21809, LR: 0.0001128, Tokens/sec: 616307.37\n",
      "Step: 1900, Training Loss: 1.06455, LR: 0.0001127, Tokens/sec: 616699.80\n",
      "Step: 1901, Training Loss: 1.07208, LR: 0.0001125, Tokens/sec: 616613.00\n",
      "Step: 1902, Training Loss: 1.09157, LR: 0.0001123, Tokens/sec: 615222.47\n",
      "Step: 1903, Training Loss: 1.11378, LR: 0.0001122, Tokens/sec: 617399.23\n",
      "Step: 1904, Training Loss: 1.08883, LR: 0.0001120, Tokens/sec: 617619.96\n",
      "Step: 1905, Training Loss: 1.08078, LR: 0.0001119, Tokens/sec: 617288.23\n",
      "Step: 1906, Training Loss: 1.16213, LR: 0.0001117, Tokens/sec: 616732.03\n",
      "Step: 1907, Training Loss: 1.11264, LR: 0.0001115, Tokens/sec: 617614.54\n",
      "Step: 1908, Training Loss: 1.09125, LR: 0.0001114, Tokens/sec: 617535.62\n",
      "Step: 1909, Training Loss: 1.06448, LR: 0.0001112, Tokens/sec: 617485.50\n",
      "Step: 1910, Training Loss: 1.08099, LR: 0.0001111, Tokens/sec: 617268.10\n",
      "Step: 1911, Training Loss: 1.09540, LR: 0.0001109, Tokens/sec: 616229.50\n",
      "Step: 1912, Training Loss: 1.15031, LR: 0.0001108, Tokens/sec: 616505.58\n",
      "Step: 1913, Training Loss: 1.31609, LR: 0.0001106, Tokens/sec: 617334.83\n",
      "Step: 1914, Training Loss: 1.07897, LR: 0.0001105, Tokens/sec: 617043.27\n",
      "Step: 1915, Training Loss: 1.11623, LR: 0.0001103, Tokens/sec: 616429.09\n",
      "Step: 1916, Training Loss: 1.08142, LR: 0.0001102, Tokens/sec: 617154.27\n",
      "Step: 1917, Training Loss: 1.06373, LR: 0.0001100, Tokens/sec: 616571.01\n",
      "Step: 1918, Training Loss: 1.01112, LR: 0.0001099, Tokens/sec: 617094.65\n",
      "Step: 1919, Training Loss: 0.97405, LR: 0.0001097, Tokens/sec: 616426.47\n",
      "Step: 1920, Training Loss: 1.13417, LR: 0.0001096, Tokens/sec: 617618.77\n",
      "Step: 1921, Training Loss: 1.10144, LR: 0.0001095, Tokens/sec: 615800.47\n",
      "Step: 1922, Training Loss: 0.96769, LR: 0.0001093, Tokens/sec: 615836.59\n",
      "Step: 1923, Training Loss: 0.94371, LR: 0.0001092, Tokens/sec: 617229.86\n",
      "Step: 1924, Training Loss: 0.86545, LR: 0.0001090, Tokens/sec: 616637.49\n",
      "Step: 1925, Training Loss: 0.96459, LR: 0.0001089, Tokens/sec: 616671.42\n",
      "Step: 1926, Training Loss: 1.03544, LR: 0.0001088, Tokens/sec: 618572.21\n",
      "Step: 1927, Training Loss: 1.01461, LR: 0.0001086, Tokens/sec: 615882.10\n",
      "Step: 1928, Training Loss: 0.98789, LR: 0.0001085, Tokens/sec: 616733.26\n",
      "Step: 1929, Training Loss: 0.97575, LR: 0.0001084, Tokens/sec: 616658.75\n",
      "Step: 1930, Training Loss: 0.92081, LR: 0.0001082, Tokens/sec: 617290.55\n",
      "Step: 1931, Training Loss: 1.03463, LR: 0.0001081, Tokens/sec: 615205.99\n",
      "Step: 1932, Training Loss: 0.98692, LR: 0.0001080, Tokens/sec: 616940.81\n",
      "Step: 1933, Training Loss: 1.01759, LR: 0.0001078, Tokens/sec: 617769.85\n",
      "Step: 1934, Training Loss: 1.02679, LR: 0.0001077, Tokens/sec: 617344.81\n",
      "Step: 1935, Training Loss: 0.96684, LR: 0.0001076, Tokens/sec: 617448.42\n",
      "Step: 1936, Training Loss: 0.98418, LR: 0.0001075, Tokens/sec: 617294.93\n",
      "Step: 1937, Training Loss: 0.85654, LR: 0.0001073, Tokens/sec: 615759.92\n",
      "Step: 1938, Training Loss: 1.02692, LR: 0.0001072, Tokens/sec: 614555.65\n",
      "Step: 1939, Training Loss: 1.00576, LR: 0.0001071, Tokens/sec: 617069.96\n",
      "Step: 1940, Training Loss: 0.92581, LR: 0.0001070, Tokens/sec: 618881.36\n",
      "Step: 1941, Training Loss: 0.99607, LR: 0.0001068, Tokens/sec: 616602.48\n",
      "Step: 1942, Training Loss: 0.95317, LR: 0.0001067, Tokens/sec: 616817.06\n",
      "Step: 1943, Training Loss: 0.94974, LR: 0.0001066, Tokens/sec: 616117.98\n",
      "Step: 1944, Training Loss: 0.99925, LR: 0.0001065, Tokens/sec: 616530.71\n",
      "Step: 1945, Training Loss: 1.05453, LR: 0.0001064, Tokens/sec: 616940.67\n",
      "Step: 1946, Training Loss: 1.10234, LR: 0.0001062, Tokens/sec: 616382.64\n",
      "Step: 1947, Training Loss: 1.04657, LR: 0.0001061, Tokens/sec: 617910.63\n",
      "Step: 1948, Training Loss: 1.05996, LR: 0.0001060, Tokens/sec: 617056.76\n",
      "Step: 1949, Training Loss: 1.07180, LR: 0.0001059, Tokens/sec: 617896.18\n",
      "Step: 1950, Training Loss: 1.40732, LR: 0.0001058, Tokens/sec: 616698.25\n",
      "Step: 1951, Training Loss: 1.02111, LR: 0.0001057, Tokens/sec: 615493.72\n",
      "Step: 1952, Training Loss: 0.99654, LR: 0.0001056, Tokens/sec: 616977.82\n",
      "Step: 1953, Training Loss: 1.02333, LR: 0.0001055, Tokens/sec: 615760.15\n",
      "Step: 1954, Training Loss: 1.02135, LR: 0.0001054, Tokens/sec: 615122.32\n",
      "Step: 1955, Training Loss: 1.01594, LR: 0.0001053, Tokens/sec: 616368.40\n",
      "Step: 1956, Training Loss: 1.00123, LR: 0.0001051, Tokens/sec: 616634.60\n",
      "Step: 1957, Training Loss: 1.00699, LR: 0.0001050, Tokens/sec: 617198.51\n",
      "Step: 1958, Training Loss: 0.99870, LR: 0.0001049, Tokens/sec: 617446.18\n",
      "Step: 1959, Training Loss: 1.07460, LR: 0.0001048, Tokens/sec: 616426.64\n",
      "Step: 1960, Training Loss: 1.00780, LR: 0.0001047, Tokens/sec: 616428.20\n",
      "Step: 1961, Training Loss: 1.00494, LR: 0.0001046, Tokens/sec: 617302.76\n",
      "Step: 1962, Training Loss: 1.02303, LR: 0.0001045, Tokens/sec: 617159.37\n",
      "Step: 1963, Training Loss: 0.97838, LR: 0.0001044, Tokens/sec: 615525.10\n",
      "Step: 1964, Training Loss: 0.78737, LR: 0.0001043, Tokens/sec: 617290.63\n",
      "Step: 1965, Training Loss: 0.95122, LR: 0.0001042, Tokens/sec: 616474.33\n",
      "Step: 1966, Training Loss: 0.99364, LR: 0.0001042, Tokens/sec: 616577.09\n",
      "Step: 1967, Training Loss: 0.85432, LR: 0.0001041, Tokens/sec: 617503.07\n",
      "Step: 1968, Training Loss: 0.94703, LR: 0.0001040, Tokens/sec: 617254.13\n",
      "Step: 1969, Training Loss: 0.96499, LR: 0.0001039, Tokens/sec: 616317.99\n",
      "Step: 1970, Training Loss: 0.98356, LR: 0.0001038, Tokens/sec: 617086.04\n",
      "Step: 1971, Training Loss: 1.00479, LR: 0.0001037, Tokens/sec: 617305.60\n",
      "Step: 1972, Training Loss: 1.01770, LR: 0.0001036, Tokens/sec: 616479.53\n",
      "Step: 1973, Training Loss: 1.00242, LR: 0.0001035, Tokens/sec: 616600.06\n",
      "Step: 1974, Training Loss: 1.30176, LR: 0.0001034, Tokens/sec: 617292.95\n",
      "Step: 1975, Training Loss: 0.94650, LR: 0.0001033, Tokens/sec: 617813.04\n",
      "Step: 1976, Training Loss: 1.00593, LR: 0.0001033, Tokens/sec: 615157.60\n",
      "Step: 1977, Training Loss: 1.02397, LR: 0.0001032, Tokens/sec: 614194.35\n",
      "Step: 1978, Training Loss: 1.03460, LR: 0.0001031, Tokens/sec: 616364.26\n",
      "Step: 1979, Training Loss: 1.06126, LR: 0.0001030, Tokens/sec: 616827.84\n",
      "Step: 1980, Training Loss: 0.96665, LR: 0.0001029, Tokens/sec: 616185.88\n",
      "Step: 1981, Training Loss: 1.07997, LR: 0.0001029, Tokens/sec: 616463.45\n",
      "Step: 1982, Training Loss: 1.04847, LR: 0.0001028, Tokens/sec: 617187.93\n",
      "Step: 1983, Training Loss: 0.95466, LR: 0.0001027, Tokens/sec: 616270.82\n",
      "Step: 1984, Training Loss: 1.03166, LR: 0.0001026, Tokens/sec: 617416.40\n",
      "Step: 1985, Training Loss: 0.99347, LR: 0.0001026, Tokens/sec: 617333.38\n",
      "Step: 1986, Training Loss: 1.02359, LR: 0.0001025, Tokens/sec: 617431.51\n",
      "Step: 1987, Training Loss: 1.01178, LR: 0.0001024, Tokens/sec: 617358.40\n",
      "Step: 1988, Training Loss: 1.03546, LR: 0.0001023, Tokens/sec: 616942.14\n",
      "Step: 1989, Training Loss: 1.03584, LR: 0.0001023, Tokens/sec: 617032.20\n",
      "Step: 1990, Training Loss: 0.97636, LR: 0.0001022, Tokens/sec: 616172.77\n",
      "Step: 1991, Training Loss: 1.04362, LR: 0.0001021, Tokens/sec: 616208.87\n",
      "Step: 1992, Training Loss: 1.07034, LR: 0.0001021, Tokens/sec: 615894.98\n",
      "Step: 1993, Training Loss: 1.02159, LR: 0.0001020, Tokens/sec: 616831.18\n",
      "Step: 1994, Training Loss: 1.01508, LR: 0.0001019, Tokens/sec: 616642.82\n",
      "Step: 1995, Training Loss: 1.00380, LR: 0.0001019, Tokens/sec: 616503.52\n",
      "Step: 1996, Training Loss: 1.05792, LR: 0.0001018, Tokens/sec: 617170.04\n",
      "Step: 1997, Training Loss: 0.98044, LR: 0.0001017, Tokens/sec: 617650.78\n",
      "Step: 1998, Training Loss: 1.01149, LR: 0.0001017, Tokens/sec: 617550.65\n",
      "Step: 1999, Training Loss: 1.03072, LR: 0.0001016, Tokens/sec: 617203.81\n",
      "Step: 2000, Training Loss: 1.06922, LR: 0.0001016, Tokens/sec: 616057.20\n",
      "Computing Eval loss, steps: 21\n",
      "Step: 2000, Eval Loss: 1.05985\n",
      "Step: 2001, Training Loss: 1.04988, LR: 0.0001015, Tokens/sec: 614354.77\n",
      "Step: 2002, Training Loss: 0.99203, LR: 0.0001015, Tokens/sec: 617580.04\n",
      "Step: 2003, Training Loss: 0.99796, LR: 0.0001014, Tokens/sec: 617929.42\n",
      "Step: 2004, Training Loss: 0.97307, LR: 0.0001013, Tokens/sec: 616362.49\n",
      "Step: 2005, Training Loss: 1.07458, LR: 0.0001013, Tokens/sec: 616347.66\n",
      "Step: 2006, Training Loss: 1.02098, LR: 0.0001012, Tokens/sec: 617310.61\n",
      "Step: 2007, Training Loss: 1.01915, LR: 0.0001012, Tokens/sec: 616872.01\n",
      "Step: 2008, Training Loss: 1.04290, LR: 0.0001011, Tokens/sec: 617395.90\n",
      "Step: 2009, Training Loss: 1.04021, LR: 0.0001011, Tokens/sec: 617755.47\n",
      "Step: 2010, Training Loss: 1.02084, LR: 0.0001010, Tokens/sec: 616141.89\n",
      "Step: 2011, Training Loss: 1.22520, LR: 0.0001010, Tokens/sec: 617399.32\n",
      "Step: 2012, Training Loss: 1.17140, LR: 0.0001009, Tokens/sec: 618352.65\n",
      "Step: 2013, Training Loss: 1.13613, LR: 0.0001009, Tokens/sec: 617569.87\n",
      "Step: 2014, Training Loss: 1.09413, LR: 0.0001009, Tokens/sec: 616707.53\n",
      "Step: 2015, Training Loss: 1.01205, LR: 0.0001008, Tokens/sec: 616721.27\n",
      "Step: 2016, Training Loss: 1.04063, LR: 0.0001008, Tokens/sec: 615560.70\n",
      "Step: 2017, Training Loss: 1.05945, LR: 0.0001007, Tokens/sec: 617196.61\n",
      "Step: 2018, Training Loss: 1.06251, LR: 0.0001007, Tokens/sec: 616647.62\n",
      "Step: 2019, Training Loss: 1.03306, LR: 0.0001007, Tokens/sec: 615433.65\n",
      "Step: 2020, Training Loss: 1.05440, LR: 0.0001006, Tokens/sec: 618262.72\n",
      "Step: 2021, Training Loss: 1.00060, LR: 0.0001006, Tokens/sec: 617667.94\n",
      "Step: 2022, Training Loss: 1.04874, LR: 0.0001005, Tokens/sec: 616683.31\n",
      "Step: 2023, Training Loss: 1.05054, LR: 0.0001005, Tokens/sec: 617243.79\n",
      "Step: 2024, Training Loss: 1.02012, LR: 0.0001005, Tokens/sec: 615552.49\n",
      "Step: 2025, Training Loss: 1.08507, LR: 0.0001005, Tokens/sec: 617005.02\n",
      "Step: 2026, Training Loss: 1.02395, LR: 0.0001004, Tokens/sec: 615988.47\n",
      "Step: 2027, Training Loss: 1.09861, LR: 0.0001004, Tokens/sec: 618566.98\n",
      "Step: 2028, Training Loss: 1.04887, LR: 0.0001004, Tokens/sec: 618135.57\n",
      "Step: 2029, Training Loss: 1.09932, LR: 0.0001003, Tokens/sec: 615846.10\n",
      "Step: 2030, Training Loss: 1.07172, LR: 0.0001003, Tokens/sec: 616998.84\n",
      "Step: 2031, Training Loss: 1.05000, LR: 0.0001003, Tokens/sec: 616924.98\n",
      "Step: 2032, Training Loss: 1.00982, LR: 0.0001003, Tokens/sec: 616630.87\n",
      "Step: 2033, Training Loss: 1.07501, LR: 0.0001002, Tokens/sec: 616596.43\n",
      "Step: 2034, Training Loss: 1.02973, LR: 0.0001002, Tokens/sec: 615289.49\n",
      "Step: 2035, Training Loss: 1.04523, LR: 0.0001002, Tokens/sec: 616590.87\n",
      "Step: 2036, Training Loss: 1.05708, LR: 0.0001002, Tokens/sec: 616294.49\n",
      "Step: 2037, Training Loss: 1.05764, LR: 0.0001002, Tokens/sec: 617166.50\n",
      "Step: 2038, Training Loss: 0.97162, LR: 0.0001001, Tokens/sec: 616336.92\n",
      "Step: 2039, Training Loss: 1.01930, LR: 0.0001001, Tokens/sec: 617629.00\n",
      "Step: 2040, Training Loss: 1.01413, LR: 0.0001001, Tokens/sec: 617939.38\n",
      "Step: 2041, Training Loss: 0.99459, LR: 0.0001001, Tokens/sec: 616833.09\n",
      "Step: 2042, Training Loss: 1.00527, LR: 0.0001001, Tokens/sec: 616433.42\n",
      "Step: 2043, Training Loss: 1.04962, LR: 0.0001001, Tokens/sec: 617765.41\n",
      "Step: 2044, Training Loss: 1.01045, LR: 0.0001001, Tokens/sec: 616263.05\n",
      "Step: 2045, Training Loss: 1.04520, LR: 0.0001000, Tokens/sec: 617688.79\n",
      "Step: 2046, Training Loss: 1.11754, LR: 0.0001000, Tokens/sec: 618352.19\n",
      "Step: 2047, Training Loss: 1.20871, LR: 0.0001000, Tokens/sec: 615816.27\n",
      "Step: 2048, Training Loss: 1.02681, LR: 0.0001000, Tokens/sec: 617252.20\n",
      "Step: 2049, Training Loss: 1.03855, LR: 0.0001000, Tokens/sec: 616288.20\n",
      "Step: 2050, Training Loss: 1.02391, LR: 0.0001000, Tokens/sec: 616904.75\n",
      "Step: 2051, Training Loss: 1.04656, LR: 0.0001000, Tokens/sec: 615703.30\n",
      "Step: 2052, Training Loss: 1.08956, LR: 0.0001000, Tokens/sec: 616355.55\n",
      "Step: 2053, Training Loss: 1.12233, LR: 0.0001000, Tokens/sec: 618061.65\n"
     ]
    }
   ],
   "source": [
    "trainer.train(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b5596eda083de0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:22:17.417599499Z",
     "start_time": "2024-12-16T06:27:52.251011Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "आज की चर्चा करता है। अपने पिता के निर्देशन के लिए प्रेम का निर्माण जब होता है तो अपने पिता के पिता के रूप में बहुत प्रसिद्ध होता है। इस प्रकार वह अपने पिता के पिता के रूप में अपने पिता के रूप में अपने पिता के रूप में प्रेम करते हैं। उन्होंने अपने पि\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer([\"आज की चर्चा\"], return_tensors=\"pt\")['input_ids'].to(trainer.device)\n",
    "idx = model.generate(input_ids, temperature=0.25, top_k=50, max_new_tokens=256)\n",
    "print(tokenizer.batch_decode(idx)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78ff348-a198-4d2d-bdc9-322660f96e02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
