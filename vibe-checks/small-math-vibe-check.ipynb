{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-09T07:40:44.989239Z",
     "start_time": "2025-02-09T07:40:42.396463Z"
    }
   },
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from datasets import load_dataset"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T07:40:44.993702Z",
     "start_time": "2025-02-09T07:40:44.992096Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#model_id = \"Qwen/Qwen2.5-0.5B\"\n",
    "model_id = \"amang1802/mathllama_400M\"\n",
    "#model_id = \"meta-llama/Llama-3.2-1B\""
   ],
   "id": "4071d666704e62b4",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T07:41:05.470175Z",
     "start_time": "2025-02-09T07:40:45.040097Z"
    }
   },
   "cell_type": "code",
   "source": "llm = LLM(model_id, max_model_len=2048, dtype=\"auto\")",
   "id": "e33b3c99c2432ea6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-08 23:40:46 config.py:2167] Downcasting torch.float32 to torch.float16.\n",
      "INFO 02-08 23:40:49 config.py:478] This model supports multiple tasks: {'classify', 'generate', 'embed', 'reward', 'score'}. Defaulting to 'generate'.\n",
      "INFO 02-08 23:40:49 llm_engine.py:249] Initializing an LLM engine (v0.6.5) with config: model='amang1802/mathllama_400M', speculative_config=None, tokenizer='amang1802/mathllama_400M', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=amang1802/mathllama_400M, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, mm_cache_preprocessor=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 02-08 23:40:50 selector.py:120] Using Flash Attention backend.\n",
      "INFO 02-08 23:40:50 model_runner.py:1092] Starting to load model amang1802/mathllama_400M...\n",
      "INFO 02-08 23:40:51 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 02-08 23:40:51 weight_utils.py:288] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1196980f73ac4fada62850c8dd39ace5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-08 23:40:52 model_runner.py:1097] Loading model weights took 0.7628 GB\n",
      "INFO 02-08 23:40:52 worker.py:241] Memory profiling takes 0.47 seconds\r\n",
      "INFO 02-08 23:40:52 worker.py:241] the current vLLM instance can use total_gpu_memory (23.69GiB) x gpu_memory_utilization (0.90) = 21.32GiB\r\n",
      "INFO 02-08 23:40:52 worker.py:241] model weights take 0.76GiB; non_torch_memory takes 0.11GiB; PyTorch activation peak memory takes 0.46GiB; the rest of the memory reserved for KV Cache is 19.99GiB.\n",
      "INFO 02-08 23:40:52 gpu_executor.py:76] # GPU blocks: 32751, # CPU blocks: 6553\n",
      "INFO 02-08 23:40:52 gpu_executor.py:80] Maximum concurrency for 2048 tokens per request: 255.87x\n",
      "INFO 02-08 23:40:54 model_runner.py:1413] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-08 23:40:54 model_runner.py:1417] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 02-08 23:41:05 model_runner.py:1527] Graph capturing finished in 11 secs, took 0.82 GiB\n",
      "INFO 02-08 23:41:05 llm_engine.py:446] init engine (profile, create kv cache, warmup model) took 13.06 seconds\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T07:41:05.476102Z",
     "start_time": "2025-02-09T07:41:05.473962Z"
    }
   },
   "cell_type": "code",
   "source": [
    "instructions = \"\"\"\n",
    "You are an AI assistant that helps people find information. User will you give you a question. Your task is to answer as faithfully as you can. While answering think step-by-step and justify your answer.\n",
    "\"\"\".strip()\n",
    "\n",
    "few_shot = \"\"\"\n",
    "Problem: James writes a 5-page letter to 3 different friends twice a week. How many pages does he write a year?\n",
    "\n",
    "Solution: We should first calculate the number of pages James writes in a week. From that, calculating how many pages James write in a year is easy, because we know there are 52 weeks in a year.\n",
    "\n",
    "In one week, James writes 5 x 3 x 2 = 30 pages of letters.\n",
    "So, in one year, James writes 30 x 52 = 1560 pages of letters.\n",
    "\n",
    "Answer: 1560\n",
    "\n",
    "Problem: In a contest, each day they are handing out some prizes. On the first day, they handed out one prize. On the second day, they handed two prizes. Every day, the number of prizes they hand out increases by 1. The contest lasts a week. How many prizes did they hand out?\n",
    "\n",
    "Solution: The total number of prizes is the sum of prizes they hand out each day. Since the contest lasts a week, this sum is\n",
    "\n",
    "Total number of prizes = 1 + 2 + 3 + 4 + 5 + 6 + 7\n",
    "\n",
    "We can use the formula for the sum of first n numbers to calculate this.\n",
    "\n",
    "Total number of prizes = 7 * (7 + 1) / 2 = 7 * 8 / 2 = 7 * 4 = 28\n",
    "\n",
    "Answer: 28\n",
    "\"\"\".strip()\n",
    "def solve(problems, attempt):\n",
    "    prompts = [f\"{instructions}\\n\\n{few_shot}\\n\\nProblem: {problem}\\n\\nSolution:\" for problem in problems]\n",
    "    outputs = llm.generate(prompts, SamplingParams(temperature=0.5, top_p=0.9, max_tokens=256, stop=[\"Problem:\"]))\n",
    "    return {f\"{model_id}--{attempt}\": [output.outputs[0].text for output in outputs]}"
   ],
   "id": "2398ae434354f623",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T07:41:19.304088Z",
     "start_time": "2025-02-09T07:41:05.515707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for ds_id in [\"amang1802/math-vibe-gsm-similar\", \"amang1802/math-vibe-new\"]:\n",
    "    ds = load_dataset(ds_id)\n",
    "    for attempt in range(3):\n",
    "        ds = ds.map(lambda problems: solve(problems, attempt), batched=True, batch_size=5, input_columns=[\"problem\"])\n",
    "    ds['train'].push_to_hub(ds_id)"
   ],
   "id": "3a8a31fe090413ee",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "README.md:   0%|          | 0.00/580 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bc70bc5521ec40e19696a5c2a04d40c5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/25.0k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6beedbd1e0a5405abb32b2432745c5f6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1ef884a1173344ef8d72599c0e74c503"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function <lambda> at 0x7f342c15df80> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4953d9a30c484ae48fe11e2edb201ce3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001B[A\n",
      "Processed prompts:  20%|██        | 1/5 [00:00<00:01,  2.27it/s, est. speed input: 920.04 toks/s, output: 165.42 toks/s]\u001B[A\n",
      "Processed prompts:  40%|████      | 2/5 [00:00<00:01,  2.72it/s, est. speed input: 1176.19 toks/s, output: 274.57 toks/s]\u001B[A\n",
      "Processed prompts:  80%|████████  | 4/5 [00:00<00:00,  5.35it/s, est. speed input: 1892.50 toks/s, output: 584.99 toks/s]\u001B[A\n",
      "Processed prompts: 100%|██████████| 5/5 [00:01<00:00,  4.51it/s, est. speed input: 1960.29 toks/s, output: 715.45 toks/s]\u001B[A\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "69ad170460c8442b8a8dafde46c2a8b6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001B[A\n",
      "Processed prompts:  20%|██        | 1/5 [00:00<00:00,  4.12it/s, est. speed input: 1698.09 toks/s, output: 222.56 toks/s]\u001B[A\n",
      "Processed prompts: 100%|██████████| 5/5 [00:00<00:00,  9.18it/s, est. speed input: 3982.86 toks/s, output: 980.10 toks/s]\u001B[A\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0ab7dc431b7e41d6831db8d276d0ac1b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001B[A\n",
      "Processed prompts:  20%|██        | 1/5 [00:00<00:00,  5.13it/s, est. speed input: 2082.30 toks/s, output: 194.88 toks/s]\u001B[A\n",
      "Processed prompts:  40%|████      | 2/5 [00:00<00:00,  7.13it/s, est. speed input: 2754.34 toks/s, output: 350.17 toks/s]\u001B[A\n",
      "Processed prompts:  80%|████████  | 4/5 [00:00<00:00,  5.91it/s, est. speed input: 2516.42 toks/s, output: 448.02 toks/s]\u001B[A\n",
      "Processed prompts: 100%|██████████| 5/5 [00:00<00:00,  6.18it/s, est. speed input: 2683.15 toks/s, output: 588.55 toks/s]\u001B[A\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "78dd9cbd51c44e738e3b959fd72ad8fa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7ab2a03001a54389b4711af730f423a5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "README.md:   0%|          | 0.00/580 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "034ab20a6364445b9621c7590b3053d0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/24.0k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ae4def0c1f6f49c3a73a0abff6282902"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e19121b0bbe54643b6308d40df6da4fb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f19f5e79db274f4e991701f637a524d1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001B[A\n",
      "Processed prompts:  20%|██        | 1/5 [00:00<00:01,  3.25it/s, est. speed input: 1519.83 toks/s, output: 253.30 toks/s]\u001B[A\n",
      "Processed prompts:  80%|████████  | 4/5 [00:00<00:00,  8.29it/s, est. speed input: 3344.60 toks/s, output: 746.12 toks/s]\u001B[A\n",
      "Processed prompts: 100%|██████████| 5/5 [00:00<00:00,  6.09it/s, est. speed input: 2737.56 toks/s, output: 802.00 toks/s]\u001B[A\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "64330e7289254671a9f1536bc4ba301e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001B[A\n",
      "Processed prompts:  20%|██        | 1/5 [00:00<00:01,  3.75it/s, est. speed input: 1614.43 toks/s, output: 229.02 toks/s]\u001B[A\n",
      "Processed prompts:  40%|████      | 2/5 [00:00<00:00,  3.88it/s, est. speed input: 1746.62 toks/s, output: 392.21 toks/s]\u001B[A\n",
      "Processed prompts:  60%|██████    | 3/5 [00:00<00:00,  5.34it/s, est. speed input: 2147.86 toks/s, output: 605.38 toks/s]\u001B[A\n",
      "Processed prompts: 100%|██████████| 5/5 [00:00<00:00,  5.34it/s, est. speed input: 2398.80 toks/s, output: 948.40 toks/s]\u001B[A\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "72a3c13a73d2427a976b57f268765dcc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001B[A\n",
      "Processed prompts:  20%|██        | 1/5 [00:00<00:01,  3.19it/s, est. speed input: 1370.11 toks/s, output: 203.92 toks/s]\u001B[A\n",
      "Processed prompts:  40%|████      | 2/5 [00:00<00:00,  3.60it/s, est. speed input: 1542.33 toks/s, output: 299.99 toks/s]\u001B[A\n",
      "Processed prompts:  80%|████████  | 4/5 [00:01<00:00,  3.98it/s, est. speed input: 1714.40 toks/s, output: 460.90 toks/s]\u001B[A\n",
      "Processed prompts: 100%|██████████| 5/5 [00:01<00:00,  4.22it/s, est. speed input: 1895.79 toks/s, output: 617.01 toks/s]\u001B[A\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8c35400da93049058f8d3e1d7f3fce05"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0a417c225e4c499cbd4980162abcaf22"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T07:41:19.310733Z",
     "start_time": "2025-02-09T07:41:19.309413Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "55143d38265cb77d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T07:41:19.353025Z",
     "start_time": "2025-02-09T07:41:19.351568Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "9da789cc1e981465",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
