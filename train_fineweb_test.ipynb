{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:22:48.329931Z",
     "start_time": "2024-12-16T09:22:47.413234Z"
    }
   },
   "outputs": [],
   "source": [
    "from model import ModelConfig, LlamaModel\n",
    "from train import TrainerConfig, FileDataLoader, Trainer\n",
    "\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f28fa23c987e72b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:22:48.333961Z",
     "start_time": "2024-12-16T09:22:48.332382Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer_id = \"HuggingFaceTB/SmolLM2-135M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bb4e51aa142abee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:22:48.533405Z",
     "start_time": "2024-12-16T09:22:48.376114Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cde027092af8291e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:22:48.540534Z",
     "start_time": "2024-12-16T09:22:48.538895Z"
    }
   },
   "outputs": [],
   "source": [
    "model_config = ModelConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=576,\n",
    "    d_head=64,\n",
    "    d_mlp_proj=1536,\n",
    "    n_layers=30,\n",
    "    n_kv_heads=3,\n",
    "    n_attn_heads=9,\n",
    "    rms_norm_eps=1e-5,\n",
    "    initializer_range=0.041666666666666664,\n",
    "    rope_theta=100000.0,\n",
    "    padding_idx=tokenizer.pad_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0897594b27eb59f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:22:48.582151Z",
     "start_time": "2024-12-16T09:22:48.580277Z"
    }
   },
   "outputs": [],
   "source": [
    "train_config = TrainerConfig(\n",
    "    per_device_train_batch_size=32,\n",
    "    max_seq_len=2048,\n",
    "    num_epochs=1,\n",
    "    eval_interval_steps=100,\n",
    "    learning_rate=1e-3,\n",
    "    grad_clip_norm=1.0,\n",
    "    tokens_folder=\"fineweb-edu_tok\",\n",
    "    max_steps=1000,\n",
    "    log_dir=\"runs/fineweb\",\n",
    "    warmup_ratio=0.1,\n",
    "    val_size=0.0005\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6504e357e2012d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:22:50.546015Z",
     "start_time": "2024-12-16T09:22:48.624998Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens                   | 10,101,737,472\n",
      "Num Trainable Params           | 162,826,560\n",
      "Train device                   | cuda, NVIDIA A100-SXM4-80GB, N=8\n",
      "Training precision             | torch.bfloat16\n",
      "Flash Attention                | True\n",
      "torch.compile()                | True\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LlamaModel(model_config)\n",
    "dataloader = FileDataLoader(train_config, tokenizer)\n",
    "trainer = Trainer(train_config, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c853027a7a843745",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-16T09:22:50.552519Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training steps                 | 1,000 \n",
      "Step: 0, Training Loss: 11.27769, LR: 0.0000100, Tokens/sec: 1425.5069511910128\n",
      "Computing Eval loss, steps: 78\n",
      "Step: 0, Eval Loss: 11.24022\n",
      "Step: 1, Training Loss: 11.23619, LR: 0.0000200, Tokens/sec: 1779.3861157874926\n",
      "Step: 2, Training Loss: 11.14878, LR: 0.0000300, Tokens/sec: 246992.93520791107\n",
      "Step: 3, Training Loss: 10.97960, LR: 0.0000400, Tokens/sec: 246286.03921751503\n",
      "Step: 4, Training Loss: 10.83010, LR: 0.0000500, Tokens/sec: 248138.47047151692\n",
      "Step: 5, Training Loss: 10.59780, LR: 0.0000600, Tokens/sec: 247080.66778849915\n",
      "Step: 6, Training Loss: 10.20184, LR: 0.0000700, Tokens/sec: 248375.73130491533\n",
      "Step: 7, Training Loss: 9.94503, LR: 0.0000800, Tokens/sec: 247456.64110105\n",
      "Step: 8, Training Loss: 9.64306, LR: 0.0000900, Tokens/sec: 247568.435538636\n",
      "Step: 9, Training Loss: 9.58912, LR: 0.0001000, Tokens/sec: 248998.8865784912\n",
      "Step: 10, Training Loss: 9.42804, LR: 0.0001100, Tokens/sec: 248277.19359325164\n",
      "Step: 11, Training Loss: 9.37120, LR: 0.0001200, Tokens/sec: 248949.32868330943\n",
      "Step: 12, Training Loss: 9.28942, LR: 0.0001300, Tokens/sec: 248755.52913010694\n",
      "Step: 13, Training Loss: 9.16352, LR: 0.0001400, Tokens/sec: 248049.13357525953\n",
      "Step: 14, Training Loss: 9.06405, LR: 0.0001500, Tokens/sec: 249451.78713386183\n",
      "Step: 15, Training Loss: 8.99500, LR: 0.0001600, Tokens/sec: 248913.79283306072\n",
      "Step: 16, Training Loss: 8.93054, LR: 0.0001700, Tokens/sec: 247830.05428749244\n",
      "Step: 17, Training Loss: 8.87935, LR: 0.0001800, Tokens/sec: 248239.6040046726\n",
      "Step: 18, Training Loss: 8.70506, LR: 0.0001900, Tokens/sec: 248585.1103193707\n",
      "Step: 19, Training Loss: 8.62195, LR: 0.0002000, Tokens/sec: 249199.70541449974\n",
      "Step: 20, Training Loss: 8.65769, LR: 0.0002100, Tokens/sec: 248894.36749975343\n",
      "Step: 21, Training Loss: 8.40225, LR: 0.0002200, Tokens/sec: 248833.36258305196\n",
      "Step: 22, Training Loss: 8.30791, LR: 0.0002300, Tokens/sec: 248516.30847952847\n",
      "Step: 23, Training Loss: 8.25003, LR: 0.0002400, Tokens/sec: 248676.2187520722\n",
      "Step: 24, Training Loss: 8.15278, LR: 0.0002500, Tokens/sec: 249566.78864671514\n",
      "Step: 25, Training Loss: 8.06455, LR: 0.0002600, Tokens/sec: 249274.49334458524\n",
      "Step: 26, Training Loss: 8.02753, LR: 0.0002700, Tokens/sec: 248718.80377213593\n",
      "Step: 27, Training Loss: 7.94360, LR: 0.0002800, Tokens/sec: 249133.28843896612\n",
      "Step: 28, Training Loss: 7.84650, LR: 0.0002900, Tokens/sec: 248848.80241211606\n",
      "Step: 29, Training Loss: 7.81629, LR: 0.0003000, Tokens/sec: 249195.49595542508\n",
      "Step: 30, Training Loss: 7.77990, LR: 0.0003100, Tokens/sec: 248200.15234994044\n",
      "Step: 31, Training Loss: 7.72904, LR: 0.0003200, Tokens/sec: 248436.4664019676\n",
      "Step: 32, Training Loss: 7.60519, LR: 0.0003300, Tokens/sec: 248571.23451277314\n",
      "Step: 33, Training Loss: 7.57093, LR: 0.0003400, Tokens/sec: 247805.11140411586\n",
      "Step: 34, Training Loss: 7.90684, LR: 0.0003500, Tokens/sec: 248524.69575264386\n",
      "Step: 35, Training Loss: 7.51568, LR: 0.0003600, Tokens/sec: 247677.541392432\n",
      "Step: 36, Training Loss: 7.40598, LR: 0.0003700, Tokens/sec: 248173.48944100825\n",
      "Step: 37, Training Loss: 7.47801, LR: 0.0003800, Tokens/sec: 247604.03259291474\n",
      "Step: 38, Training Loss: 7.40555, LR: 0.0003900, Tokens/sec: 248336.25390142793\n",
      "Step: 39, Training Loss: 7.33370, LR: 0.0004000, Tokens/sec: 247848.7027654231\n",
      "Step: 40, Training Loss: 7.45327, LR: 0.0004100, Tokens/sec: 247784.09799853215\n",
      "Step: 41, Training Loss: 7.34446, LR: 0.0004200, Tokens/sec: 247406.26859307464\n",
      "Step: 42, Training Loss: 7.36859, LR: 0.0004300, Tokens/sec: 247202.3677894056\n",
      "Step: 43, Training Loss: 7.17243, LR: 0.0004400, Tokens/sec: 247755.29826842397\n",
      "Step: 44, Training Loss: 7.39903, LR: 0.0004500, Tokens/sec: 246817.9511667458\n",
      "Step: 45, Training Loss: 7.29419, LR: 0.0004600, Tokens/sec: 246658.67115751194\n",
      "Step: 46, Training Loss: 7.27807, LR: 0.0004700, Tokens/sec: 246725.56575709197\n",
      "Step: 47, Training Loss: 7.28728, LR: 0.0004800, Tokens/sec: 246477.93722027302\n",
      "Step: 48, Training Loss: 7.14709, LR: 0.0004900, Tokens/sec: 244068.5198809826\n",
      "Step: 49, Training Loss: 6.99808, LR: 0.0005000, Tokens/sec: 246522.62086558738\n",
      "Step: 50, Training Loss: 7.26857, LR: 0.0005100, Tokens/sec: 246941.96096283605\n",
      "Step: 51, Training Loss: 7.09532, LR: 0.0005200, Tokens/sec: 246279.2624766699\n",
      "Step: 52, Training Loss: 7.23310, LR: 0.0005300, Tokens/sec: 247028.33919404165\n",
      "Step: 53, Training Loss: 7.29241, LR: 0.0005400, Tokens/sec: 246834.65362237705\n",
      "Step: 54, Training Loss: 7.03730, LR: 0.0005500, Tokens/sec: 246737.76030943776\n",
      "Step: 55, Training Loss: 7.07024, LR: 0.0005600, Tokens/sec: 246374.43302991358\n",
      "Step: 56, Training Loss: 7.18418, LR: 0.0005700, Tokens/sec: 246197.83801505345\n",
      "Step: 57, Training Loss: 7.08859, LR: 0.0005800, Tokens/sec: 245975.58885067032\n",
      "Step: 58, Training Loss: 6.91861, LR: 0.0005900, Tokens/sec: 246802.07155847413\n",
      "Step: 59, Training Loss: 6.98807, LR: 0.0006000, Tokens/sec: 246695.1936203884\n",
      "Step: 60, Training Loss: 7.21429, LR: 0.0006100, Tokens/sec: 246530.8136458155\n",
      "Step: 61, Training Loss: 6.81212, LR: 0.0006200, Tokens/sec: 246155.65200101532\n",
      "Step: 62, Training Loss: 6.99664, LR: 0.0006300, Tokens/sec: 245976.60343468605\n",
      "Step: 63, Training Loss: 7.03561, LR: 0.0006400, Tokens/sec: 246392.00380150045\n",
      "Step: 64, Training Loss: 7.04119, LR: 0.0006500, Tokens/sec: 246003.7698842257\n",
      "Step: 65, Training Loss: 7.05817, LR: 0.0006600, Tokens/sec: 245806.9730114427\n",
      "Step: 66, Training Loss: 7.03608, LR: 0.0006700, Tokens/sec: 246273.99787359938\n",
      "Step: 67, Training Loss: 6.87327, LR: 0.0006800, Tokens/sec: 246021.35835519736\n",
      "Step: 68, Training Loss: 6.95314, LR: 0.0006900, Tokens/sec: 245941.5019730823\n",
      "Step: 69, Training Loss: 6.95235, LR: 0.0007000, Tokens/sec: 245822.94981298503\n",
      "Step: 70, Training Loss: 6.86011, LR: 0.0007100, Tokens/sec: 245581.65581409502\n",
      "Step: 71, Training Loss: 6.91333, LR: 0.0007200, Tokens/sec: 246353.18202535616\n",
      "Step: 72, Training Loss: 6.90965, LR: 0.0007300, Tokens/sec: 246109.63971152483\n",
      "Step: 73, Training Loss: 6.70070, LR: 0.0007400, Tokens/sec: 245813.26868464297\n",
      "Step: 74, Training Loss: 6.78615, LR: 0.0007500, Tokens/sec: 246862.69583570035\n",
      "Step: 75, Training Loss: 6.72359, LR: 0.0007600, Tokens/sec: 245678.56771714138\n",
      "Step: 76, Training Loss: 6.76110, LR: 0.0007700, Tokens/sec: 245656.52067812352\n",
      "Step: 77, Training Loss: 6.95671, LR: 0.0007800, Tokens/sec: 245833.15386411687\n",
      "Step: 78, Training Loss: 6.86937, LR: 0.0007900, Tokens/sec: 246267.3476260489\n",
      "Step: 79, Training Loss: 6.82828, LR: 0.0008000, Tokens/sec: 245689.43745626864\n",
      "Step: 80, Training Loss: 6.41615, LR: 0.0008100, Tokens/sec: 245609.6643534968\n",
      "Step: 81, Training Loss: 6.72442, LR: 0.0008200, Tokens/sec: 245808.3777463612\n",
      "Step: 82, Training Loss: 6.63129, LR: 0.0008300, Tokens/sec: 246039.3398984248\n",
      "Step: 83, Training Loss: 6.82444, LR: 0.0008400, Tokens/sec: 246139.51827815644\n",
      "Step: 84, Training Loss: 6.65482, LR: 0.0008500, Tokens/sec: 245309.9562391718\n",
      "Step: 85, Training Loss: 6.72710, LR: 0.0008600, Tokens/sec: 246237.51729052127\n",
      "Step: 86, Training Loss: 6.66092, LR: 0.0008700, Tokens/sec: 245437.1306249626\n",
      "Step: 87, Training Loss: 6.65425, LR: 0.0008800, Tokens/sec: 245516.7094378712\n",
      "Step: 88, Training Loss: 6.62861, LR: 0.0008900, Tokens/sec: 244002.36435701864\n",
      "Step: 89, Training Loss: 6.62011, LR: 0.0009000, Tokens/sec: 245778.84906365947\n",
      "Step: 90, Training Loss: 6.56328, LR: 0.0009100, Tokens/sec: 245230.45155424482\n",
      "Step: 91, Training Loss: 6.53725, LR: 0.0009200, Tokens/sec: 246155.7777176947\n",
      "Step: 92, Training Loss: 6.60494, LR: 0.0009300, Tokens/sec: 246114.28785377697\n",
      "Step: 93, Training Loss: 6.60690, LR: 0.0009400, Tokens/sec: 246143.01899063835\n",
      "Step: 94, Training Loss: 6.54989, LR: 0.0009500, Tokens/sec: 245172.22528276854\n",
      "Step: 95, Training Loss: 6.57800, LR: 0.0009600, Tokens/sec: 245723.89210091945\n",
      "Step: 96, Training Loss: 6.53998, LR: 0.0009700, Tokens/sec: 245738.33229382685\n",
      "Step: 97, Training Loss: 6.51417, LR: 0.0009800, Tokens/sec: 242748.82378787472\n",
      "Step: 98, Training Loss: 6.55954, LR: 0.0009900, Tokens/sec: 245218.20044843986\n",
      "Step: 99, Training Loss: 6.60083, LR: 0.0010000, Tokens/sec: 244993.85508677596\n",
      "Step: 100, Training Loss: 6.44858, LR: 0.0010000, Tokens/sec: 244552.66736955353\n",
      "Computing Eval loss, steps: 78\n",
      "Step: 100, Eval Loss: 6.46349\n",
      "Step: 101, Training Loss: 6.42017, LR: 0.0010000, Tokens/sec: 244757.5802036731\n",
      "Step: 102, Training Loss: 6.54514, LR: 0.0010000, Tokens/sec: 245235.56053131522\n",
      "Step: 103, Training Loss: 6.42672, LR: 0.0010000, Tokens/sec: 245713.75031305404\n",
      "Step: 104, Training Loss: 6.34841, LR: 0.0010000, Tokens/sec: 245302.89961032712\n",
      "Step: 105, Training Loss: 6.34232, LR: 0.0009999, Tokens/sec: 245125.95483145016\n",
      "Step: 106, Training Loss: 6.41928, LR: 0.0009999, Tokens/sec: 245318.82805462775\n",
      "Step: 107, Training Loss: 6.42794, LR: 0.0009999, Tokens/sec: 245223.32771542462\n",
      "Step: 108, Training Loss: 6.38255, LR: 0.0009998, Tokens/sec: 244897.87457239465\n",
      "Step: 109, Training Loss: 6.38807, LR: 0.0009998, Tokens/sec: 245601.9355664859\n",
      "Step: 110, Training Loss: 6.37272, LR: 0.0009997, Tokens/sec: 244805.30255057028\n",
      "Step: 111, Training Loss: 6.33570, LR: 0.0009997, Tokens/sec: 244614.66248631044\n",
      "Step: 112, Training Loss: 6.36882, LR: 0.0009996, Tokens/sec: 245263.77227594377\n",
      "Step: 113, Training Loss: 6.37630, LR: 0.0009995, Tokens/sec: 244664.23289730333\n",
      "Step: 114, Training Loss: 6.11703, LR: 0.0009995, Tokens/sec: 245166.37240137038\n",
      "Step: 115, Training Loss: 6.38363, LR: 0.0009994, Tokens/sec: 244800.08374233975\n",
      "Step: 116, Training Loss: 6.23728, LR: 0.0009993, Tokens/sec: 245233.29915454998\n",
      "Step: 117, Training Loss: 6.27642, LR: 0.0009992, Tokens/sec: 244771.0624394053\n",
      "Step: 118, Training Loss: 6.45363, LR: 0.0009991, Tokens/sec: 244799.77545808294\n",
      "Step: 119, Training Loss: 6.17740, LR: 0.0009990, Tokens/sec: 243974.87174730827\n",
      "Step: 120, Training Loss: 6.34795, LR: 0.0009989, Tokens/sec: 245231.5659733788\n",
      "Step: 121, Training Loss: 6.32241, LR: 0.0009988, Tokens/sec: 245066.3067776179\n",
      "Step: 122, Training Loss: 6.26268, LR: 0.0009987, Tokens/sec: 245381.30126824678\n",
      "Step: 123, Training Loss: 6.27586, LR: 0.0009986, Tokens/sec: 243978.5107910844\n",
      "Step: 124, Training Loss: 6.26365, LR: 0.0009984, Tokens/sec: 245034.48166309518\n",
      "Step: 125, Training Loss: 6.22983, LR: 0.0009983, Tokens/sec: 245197.98401520346\n",
      "Step: 126, Training Loss: 6.33879, LR: 0.0009981, Tokens/sec: 244769.19445324532\n",
      "Step: 127, Training Loss: 6.37175, LR: 0.0009980, Tokens/sec: 245382.32293500935\n",
      "Step: 128, Training Loss: 6.24336, LR: 0.0009979, Tokens/sec: 245266.84804421137\n",
      "Step: 129, Training Loss: 6.28273, LR: 0.0009977, Tokens/sec: 244582.0772549061\n",
      "Step: 130, Training Loss: 6.24478, LR: 0.0009975, Tokens/sec: 244512.13412221576\n",
      "Step: 131, Training Loss: 6.04813, LR: 0.0009974, Tokens/sec: 245349.6919879459\n",
      "Step: 132, Training Loss: 6.32223, LR: 0.0009972, Tokens/sec: 244891.30525436148\n",
      "Step: 133, Training Loss: 6.28410, LR: 0.0009970, Tokens/sec: 244890.42573567916\n",
      "Step: 134, Training Loss: 6.18395, LR: 0.0009968, Tokens/sec: 244993.39107536618\n",
      "Step: 135, Training Loss: 5.99423, LR: 0.0009966, Tokens/sec: 244945.49476546625\n",
      "Step: 136, Training Loss: 6.33298, LR: 0.0009965, Tokens/sec: 245491.6921702933\n",
      "Step: 137, Training Loss: 6.33021, LR: 0.0009963, Tokens/sec: 242795.1594347128\n",
      "Step: 138, Training Loss: 6.19040, LR: 0.0009960, Tokens/sec: 244962.275615543\n",
      "Step: 139, Training Loss: 6.25271, LR: 0.0009958, Tokens/sec: 244595.50275375237\n",
      "Step: 140, Training Loss: 6.18733, LR: 0.0009956, Tokens/sec: 244924.50659105106\n",
      "Step: 141, Training Loss: 6.22889, LR: 0.0009954, Tokens/sec: 244019.98257861118\n",
      "Step: 142, Training Loss: 6.30892, LR: 0.0009952, Tokens/sec: 244580.08293913194\n",
      "Step: 143, Training Loss: 6.10454, LR: 0.0009949, Tokens/sec: 245166.1896101\n",
      "Step: 144, Training Loss: 6.09379, LR: 0.0009947, Tokens/sec: 244742.63535387118\n",
      "Step: 145, Training Loss: 6.15301, LR: 0.0009945, Tokens/sec: 245362.79979057072\n",
      "Step: 146, Training Loss: 6.25618, LR: 0.0009942, Tokens/sec: 244616.29172069792\n",
      "Step: 147, Training Loss: 6.37852, LR: 0.0009940, Tokens/sec: 245115.52423028136\n",
      "Step: 148, Training Loss: 6.26551, LR: 0.0009937, Tokens/sec: 245045.38493428656\n",
      "Step: 149, Training Loss: 6.42983, LR: 0.0009934, Tokens/sec: 245075.965005885\n",
      "Step: 150, Training Loss: 6.21269, LR: 0.0009932, Tokens/sec: 244617.86995591348\n",
      "Step: 151, Training Loss: 6.15392, LR: 0.0009929, Tokens/sec: 244942.00586523526\n",
      "Step: 152, Training Loss: 6.13286, LR: 0.0009926, Tokens/sec: 244556.94071366178\n",
      "Step: 153, Training Loss: 6.12917, LR: 0.0009923, Tokens/sec: 244551.29054825025\n",
      "Step: 154, Training Loss: 6.02736, LR: 0.0009920, Tokens/sec: 245141.76636453086\n",
      "Step: 155, Training Loss: 6.14212, LR: 0.0009917, Tokens/sec: 244952.12667828705\n",
      "Step: 156, Training Loss: 6.07544, LR: 0.0009914, Tokens/sec: 245031.37247165295\n",
      "Step: 157, Training Loss: 5.70673, LR: 0.0009911, Tokens/sec: 245155.43786296507\n",
      "Step: 158, Training Loss: 6.12399, LR: 0.0009908, Tokens/sec: 244665.19756100362\n",
      "Step: 159, Training Loss: 6.24216, LR: 0.0009905, Tokens/sec: 244496.9813679274\n",
      "Step: 160, Training Loss: 6.07990, LR: 0.0009902, Tokens/sec: 244203.431834214\n",
      "Step: 161, Training Loss: 5.95263, LR: 0.0009898, Tokens/sec: 244779.1443197118\n",
      "Step: 162, Training Loss: 6.11376, LR: 0.0009895, Tokens/sec: 245113.81661982564\n",
      "Step: 163, Training Loss: 5.95859, LR: 0.0009892, Tokens/sec: 244255.92007728692\n",
      "Step: 164, Training Loss: 5.90990, LR: 0.0009888, Tokens/sec: 245321.5049477911\n",
      "Step: 165, Training Loss: 6.51428, LR: 0.0009885, Tokens/sec: 244147.48289987852\n",
      "Step: 166, Training Loss: 6.26126, LR: 0.0009881, Tokens/sec: 244703.81587606878\n",
      "Step: 167, Training Loss: 6.07397, LR: 0.0009877, Tokens/sec: 244241.3246538615\n",
      "Step: 168, Training Loss: 6.25184, LR: 0.0009874, Tokens/sec: 245266.4530968874\n",
      "Step: 169, Training Loss: 6.04841, LR: 0.0009870, Tokens/sec: 244899.24507134172\n",
      "Step: 170, Training Loss: 6.21922, LR: 0.0009866, Tokens/sec: 244612.1863637045\n",
      "Step: 171, Training Loss: 5.99197, LR: 0.0009863, Tokens/sec: 245151.97714213314\n",
      "Step: 172, Training Loss: 6.04646, LR: 0.0009859, Tokens/sec: 244268.7535541929\n",
      "Step: 173, Training Loss: 6.10902, LR: 0.0009855, Tokens/sec: 244158.45647431965\n",
      "Step: 174, Training Loss: 6.12655, LR: 0.0009851, Tokens/sec: 244833.58064157312\n",
      "Step: 175, Training Loss: 5.93326, LR: 0.0009847, Tokens/sec: 244773.9589811631\n",
      "Step: 176, Training Loss: 6.04010, LR: 0.0009843, Tokens/sec: 244960.66564566456\n",
      "Step: 177, Training Loss: 5.95715, LR: 0.0009838, Tokens/sec: 244975.24141985102\n",
      "Step: 178, Training Loss: 6.05649, LR: 0.0009834, Tokens/sec: 244696.41117347497\n",
      "Step: 179, Training Loss: 6.10112, LR: 0.0009830, Tokens/sec: 244934.426461148\n",
      "Step: 180, Training Loss: 5.92300, LR: 0.0009826, Tokens/sec: 244849.73104345956\n",
      "Step: 181, Training Loss: 5.97006, LR: 0.0009821, Tokens/sec: 244094.82279295943\n",
      "Step: 182, Training Loss: 6.01655, LR: 0.0009817, Tokens/sec: 244588.7422119549\n",
      "Step: 183, Training Loss: 5.90638, LR: 0.0009812, Tokens/sec: 244639.19173267338\n",
      "Step: 184, Training Loss: 5.86493, LR: 0.0009808, Tokens/sec: 244361.28843371785\n",
      "Step: 185, Training Loss: 5.98029, LR: 0.0009803, Tokens/sec: 243874.6165912571\n",
      "Step: 186, Training Loss: 5.72710, LR: 0.0009799, Tokens/sec: 245299.23634361455\n",
      "Step: 187, Training Loss: 5.94592, LR: 0.0009794, Tokens/sec: 244804.89035200968\n",
      "Step: 188, Training Loss: 5.95018, LR: 0.0009789, Tokens/sec: 244839.00533057115\n",
      "Step: 189, Training Loss: 5.86429, LR: 0.0009785, Tokens/sec: 244158.06000563604\n",
      "Step: 190, Training Loss: 5.96073, LR: 0.0009780, Tokens/sec: 245207.45781028474\n",
      "Step: 191, Training Loss: 5.88710, LR: 0.0009775, Tokens/sec: 244548.7902124135\n",
      "Step: 192, Training Loss: 5.87801, LR: 0.0009770, Tokens/sec: 243540.35887623648\n",
      "Step: 193, Training Loss: 5.98745, LR: 0.0009765, Tokens/sec: 244845.54287725934\n",
      "Step: 194, Training Loss: 6.05082, LR: 0.0009760, Tokens/sec: 245287.81114309884\n",
      "Step: 195, Training Loss: 5.86685, LR: 0.0009755, Tokens/sec: 245058.839157272\n",
      "Step: 196, Training Loss: 5.95866, LR: 0.0009750, Tokens/sec: 244443.8052730551\n",
      "Step: 197, Training Loss: 5.84082, LR: 0.0009745, Tokens/sec: 245275.28585908332\n",
      "Step: 198, Training Loss: 5.92430, LR: 0.0009739, Tokens/sec: 243631.66274089896\n",
      "Step: 199, Training Loss: 5.78651, LR: 0.0009734, Tokens/sec: 244696.58645773947\n",
      "Step: 200, Training Loss: 6.32552, LR: 0.0009729, Tokens/sec: 244106.3352752292\n",
      "Computing Eval loss, steps: 78\n",
      "Step: 200, Eval Loss: 5.90901\n",
      "Step: 201, Training Loss: 6.00101, LR: 0.0009723, Tokens/sec: 244505.3527011621\n",
      "Step: 202, Training Loss: 6.00824, LR: 0.0009718, Tokens/sec: 244901.1320893948\n",
      "Step: 203, Training Loss: 5.93424, LR: 0.0009712, Tokens/sec: 244863.9613147984\n",
      "Step: 204, Training Loss: 5.89248, LR: 0.0009707, Tokens/sec: 243959.8447121039\n",
      "Step: 205, Training Loss: 5.71357, LR: 0.0009701, Tokens/sec: 244403.3301715492\n",
      "Step: 206, Training Loss: 5.81153, LR: 0.0009695, Tokens/sec: 244535.62477344865\n",
      "Step: 207, Training Loss: 5.89582, LR: 0.0009690, Tokens/sec: 244931.69321632985\n",
      "Step: 208, Training Loss: 6.03845, LR: 0.0009684, Tokens/sec: 244373.4489724269\n",
      "Step: 209, Training Loss: 6.17099, LR: 0.0009678, Tokens/sec: 244574.88392952163\n",
      "Step: 210, Training Loss: 5.93694, LR: 0.0009672, Tokens/sec: 244524.36236072905\n",
      "Step: 211, Training Loss: 5.90860, LR: 0.0009666, Tokens/sec: 244611.13708786\n",
      "Step: 212, Training Loss: 5.86377, LR: 0.0009660, Tokens/sec: 244738.44574347453\n",
      "Step: 213, Training Loss: 5.89920, LR: 0.0009654, Tokens/sec: 244348.6030485305\n",
      "Step: 214, Training Loss: 5.85819, LR: 0.0009648, Tokens/sec: 245017.8342450014\n",
      "Step: 215, Training Loss: 5.79114, LR: 0.0009642, Tokens/sec: 244441.53979389623\n",
      "Step: 216, Training Loss: 5.75448, LR: 0.0009636, Tokens/sec: 244373.33355618228\n",
      "Step: 217, Training Loss: 5.81553, LR: 0.0009630, Tokens/sec: 244512.32783408102\n",
      "Step: 218, Training Loss: 5.71891, LR: 0.0009624, Tokens/sec: 244728.3561977434\n",
      "Step: 219, Training Loss: 5.94562, LR: 0.0009617, Tokens/sec: 244025.7994687737\n",
      "Step: 220, Training Loss: 5.88453, LR: 0.0009611, Tokens/sec: 245010.2023750892\n",
      "Step: 221, Training Loss: 5.97002, LR: 0.0009605, Tokens/sec: 244428.42835142766\n",
      "Step: 222, Training Loss: 5.81565, LR: 0.0009598, Tokens/sec: 244185.5295205621\n",
      "Step: 223, Training Loss: 5.86529, LR: 0.0009592, Tokens/sec: 244451.28131255167\n",
      "Step: 224, Training Loss: 5.91510, LR: 0.0009585, Tokens/sec: 244071.84341358556\n",
      "Step: 225, Training Loss: 5.60042, LR: 0.0009578, Tokens/sec: 245092.76715258896\n",
      "Step: 226, Training Loss: 5.73296, LR: 0.0009572, Tokens/sec: 244805.52398012223\n",
      "Step: 227, Training Loss: 5.79488, LR: 0.0009565, Tokens/sec: 244755.0262722932\n",
      "Step: 228, Training Loss: 5.83433, LR: 0.0009558, Tokens/sec: 244288.42354254515\n",
      "Step: 229, Training Loss: 5.78538, LR: 0.0009551, Tokens/sec: 245440.19362135697\n",
      "Step: 230, Training Loss: 5.78527, LR: 0.0009545, Tokens/sec: 244078.91913163487\n",
      "Step: 231, Training Loss: 5.84297, LR: 0.0009538, Tokens/sec: 244640.00310853054\n",
      "Step: 232, Training Loss: 5.84084, LR: 0.0009531, Tokens/sec: 243641.86114315505\n",
      "Step: 233, Training Loss: 5.83136, LR: 0.0009524, Tokens/sec: 244980.33123987284\n",
      "Step: 234, Training Loss: 5.79564, LR: 0.0009517, Tokens/sec: 245071.13921060192\n",
      "Step: 235, Training Loss: 5.83680, LR: 0.0009510, Tokens/sec: 244529.23292526003\n",
      "Step: 236, Training Loss: 5.87514, LR: 0.0009502, Tokens/sec: 244920.6721917654\n",
      "Step: 237, Training Loss: 5.73466, LR: 0.0009495, Tokens/sec: 245006.58026115585\n",
      "Step: 238, Training Loss: 5.79649, LR: 0.0009488, Tokens/sec: 242975.6434536432\n",
      "Step: 239, Training Loss: 5.79696, LR: 0.0009481, Tokens/sec: 244132.13137761215\n",
      "Step: 240, Training Loss: 5.78679, LR: 0.0009473, Tokens/sec: 244888.67183065598\n",
      "Step: 241, Training Loss: 5.78000, LR: 0.0009466, Tokens/sec: 244986.1957116255\n",
      "Step: 242, Training Loss: 5.70555, LR: 0.0009458, Tokens/sec: 244445.60376054852\n",
      "Step: 243, Training Loss: 5.70706, LR: 0.0009451, Tokens/sec: 244320.1280952774\n",
      "Step: 244, Training Loss: 5.70790, LR: 0.0009443, Tokens/sec: 244213.67477190585\n",
      "Step: 245, Training Loss: 5.72408, LR: 0.0009436, Tokens/sec: 244633.77589416335\n",
      "Step: 246, Training Loss: 5.61644, LR: 0.0009428, Tokens/sec: 244683.59568110204\n",
      "Step: 247, Training Loss: 5.65400, LR: 0.0009420, Tokens/sec: 244548.64743475185\n",
      "Step: 248, Training Loss: 5.48495, LR: 0.0009413, Tokens/sec: 244373.43709133787\n",
      "Step: 249, Training Loss: 5.80038, LR: 0.0009405, Tokens/sec: 244615.96008980976\n",
      "Step: 250, Training Loss: 5.80393, LR: 0.0009397, Tokens/sec: 244571.76258978294\n",
      "Step: 251, Training Loss: 5.69709, LR: 0.0009389, Tokens/sec: 244380.260421226\n",
      "Step: 252, Training Loss: 5.70706, LR: 0.0009381, Tokens/sec: 245385.32295374994\n",
      "Step: 253, Training Loss: 5.85676, LR: 0.0009373, Tokens/sec: 245082.32742545402\n",
      "Step: 254, Training Loss: 5.53396, LR: 0.0009365, Tokens/sec: 244737.40730066696\n",
      "Step: 255, Training Loss: 5.70608, LR: 0.0009357, Tokens/sec: 245007.55786307872\n",
      "Step: 256, Training Loss: 5.73465, LR: 0.0009349, Tokens/sec: 245047.40391222428\n",
      "Step: 257, Training Loss: 5.77705, LR: 0.0009341, Tokens/sec: 244909.93173912278\n",
      "Step: 258, Training Loss: 5.61501, LR: 0.0009333, Tokens/sec: 244878.2171007569\n",
      "Step: 259, Training Loss: 5.61125, LR: 0.0009325, Tokens/sec: 244677.92434807433\n",
      "Step: 260, Training Loss: 5.63943, LR: 0.0009316, Tokens/sec: 244666.51781853873\n",
      "Step: 261, Training Loss: 5.73301, LR: 0.0009308, Tokens/sec: 244342.37369141594\n",
      "Step: 262, Training Loss: 5.73593, LR: 0.0009299, Tokens/sec: 244183.3806736589\n",
      "Step: 263, Training Loss: 5.65243, LR: 0.0009291, Tokens/sec: 244829.22947872238\n",
      "Step: 264, Training Loss: 5.66014, LR: 0.0009283, Tokens/sec: 245278.62694967486\n",
      "Step: 265, Training Loss: 5.79993, LR: 0.0009274, Tokens/sec: 244950.73853413586\n",
      "Step: 266, Training Loss: 5.62475, LR: 0.0009265, Tokens/sec: 244916.41681404144\n",
      "Step: 267, Training Loss: 5.59784, LR: 0.0009257, Tokens/sec: 245717.9802463242\n",
      "Step: 268, Training Loss: 5.61666, LR: 0.0009248, Tokens/sec: 243761.1352976663\n",
      "Step: 269, Training Loss: 5.67846, LR: 0.0009239, Tokens/sec: 245224.01478549122\n",
      "Step: 270, Training Loss: 5.81125, LR: 0.0009231, Tokens/sec: 244276.78194592614\n",
      "Step: 271, Training Loss: 5.71297, LR: 0.0009222, Tokens/sec: 244872.49924904542\n",
      "Step: 272, Training Loss: 5.80445, LR: 0.0009213, Tokens/sec: 244226.05965308356\n",
      "Step: 273, Training Loss: 5.73202, LR: 0.0009204, Tokens/sec: 244569.03233287417\n",
      "Step: 274, Training Loss: 5.60590, LR: 0.0009195, Tokens/sec: 245155.65480182934\n",
      "Step: 275, Training Loss: 5.57526, LR: 0.0009186, Tokens/sec: 244861.7817659044\n",
      "Step: 276, Training Loss: 5.59859, LR: 0.0009177, Tokens/sec: 244658.05208448105\n",
      "Step: 277, Training Loss: 5.73176, LR: 0.0009168, Tokens/sec: 244462.95993038494\n",
      "Step: 278, Training Loss: 5.57875, LR: 0.0009159, Tokens/sec: 244771.15779771344\n",
      "Step: 279, Training Loss: 5.76612, LR: 0.0009150, Tokens/sec: 244571.451480221\n",
      "Step: 280, Training Loss: 5.69316, LR: 0.0009141, Tokens/sec: 244467.42887645774\n",
      "Step: 281, Training Loss: 5.74981, LR: 0.0009131, Tokens/sec: 245338.10127870046\n",
      "Step: 282, Training Loss: 5.68239, LR: 0.0009122, Tokens/sec: 244186.50227599623\n",
      "Step: 283, Training Loss: 5.65253, LR: 0.0009113, Tokens/sec: 244850.23881297326\n",
      "Step: 284, Training Loss: 5.61765, LR: 0.0009103, Tokens/sec: 244740.1157822748\n",
      "Step: 285, Training Loss: 5.66738, LR: 0.0009094, Tokens/sec: 245726.49889872965\n",
      "Step: 286, Training Loss: 5.75448, LR: 0.0009084, Tokens/sec: 244540.0317913414\n",
      "Step: 287, Training Loss: 5.61707, LR: 0.0009075, Tokens/sec: 244665.2332894349\n",
      "Step: 288, Training Loss: 5.55357, LR: 0.0009065, Tokens/sec: 245076.41567351308\n",
      "Step: 289, Training Loss: 5.54371, LR: 0.0009056, Tokens/sec: 243412.37860222763\n",
      "Step: 290, Training Loss: 5.66770, LR: 0.0009046, Tokens/sec: 245265.32126355002\n",
      "Step: 291, Training Loss: 5.65654, LR: 0.0009036, Tokens/sec: 244057.78123976447\n",
      "Step: 292, Training Loss: 5.63486, LR: 0.0009027, Tokens/sec: 244768.5133350297\n",
      "Step: 293, Training Loss: 5.62809, LR: 0.0009017, Tokens/sec: 244956.21787277298\n",
      "Step: 294, Training Loss: 5.57429, LR: 0.0009007, Tokens/sec: 244690.46697324238\n",
      "Step: 295, Training Loss: 5.84038, LR: 0.0008997, Tokens/sec: 245032.625012392\n",
      "Step: 296, Training Loss: 5.55550, LR: 0.0008987, Tokens/sec: 244860.58209331174\n",
      "Step: 297, Training Loss: 5.59549, LR: 0.0008977, Tokens/sec: 243711.10631960514\n",
      "Step: 298, Training Loss: 5.68545, LR: 0.0008967, Tokens/sec: 245171.2224487502\n",
      "Step: 299, Training Loss: 5.60370, LR: 0.0008957, Tokens/sec: 244495.84133421714\n",
      "Step: 300, Training Loss: 5.67590, LR: 0.0008947, Tokens/sec: 244847.37624727207\n",
      "Computing Eval loss, steps: 78\n",
      "Step: 300, Eval Loss: 5.58351\n",
      "Step: 301, Training Loss: 5.46457, LR: 0.0008937, Tokens/sec: 245019.70261557854\n",
      "Step: 302, Training Loss: 5.52605, LR: 0.0008927, Tokens/sec: 245073.95238545703\n",
      "Step: 303, Training Loss: 5.67946, LR: 0.0008917, Tokens/sec: 244986.48740734102\n",
      "Step: 304, Training Loss: 5.70601, LR: 0.0008906, Tokens/sec: 245286.87405368887\n",
      "Step: 305, Training Loss: 5.46175, LR: 0.0008896, Tokens/sec: 245432.35907747204\n",
      "Step: 306, Training Loss: 5.49019, LR: 0.0008886, Tokens/sec: 245727.71908444987\n",
      "Step: 307, Training Loss: 5.58955, LR: 0.0008875, Tokens/sec: 244513.1638572308\n",
      "Step: 308, Training Loss: 5.58732, LR: 0.0008865, Tokens/sec: 245530.79802738078\n",
      "Step: 309, Training Loss: 5.50397, LR: 0.0008855, Tokens/sec: 244694.08995699463\n",
      "Step: 310, Training Loss: 5.53313, LR: 0.0008844, Tokens/sec: 244504.93131732865\n",
      "Step: 311, Training Loss: 5.48631, LR: 0.0008834, Tokens/sec: 244170.12746263432\n",
      "Step: 312, Training Loss: 5.56027, LR: 0.0008823, Tokens/sec: 245260.85900126447\n",
      "Step: 313, Training Loss: 5.43848, LR: 0.0008812, Tokens/sec: 243730.52620050855\n",
      "Step: 314, Training Loss: 5.57875, LR: 0.0008802, Tokens/sec: 244179.55588752817\n",
      "Step: 315, Training Loss: 5.59259, LR: 0.0008791, Tokens/sec: 244859.41992169758\n",
      "Step: 316, Training Loss: 5.47775, LR: 0.0008780, Tokens/sec: 245093.34593044824\n",
      "Step: 317, Training Loss: 5.54818, LR: 0.0008770, Tokens/sec: 245382.2373679524\n",
      "Step: 318, Training Loss: 5.54775, LR: 0.0008759, Tokens/sec: 244736.89829667314\n",
      "Step: 319, Training Loss: 5.60166, LR: 0.0008748, Tokens/sec: 245503.68625518915\n",
      "Step: 320, Training Loss: 5.73700, LR: 0.0008737, Tokens/sec: 245153.50763643289\n",
      "Step: 321, Training Loss: 5.47298, LR: 0.0008726, Tokens/sec: 244514.98715519035\n",
      "Step: 322, Training Loss: 5.49292, LR: 0.0008715, Tokens/sec: 245476.3115829463\n",
      "Step: 323, Training Loss: 5.43178, LR: 0.0008704, Tokens/sec: 244471.12849248978\n",
      "Step: 324, Training Loss: 5.40672, LR: 0.0008693, Tokens/sec: 245124.3085544567\n",
      "Step: 325, Training Loss: 5.50097, LR: 0.0008682, Tokens/sec: 244502.06832449927\n",
      "Step: 326, Training Loss: 5.59980, LR: 0.0008671, Tokens/sec: 245176.88079515184\n",
      "Step: 327, Training Loss: 5.46025, LR: 0.0008660, Tokens/sec: 244739.04327767444\n",
      "Step: 328, Training Loss: 5.37314, LR: 0.0008648, Tokens/sec: 244876.97125100013\n",
      "Step: 329, Training Loss: 5.59838, LR: 0.0008637, Tokens/sec: 245196.55378182206\n",
      "Step: 330, Training Loss: 5.57617, LR: 0.0008626, Tokens/sec: 244686.05453154663\n",
      "Step: 331, Training Loss: 5.52240, LR: 0.0008615, Tokens/sec: 244722.71513052576\n",
      "Step: 332, Training Loss: 5.52731, LR: 0.0008603, Tokens/sec: 245105.9294758756\n",
      "Step: 333, Training Loss: 5.54166, LR: 0.0008592, Tokens/sec: 244843.77257669493\n",
      "Step: 334, Training Loss: 5.53650, LR: 0.0008580, Tokens/sec: 244142.27346575077\n",
      "Step: 335, Training Loss: 5.58043, LR: 0.0008569, Tokens/sec: 243745.48953743043\n",
      "Step: 336, Training Loss: 5.54879, LR: 0.0008557, Tokens/sec: 245112.31906503846\n",
      "Step: 337, Training Loss: 5.25478, LR: 0.0008546, Tokens/sec: 244742.0122657968\n",
      "Step: 338, Training Loss: 5.38484, LR: 0.0008534, Tokens/sec: 244611.20341135026\n",
      "Step: 339, Training Loss: 5.43725, LR: 0.0008523, Tokens/sec: 244464.83513529517\n",
      "Step: 340, Training Loss: 5.50849, LR: 0.0008511, Tokens/sec: 245146.99633538938\n",
      "Step: 341, Training Loss: 5.51968, LR: 0.0008499, Tokens/sec: 244694.65323935184\n",
      "Step: 342, Training Loss: 5.34795, LR: 0.0008488, Tokens/sec: 245138.79962526434\n",
      "Step: 343, Training Loss: 5.38798, LR: 0.0008476, Tokens/sec: 244799.29174291342\n",
      "Step: 344, Training Loss: 5.41609, LR: 0.0008464, Tokens/sec: 244705.60798015608\n",
      "Step: 345, Training Loss: 5.32617, LR: 0.0008452, Tokens/sec: 244682.64618668242\n",
      "Step: 346, Training Loss: 5.14097, LR: 0.0008440, Tokens/sec: 245181.44080601516\n",
      "Step: 347, Training Loss: 5.27668, LR: 0.0008428, Tokens/sec: 245580.7164781557\n",
      "Step: 348, Training Loss: 5.42955, LR: 0.0008417, Tokens/sec: 244549.26783889814\n",
      "Step: 349, Training Loss: 5.56256, LR: 0.0008405, Tokens/sec: 245045.71431725804\n",
      "Step: 350, Training Loss: 5.38415, LR: 0.0008393, Tokens/sec: 243995.04096164208\n",
      "Step: 351, Training Loss: 5.48008, LR: 0.0008380, Tokens/sec: 245145.61452192036\n",
      "Step: 352, Training Loss: 5.48417, LR: 0.0008368, Tokens/sec: 244829.66220294946\n",
      "Step: 353, Training Loss: 5.37643, LR: 0.0008356, Tokens/sec: 244164.0156430422\n",
      "Step: 354, Training Loss: 5.46294, LR: 0.0008344, Tokens/sec: 245303.8658963939\n",
      "Step: 355, Training Loss: 5.41319, LR: 0.0008332, Tokens/sec: 245113.12162918248\n",
      "Step: 356, Training Loss: 5.35500, LR: 0.0008320, Tokens/sec: 245159.46239762972\n",
      "Step: 357, Training Loss: 5.23485, LR: 0.0008307, Tokens/sec: 244675.63070629167\n",
      "Step: 358, Training Loss: 5.33236, LR: 0.0008295, Tokens/sec: 244915.1773948912\n",
      "Step: 359, Training Loss: 5.46417, LR: 0.0008283, Tokens/sec: 244705.64542230678\n",
      "Step: 360, Training Loss: 5.42640, LR: 0.0008270, Tokens/sec: 244937.87251675487\n",
      "Step: 361, Training Loss: 5.35769, LR: 0.0008258, Tokens/sec: 245468.32402938412\n",
      "Step: 362, Training Loss: 5.38021, LR: 0.0008246, Tokens/sec: 244946.4463009607\n",
      "Step: 363, Training Loss: 5.32952, LR: 0.0008233, Tokens/sec: 244926.15530030773\n",
      "Step: 364, Training Loss: 5.54115, LR: 0.0008221, Tokens/sec: 244698.73924127515\n",
      "Step: 365, Training Loss: 5.36886, LR: 0.0008208, Tokens/sec: 245319.32921931832\n",
      "Step: 366, Training Loss: 5.37089, LR: 0.0008196, Tokens/sec: 244251.0180043599\n",
      "Step: 367, Training Loss: 5.35748, LR: 0.0008183, Tokens/sec: 244797.9070336484\n",
      "Step: 368, Training Loss: 5.33636, LR: 0.0008170, Tokens/sec: 244961.77590864425\n",
      "Step: 369, Training Loss: 5.37677, LR: 0.0008158, Tokens/sec: 244425.46866463756\n",
      "Step: 370, Training Loss: 5.31875, LR: 0.0008145, Tokens/sec: 245012.918604274\n",
      "Step: 371, Training Loss: 5.28073, LR: 0.0008132, Tokens/sec: 245515.6592405129\n",
      "Step: 372, Training Loss: 5.53829, LR: 0.0008120, Tokens/sec: 244836.34577178909\n",
      "Step: 373, Training Loss: 5.43766, LR: 0.0008107, Tokens/sec: 244505.50902134064\n",
      "Step: 374, Training Loss: 5.37285, LR: 0.0008094, Tokens/sec: 243410.0496877509\n",
      "Step: 375, Training Loss: 5.21171, LR: 0.0008081, Tokens/sec: 244550.81461362774\n",
      "Step: 376, Training Loss: 5.30225, LR: 0.0008068, Tokens/sec: 244843.43010571998\n",
      "Step: 377, Training Loss: 5.40562, LR: 0.0008055, Tokens/sec: 244553.98471309096\n",
      "Step: 378, Training Loss: 5.38226, LR: 0.0008042, Tokens/sec: 245140.53320339203\n",
      "Step: 379, Training Loss: 5.31276, LR: 0.0008029, Tokens/sec: 245237.22539619292\n",
      "Step: 380, Training Loss: 5.38781, LR: 0.0008016, Tokens/sec: 244851.84221979286\n",
      "Step: 381, Training Loss: 5.40841, LR: 0.0008003, Tokens/sec: 244846.69980551163\n",
      "Step: 382, Training Loss: 5.35520, LR: 0.0007990, Tokens/sec: 245000.51862094968\n",
      "Step: 383, Training Loss: 5.33092, LR: 0.0007977, Tokens/sec: 244731.48664265653\n",
      "Step: 384, Training Loss: 5.23820, LR: 0.0007964, Tokens/sec: 244902.6287693787\n",
      "Step: 385, Training Loss: 5.30946, LR: 0.0007951, Tokens/sec: 244818.80878283476\n",
      "Step: 386, Training Loss: 5.42290, LR: 0.0007938, Tokens/sec: 244667.39913240095\n",
      "Step: 387, Training Loss: 5.42146, LR: 0.0007924, Tokens/sec: 244980.29712509358\n",
      "Step: 388, Training Loss: 5.51313, LR: 0.0007911, Tokens/sec: 244687.83786714688\n",
      "Step: 389, Training Loss: 5.34243, LR: 0.0007898, Tokens/sec: 245188.9739943132\n",
      "Step: 390, Training Loss: 5.41244, LR: 0.0007885, Tokens/sec: 245021.25705336238\n",
      "Step: 391, Training Loss: 5.42397, LR: 0.0007871, Tokens/sec: 245055.357268162\n",
      "Step: 392, Training Loss: 5.39388, LR: 0.0007858, Tokens/sec: 245404.17700381423\n",
      "Step: 393, Training Loss: 5.30440, LR: 0.0007845, Tokens/sec: 244447.9661200258\n",
      "Step: 394, Training Loss: 5.35140, LR: 0.0007831, Tokens/sec: 244893.25521753574\n",
      "Step: 395, Training Loss: 5.30591, LR: 0.0007818, Tokens/sec: 244668.02864618978\n",
      "Step: 396, Training Loss: 5.47322, LR: 0.0007804, Tokens/sec: 245210.9611125552\n",
      "Step: 397, Training Loss: 5.33410, LR: 0.0007791, Tokens/sec: 244652.02639034836\n",
      "Step: 398, Training Loss: 5.29972, LR: 0.0007777, Tokens/sec: 244749.71256321477\n",
      "Step: 399, Training Loss: 5.33994, LR: 0.0007764, Tokens/sec: 244854.9400407994\n",
      "Step: 400, Training Loss: 5.47993, LR: 0.0007750, Tokens/sec: 245061.95075268063\n",
      "Computing Eval loss, steps: 78\n",
      "Step: 400, Eval Loss: 5.28742\n",
      "Step: 401, Training Loss: 5.24597, LR: 0.0007736, Tokens/sec: 245617.51707981116\n",
      "Step: 402, Training Loss: 5.30338, LR: 0.0007723, Tokens/sec: 244618.53833106026\n",
      "Step: 403, Training Loss: 5.24364, LR: 0.0007709, Tokens/sec: 244485.94166733022\n",
      "Step: 404, Training Loss: 5.22944, LR: 0.0007695, Tokens/sec: 244974.91563763472\n",
      "Step: 405, Training Loss: 5.11154, LR: 0.0007682, Tokens/sec: 244285.30951026868\n",
      "Step: 406, Training Loss: 5.23535, LR: 0.0007668, Tokens/sec: 245172.17403040515\n",
      "Step: 407, Training Loss: 5.38262, LR: 0.0007654, Tokens/sec: 245886.40523811892\n",
      "Step: 408, Training Loss: 5.25093, LR: 0.0007640, Tokens/sec: 245353.7297468126\n",
      "Step: 409, Training Loss: 5.18389, LR: 0.0007626, Tokens/sec: 245087.38925893864\n",
      "Step: 410, Training Loss: 5.41943, LR: 0.0007613, Tokens/sec: 245006.84982600305\n",
      "Step: 411, Training Loss: 5.25956, LR: 0.0007599, Tokens/sec: 244460.6006781167\n",
      "Step: 412, Training Loss: 5.32269, LR: 0.0007585, Tokens/sec: 245065.48062305764\n",
      "Step: 413, Training Loss: 5.33287, LR: 0.0007571, Tokens/sec: 244243.20663303777\n",
      "Step: 414, Training Loss: 5.33437, LR: 0.0007557, Tokens/sec: 244365.43798605003\n",
      "Step: 415, Training Loss: 5.34988, LR: 0.0007543, Tokens/sec: 245187.98469233117\n",
      "Step: 416, Training Loss: 5.20407, LR: 0.0007529, Tokens/sec: 244896.71545590486\n",
      "Step: 417, Training Loss: 5.16379, LR: 0.0007515, Tokens/sec: 244997.4290570911\n",
      "Step: 418, Training Loss: 5.33494, LR: 0.0007501, Tokens/sec: 244909.955605751\n",
      "Step: 419, Training Loss: 5.28903, LR: 0.0007487, Tokens/sec: 244732.39395895213\n",
      "Step: 420, Training Loss: 5.26171, LR: 0.0007473, Tokens/sec: 243999.35744763614\n",
      "Step: 421, Training Loss: 5.30247, LR: 0.0007459, Tokens/sec: 245327.7791985203\n",
      "Step: 422, Training Loss: 5.22845, LR: 0.0007444, Tokens/sec: 244458.37395262986\n",
      "Step: 423, Training Loss: 5.17214, LR: 0.0007430, Tokens/sec: 244347.09107240773\n",
      "Step: 424, Training Loss: 5.22054, LR: 0.0007416, Tokens/sec: 244669.33532298915\n",
      "Step: 425, Training Loss: 5.35438, LR: 0.0007402, Tokens/sec: 245253.50431641671\n",
      "Step: 426, Training Loss: 5.13699, LR: 0.0007388, Tokens/sec: 244494.88140349585\n",
      "Step: 427, Training Loss: 5.20199, LR: 0.0007373, Tokens/sec: 244829.7899761437\n",
      "Step: 428, Training Loss: 5.26089, LR: 0.0007359, Tokens/sec: 245450.25804605326\n",
      "Step: 429, Training Loss: 5.10739, LR: 0.0007345, Tokens/sec: 245086.09689236776\n",
      "Step: 430, Training Loss: 5.17265, LR: 0.0007330, Tokens/sec: 243850.98066009243\n",
      "Step: 431, Training Loss: 5.30545, LR: 0.0007316, Tokens/sec: 244669.4237964019\n",
      "Step: 432, Training Loss: 5.31147, LR: 0.0007302, Tokens/sec: 245074.7034867547\n",
      "Step: 433, Training Loss: 5.28604, LR: 0.0007287, Tokens/sec: 244146.40541819428\n",
      "Step: 434, Training Loss: 5.22609, LR: 0.0007273, Tokens/sec: 245037.83838150933\n",
      "Step: 435, Training Loss: 5.23044, LR: 0.0007258, Tokens/sec: 245313.05882154356\n",
      "Step: 436, Training Loss: 5.12794, LR: 0.0007244, Tokens/sec: 244440.14044843926\n",
      "Step: 437, Training Loss: 5.19282, LR: 0.0007229, Tokens/sec: 245028.8998504827\n",
      "Step: 438, Training Loss: 5.14515, LR: 0.0007215, Tokens/sec: 244784.53934847593\n",
      "Step: 439, Training Loss: 5.17839, LR: 0.0007200, Tokens/sec: 245295.61593961797\n",
      "Step: 440, Training Loss: 5.37915, LR: 0.0007186, Tokens/sec: 245055.43578025745\n",
      "Step: 441, Training Loss: 5.47615, LR: 0.0007171, Tokens/sec: 243154.66684629404\n",
      "Step: 442, Training Loss: 5.18366, LR: 0.0007157, Tokens/sec: 244925.25677658533\n",
      "Step: 443, Training Loss: 5.30535, LR: 0.0007142, Tokens/sec: 244431.95018342798\n",
      "Step: 444, Training Loss: 5.16594, LR: 0.0007127, Tokens/sec: 244455.66830645836\n",
      "Step: 445, Training Loss: 5.17653, LR: 0.0007113, Tokens/sec: 244564.8690634433\n",
      "Step: 446, Training Loss: 5.25998, LR: 0.0007098, Tokens/sec: 245092.1678898982\n",
      "Step: 447, Training Loss: 5.19342, LR: 0.0007083, Tokens/sec: 245373.42770313416\n",
      "Step: 448, Training Loss: 5.18235, LR: 0.0007069, Tokens/sec: 244382.942331813\n",
      "Step: 449, Training Loss: 5.12989, LR: 0.0007054, Tokens/sec: 244738.9445396282\n",
      "Step: 450, Training Loss: 5.22517, LR: 0.0007039, Tokens/sec: 245204.91498821424\n",
      "Step: 451, Training Loss: 5.43173, LR: 0.0007024, Tokens/sec: 244883.17676514192\n",
      "Step: 452, Training Loss: 5.25942, LR: 0.0007010, Tokens/sec: 245093.23666244248\n",
      "Step: 453, Training Loss: 5.19826, LR: 0.0006995, Tokens/sec: 245303.8693168891\n",
      "Step: 454, Training Loss: 5.17834, LR: 0.0006980, Tokens/sec: 244564.71776722337\n",
      "Step: 455, Training Loss: 5.21584, LR: 0.0006965, Tokens/sec: 244872.5486719549\n",
      "Step: 456, Training Loss: 5.26988, LR: 0.0006950, Tokens/sec: 244637.12504489816\n",
      "Step: 457, Training Loss: 5.21798, LR: 0.0006935, Tokens/sec: 244956.89833062986\n",
      "Step: 458, Training Loss: 5.31949, LR: 0.0006920, Tokens/sec: 244216.11571123722\n",
      "Step: 459, Training Loss: 5.12444, LR: 0.0006906, Tokens/sec: 244632.05627951797\n",
      "Step: 460, Training Loss: 5.09431, LR: 0.0006891, Tokens/sec: 245421.42819065112\n",
      "Step: 461, Training Loss: 5.23250, LR: 0.0006876, Tokens/sec: 244279.73633697216\n",
      "Step: 462, Training Loss: 5.24609, LR: 0.0006861, Tokens/sec: 245100.4929525402\n",
      "Step: 463, Training Loss: 5.13119, LR: 0.0006846, Tokens/sec: 244783.79683671772\n",
      "Step: 464, Training Loss: 5.18877, LR: 0.0006831, Tokens/sec: 245342.6159649775\n",
      "Step: 465, Training Loss: 5.23485, LR: 0.0006816, Tokens/sec: 244179.35592415888\n",
      "Step: 466, Training Loss: 5.23178, LR: 0.0006801, Tokens/sec: 244528.98480388388\n",
      "Step: 467, Training Loss: 5.13401, LR: 0.0006786, Tokens/sec: 244949.5959702876\n",
      "Step: 468, Training Loss: 5.23130, LR: 0.0006771, Tokens/sec: 244519.2234967201\n",
      "Step: 469, Training Loss: 5.11614, LR: 0.0006755, Tokens/sec: 244358.60869546857\n",
      "Step: 470, Training Loss: 5.16146, LR: 0.0006740, Tokens/sec: 244753.31857324488\n",
      "Step: 471, Training Loss: 5.12575, LR: 0.0006725, Tokens/sec: 245157.27929324322\n",
      "Step: 472, Training Loss: 4.99459, LR: 0.0006710, Tokens/sec: 244783.24676783817\n",
      "Step: 473, Training Loss: 5.10260, LR: 0.0006695, Tokens/sec: 244136.8745242299\n",
      "Step: 474, Training Loss: 5.17880, LR: 0.0006680, Tokens/sec: 245293.474872381\n",
      "Step: 475, Training Loss: 5.22102, LR: 0.0006665, Tokens/sec: 244590.24527797365\n",
      "Step: 476, Training Loss: 5.06391, LR: 0.0006650, Tokens/sec: 244791.20512594486\n",
      "Step: 477, Training Loss: 5.06437, LR: 0.0006634, Tokens/sec: 244472.45854710494\n",
      "Step: 478, Training Loss: 5.16716, LR: 0.0006619, Tokens/sec: 245182.98875384757\n",
      "Step: 479, Training Loss: 5.25647, LR: 0.0006604, Tokens/sec: 244611.09117161855\n",
      "Step: 480, Training Loss: 5.19464, LR: 0.0006589, Tokens/sec: 245198.62822374178\n",
      "Step: 481, Training Loss: 5.19716, LR: 0.0006573, Tokens/sec: 244767.33841501703\n",
      "Step: 482, Training Loss: 5.02539, LR: 0.0006558, Tokens/sec: 245284.63396021028\n",
      "Step: 483, Training Loss: 5.25180, LR: 0.0006543, Tokens/sec: 245182.3292505049\n",
      "Step: 484, Training Loss: 4.99108, LR: 0.0006528, Tokens/sec: 244936.93127868063\n",
      "Step: 485, Training Loss: 5.12731, LR: 0.0006512, Tokens/sec: 245084.1097132937\n",
      "Step: 486, Training Loss: 5.12520, LR: 0.0006497, Tokens/sec: 244335.42182561685\n",
      "Step: 487, Training Loss: 5.01107, LR: 0.0006482, Tokens/sec: 244835.2894608221\n",
      "Step: 488, Training Loss: 5.20090, LR: 0.0006466, Tokens/sec: 245475.65906389715\n",
      "Step: 489, Training Loss: 5.08557, LR: 0.0006451, Tokens/sec: 244922.72321388786\n",
      "Step: 490, Training Loss: 5.19087, LR: 0.0006436, Tokens/sec: 243893.19693263463\n",
      "Step: 491, Training Loss: 5.07730, LR: 0.0006420, Tokens/sec: 244849.54702012887\n",
      "Step: 492, Training Loss: 5.21735, LR: 0.0006405, Tokens/sec: 245139.07460498635\n",
      "Step: 493, Training Loss: 5.05253, LR: 0.0006389, Tokens/sec: 244916.81574942145\n",
      "Step: 494, Training Loss: 5.04208, LR: 0.0006374, Tokens/sec: 244241.63661957544\n",
      "Step: 495, Training Loss: 5.08616, LR: 0.0006359, Tokens/sec: 245472.33657975856\n",
      "Step: 496, Training Loss: 5.05730, LR: 0.0006343, Tokens/sec: 244183.21798647442\n",
      "Step: 497, Training Loss: 5.19425, LR: 0.0006328, Tokens/sec: 244956.8829819147\n",
      "Step: 498, Training Loss: 5.17922, LR: 0.0006312, Tokens/sec: 244992.2071718649\n",
      "Step: 499, Training Loss: 5.10265, LR: 0.0006297, Tokens/sec: 244310.7481866851\n",
      "Step: 500, Training Loss: 5.16162, LR: 0.0006281, Tokens/sec: 244776.26468421693\n",
      "Computing Eval loss, steps: 78\n",
      "Step: 500, Eval Loss: 5.09321\n",
      "Step: 501, Training Loss: 5.16144, LR: 0.0006266, Tokens/sec: 243131.36514524987\n",
      "Step: 502, Training Loss: 5.09711, LR: 0.0006250, Tokens/sec: 245300.73618328446\n",
      "Step: 503, Training Loss: 5.27211, LR: 0.0006235, Tokens/sec: 244988.58217826483\n",
      "Step: 504, Training Loss: 5.14560, LR: 0.0006219, Tokens/sec: 245382.271594768\n",
      "Step: 505, Training Loss: 5.05355, LR: 0.0006204, Tokens/sec: 244697.95981013437\n",
      "Step: 506, Training Loss: 4.94411, LR: 0.0006188, Tokens/sec: 245315.28233017324\n",
      "Step: 507, Training Loss: 5.11624, LR: 0.0006173, Tokens/sec: 244485.3878397309\n",
      "Step: 508, Training Loss: 4.98015, LR: 0.0006157, Tokens/sec: 245185.8164573608\n",
      "Step: 509, Training Loss: 5.01359, LR: 0.0006142, Tokens/sec: 244828.0045707981\n",
      "Step: 510, Training Loss: 5.00498, LR: 0.0006126, Tokens/sec: 244071.542040112\n",
      "Step: 511, Training Loss: 5.02920, LR: 0.0006111, Tokens/sec: 244997.82655044788\n",
      "Step: 512, Training Loss: 5.11443, LR: 0.0006095, Tokens/sec: 244760.18697126448\n",
      "Step: 513, Training Loss: 5.08784, LR: 0.0006080, Tokens/sec: 244566.490834682\n",
      "Step: 514, Training Loss: 5.07505, LR: 0.0006064, Tokens/sec: 245566.34964764945\n",
      "Step: 515, Training Loss: 5.18772, LR: 0.0006048, Tokens/sec: 244430.27076293493\n",
      "Step: 516, Training Loss: 5.06003, LR: 0.0006033, Tokens/sec: 245193.90182811502\n",
      "Step: 517, Training Loss: 4.97259, LR: 0.0006017, Tokens/sec: 245418.19276643248\n",
      "Step: 518, Training Loss: 5.13668, LR: 0.0006002, Tokens/sec: 244681.96384846917\n",
      "Step: 519, Training Loss: 4.99143, LR: 0.0005986, Tokens/sec: 245664.4261440657\n",
      "Step: 520, Training Loss: 5.02471, LR: 0.0005970, Tokens/sec: 245296.1922556341\n",
      "Step: 521, Training Loss: 5.05791, LR: 0.0005955, Tokens/sec: 244859.47104338414\n",
      "Step: 522, Training Loss: 5.06112, LR: 0.0005939, Tokens/sec: 244886.6060388855\n",
      "Step: 523, Training Loss: 5.02285, LR: 0.0005923, Tokens/sec: 245165.09115988863\n",
      "Step: 524, Training Loss: 5.14742, LR: 0.0005908, Tokens/sec: 244473.2501318171\n",
      "Step: 525, Training Loss: 5.07269, LR: 0.0005892, Tokens/sec: 244648.67172822522\n",
      "Step: 526, Training Loss: 5.03745, LR: 0.0005877, Tokens/sec: 244905.4380768745\n",
      "Step: 527, Training Loss: 5.03559, LR: 0.0005861, Tokens/sec: 244892.45238802765\n",
      "Step: 528, Training Loss: 5.13204, LR: 0.0005845, Tokens/sec: 244833.83960367454\n",
      "Step: 529, Training Loss: 5.04291, LR: 0.0005830, Tokens/sec: 245042.69358432374\n",
      "Step: 530, Training Loss: 4.99109, LR: 0.0005814, Tokens/sec: 243816.858283033\n",
      "Step: 531, Training Loss: 5.10941, LR: 0.0005798, Tokens/sec: 244805.357056269\n",
      "Step: 532, Training Loss: 5.04610, LR: 0.0005783, Tokens/sec: 245069.80604774397\n",
      "Step: 533, Training Loss: 4.98441, LR: 0.0005767, Tokens/sec: 245247.43387096675\n",
      "Step: 534, Training Loss: 5.01964, LR: 0.0005751, Tokens/sec: 245596.33644301235\n",
      "Step: 535, Training Loss: 5.11451, LR: 0.0005736, Tokens/sec: 245135.67753851824\n",
      "Step: 536, Training Loss: 4.98047, LR: 0.0005720, Tokens/sec: 244927.22944547722\n",
      "Step: 537, Training Loss: 4.99073, LR: 0.0005704, Tokens/sec: 243621.2138491933\n",
      "Step: 538, Training Loss: 5.02433, LR: 0.0005688, Tokens/sec: 245461.6350244822\n",
      "Step: 539, Training Loss: 5.03378, LR: 0.0005673, Tokens/sec: 244730.43634307594\n",
      "Step: 540, Training Loss: 4.86265, LR: 0.0005657, Tokens/sec: 244847.21948965482\n",
      "Step: 541, Training Loss: 4.99656, LR: 0.0005641, Tokens/sec: 245037.49366089192\n",
      "Step: 542, Training Loss: 5.00634, LR: 0.0005626, Tokens/sec: 244837.16356718465\n",
      "Step: 543, Training Loss: 5.14292, LR: 0.0005610, Tokens/sec: 244768.41457320313\n",
      "Step: 544, Training Loss: 5.02656, LR: 0.0005594, Tokens/sec: 245002.87465768357\n",
      "Step: 545, Training Loss: 4.98959, LR: 0.0005579, Tokens/sec: 245286.69792271958\n",
      "Step: 546, Training Loss: 4.94463, LR: 0.0005563, Tokens/sec: 244690.46186812726\n",
      "Step: 547, Training Loss: 4.90083, LR: 0.0005547, Tokens/sec: 245413.1857304511\n",
      "Step: 548, Training Loss: 4.98764, LR: 0.0005531, Tokens/sec: 245212.50088829352\n",
      "Step: 549, Training Loss: 5.13082, LR: 0.0005516, Tokens/sec: 244209.15916277457\n",
      "Step: 550, Training Loss: 5.16520, LR: 0.0005500, Tokens/sec: 244680.054682966\n",
      "Step: 551, Training Loss: 5.12024, LR: 0.0005484, Tokens/sec: 245438.93519827002\n",
      "Step: 552, Training Loss: 4.92859, LR: 0.0005469, Tokens/sec: 244511.46462918533\n",
      "Step: 553, Training Loss: 5.01700, LR: 0.0005453, Tokens/sec: 244385.92645088007\n",
      "Step: 554, Training Loss: 5.04489, LR: 0.0005437, Tokens/sec: 245032.70862932783\n",
      "Step: 555, Training Loss: 4.98988, LR: 0.0005421, Tokens/sec: 245083.7836414546\n",
      "Step: 556, Training Loss: 4.93920, LR: 0.0005406, Tokens/sec: 244227.05476991308\n",
      "Step: 557, Training Loss: 5.10256, LR: 0.0005390, Tokens/sec: 245006.91295203398\n",
      "Step: 558, Training Loss: 4.95440, LR: 0.0005374, Tokens/sec: 245464.9349565056\n",
      "Step: 559, Training Loss: 5.06054, LR: 0.0005359, Tokens/sec: 244360.19888137357\n",
      "Step: 560, Training Loss: 5.11081, LR: 0.0005343, Tokens/sec: 244846.31643337736\n",
      "Step: 561, Training Loss: 4.93914, LR: 0.0005327, Tokens/sec: 244738.7351471386\n",
      "Step: 562, Training Loss: 4.78807, LR: 0.0005312, Tokens/sec: 243770.60816466736\n",
      "Step: 563, Training Loss: 4.98026, LR: 0.0005296, Tokens/sec: 245068.0564001896\n",
      "Step: 564, Training Loss: 5.04872, LR: 0.0005280, Tokens/sec: 244456.97950980405\n",
      "Step: 565, Training Loss: 5.00535, LR: 0.0005264, Tokens/sec: 245288.23351968112\n",
      "Step: 566, Training Loss: 4.97548, LR: 0.0005249, Tokens/sec: 245000.77623029056\n",
      "Step: 567, Training Loss: 4.94182, LR: 0.0005233, Tokens/sec: 244986.41576271495\n",
      "Step: 568, Training Loss: 4.74826, LR: 0.0005217, Tokens/sec: 245387.64874608125\n",
      "Step: 569, Training Loss: 5.06417, LR: 0.0005202, Tokens/sec: 245079.39285281824\n",
      "Step: 570, Training Loss: 4.90596, LR: 0.0005186, Tokens/sec: 244308.48517316135\n",
      "Step: 571, Training Loss: 4.93302, LR: 0.0005170, Tokens/sec: 243953.25965458874\n",
      "Step: 572, Training Loss: 5.00745, LR: 0.0005155, Tokens/sec: 245446.11951223065\n",
      "Step: 573, Training Loss: 5.04419, LR: 0.0005139, Tokens/sec: 244941.85069162934\n",
      "Step: 574, Training Loss: 4.94096, LR: 0.0005123, Tokens/sec: 244356.0138687795\n",
      "Step: 575, Training Loss: 4.84871, LR: 0.0005108, Tokens/sec: 244896.2364712652\n",
      "Step: 576, Training Loss: 4.93823, LR: 0.0005092, Tokens/sec: 244409.4828406025\n",
      "Step: 577, Training Loss: 4.91715, LR: 0.0005077, Tokens/sec: 245068.56166169827\n",
      "Step: 578, Training Loss: 4.95178, LR: 0.0005061, Tokens/sec: 243942.82036900453\n",
      "Step: 579, Training Loss: 5.06762, LR: 0.0005045, Tokens/sec: 244758.86059466313\n",
      "Step: 580, Training Loss: 4.94697, LR: 0.0005030, Tokens/sec: 245353.7246139839\n",
      "Step: 581, Training Loss: 5.13007, LR: 0.0005014, Tokens/sec: 245109.50330176787\n",
      "Step: 582, Training Loss: 4.59328, LR: 0.0004998, Tokens/sec: 244690.21001604552\n",
      "Step: 583, Training Loss: 4.89616, LR: 0.0004983, Tokens/sec: 244787.5537199961\n",
      "Step: 584, Training Loss: 4.74833, LR: 0.0004967, Tokens/sec: 244425.88807659375\n",
      "Step: 585, Training Loss: 4.91165, LR: 0.0004952, Tokens/sec: 245073.77656013644\n",
      "Step: 586, Training Loss: 5.09234, LR: 0.0004936, Tokens/sec: 244468.86420547173\n",
      "Step: 587, Training Loss: 4.98507, LR: 0.0004920, Tokens/sec: 245564.32725101133\n",
      "Step: 588, Training Loss: 5.05801, LR: 0.0004905, Tokens/sec: 244523.2237710421\n",
      "Step: 589, Training Loss: 4.86954, LR: 0.0004889, Tokens/sec: 245126.95387961867\n",
      "Step: 590, Training Loss: 4.88581, LR: 0.0004874, Tokens/sec: 245233.65980972708\n",
      "Step: 591, Training Loss: 4.95109, LR: 0.0004858, Tokens/sec: 245348.76640177754\n",
      "Step: 592, Training Loss: 4.95674, LR: 0.0004843, Tokens/sec: 244804.95507731164\n",
      "Step: 593, Training Loss: 4.99383, LR: 0.0004827, Tokens/sec: 244630.569708299\n",
      "Step: 594, Training Loss: 5.00928, LR: 0.0004812, Tokens/sec: 245348.51832534058\n",
      "Step: 595, Training Loss: 5.00439, LR: 0.0004796, Tokens/sec: 244546.7981391509\n",
      "Step: 596, Training Loss: 5.02369, LR: 0.0004781, Tokens/sec: 244604.20052960864\n",
      "Step: 597, Training Loss: 4.86258, LR: 0.0004765, Tokens/sec: 245143.54439294385\n",
      "Step: 598, Training Loss: 4.85894, LR: 0.0004750, Tokens/sec: 244849.96959263322\n",
      "Step: 599, Training Loss: 4.99214, LR: 0.0004734, Tokens/sec: 244715.66841733706\n",
      "Step: 600, Training Loss: 4.93788, LR: 0.0004719, Tokens/sec: 244143.11034847266\n",
      "Computing Eval loss, steps: 78\n",
      "Step: 600, Eval Loss: 4.94189\n",
      "Step: 601, Training Loss: 4.91854, LR: 0.0004703, Tokens/sec: 244956.44810245174\n",
      "Step: 602, Training Loss: 4.80546, LR: 0.0004688, Tokens/sec: 244847.71361760746\n",
      "Step: 603, Training Loss: 4.86559, LR: 0.0004672, Tokens/sec: 245095.94619626872\n",
      "Step: 604, Training Loss: 4.89606, LR: 0.0004657, Tokens/sec: 245467.942133219\n",
      "Step: 605, Training Loss: 5.00758, LR: 0.0004641, Tokens/sec: 244873.0377894106\n",
      "Step: 606, Training Loss: 4.85638, LR: 0.0004626, Tokens/sec: 245399.00451687997\n",
      "Step: 607, Training Loss: 4.89999, LR: 0.0004611, Tokens/sec: 245473.32646511105\n",
      "Step: 608, Training Loss: 4.96000, LR: 0.0004595, Tokens/sec: 244408.40813849718\n",
      "Step: 609, Training Loss: 4.95491, LR: 0.0004580, Tokens/sec: 244544.28089845023\n",
      "Step: 610, Training Loss: 4.82921, LR: 0.0004564, Tokens/sec: 245717.1496921553\n",
      "Step: 611, Training Loss: 4.95256, LR: 0.0004549, Tokens/sec: 244788.63346209092\n",
      "Step: 612, Training Loss: 5.05456, LR: 0.0004534, Tokens/sec: 245141.12416272203\n",
      "Step: 613, Training Loss: 5.00094, LR: 0.0004518, Tokens/sec: 244601.88446317692\n",
      "Step: 614, Training Loss: 4.89043, LR: 0.0004503, Tokens/sec: 245584.60071312453\n",
      "Step: 615, Training Loss: 4.82558, LR: 0.0004488, Tokens/sec: 244938.4966025098\n",
      "Step: 616, Training Loss: 4.80831, LR: 0.0004472, Tokens/sec: 242583.34192231353\n",
      "Step: 617, Training Loss: 4.98455, LR: 0.0004457, Tokens/sec: 244778.96210540461\n",
      "Step: 618, Training Loss: 4.89163, LR: 0.0004442, Tokens/sec: 244873.33944121975\n",
      "Step: 619, Training Loss: 5.05219, LR: 0.0004427, Tokens/sec: 244685.58658042268\n",
      "Step: 620, Training Loss: 4.97385, LR: 0.0004411, Tokens/sec: 244000.31179914193\n",
      "Step: 621, Training Loss: 4.94289, LR: 0.0004396, Tokens/sec: 245129.78199858905\n",
      "Step: 622, Training Loss: 4.98656, LR: 0.0004381, Tokens/sec: 244876.89626181594\n",
      "Step: 623, Training Loss: 5.00336, LR: 0.0004366, Tokens/sec: 244635.0141677524\n",
      "Step: 624, Training Loss: 5.06258, LR: 0.0004350, Tokens/sec: 244542.7664997412\n",
      "Step: 625, Training Loss: 4.83568, LR: 0.0004335, Tokens/sec: 245194.23844534895\n",
      "Step: 626, Training Loss: 5.01523, LR: 0.0004320, Tokens/sec: 245219.72493342552\n",
      "Step: 627, Training Loss: 4.85758, LR: 0.0004305, Tokens/sec: 244369.74722043637\n",
      "Step: 628, Training Loss: 4.71070, LR: 0.0004290, Tokens/sec: 245640.92735612934\n",
      "Step: 629, Training Loss: 4.93946, LR: 0.0004275, Tokens/sec: 243800.91142099912\n",
      "Step: 630, Training Loss: 4.77551, LR: 0.0004260, Tokens/sec: 245576.86664717604\n",
      "Step: 631, Training Loss: 4.96165, LR: 0.0004245, Tokens/sec: 244576.4888323833\n",
      "Step: 632, Training Loss: 4.83821, LR: 0.0004229, Tokens/sec: 244051.32801307898\n",
      "Step: 633, Training Loss: 5.01350, LR: 0.0004214, Tokens/sec: 244413.02450084756\n",
      "Step: 634, Training Loss: 4.80086, LR: 0.0004199, Tokens/sec: 244991.83699112988\n",
      "Step: 635, Training Loss: 4.90426, LR: 0.0004184, Tokens/sec: 244162.59236330166\n",
      "Step: 636, Training Loss: 4.95817, LR: 0.0004169, Tokens/sec: 244685.31091550743\n",
      "Step: 637, Training Loss: 4.86047, LR: 0.0004154, Tokens/sec: 245296.85921226756\n",
      "Step: 638, Training Loss: 4.84826, LR: 0.0004139, Tokens/sec: 244547.23496472478\n",
      "Step: 639, Training Loss: 4.81130, LR: 0.0004124, Tokens/sec: 245175.02198588496\n",
      "Step: 640, Training Loss: 4.88408, LR: 0.0004109, Tokens/sec: 245299.4142027765\n",
      "Step: 641, Training Loss: 5.04559, LR: 0.0004094, Tokens/sec: 244215.30205937178\n",
      "Step: 642, Training Loss: 4.85785, LR: 0.0004080, Tokens/sec: 244486.8709481111\n",
      "Step: 643, Training Loss: 4.84998, LR: 0.0004065, Tokens/sec: 245246.20990316363\n",
      "Step: 644, Training Loss: 4.87276, LR: 0.0004050, Tokens/sec: 244832.52094710243\n",
      "Step: 645, Training Loss: 4.90148, LR: 0.0004035, Tokens/sec: 245121.91602741723\n",
      "Step: 646, Training Loss: 5.15949, LR: 0.0004020, Tokens/sec: 245165.034785571\n",
      "Step: 647, Training Loss: 4.93672, LR: 0.0004005, Tokens/sec: 244560.81812924964\n",
      "Step: 648, Training Loss: 4.92907, LR: 0.0003990, Tokens/sec: 244709.82539989767\n",
      "Step: 649, Training Loss: 4.96311, LR: 0.0003976, Tokens/sec: 245251.41185070254\n",
      "Step: 650, Training Loss: 4.96130, LR: 0.0003961, Tokens/sec: 245133.69467898438\n",
      "Step: 651, Training Loss: 4.81029, LR: 0.0003946, Tokens/sec: 243841.85135904737\n",
      "Step: 652, Training Loss: 4.91550, LR: 0.0003931, Tokens/sec: 245452.8624659432\n",
      "Step: 653, Training Loss: 4.98184, LR: 0.0003917, Tokens/sec: 245335.49245086045\n",
      "Step: 654, Training Loss: 4.82656, LR: 0.0003902, Tokens/sec: 244781.5046155338\n",
      "Step: 655, Training Loss: 4.79509, LR: 0.0003887, Tokens/sec: 244944.1322750045\n",
      "Step: 656, Training Loss: 4.71131, LR: 0.0003873, Tokens/sec: 245113.01917389274\n",
      "Step: 657, Training Loss: 4.79420, LR: 0.0003858, Tokens/sec: 244455.31333284255\n",
      "Step: 658, Training Loss: 4.94065, LR: 0.0003843, Tokens/sec: 245015.46768317485\n",
      "Step: 659, Training Loss: 4.89946, LR: 0.0003829, Tokens/sec: 245458.11941870267\n",
      "Step: 660, Training Loss: 4.88602, LR: 0.0003814, Tokens/sec: 245166.86952667127\n",
      "Step: 661, Training Loss: 4.92160, LR: 0.0003800, Tokens/sec: 245025.11849184678\n",
      "Step: 662, Training Loss: 4.77952, LR: 0.0003785, Tokens/sec: 244427.21933703416\n",
      "Step: 663, Training Loss: 4.84422, LR: 0.0003771, Tokens/sec: 244939.43103193553\n",
      "Step: 664, Training Loss: 4.88557, LR: 0.0003756, Tokens/sec: 244615.90906975293\n",
      "Step: 665, Training Loss: 4.74347, LR: 0.0003742, Tokens/sec: 245240.2252867865\n",
      "Step: 666, Training Loss: 4.87060, LR: 0.0003727, Tokens/sec: 244395.12879225906\n",
      "Step: 667, Training Loss: 4.90201, LR: 0.0003713, Tokens/sec: 245228.63124416137\n",
      "Step: 668, Training Loss: 4.73923, LR: 0.0003698, Tokens/sec: 245161.0510661083\n",
      "Step: 669, Training Loss: 4.86507, LR: 0.0003684, Tokens/sec: 245019.68043384294\n",
      "Step: 670, Training Loss: 4.79024, LR: 0.0003670, Tokens/sec: 244371.6006283223\n",
      "Step: 671, Training Loss: 4.89599, LR: 0.0003655, Tokens/sec: 244922.16229087565\n",
      "Step: 672, Training Loss: 4.87075, LR: 0.0003641, Tokens/sec: 244809.46889989302\n",
      "Step: 673, Training Loss: 4.85155, LR: 0.0003627, Tokens/sec: 244623.76640924453\n",
      "Step: 674, Training Loss: 4.95566, LR: 0.0003612, Tokens/sec: 244641.33330030137\n",
      "Step: 675, Training Loss: 4.96693, LR: 0.0003598, Tokens/sec: 245332.05743626834\n",
      "Step: 676, Training Loss: 4.87568, LR: 0.0003584, Tokens/sec: 244298.49889680787\n",
      "Step: 677, Training Loss: 4.87514, LR: 0.0003570, Tokens/sec: 244513.76199112268\n",
      "Step: 678, Training Loss: 4.83113, LR: 0.0003556, Tokens/sec: 244853.30251398854\n",
      "Step: 679, Training Loss: 4.87394, LR: 0.0003541, Tokens/sec: 245425.5761635246\n",
      "Step: 680, Training Loss: 4.98365, LR: 0.0003527, Tokens/sec: 244880.67986431788\n",
      "Step: 681, Training Loss: 4.82529, LR: 0.0003513, Tokens/sec: 245309.45340245124\n",
      "Step: 682, Training Loss: 4.70399, LR: 0.0003499, Tokens/sec: 244216.16825977323\n",
      "Step: 683, Training Loss: 4.78684, LR: 0.0003485, Tokens/sec: 244737.31196865413\n",
      "Step: 684, Training Loss: 4.93959, LR: 0.0003471, Tokens/sec: 244567.43092785764\n",
      "Step: 685, Training Loss: 4.65179, LR: 0.0003457, Tokens/sec: 245063.59960597445\n",
      "Step: 686, Training Loss: 4.90270, LR: 0.0003443, Tokens/sec: 245271.55331536796\n",
      "Step: 687, Training Loss: 4.81848, LR: 0.0003429, Tokens/sec: 244567.61452736842\n",
      "Step: 688, Training Loss: 4.92154, LR: 0.0003415, Tokens/sec: 245138.9277213314\n",
      "Step: 689, Training Loss: 4.87047, LR: 0.0003401, Tokens/sec: 245370.51011170846\n",
      "Step: 690, Training Loss: 4.91258, LR: 0.0003387, Tokens/sec: 243796.8046688978\n",
      "Step: 691, Training Loss: 4.86627, LR: 0.0003374, Tokens/sec: 244779.5666492425\n",
      "Step: 692, Training Loss: 4.91225, LR: 0.0003360, Tokens/sec: 245483.22747049946\n",
      "Step: 693, Training Loss: 4.86689, LR: 0.0003346, Tokens/sec: 245625.65323964064\n",
      "Step: 694, Training Loss: 4.92742, LR: 0.0003332, Tokens/sec: 245020.95162440336\n",
      "Step: 695, Training Loss: 4.88465, LR: 0.0003318, Tokens/sec: 245116.08433169266\n",
      "Step: 696, Training Loss: 4.97901, LR: 0.0003305, Tokens/sec: 245282.29302074618\n",
      "Step: 697, Training Loss: 4.87624, LR: 0.0003291, Tokens/sec: 245007.27976610075\n",
      "Step: 698, Training Loss: 4.73431, LR: 0.0003277, Tokens/sec: 244349.09855992786\n",
      "Step: 699, Training Loss: 4.88849, LR: 0.0003264, Tokens/sec: 245338.5323821508\n",
      "Step: 700, Training Loss: 4.86279, LR: 0.0003250, Tokens/sec: 244334.4903022615\n",
      "Computing Eval loss, steps: 78\n",
      "Step: 700, Eval Loss: 4.83797\n",
      "Step: 701, Training Loss: 4.84265, LR: 0.0003236, Tokens/sec: 243397.28102317313\n",
      "Step: 702, Training Loss: 4.78408, LR: 0.0003223, Tokens/sec: 245434.35190789847\n",
      "Step: 703, Training Loss: 5.03827, LR: 0.0003209, Tokens/sec: 245116.0279799105\n",
      "Step: 704, Training Loss: 4.73685, LR: 0.0003196, Tokens/sec: 245314.65803329588\n",
      "Step: 705, Training Loss: 4.89796, LR: 0.0003182, Tokens/sec: 244453.9613853534\n",
      "Step: 706, Training Loss: 4.82646, LR: 0.0003169, Tokens/sec: 245179.54434028023\n",
      "Step: 707, Training Loss: 4.87197, LR: 0.0003155, Tokens/sec: 245168.46512459024\n",
      "Step: 708, Training Loss: 4.83716, LR: 0.0003142, Tokens/sec: 244720.6283079485\n",
      "Step: 709, Training Loss: 4.78673, LR: 0.0003129, Tokens/sec: 245090.89596027965\n",
      "Step: 710, Training Loss: 4.83355, LR: 0.0003115, Tokens/sec: 244119.36306523642\n",
      "Step: 711, Training Loss: 4.87369, LR: 0.0003102, Tokens/sec: 245224.1617711298\n",
      "Step: 712, Training Loss: 4.88559, LR: 0.0003089, Tokens/sec: 245705.60660012582\n",
      "Step: 713, Training Loss: 4.66955, LR: 0.0003076, Tokens/sec: 245001.70431081802\n",
      "Step: 714, Training Loss: 4.81932, LR: 0.0003062, Tokens/sec: 244698.2031691059\n",
      "Step: 715, Training Loss: 4.94778, LR: 0.0003049, Tokens/sec: 245413.7831404202\n",
      "Step: 716, Training Loss: 4.82764, LR: 0.0003036, Tokens/sec: 245021.0625342725\n",
      "Step: 717, Training Loss: 4.82042, LR: 0.0003023, Tokens/sec: 244211.50508901855\n",
      "Step: 718, Training Loss: 4.70746, LR: 0.0003010, Tokens/sec: 245259.75457367417\n",
      "Step: 719, Training Loss: 4.58454, LR: 0.0002997, Tokens/sec: 244883.44435412134\n",
      "Step: 720, Training Loss: 4.81417, LR: 0.0002984, Tokens/sec: 244344.32171012103\n",
      "Step: 721, Training Loss: 4.77091, LR: 0.0002971, Tokens/sec: 245463.8252652593\n",
      "Step: 722, Training Loss: 4.88619, LR: 0.0002958, Tokens/sec: 245316.95511938768\n",
      "Step: 723, Training Loss: 4.69899, LR: 0.0002945, Tokens/sec: 244855.9862972846\n",
      "Step: 724, Training Loss: 4.87049, LR: 0.0002932, Tokens/sec: 244493.40329482369\n",
      "Step: 725, Training Loss: 4.85339, LR: 0.0002919, Tokens/sec: 244959.19042701475\n",
      "Step: 726, Training Loss: 4.86844, LR: 0.0002906, Tokens/sec: 245095.77375408268\n",
      "Step: 727, Training Loss: 4.79688, LR: 0.0002893, Tokens/sec: 244600.51218900134\n",
      "Step: 728, Training Loss: 4.79555, LR: 0.0002880, Tokens/sec: 245187.1201275333\n",
      "Step: 729, Training Loss: 4.77499, LR: 0.0002868, Tokens/sec: 244944.88769650468\n",
      "Step: 730, Training Loss: 4.76647, LR: 0.0002855, Tokens/sec: 244969.00565519842\n",
      "Step: 731, Training Loss: 4.71778, LR: 0.0002842, Tokens/sec: 245519.2090465168\n",
      "Step: 732, Training Loss: 4.74668, LR: 0.0002830, Tokens/sec: 244389.8035380843\n",
      "Step: 733, Training Loss: 4.94199, LR: 0.0002817, Tokens/sec: 244667.36850748843\n",
      "Step: 734, Training Loss: 4.79840, LR: 0.0002804, Tokens/sec: 244515.17407450484\n",
      "Step: 735, Training Loss: 4.66738, LR: 0.0002792, Tokens/sec: 245359.47522771385\n",
      "Step: 736, Training Loss: 4.76500, LR: 0.0002779, Tokens/sec: 244726.54503617057\n",
      "Step: 737, Training Loss: 4.75714, LR: 0.0002767, Tokens/sec: 245047.00455131134\n",
      "Step: 738, Training Loss: 4.88497, LR: 0.0002754, Tokens/sec: 244996.36623784123\n",
      "Step: 739, Training Loss: 4.91843, LR: 0.0002742, Tokens/sec: 245504.9470554057\n",
      "Step: 740, Training Loss: 4.79085, LR: 0.0002730, Tokens/sec: 244436.1853462793\n",
      "Step: 741, Training Loss: 4.88794, LR: 0.0002717, Tokens/sec: 245642.7212077105\n",
      "Step: 742, Training Loss: 4.76146, LR: 0.0002705, Tokens/sec: 244810.8895109461\n",
      "Step: 743, Training Loss: 4.82208, LR: 0.0002693, Tokens/sec: 245079.07362198792\n",
      "Step: 744, Training Loss: 4.84456, LR: 0.0002680, Tokens/sec: 244622.81567833785\n",
      "Step: 745, Training Loss: 4.85137, LR: 0.0002668, Tokens/sec: 244872.42085412648\n",
      "Step: 746, Training Loss: 4.81289, LR: 0.0002656, Tokens/sec: 244857.8982092694\n",
      "Step: 747, Training Loss: 4.43261, LR: 0.0002644, Tokens/sec: 244396.2560036489\n",
      "Step: 748, Training Loss: 4.75293, LR: 0.0002632, Tokens/sec: 244678.9384645306\n",
      "Step: 749, Training Loss: 4.81136, LR: 0.0002620, Tokens/sec: 245605.86504869696\n",
      "Step: 750, Training Loss: 4.79195, LR: 0.0002607, Tokens/sec: 244753.99449913146\n",
      "Step: 751, Training Loss: 4.66383, LR: 0.0002595, Tokens/sec: 245137.22149270255\n",
      "Step: 752, Training Loss: 4.86651, LR: 0.0002583, Tokens/sec: 245252.01360189082\n",
      "Step: 753, Training Loss: 4.85437, LR: 0.0002572, Tokens/sec: 245097.11915104228\n",
      "Step: 754, Training Loss: 4.91328, LR: 0.0002560, Tokens/sec: 244493.09408354582\n",
      "Step: 755, Training Loss: 4.68742, LR: 0.0002548, Tokens/sec: 245192.5382814649\n",
      "Step: 756, Training Loss: 4.81346, LR: 0.0002536, Tokens/sec: 244838.9405872625\n",
      "Step: 757, Training Loss: 4.55526, LR: 0.0002524, Tokens/sec: 243932.0217566149\n",
      "Step: 758, Training Loss: 4.73774, LR: 0.0002512, Tokens/sec: 245476.79626469532\n",
      "Step: 759, Training Loss: 4.64333, LR: 0.0002501, Tokens/sec: 244337.70569981812\n",
      "Step: 760, Training Loss: 4.65421, LR: 0.0002489, Tokens/sec: 244994.5169884465\n",
      "Step: 761, Training Loss: 4.73086, LR: 0.0002477, Tokens/sec: 244899.83486789814\n",
      "Step: 762, Training Loss: 4.81685, LR: 0.0002466, Tokens/sec: 245494.33172764315\n",
      "Step: 763, Training Loss: 4.76465, LR: 0.0002454, Tokens/sec: 245043.63222217534\n",
      "Step: 764, Training Loss: 4.79477, LR: 0.0002443, Tokens/sec: 244291.9057216796\n",
      "Step: 765, Training Loss: 4.74530, LR: 0.0002431, Tokens/sec: 244171.21362616413\n",
      "Step: 766, Training Loss: 4.83076, LR: 0.0002420, Tokens/sec: 245308.078308477\n",
      "Step: 767, Training Loss: 4.81604, LR: 0.0002408, Tokens/sec: 244247.5539462612\n",
      "Step: 768, Training Loss: 4.66538, LR: 0.0002397, Tokens/sec: 245208.07130818392\n",
      "Step: 769, Training Loss: 4.80398, LR: 0.0002385, Tokens/sec: 244814.65233619898\n",
      "Step: 770, Training Loss: 4.76910, LR: 0.0002374, Tokens/sec: 245032.22569964878\n",
      "Step: 771, Training Loss: 4.97022, LR: 0.0002363, Tokens/sec: 244048.88359555282\n",
      "Step: 772, Training Loss: 4.75254, LR: 0.0002352, Tokens/sec: 244845.66725930094\n",
      "Step: 773, Training Loss: 4.80975, LR: 0.0002340, Tokens/sec: 244611.9703856069\n",
      "Step: 774, Training Loss: 5.34457, LR: 0.0002329, Tokens/sec: 244442.32608362884\n",
      "Step: 775, Training Loss: 4.95509, LR: 0.0002318, Tokens/sec: 245681.62988061446\n",
      "Step: 776, Training Loss: 4.84578, LR: 0.0002307, Tokens/sec: 245119.46548604083\n",
      "Step: 777, Training Loss: 4.79910, LR: 0.0002296, Tokens/sec: 244957.20018908512\n",
      "Step: 778, Training Loss: 4.78124, LR: 0.0002285, Tokens/sec: 245213.8219321693\n",
      "Step: 779, Training Loss: 4.90523, LR: 0.0002274, Tokens/sec: 245125.8284567065\n",
      "Step: 780, Training Loss: 4.82846, LR: 0.0002263, Tokens/sec: 245109.04055971824\n",
      "Step: 781, Training Loss: 4.85966, LR: 0.0002252, Tokens/sec: 245532.8061642486\n",
      "Step: 782, Training Loss: 4.79581, LR: 0.0002241, Tokens/sec: 244534.18526133464\n",
      "Step: 783, Training Loss: 4.80241, LR: 0.0002230, Tokens/sec: 245261.953180822\n",
      "Step: 784, Training Loss: 4.85139, LR: 0.0002220, Tokens/sec: 245130.0159702913\n",
      "Step: 785, Training Loss: 4.85030, LR: 0.0002209, Tokens/sec: 245506.50080391992\n",
      "Step: 786, Training Loss: 4.72194, LR: 0.0002198, Tokens/sec: 244915.87637801399\n",
      "Step: 787, Training Loss: 4.79845, LR: 0.0002188, Tokens/sec: 245709.11385573706\n",
      "Step: 788, Training Loss: 4.82521, LR: 0.0002177, Tokens/sec: 244332.61709647047\n",
      "Step: 789, Training Loss: 4.76197, LR: 0.0002166, Tokens/sec: 243171.40488804327\n",
      "Step: 790, Training Loss: 4.83671, LR: 0.0002156, Tokens/sec: 245137.24881950044\n",
      "Step: 791, Training Loss: 4.79314, LR: 0.0002145, Tokens/sec: 244710.8414828007\n",
      "Step: 792, Training Loss: 4.73476, LR: 0.0002135, Tokens/sec: 243886.23341050654\n",
      "Step: 793, Training Loss: 4.83405, LR: 0.0002125, Tokens/sec: 244743.6261725888\n",
      "Step: 794, Training Loss: 4.86951, LR: 0.0002114, Tokens/sec: 244085.6650653686\n",
      "Step: 795, Training Loss: 4.76949, LR: 0.0002104, Tokens/sec: 244780.87111175328\n",
      "Step: 796, Training Loss: 4.79434, LR: 0.0002094, Tokens/sec: 245155.50448194554\n",
      "Step: 797, Training Loss: 4.85842, LR: 0.0002083, Tokens/sec: 244394.78757519874\n",
      "Step: 798, Training Loss: 4.71412, LR: 0.0002073, Tokens/sec: 244861.97432808302\n",
      "Step: 799, Training Loss: 4.82908, LR: 0.0002063, Tokens/sec: 245266.73862144875\n",
      "Step: 800, Training Loss: 4.85656, LR: 0.0002053, Tokens/sec: 243808.7925154375\n",
      "Computing Eval loss, steps: 78\n",
      "Step: 800, Eval Loss: 4.75032\n",
      "Step: 801, Training Loss: 4.82111, LR: 0.0002043, Tokens/sec: 244862.85364241162\n",
      "Step: 802, Training Loss: 4.67241, LR: 0.0002033, Tokens/sec: 244889.4371369498\n",
      "Step: 803, Training Loss: 4.79185, LR: 0.0002023, Tokens/sec: 243975.65842121473\n",
      "Step: 804, Training Loss: 4.86929, LR: 0.0002013, Tokens/sec: 245011.1066406806\n",
      "Step: 805, Training Loss: 4.68526, LR: 0.0002003, Tokens/sec: 244929.53633194763\n",
      "Step: 806, Training Loss: 4.76708, LR: 0.0001993, Tokens/sec: 244360.96258526942\n",
      "Step: 807, Training Loss: 4.78367, LR: 0.0001983, Tokens/sec: 245080.26348658235\n",
      "Step: 808, Training Loss: 4.79359, LR: 0.0001973, Tokens/sec: 245052.85856166485\n",
      "Step: 809, Training Loss: 4.78689, LR: 0.0001964, Tokens/sec: 245063.91879649422\n",
      "Step: 810, Training Loss: 4.72237, LR: 0.0001954, Tokens/sec: 244820.12218117755\n",
      "Step: 811, Training Loss: 4.82935, LR: 0.0001944, Tokens/sec: 244860.11688252914\n",
      "Step: 812, Training Loss: 4.79526, LR: 0.0001935, Tokens/sec: 245103.06434338115\n",
      "Step: 813, Training Loss: 4.72034, LR: 0.0001925, Tokens/sec: 244171.28987825703\n",
      "Step: 814, Training Loss: 4.62739, LR: 0.0001916, Tokens/sec: 245158.6287819764\n",
      "Step: 815, Training Loss: 4.66580, LR: 0.0001906, Tokens/sec: 245096.38669325155\n",
      "Step: 816, Training Loss: 4.71323, LR: 0.0001897, Tokens/sec: 244700.39512347538\n",
      "Step: 817, Training Loss: 4.74240, LR: 0.0001887, Tokens/sec: 244996.0625768921\n",
      "Step: 818, Training Loss: 4.79132, LR: 0.0001878, Tokens/sec: 245192.82021655864\n",
      "Step: 819, Training Loss: 4.73978, LR: 0.0001869, Tokens/sec: 245089.3867347441\n",
      "Step: 820, Training Loss: 4.70379, LR: 0.0001859, Tokens/sec: 244557.76174317684\n",
      "Step: 821, Training Loss: 4.90901, LR: 0.0001850, Tokens/sec: 245361.22048365162\n",
      "Step: 822, Training Loss: 4.78883, LR: 0.0001841, Tokens/sec: 244677.0803917949\n",
      "Step: 823, Training Loss: 4.92424, LR: 0.0001832, Tokens/sec: 244882.88020224284\n",
      "Step: 824, Training Loss: 4.72463, LR: 0.0001823, Tokens/sec: 245289.55195958444\n",
      "Step: 825, Training Loss: 4.85387, LR: 0.0001814, Tokens/sec: 245063.18995091072\n",
      "Step: 826, Training Loss: 4.67774, LR: 0.0001805, Tokens/sec: 245102.50942379367\n",
      "Step: 827, Training Loss: 4.80848, LR: 0.0001796, Tokens/sec: 50622.84809141464\n",
      "Step: 828, Training Loss: 4.75469, LR: 0.0001787, Tokens/sec: 245923.57941865074\n",
      "Step: 829, Training Loss: 4.87059, LR: 0.0001778, Tokens/sec: 244893.36771635863\n",
      "Step: 830, Training Loss: 4.78366, LR: 0.0001769, Tokens/sec: 245973.29660946483\n",
      "Step: 831, Training Loss: 4.63810, LR: 0.0001761, Tokens/sec: 244499.06607703617\n",
      "Step: 832, Training Loss: 4.81400, LR: 0.0001752, Tokens/sec: 245388.03039244798\n",
      "Step: 833, Training Loss: 4.69079, LR: 0.0001743, Tokens/sec: 245608.92368557563\n",
      "Step: 834, Training Loss: 4.80091, LR: 0.0001735, Tokens/sec: 245073.22177315748\n",
      "Step: 835, Training Loss: 4.66879, LR: 0.0001726, Tokens/sec: 245252.36234540836\n",
      "Step: 836, Training Loss: 4.58928, LR: 0.0001717, Tokens/sec: 245289.0731470856\n",
      "Step: 837, Training Loss: 4.78279, LR: 0.0001709, Tokens/sec: 245238.49029837758\n",
      "Step: 838, Training Loss: 4.73925, LR: 0.0001701, Tokens/sec: 244998.17286545387\n",
      "Step: 839, Training Loss: 4.70044, LR: 0.0001692, Tokens/sec: 245270.08631829137\n",
      "Step: 840, Training Loss: 4.55840, LR: 0.0001684, Tokens/sec: 244948.2487818315\n",
      "Step: 841, Training Loss: 4.66741, LR: 0.0001675, Tokens/sec: 244885.14193859466\n",
      "Step: 842, Training Loss: 4.80626, LR: 0.0001667, Tokens/sec: 244881.3309284307\n",
      "Step: 843, Training Loss: 4.77511, LR: 0.0001659, Tokens/sec: 243966.48256627115\n",
      "Step: 844, Training Loss: 4.70350, LR: 0.0001651, Tokens/sec: 244898.38595080483\n",
      "Step: 845, Training Loss: 4.80930, LR: 0.0001643, Tokens/sec: 245587.36226293573\n",
      "Step: 846, Training Loss: 4.75593, LR: 0.0001635, Tokens/sec: 245275.21746513963\n",
      "Step: 847, Training Loss: 4.70007, LR: 0.0001627, Tokens/sec: 244312.51248418252\n",
      "Step: 848, Training Loss: 4.68535, LR: 0.0001619, Tokens/sec: 245401.40930251218\n",
      "Step: 849, Training Loss: 4.75647, LR: 0.0001611, Tokens/sec: 244886.31628540118\n",
      "Step: 850, Training Loss: 4.65629, LR: 0.0001603, Tokens/sec: 242978.31308090274\n",
      "Step: 851, Training Loss: 4.78049, LR: 0.0001595, Tokens/sec: 245327.248919549\n",
      "Step: 852, Training Loss: 4.68611, LR: 0.0001587, Tokens/sec: 244365.26996462903\n",
      "Step: 853, Training Loss: 4.56677, LR: 0.0001580, Tokens/sec: 245335.77984688012\n",
      "Step: 854, Training Loss: 4.64458, LR: 0.0001572, Tokens/sec: 244361.88412765917\n",
      "Step: 855, Training Loss: 4.65451, LR: 0.0001564, Tokens/sec: 244699.07790395146\n",
      "Step: 856, Training Loss: 4.63609, LR: 0.0001557, Tokens/sec: 244815.01516792638\n",
      "Step: 857, Training Loss: 4.81488, LR: 0.0001549, Tokens/sec: 243920.7184855584\n",
      "Step: 858, Training Loss: 4.61517, LR: 0.0001542, Tokens/sec: 245194.4417831106\n",
      "Step: 859, Training Loss: 4.63757, LR: 0.0001534, Tokens/sec: 245500.63537781226\n",
      "Step: 860, Training Loss: 4.65359, LR: 0.0001527, Tokens/sec: 244112.55768372712\n",
      "Step: 861, Training Loss: 4.71558, LR: 0.0001519, Tokens/sec: 245280.42577309243\n",
      "Step: 862, Training Loss: 4.81382, LR: 0.0001512, Tokens/sec: 245397.26556958706\n",
      "Step: 863, Training Loss: 4.64991, LR: 0.0001505, Tokens/sec: 243946.24872670925\n",
      "Step: 864, Training Loss: 4.69953, LR: 0.0001498, Tokens/sec: 245492.82438099766\n",
      "Step: 865, Training Loss: 4.77611, LR: 0.0001490, Tokens/sec: 245137.74582670012\n",
      "Step: 866, Training Loss: 4.73958, LR: 0.0001483, Tokens/sec: 244773.79380321733\n",
      "Step: 867, Training Loss: 4.75342, LR: 0.0001476, Tokens/sec: 244156.4944752093\n",
      "Step: 868, Training Loss: 4.95900, LR: 0.0001469, Tokens/sec: 245631.09593700137\n",
      "Step: 869, Training Loss: 4.72638, LR: 0.0001462, Tokens/sec: 244763.22287836307\n",
      "Step: 870, Training Loss: 4.71978, LR: 0.0001455, Tokens/sec: 245165.3234906833\n",
      "Step: 871, Training Loss: 4.57458, LR: 0.0001449, Tokens/sec: 245550.23995458093\n",
      "Step: 872, Training Loss: 4.76863, LR: 0.0001442, Tokens/sec: 245419.47494286267\n",
      "Step: 873, Training Loss: 4.64136, LR: 0.0001435, Tokens/sec: 245201.54514667514\n",
      "Step: 874, Training Loss: 4.90048, LR: 0.0001428, Tokens/sec: 244766.77309520417\n",
      "Step: 875, Training Loss: 4.72675, LR: 0.0001422, Tokens/sec: 245506.63271070828\n",
      "Step: 876, Training Loss: 4.60737, LR: 0.0001415, Tokens/sec: 244791.74501190783\n",
      "Step: 877, Training Loss: 4.73438, LR: 0.0001408, Tokens/sec: 244478.07789031992\n",
      "Step: 878, Training Loss: 4.73759, LR: 0.0001402, Tokens/sec: 245048.77437858848\n",
      "Step: 879, Training Loss: 4.59647, LR: 0.0001395, Tokens/sec: 244962.04708039435\n",
      "Step: 880, Training Loss: 4.75698, LR: 0.0001389, Tokens/sec: 244405.25199848186\n",
      "Step: 881, Training Loss: 4.69335, LR: 0.0001383, Tokens/sec: 243320.254607998\n",
      "Step: 882, Training Loss: 4.71681, LR: 0.0001376, Tokens/sec: 245329.88665244586\n",
      "Step: 883, Training Loss: 4.70409, LR: 0.0001370, Tokens/sec: 244914.58240996185\n",
      "Step: 884, Training Loss: 4.39310, LR: 0.0001364, Tokens/sec: 244562.0607641184\n",
      "Step: 885, Training Loss: 4.62286, LR: 0.0001358, Tokens/sec: 245526.61908335186\n",
      "Step: 886, Training Loss: 4.72142, LR: 0.0001352, Tokens/sec: 245368.95295054754\n",
      "Step: 887, Training Loss: 4.72105, LR: 0.0001346, Tokens/sec: 244267.78692665193\n",
      "Step: 888, Training Loss: 4.79320, LR: 0.0001340, Tokens/sec: 245513.85182563966\n",
      "Step: 889, Training Loss: 4.76125, LR: 0.0001334, Tokens/sec: 244739.61357656945\n",
      "Step: 890, Training Loss: 4.55157, LR: 0.0001328, Tokens/sec: 244434.54832126532\n",
      "Step: 891, Training Loss: 4.79475, LR: 0.0001322, Tokens/sec: 245658.13637746236\n",
      "Step: 892, Training Loss: 4.60705, LR: 0.0001316, Tokens/sec: 245480.24732258494\n",
      "Step: 893, Training Loss: 4.66356, LR: 0.0001310, Tokens/sec: 245115.63181053675\n",
      "Step: 894, Training Loss: 4.70582, LR: 0.0001305, Tokens/sec: 245330.36904553097\n",
      "Step: 895, Training Loss: 4.80381, LR: 0.0001299, Tokens/sec: 244650.04114054554\n",
      "Step: 896, Training Loss: 4.71407, LR: 0.0001293, Tokens/sec: 244949.69828900366\n",
      "Step: 897, Training Loss: 4.68177, LR: 0.0001288, Tokens/sec: 245307.6421811456\n",
      "Step: 898, Training Loss: 4.72311, LR: 0.0001282, Tokens/sec: 245839.07126211966\n",
      "Step: 899, Training Loss: 4.89949, LR: 0.0001277, Tokens/sec: 245053.00363544378\n",
      "Step: 900, Training Loss: 4.88247, LR: 0.0001271, Tokens/sec: 244045.25599414908\n",
      "Computing Eval loss, steps: 78\n",
      "Step: 900, Eval Loss: 4.69107\n",
      "Step: 901, Training Loss: 4.72999, LR: 0.0001266, Tokens/sec: 245161.93082148928\n",
      "Step: 902, Training Loss: 4.83282, LR: 0.0001261, Tokens/sec: 245441.71744345687\n",
      "Step: 903, Training Loss: 4.91867, LR: 0.0001255, Tokens/sec: 245251.9606066392\n",
      "Step: 904, Training Loss: 4.75427, LR: 0.0001250, Tokens/sec: 244429.03286310952\n",
      "Step: 905, Training Loss: 4.78942, LR: 0.0001245, Tokens/sec: 244864.00391778263\n",
      "Step: 906, Training Loss: 4.70029, LR: 0.0001240, Tokens/sec: 244447.7504318243\n",
      "Step: 907, Training Loss: 4.67613, LR: 0.0001235, Tokens/sec: 244469.29565677763\n",
      "Step: 908, Training Loss: 4.56963, LR: 0.0001230, Tokens/sec: 244992.717238397\n",
      "Step: 909, Training Loss: 4.67818, LR: 0.0001225, Tokens/sec: 244729.7401202837\n",
      "Step: 910, Training Loss: 4.75429, LR: 0.0001220, Tokens/sec: 244931.28741225632\n",
      "Step: 911, Training Loss: 4.69754, LR: 0.0001215, Tokens/sec: 244842.5918226\n",
      "Step: 912, Training Loss: 4.61996, LR: 0.0001211, Tokens/sec: 245163.37602558735\n",
      "Step: 913, Training Loss: 4.87736, LR: 0.0001206, Tokens/sec: 245180.0534786507\n",
      "Step: 914, Training Loss: 4.61919, LR: 0.0001201, Tokens/sec: 245272.6253627909\n",
      "Step: 915, Training Loss: 4.71599, LR: 0.0001197, Tokens/sec: 244758.58476462096\n",
      "Step: 916, Training Loss: 4.70600, LR: 0.0001192, Tokens/sec: 245188.84755458208\n",
      "Step: 917, Training Loss: 4.79013, LR: 0.0001188, Tokens/sec: 244591.41169713938\n",
      "Step: 918, Training Loss: 4.67891, LR: 0.0001183, Tokens/sec: 245151.17261540418\n",
      "Step: 919, Training Loss: 4.75382, LR: 0.0001179, Tokens/sec: 244537.11359011062\n",
      "Step: 920, Training Loss: 4.68392, LR: 0.0001174, Tokens/sec: 244765.22529016345\n",
      "Step: 921, Training Loss: 4.71610, LR: 0.0001170, Tokens/sec: 244943.79293368023\n",
      "Step: 922, Training Loss: 4.47180, LR: 0.0001166, Tokens/sec: 245082.6791008868\n",
      "Step: 923, Training Loss: 4.78176, LR: 0.0001162, Tokens/sec: 244697.33354591584\n",
      "Step: 924, Training Loss: 4.68911, LR: 0.0001157, Tokens/sec: 244825.70300366383\n",
      "Step: 925, Training Loss: 4.33576, LR: 0.0001153, Tokens/sec: 245310.69681537457\n",
      "Step: 926, Training Loss: 4.49685, LR: 0.0001149, Tokens/sec: 244868.5625227548\n",
      "Step: 927, Training Loss: 4.69467, LR: 0.0001145, Tokens/sec: 244770.95005292323\n",
      "Step: 928, Training Loss: 4.68721, LR: 0.0001141, Tokens/sec: 245131.29855303207\n",
      "Step: 929, Training Loss: 4.69629, LR: 0.0001137, Tokens/sec: 244922.54760549532\n",
      "Step: 930, Training Loss: 4.70498, LR: 0.0001134, Tokens/sec: 245066.9485874844\n",
      "Step: 931, Training Loss: 4.43423, LR: 0.0001130, Tokens/sec: 245312.56623431644\n",
      "Step: 932, Training Loss: 4.68986, LR: 0.0001126, Tokens/sec: 245166.67990131865\n",
      "Step: 933, Training Loss: 4.70029, LR: 0.0001123, Tokens/sec: 244984.80547550847\n",
      "Step: 934, Training Loss: 4.60704, LR: 0.0001119, Tokens/sec: 245350.75274223497\n",
      "Step: 935, Training Loss: 4.80763, LR: 0.0001115, Tokens/sec: 244392.6367413326\n",
      "Step: 936, Training Loss: 4.76060, LR: 0.0001112, Tokens/sec: 244728.50769649592\n",
      "Step: 937, Training Loss: 4.72769, LR: 0.0001108, Tokens/sec: 245501.86188649788\n",
      "Step: 938, Training Loss: 4.73964, LR: 0.0001105, Tokens/sec: 244518.7340923652\n",
      "Step: 939, Training Loss: 4.69489, LR: 0.0001102, Tokens/sec: 244559.4259172211\n",
      "Step: 940, Training Loss: 4.66869, LR: 0.0001098, Tokens/sec: 244946.12400585238\n",
      "Step: 941, Training Loss: 4.64944, LR: 0.0001095, Tokens/sec: 245077.65843222616\n",
      "Step: 942, Training Loss: 5.24042, LR: 0.0001092, Tokens/sec: 245963.8615548212\n",
      "Step: 943, Training Loss: 4.80654, LR: 0.0001089, Tokens/sec: 244553.22830119578\n",
      "Step: 944, Training Loss: 4.68030, LR: 0.0001086, Tokens/sec: 245232.58297513938\n",
      "Step: 945, Training Loss: 4.73668, LR: 0.0001083, Tokens/sec: 244451.03674609654\n",
      "Step: 946, Training Loss: 4.72368, LR: 0.0001080, Tokens/sec: 244950.00183503094\n",
      "Step: 947, Training Loss: 4.71813, LR: 0.0001077, Tokens/sec: 245004.25656852397\n",
      "Step: 948, Training Loss: 4.88476, LR: 0.0001074, Tokens/sec: 244852.79984365887\n",
      "Step: 949, Training Loss: 4.85581, LR: 0.0001071, Tokens/sec: 244939.02861254968\n",
      "Step: 950, Training Loss: 4.60631, LR: 0.0001068, Tokens/sec: 244843.62775054385\n",
      "Step: 951, Training Loss: 4.83680, LR: 0.0001066, Tokens/sec: 244487.97522147803\n",
      "Step: 952, Training Loss: 4.50762, LR: 0.0001063, Tokens/sec: 244936.14862426862\n",
      "Step: 953, Training Loss: 4.68257, LR: 0.0001060, Tokens/sec: 244967.15511326244\n",
      "Step: 954, Training Loss: 4.71886, LR: 0.0001058, Tokens/sec: 245574.1464698329\n",
      "Step: 955, Training Loss: 4.67138, LR: 0.0001055, Tokens/sec: 245176.79878826634\n",
      "Step: 956, Training Loss: 4.84262, LR: 0.0001053, Tokens/sec: 245104.85205526158\n",
      "Step: 957, Training Loss: 4.70725, LR: 0.0001051, Tokens/sec: 244688.08801267346\n",
      "Step: 958, Training Loss: 4.61549, LR: 0.0001048, Tokens/sec: 244965.93053037688\n",
      "Step: 959, Training Loss: 4.68237, LR: 0.0001046, Tokens/sec: 245137.9183279519\n",
      "Step: 960, Training Loss: 4.80788, LR: 0.0001044, Tokens/sec: 244525.03702260612\n",
      "Step: 961, Training Loss: 4.66123, LR: 0.0001042, Tokens/sec: 245313.15973375438\n",
      "Step: 962, Training Loss: 4.73619, LR: 0.0001040, Tokens/sec: 245643.7124677199\n",
      "Step: 963, Training Loss: 4.73831, LR: 0.0001037, Tokens/sec: 244012.13520900652\n",
      "Step: 964, Training Loss: 4.77980, LR: 0.0001035, Tokens/sec: 244710.53172128962\n",
      "Step: 965, Training Loss: 4.59466, LR: 0.0001034, Tokens/sec: 244513.31508967138\n",
      "Step: 966, Training Loss: 4.65513, LR: 0.0001032, Tokens/sec: 244553.01922637376\n",
      "Step: 967, Training Loss: 4.66186, LR: 0.0001030, Tokens/sec: 243936.55925597585\n",
      "Step: 968, Training Loss: 4.73060, LR: 0.0001028, Tokens/sec: 245271.85424001428\n",
      "Step: 969, Training Loss: 4.64951, LR: 0.0001026, Tokens/sec: 244569.49473822472\n",
      "Step: 970, Training Loss: 4.76888, LR: 0.0001025, Tokens/sec: 245117.18917338355\n",
      "Step: 971, Training Loss: 4.60645, LR: 0.0001023, Tokens/sec: 245387.9995868676\n",
      "Step: 972, Training Loss: 4.76987, LR: 0.0001021, Tokens/sec: 245028.44594477126\n",
      "Step: 973, Training Loss: 4.56865, LR: 0.0001020, Tokens/sec: 244856.14306612755\n",
      "Step: 974, Training Loss: 4.42554, LR: 0.0001019, Tokens/sec: 245627.79839496245\n",
      "Step: 975, Training Loss: 4.65428, LR: 0.0001017, Tokens/sec: 244402.376060218\n",
      "Step: 976, Training Loss: 4.70348, LR: 0.0001016, Tokens/sec: 244508.29053445964\n",
      "Step: 977, Training Loss: 4.64017, LR: 0.0001014, Tokens/sec: 245183.42443805374\n",
      "Step: 978, Training Loss: 4.63402, LR: 0.0001013, Tokens/sec: 245342.32513137363\n",
      "Step: 979, Training Loss: 4.69889, LR: 0.0001012, Tokens/sec: 244095.98449168826\n",
      "Step: 980, Training Loss: 4.68706, LR: 0.0001011, Tokens/sec: 245303.21258356477\n",
      "Step: 981, Training Loss: 4.64113, LR: 0.0001010, Tokens/sec: 245101.6283873949\n",
      "Step: 982, Training Loss: 4.64983, LR: 0.0001009, Tokens/sec: 245096.75548264093\n",
      "Step: 983, Training Loss: 4.76753, LR: 0.0001008, Tokens/sec: 244943.87137430478\n",
      "Step: 984, Training Loss: 4.68257, LR: 0.0001007, Tokens/sec: 245286.7081827691\n",
      "Step: 985, Training Loss: 4.67091, LR: 0.0001006, Tokens/sec: 244807.5304907846\n",
      "Step: 986, Training Loss: 4.68268, LR: 0.0001005, Tokens/sec: 244480.59886462532\n",
      "Step: 987, Training Loss: 4.88776, LR: 0.0001005, Tokens/sec: 245348.05125865169\n",
      "Step: 988, Training Loss: 4.59425, LR: 0.0001004, Tokens/sec: 244318.39422539194\n",
      "Step: 989, Training Loss: 4.78710, LR: 0.0001003, Tokens/sec: 244552.99202967522\n",
      "Step: 990, Training Loss: 4.80467, LR: 0.0001003, Tokens/sec: 245131.08678078896\n",
      "Step: 991, Training Loss: 4.62811, LR: 0.0001002, Tokens/sec: 244801.20617611104\n",
      "Step: 992, Training Loss: 4.66692, LR: 0.0001002, Tokens/sec: 245182.43176374497\n",
      "Step: 993, Training Loss: 4.60700, LR: 0.0001001, Tokens/sec: 245230.4293343344\n",
      "Step: 994, Training Loss: 4.58376, LR: 0.0001001, Tokens/sec: 244897.0921675605\n",
      "Step: 995, Training Loss: 4.49192, LR: 0.0001001, Tokens/sec: 245049.99808317822\n",
      "Step: 996, Training Loss: 4.68426, LR: 0.0001000, Tokens/sec: 244780.04347465772\n",
      "Step: 997, Training Loss: 4.35374, LR: 0.0001000, Tokens/sec: 245245.7876712948\n",
      "Step: 998, Training Loss: 4.70975, LR: 0.0001000, Tokens/sec: 243557.9947397318\n",
      "Step: 999, Training Loss: 4.57086, LR: 0.0001000, Tokens/sec: 244903.7487343624\n"
     ]
    }
   ],
   "source": [
    "trainer.train(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b5596eda083de0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:22:17.417599499Z",
     "start_time": "2024-12-16T06:27:52.251011Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The world is a very important part of the world.\n",
      "The first time the world is the world’s largest and most important. The world is the world’s largest and most important. The world is the world’s largest and most important.\n",
      "The world is the world’s largest and most important. The world is the world’s largest and most important and most important. The world is the world’s largest and most important.\n",
      "The world is the world’s largest and most important and most important.\n",
      "The world is the world’s most important and most important.\n",
      "The world is the world’s most important and most important.\n",
      "The world is the world’s most important and most important.\n",
      "The world is the world’s largest nation.\n",
      "The world is the world’s largest and most important.\n",
      "The world is the world’s largest.\n",
      "The world is the world’s largest.\n",
      "The world is the world’s largest.\n",
      "The world is the world’s largest.\n",
      "The world is the world’s largest.\n",
      "The world is the world’s largest.\n",
      "The world is the world’s largest.\n",
      "The world is the world’s largest.\n",
      "The world is the world\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer([\"The world is\"], return_tensors=\"pt\")['input_ids'].to(trainer.device)\n",
    "idx = model.generate(input_ids, temperature=0.25, top_k=50, max_new_tokens=256)\n",
    "print(tokenizer.batch_decode(idx)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3bcc343073d67a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
