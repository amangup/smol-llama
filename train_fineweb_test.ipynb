{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:22:48.329931Z",
     "start_time": "2024-12-16T09:22:47.413234Z"
    }
   },
   "outputs": [],
   "source": [
    "from model import ModelConfig, LlamaModel\n",
    "from train import TrainerConfig, FileDataLoader, Trainer\n",
    "\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f28fa23c987e72b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:22:48.333961Z",
     "start_time": "2024-12-16T09:22:48.332382Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer_id = \"HuggingFaceTB/SmolLM2-135M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bb4e51aa142abee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:22:48.533405Z",
     "start_time": "2024-12-16T09:22:48.376114Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cde027092af8291e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:22:48.540534Z",
     "start_time": "2024-12-16T09:22:48.538895Z"
    }
   },
   "outputs": [],
   "source": [
    "model_config = ModelConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=576,\n",
    "    d_head=64,\n",
    "    d_mlp_proj=1536,\n",
    "    n_layers=30,\n",
    "    n_kv_heads=3,\n",
    "    n_attn_heads=9,\n",
    "    rms_norm_eps=1e-5,\n",
    "    initializer_range=0.041666666666666664,\n",
    "    rope_theta=100000.0,\n",
    "    padding_idx=tokenizer.pad_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0897594b27eb59f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:22:48.582151Z",
     "start_time": "2024-12-16T09:22:48.580277Z"
    }
   },
   "outputs": [],
   "source": [
    "train_config = TrainerConfig(\n",
    "    per_device_train_batch_size=32,\n",
    "    max_seq_len=2048,\n",
    "    num_epochs=1,\n",
    "    eval_interval_steps=100,\n",
    "    learning_rate=1e-3,\n",
    "    grad_clip_norm=1.0,\n",
    "    tokens_folder=\"fineweb-edu_tok\",\n",
    "    max_steps=1000,\n",
    "    log_dir=\"runs/fineweb\",\n",
    "    warmup_ratio=0.1,\n",
    "    val_size=0.0005\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6504e357e2012d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:22:50.546015Z",
     "start_time": "2024-12-16T09:22:48.624998Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens                   | 10,101,737,472\n",
      "Num Trainable Params           | 162,826,560\n",
      "Train device                   | cuda, NVIDIA A100-SXM4-80GB, N=8\n",
      "Training precision             | torch.bfloat16\n",
      "Flash Attention                | True\n",
      "torch.compile()                | True\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LlamaModel(model_config)\n",
    "dataloader = FileDataLoader(train_config, tokenizer)\n",
    "trainer = Trainer(train_config, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c853027a7a843745",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-16T09:22:50.552519Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training steps                 | 1,000 \n",
      "Step: 0, Training Loss: 11.27769, LR: 0.0000100, Tokens/sec: 1425.5069511910128\n",
      "Computing Eval loss, steps: 78\n",
      "Step: 0, Eval Loss: 11.24022\n",
      "Step: 1, Training Loss: 11.23619, LR: 0.0000200, Tokens/sec: 1779.3861157874926\n",
      "Step: 2, Training Loss: 11.14878, LR: 0.0000300, Tokens/sec: 246992.93520791107\n",
      "Step: 3, Training Loss: 10.97960, LR: 0.0000400, Tokens/sec: 246286.03921751503\n",
      "Step: 4, Training Loss: 10.83010, LR: 0.0000500, Tokens/sec: 248138.47047151692\n",
      "Step: 5, Training Loss: 10.59780, LR: 0.0000600, Tokens/sec: 247080.66778849915\n",
      "Step: 6, Training Loss: 10.20184, LR: 0.0000700, Tokens/sec: 248375.73130491533\n",
      "Step: 7, Training Loss: 9.94503, LR: 0.0000800, Tokens/sec: 247456.64110105\n",
      "Step: 8, Training Loss: 9.64306, LR: 0.0000900, Tokens/sec: 247568.435538636\n",
      "Step: 9, Training Loss: 9.58912, LR: 0.0001000, Tokens/sec: 248998.8865784912\n",
      "Step: 10, Training Loss: 9.42804, LR: 0.0001100, Tokens/sec: 248277.19359325164\n",
      "Step: 11, Training Loss: 9.37120, LR: 0.0001200, Tokens/sec: 248949.32868330943\n",
      "Step: 12, Training Loss: 9.28942, LR: 0.0001300, Tokens/sec: 248755.52913010694\n",
      "Step: 13, Training Loss: 9.16352, LR: 0.0001400, Tokens/sec: 248049.13357525953\n",
      "Step: 14, Training Loss: 9.06405, LR: 0.0001500, Tokens/sec: 249451.78713386183\n",
      "Step: 15, Training Loss: 8.99500, LR: 0.0001600, Tokens/sec: 248913.79283306072\n",
      "Step: 16, Training Loss: 8.93054, LR: 0.0001700, Tokens/sec: 247830.05428749244\n",
      "Step: 17, Training Loss: 8.87935, LR: 0.0001800, Tokens/sec: 248239.6040046726\n",
      "Step: 18, Training Loss: 8.70506, LR: 0.0001900, Tokens/sec: 248585.1103193707\n",
      "Step: 19, Training Loss: 8.62195, LR: 0.0002000, Tokens/sec: 249199.70541449974\n",
      "Step: 20, Training Loss: 8.65769, LR: 0.0002100, Tokens/sec: 248894.36749975343\n",
      "Step: 21, Training Loss: 8.40225, LR: 0.0002200, Tokens/sec: 248833.36258305196\n",
      "Step: 22, Training Loss: 8.30791, LR: 0.0002300, Tokens/sec: 248516.30847952847\n",
      "Step: 23, Training Loss: 8.25003, LR: 0.0002400, Tokens/sec: 248676.2187520722\n",
      "Step: 24, Training Loss: 8.15278, LR: 0.0002500, Tokens/sec: 249566.78864671514\n",
      "Step: 25, Training Loss: 8.06455, LR: 0.0002600, Tokens/sec: 249274.49334458524\n",
      "Step: 26, Training Loss: 8.02753, LR: 0.0002700, Tokens/sec: 248718.80377213593\n",
      "Step: 27, Training Loss: 7.94360, LR: 0.0002800, Tokens/sec: 249133.28843896612\n",
      "Step: 28, Training Loss: 7.84650, LR: 0.0002900, Tokens/sec: 248848.80241211606\n",
      "Step: 29, Training Loss: 7.81629, LR: 0.0003000, Tokens/sec: 249195.49595542508\n",
      "Step: 30, Training Loss: 7.77990, LR: 0.0003100, Tokens/sec: 248200.15234994044\n",
      "Step: 31, Training Loss: 7.72904, LR: 0.0003200, Tokens/sec: 248436.4664019676\n",
      "Step: 32, Training Loss: 7.60519, LR: 0.0003300, Tokens/sec: 248571.23451277314\n",
      "Step: 33, Training Loss: 7.57093, LR: 0.0003400, Tokens/sec: 247805.11140411586\n",
      "Step: 34, Training Loss: 7.90684, LR: 0.0003500, Tokens/sec: 248524.69575264386\n",
      "Step: 35, Training Loss: 7.51568, LR: 0.0003600, Tokens/sec: 247677.541392432\n",
      "Step: 36, Training Loss: 7.40598, LR: 0.0003700, Tokens/sec: 248173.48944100825\n",
      "Step: 37, Training Loss: 7.47801, LR: 0.0003800, Tokens/sec: 247604.03259291474\n",
      "Step: 38, Training Loss: 7.40555, LR: 0.0003900, Tokens/sec: 248336.25390142793\n",
      "Step: 39, Training Loss: 7.33370, LR: 0.0004000, Tokens/sec: 247848.7027654231\n",
      "Step: 40, Training Loss: 7.45327, LR: 0.0004100, Tokens/sec: 247784.09799853215\n",
      "Step: 41, Training Loss: 7.34446, LR: 0.0004200, Tokens/sec: 247406.26859307464\n",
      "Step: 42, Training Loss: 7.36859, LR: 0.0004300, Tokens/sec: 247202.3677894056\n",
      "Step: 43, Training Loss: 7.17243, LR: 0.0004400, Tokens/sec: 247755.29826842397\n",
      "Step: 44, Training Loss: 7.39903, LR: 0.0004500, Tokens/sec: 246817.9511667458\n",
      "Step: 45, Training Loss: 7.29419, LR: 0.0004600, Tokens/sec: 246658.67115751194\n",
      "Step: 46, Training Loss: 7.27807, LR: 0.0004700, Tokens/sec: 246725.56575709197\n",
      "Step: 47, Training Loss: 7.28728, LR: 0.0004800, Tokens/sec: 246477.93722027302\n",
      "Step: 48, Training Loss: 7.14709, LR: 0.0004900, Tokens/sec: 244068.5198809826\n",
      "Step: 49, Training Loss: 6.99808, LR: 0.0005000, Tokens/sec: 246522.62086558738\n",
      "Step: 50, Training Loss: 7.26857, LR: 0.0005100, Tokens/sec: 246941.96096283605\n",
      "Step: 51, Training Loss: 7.09532, LR: 0.0005200, Tokens/sec: 246279.2624766699\n",
      "Step: 52, Training Loss: 7.23310, LR: 0.0005300, Tokens/sec: 247028.33919404165\n",
      "Step: 53, Training Loss: 7.29241, LR: 0.0005400, Tokens/sec: 246834.65362237705\n",
      "Step: 54, Training Loss: 7.03730, LR: 0.0005500, Tokens/sec: 246737.76030943776\n",
      "Step: 55, Training Loss: 7.07024, LR: 0.0005600, Tokens/sec: 246374.43302991358\n",
      "Step: 56, Training Loss: 7.18418, LR: 0.0005700, Tokens/sec: 246197.83801505345\n",
      "Step: 57, Training Loss: 7.08859, LR: 0.0005800, Tokens/sec: 245975.58885067032\n",
      "Step: 58, Training Loss: 6.91861, LR: 0.0005900, Tokens/sec: 246802.07155847413\n",
      "Step: 59, Training Loss: 6.98807, LR: 0.0006000, Tokens/sec: 246695.1936203884\n",
      "Step: 60, Training Loss: 7.21429, LR: 0.0006100, Tokens/sec: 246530.8136458155\n",
      "Step: 61, Training Loss: 6.81212, LR: 0.0006200, Tokens/sec: 246155.65200101532\n",
      "Step: 62, Training Loss: 6.99664, LR: 0.0006300, Tokens/sec: 245976.60343468605\n",
      "Step: 63, Training Loss: 7.03561, LR: 0.0006400, Tokens/sec: 246392.00380150045\n",
      "Step: 64, Training Loss: 7.04119, LR: 0.0006500, Tokens/sec: 246003.7698842257\n",
      "Step: 65, Training Loss: 7.05817, LR: 0.0006600, Tokens/sec: 245806.9730114427\n",
      "Step: 66, Training Loss: 7.03608, LR: 0.0006700, Tokens/sec: 246273.99787359938\n",
      "Step: 67, Training Loss: 6.87327, LR: 0.0006800, Tokens/sec: 246021.35835519736\n",
      "Step: 68, Training Loss: 6.95314, LR: 0.0006900, Tokens/sec: 245941.5019730823\n",
      "Step: 69, Training Loss: 6.95235, LR: 0.0007000, Tokens/sec: 245822.94981298503\n",
      "Step: 70, Training Loss: 6.86011, LR: 0.0007100, Tokens/sec: 245581.65581409502\n",
      "Step: 71, Training Loss: 6.91333, LR: 0.0007200, Tokens/sec: 246353.18202535616\n",
      "Step: 72, Training Loss: 6.90965, LR: 0.0007300, Tokens/sec: 246109.63971152483\n",
      "Step: 73, Training Loss: 6.70070, LR: 0.0007400, Tokens/sec: 245813.26868464297\n",
      "Step: 74, Training Loss: 6.78615, LR: 0.0007500, Tokens/sec: 246862.69583570035\n",
      "Step: 75, Training Loss: 6.72359, LR: 0.0007600, Tokens/sec: 245678.56771714138\n",
      "Step: 76, Training Loss: 6.76110, LR: 0.0007700, Tokens/sec: 245656.52067812352\n",
      "Step: 77, Training Loss: 6.95671, LR: 0.0007800, Tokens/sec: 245833.15386411687\n",
      "Step: 78, Training Loss: 6.86937, LR: 0.0007900, Tokens/sec: 246267.3476260489\n",
      "Step: 79, Training Loss: 6.82828, LR: 0.0008000, Tokens/sec: 245689.43745626864\n",
      "Step: 80, Training Loss: 6.41615, LR: 0.0008100, Tokens/sec: 245609.6643534968\n",
      "Step: 81, Training Loss: 6.72442, LR: 0.0008200, Tokens/sec: 245808.3777463612\n",
      "Step: 82, Training Loss: 6.63129, LR: 0.0008300, Tokens/sec: 246039.3398984248\n",
      "Step: 83, Training Loss: 6.82444, LR: 0.0008400, Tokens/sec: 246139.51827815644\n",
      "Step: 84, Training Loss: 6.65482, LR: 0.0008500, Tokens/sec: 245309.9562391718\n",
      "Step: 85, Training Loss: 6.72710, LR: 0.0008600, Tokens/sec: 246237.51729052127\n",
      "Step: 86, Training Loss: 6.66092, LR: 0.0008700, Tokens/sec: 245437.1306249626\n",
      "Step: 87, Training Loss: 6.65425, LR: 0.0008800, Tokens/sec: 245516.7094378712\n",
      "Step: 88, Training Loss: 6.62861, LR: 0.0008900, Tokens/sec: 244002.36435701864\n",
      "Step: 89, Training Loss: 6.62011, LR: 0.0009000, Tokens/sec: 245778.84906365947\n",
      "Step: 90, Training Loss: 6.56328, LR: 0.0009100, Tokens/sec: 245230.45155424482\n",
      "Step: 91, Training Loss: 6.53725, LR: 0.0009200, Tokens/sec: 246155.7777176947\n",
      "Step: 92, Training Loss: 6.60494, LR: 0.0009300, Tokens/sec: 246114.28785377697\n",
      "Step: 93, Training Loss: 6.60690, LR: 0.0009400, Tokens/sec: 246143.01899063835\n",
      "Step: 94, Training Loss: 6.54989, LR: 0.0009500, Tokens/sec: 245172.22528276854\n",
      "Step: 95, Training Loss: 6.57800, LR: 0.0009600, Tokens/sec: 245723.89210091945\n",
      "Step: 96, Training Loss: 6.53998, LR: 0.0009700, Tokens/sec: 245738.33229382685\n",
      "Step: 97, Training Loss: 6.51417, LR: 0.0009800, Tokens/sec: 242748.82378787472\n",
      "Step: 98, Training Loss: 6.55954, LR: 0.0009900, Tokens/sec: 245218.20044843986\n",
      "Step: 99, Training Loss: 6.60083, LR: 0.0010000, Tokens/sec: 244993.85508677596\n",
      "Step: 100, Training Loss: 6.44858, LR: 0.0010000, Tokens/sec: 244552.66736955353\n",
      "Computing Eval loss, steps: 78\n",
      "Step: 100, Eval Loss: 6.46349\n",
      "Step: 101, Training Loss: 6.42017, LR: 0.0010000, Tokens/sec: 244757.5802036731\n",
      "Step: 102, Training Loss: 6.54514, LR: 0.0010000, Tokens/sec: 245235.56053131522\n",
      "Step: 103, Training Loss: 6.42672, LR: 0.0010000, Tokens/sec: 245713.75031305404\n",
      "Step: 104, Training Loss: 6.34841, LR: 0.0010000, Tokens/sec: 245302.89961032712\n",
      "Step: 105, Training Loss: 6.34232, LR: 0.0009999, Tokens/sec: 245125.95483145016\n",
      "Step: 106, Training Loss: 6.41928, LR: 0.0009999, Tokens/sec: 245318.82805462775\n",
      "Step: 107, Training Loss: 6.42794, LR: 0.0009999, Tokens/sec: 245223.32771542462\n",
      "Step: 108, Training Loss: 6.38255, LR: 0.0009998, Tokens/sec: 244897.87457239465\n",
      "Step: 109, Training Loss: 6.38807, LR: 0.0009998, Tokens/sec: 245601.9355664859\n",
      "Step: 110, Training Loss: 6.37272, LR: 0.0009997, Tokens/sec: 244805.30255057028\n",
      "Step: 111, Training Loss: 6.33570, LR: 0.0009997, Tokens/sec: 244614.66248631044\n",
      "Step: 112, Training Loss: 6.36882, LR: 0.0009996, Tokens/sec: 245263.77227594377\n",
      "Step: 113, Training Loss: 6.37630, LR: 0.0009995, Tokens/sec: 244664.23289730333\n",
      "Step: 114, Training Loss: 6.11703, LR: 0.0009995, Tokens/sec: 245166.37240137038\n",
      "Step: 115, Training Loss: 6.38363, LR: 0.0009994, Tokens/sec: 244800.08374233975\n",
      "Step: 116, Training Loss: 6.23728, LR: 0.0009993, Tokens/sec: 245233.29915454998\n",
      "Step: 117, Training Loss: 6.27642, LR: 0.0009992, Tokens/sec: 244771.0624394053\n",
      "Step: 118, Training Loss: 6.45363, LR: 0.0009991, Tokens/sec: 244799.77545808294\n",
      "Step: 119, Training Loss: 6.17740, LR: 0.0009990, Tokens/sec: 243974.87174730827\n",
      "Step: 120, Training Loss: 6.34795, LR: 0.0009989, Tokens/sec: 245231.5659733788\n",
      "Step: 121, Training Loss: 6.32241, LR: 0.0009988, Tokens/sec: 245066.3067776179\n",
      "Step: 122, Training Loss: 6.26268, LR: 0.0009987, Tokens/sec: 245381.30126824678\n",
      "Step: 123, Training Loss: 6.27586, LR: 0.0009986, Tokens/sec: 243978.5107910844\n",
      "Step: 124, Training Loss: 6.26365, LR: 0.0009984, Tokens/sec: 245034.48166309518\n",
      "Step: 125, Training Loss: 6.22983, LR: 0.0009983, Tokens/sec: 245197.98401520346\n",
      "Step: 126, Training Loss: 6.33879, LR: 0.0009981, Tokens/sec: 244769.19445324532\n",
      "Step: 127, Training Loss: 6.37175, LR: 0.0009980, Tokens/sec: 245382.32293500935\n",
      "Step: 128, Training Loss: 6.24336, LR: 0.0009979, Tokens/sec: 245266.84804421137\n",
      "Step: 129, Training Loss: 6.28273, LR: 0.0009977, Tokens/sec: 244582.0772549061\n",
      "Step: 130, Training Loss: 6.24478, LR: 0.0009975, Tokens/sec: 244512.13412221576\n",
      "Step: 131, Training Loss: 6.04813, LR: 0.0009974, Tokens/sec: 245349.6919879459\n",
      "Step: 132, Training Loss: 6.32223, LR: 0.0009972, Tokens/sec: 244891.30525436148\n",
      "Step: 133, Training Loss: 6.28410, LR: 0.0009970, Tokens/sec: 244890.42573567916\n",
      "Step: 134, Training Loss: 6.18395, LR: 0.0009968, Tokens/sec: 244993.39107536618\n",
      "Step: 135, Training Loss: 5.99423, LR: 0.0009966, Tokens/sec: 244945.49476546625\n",
      "Step: 136, Training Loss: 6.33298, LR: 0.0009965, Tokens/sec: 245491.6921702933\n",
      "Step: 137, Training Loss: 6.33021, LR: 0.0009963, Tokens/sec: 242795.1594347128\n",
      "Step: 138, Training Loss: 6.19040, LR: 0.0009960, Tokens/sec: 244962.275615543\n",
      "Step: 139, Training Loss: 6.25271, LR: 0.0009958, Tokens/sec: 244595.50275375237\n",
      "Step: 140, Training Loss: 6.18733, LR: 0.0009956, Tokens/sec: 244924.50659105106\n",
      "Step: 141, Training Loss: 6.22889, LR: 0.0009954, Tokens/sec: 244019.98257861118\n",
      "Step: 142, Training Loss: 6.30892, LR: 0.0009952, Tokens/sec: 244580.08293913194\n",
      "Step: 143, Training Loss: 6.10454, LR: 0.0009949, Tokens/sec: 245166.1896101\n",
      "Step: 144, Training Loss: 6.09379, LR: 0.0009947, Tokens/sec: 244742.63535387118\n",
      "Step: 145, Training Loss: 6.15301, LR: 0.0009945, Tokens/sec: 245362.79979057072\n",
      "Step: 146, Training Loss: 6.25618, LR: 0.0009942, Tokens/sec: 244616.29172069792\n",
      "Step: 147, Training Loss: 6.37852, LR: 0.0009940, Tokens/sec: 245115.52423028136\n",
      "Step: 148, Training Loss: 6.26551, LR: 0.0009937, Tokens/sec: 245045.38493428656\n",
      "Step: 149, Training Loss: 6.42983, LR: 0.0009934, Tokens/sec: 245075.965005885\n",
      "Step: 150, Training Loss: 6.21269, LR: 0.0009932, Tokens/sec: 244617.86995591348\n",
      "Step: 151, Training Loss: 6.15392, LR: 0.0009929, Tokens/sec: 244942.00586523526\n",
      "Step: 152, Training Loss: 6.13286, LR: 0.0009926, Tokens/sec: 244556.94071366178\n",
      "Step: 153, Training Loss: 6.12917, LR: 0.0009923, Tokens/sec: 244551.29054825025\n",
      "Step: 154, Training Loss: 6.02736, LR: 0.0009920, Tokens/sec: 245141.76636453086\n",
      "Step: 155, Training Loss: 6.14212, LR: 0.0009917, Tokens/sec: 244952.12667828705\n",
      "Step: 156, Training Loss: 6.07544, LR: 0.0009914, Tokens/sec: 245031.37247165295\n",
      "Step: 157, Training Loss: 5.70673, LR: 0.0009911, Tokens/sec: 245155.43786296507\n",
      "Step: 158, Training Loss: 6.12399, LR: 0.0009908, Tokens/sec: 244665.19756100362\n",
      "Step: 159, Training Loss: 6.24216, LR: 0.0009905, Tokens/sec: 244496.9813679274\n",
      "Step: 160, Training Loss: 6.07990, LR: 0.0009902, Tokens/sec: 244203.431834214\n",
      "Step: 161, Training Loss: 5.95263, LR: 0.0009898, Tokens/sec: 244779.1443197118\n",
      "Step: 162, Training Loss: 6.11376, LR: 0.0009895, Tokens/sec: 245113.81661982564\n",
      "Step: 163, Training Loss: 5.95859, LR: 0.0009892, Tokens/sec: 244255.92007728692\n",
      "Step: 164, Training Loss: 5.90990, LR: 0.0009888, Tokens/sec: 245321.5049477911\n",
      "Step: 165, Training Loss: 6.51428, LR: 0.0009885, Tokens/sec: 244147.48289987852\n",
      "Step: 166, Training Loss: 6.26126, LR: 0.0009881, Tokens/sec: 244703.81587606878\n",
      "Step: 167, Training Loss: 6.07397, LR: 0.0009877, Tokens/sec: 244241.3246538615\n",
      "Step: 168, Training Loss: 6.25184, LR: 0.0009874, Tokens/sec: 245266.4530968874\n",
      "Step: 169, Training Loss: 6.04841, LR: 0.0009870, Tokens/sec: 244899.24507134172\n",
      "Step: 170, Training Loss: 6.21922, LR: 0.0009866, Tokens/sec: 244612.1863637045\n",
      "Step: 171, Training Loss: 5.99197, LR: 0.0009863, Tokens/sec: 245151.97714213314\n",
      "Step: 172, Training Loss: 6.04646, LR: 0.0009859, Tokens/sec: 244268.7535541929\n",
      "Step: 173, Training Loss: 6.10902, LR: 0.0009855, Tokens/sec: 244158.45647431965\n",
      "Step: 174, Training Loss: 6.12655, LR: 0.0009851, Tokens/sec: 244833.58064157312\n",
      "Step: 175, Training Loss: 5.93326, LR: 0.0009847, Tokens/sec: 244773.9589811631\n",
      "Step: 176, Training Loss: 6.04010, LR: 0.0009843, Tokens/sec: 244960.66564566456\n",
      "Step: 177, Training Loss: 5.95715, LR: 0.0009838, Tokens/sec: 244975.24141985102\n",
      "Step: 178, Training Loss: 6.05649, LR: 0.0009834, Tokens/sec: 244696.41117347497\n",
      "Step: 179, Training Loss: 6.10112, LR: 0.0009830, Tokens/sec: 244934.426461148\n",
      "Step: 180, Training Loss: 5.92300, LR: 0.0009826, Tokens/sec: 244849.73104345956\n",
      "Step: 181, Training Loss: 5.97006, LR: 0.0009821, Tokens/sec: 244094.82279295943\n",
      "Step: 182, Training Loss: 6.01655, LR: 0.0009817, Tokens/sec: 244588.7422119549\n",
      "Step: 183, Training Loss: 5.90638, LR: 0.0009812, Tokens/sec: 244639.19173267338\n",
      "Step: 184, Training Loss: 5.86493, LR: 0.0009808, Tokens/sec: 244361.28843371785\n",
      "Step: 185, Training Loss: 5.98029, LR: 0.0009803, Tokens/sec: 243874.6165912571\n",
      "Step: 186, Training Loss: 5.72710, LR: 0.0009799, Tokens/sec: 245299.23634361455\n",
      "Step: 187, Training Loss: 5.94592, LR: 0.0009794, Tokens/sec: 244804.89035200968\n",
      "Step: 188, Training Loss: 5.95018, LR: 0.0009789, Tokens/sec: 244839.00533057115\n",
      "Step: 189, Training Loss: 5.86429, LR: 0.0009785, Tokens/sec: 244158.06000563604\n",
      "Step: 190, Training Loss: 5.96073, LR: 0.0009780, Tokens/sec: 245207.45781028474\n",
      "Step: 191, Training Loss: 5.88710, LR: 0.0009775, Tokens/sec: 244548.7902124135\n",
      "Step: 192, Training Loss: 5.87801, LR: 0.0009770, Tokens/sec: 243540.35887623648\n",
      "Step: 193, Training Loss: 5.98745, LR: 0.0009765, Tokens/sec: 244845.54287725934\n",
      "Step: 194, Training Loss: 6.05082, LR: 0.0009760, Tokens/sec: 245287.81114309884\n",
      "Step: 195, Training Loss: 5.86685, LR: 0.0009755, Tokens/sec: 245058.839157272\n",
      "Step: 196, Training Loss: 5.95866, LR: 0.0009750, Tokens/sec: 244443.8052730551\n",
      "Step: 197, Training Loss: 5.84082, LR: 0.0009745, Tokens/sec: 245275.28585908332\n",
      "Step: 198, Training Loss: 5.92430, LR: 0.0009739, Tokens/sec: 243631.66274089896\n",
      "Step: 199, Training Loss: 5.78651, LR: 0.0009734, Tokens/sec: 244696.58645773947\n",
      "Step: 200, Training Loss: 6.32552, LR: 0.0009729, Tokens/sec: 244106.3352752292\n",
      "Computing Eval loss, steps: 78\n",
      "Step: 200, Eval Loss: 5.90901\n",
      "Step: 201, Training Loss: 6.00101, LR: 0.0009723, Tokens/sec: 244505.3527011621\n",
      "Step: 202, Training Loss: 6.00824, LR: 0.0009718, Tokens/sec: 244901.1320893948\n",
      "Step: 203, Training Loss: 5.93424, LR: 0.0009712, Tokens/sec: 244863.9613147984\n",
      "Step: 204, Training Loss: 5.89248, LR: 0.0009707, Tokens/sec: 243959.8447121039\n",
      "Step: 205, Training Loss: 5.71357, LR: 0.0009701, Tokens/sec: 244403.3301715492\n",
      "Step: 206, Training Loss: 5.81153, LR: 0.0009695, Tokens/sec: 244535.62477344865\n",
      "Step: 207, Training Loss: 5.89582, LR: 0.0009690, Tokens/sec: 244931.69321632985\n",
      "Step: 208, Training Loss: 6.03845, LR: 0.0009684, Tokens/sec: 244373.4489724269\n",
      "Step: 209, Training Loss: 6.17099, LR: 0.0009678, Tokens/sec: 244574.88392952163\n",
      "Step: 210, Training Loss: 5.93694, LR: 0.0009672, Tokens/sec: 244524.36236072905\n",
      "Step: 211, Training Loss: 5.90860, LR: 0.0009666, Tokens/sec: 244611.13708786\n",
      "Step: 212, Training Loss: 5.86377, LR: 0.0009660, Tokens/sec: 244738.44574347453\n",
      "Step: 213, Training Loss: 5.89920, LR: 0.0009654, Tokens/sec: 244348.6030485305\n",
      "Step: 214, Training Loss: 5.85819, LR: 0.0009648, Tokens/sec: 245017.8342450014\n",
      "Step: 215, Training Loss: 5.79114, LR: 0.0009642, Tokens/sec: 244441.53979389623\n",
      "Step: 216, Training Loss: 5.75448, LR: 0.0009636, Tokens/sec: 244373.33355618228\n",
      "Step: 217, Training Loss: 5.81553, LR: 0.0009630, Tokens/sec: 244512.32783408102\n",
      "Step: 218, Training Loss: 5.71891, LR: 0.0009624, Tokens/sec: 244728.3561977434\n",
      "Step: 219, Training Loss: 5.94562, LR: 0.0009617, Tokens/sec: 244025.7994687737\n",
      "Step: 220, Training Loss: 5.88453, LR: 0.0009611, Tokens/sec: 245010.2023750892\n",
      "Step: 221, Training Loss: 5.97002, LR: 0.0009605, Tokens/sec: 244428.42835142766\n",
      "Step: 222, Training Loss: 5.81565, LR: 0.0009598, Tokens/sec: 244185.5295205621\n",
      "Step: 223, Training Loss: 5.86529, LR: 0.0009592, Tokens/sec: 244451.28131255167\n",
      "Step: 224, Training Loss: 5.91510, LR: 0.0009585, Tokens/sec: 244071.84341358556\n",
      "Step: 225, Training Loss: 5.60042, LR: 0.0009578, Tokens/sec: 245092.76715258896\n",
      "Step: 226, Training Loss: 5.73296, LR: 0.0009572, Tokens/sec: 244805.52398012223\n",
      "Step: 227, Training Loss: 5.79488, LR: 0.0009565, Tokens/sec: 244755.0262722932\n",
      "Step: 228, Training Loss: 5.83433, LR: 0.0009558, Tokens/sec: 244288.42354254515\n",
      "Step: 229, Training Loss: 5.78538, LR: 0.0009551, Tokens/sec: 245440.19362135697\n",
      "Step: 230, Training Loss: 5.78527, LR: 0.0009545, Tokens/sec: 244078.91913163487\n",
      "Step: 231, Training Loss: 5.84297, LR: 0.0009538, Tokens/sec: 244640.00310853054\n",
      "Step: 232, Training Loss: 5.84084, LR: 0.0009531, Tokens/sec: 243641.86114315505\n",
      "Step: 233, Training Loss: 5.83136, LR: 0.0009524, Tokens/sec: 244980.33123987284\n",
      "Step: 234, Training Loss: 5.79564, LR: 0.0009517, Tokens/sec: 245071.13921060192\n",
      "Step: 235, Training Loss: 5.83680, LR: 0.0009510, Tokens/sec: 244529.23292526003\n",
      "Step: 236, Training Loss: 5.87514, LR: 0.0009502, Tokens/sec: 244920.6721917654\n",
      "Step: 237, Training Loss: 5.73466, LR: 0.0009495, Tokens/sec: 245006.58026115585\n",
      "Step: 238, Training Loss: 5.79649, LR: 0.0009488, Tokens/sec: 242975.6434536432\n",
      "Step: 239, Training Loss: 5.79696, LR: 0.0009481, Tokens/sec: 244132.13137761215\n",
      "Step: 240, Training Loss: 5.78679, LR: 0.0009473, Tokens/sec: 244888.67183065598\n",
      "Step: 241, Training Loss: 5.78000, LR: 0.0009466, Tokens/sec: 244986.1957116255\n",
      "Step: 242, Training Loss: 5.70555, LR: 0.0009458, Tokens/sec: 244445.60376054852\n",
      "Step: 243, Training Loss: 5.70706, LR: 0.0009451, Tokens/sec: 244320.1280952774\n",
      "Step: 244, Training Loss: 5.70790, LR: 0.0009443, Tokens/sec: 244213.67477190585\n",
      "Step: 245, Training Loss: 5.72408, LR: 0.0009436, Tokens/sec: 244633.77589416335\n",
      "Step: 246, Training Loss: 5.61644, LR: 0.0009428, Tokens/sec: 244683.59568110204\n",
      "Step: 247, Training Loss: 5.65400, LR: 0.0009420, Tokens/sec: 244548.64743475185\n",
      "Step: 248, Training Loss: 5.48495, LR: 0.0009413, Tokens/sec: 244373.43709133787\n",
      "Step: 249, Training Loss: 5.80038, LR: 0.0009405, Tokens/sec: 244615.96008980976\n",
      "Step: 250, Training Loss: 5.80393, LR: 0.0009397, Tokens/sec: 244571.76258978294\n",
      "Step: 251, Training Loss: 5.69709, LR: 0.0009389, Tokens/sec: 244380.260421226\n",
      "Step: 252, Training Loss: 5.70706, LR: 0.0009381, Tokens/sec: 245385.32295374994\n",
      "Step: 253, Training Loss: 5.85676, LR: 0.0009373, Tokens/sec: 245082.32742545402\n",
      "Step: 254, Training Loss: 5.53396, LR: 0.0009365, Tokens/sec: 244737.40730066696\n",
      "Step: 255, Training Loss: 5.70608, LR: 0.0009357, Tokens/sec: 245007.55786307872\n",
      "Step: 256, Training Loss: 5.73465, LR: 0.0009349, Tokens/sec: 245047.40391222428\n",
      "Step: 257, Training Loss: 5.77705, LR: 0.0009341, Tokens/sec: 244909.93173912278\n",
      "Step: 258, Training Loss: 5.61501, LR: 0.0009333, Tokens/sec: 244878.2171007569\n",
      "Step: 259, Training Loss: 5.61125, LR: 0.0009325, Tokens/sec: 244677.92434807433\n",
      "Step: 260, Training Loss: 5.63943, LR: 0.0009316, Tokens/sec: 244666.51781853873\n",
      "Step: 261, Training Loss: 5.73301, LR: 0.0009308, Tokens/sec: 244342.37369141594\n",
      "Step: 262, Training Loss: 5.73593, LR: 0.0009299, Tokens/sec: 244183.3806736589\n",
      "Step: 263, Training Loss: 5.65243, LR: 0.0009291, Tokens/sec: 244829.22947872238\n",
      "Step: 264, Training Loss: 5.66014, LR: 0.0009283, Tokens/sec: 245278.62694967486\n",
      "Step: 265, Training Loss: 5.79993, LR: 0.0009274, Tokens/sec: 244950.73853413586\n",
      "Step: 266, Training Loss: 5.62475, LR: 0.0009265, Tokens/sec: 244916.41681404144\n",
      "Step: 267, Training Loss: 5.59784, LR: 0.0009257, Tokens/sec: 245717.9802463242\n",
      "Step: 268, Training Loss: 5.61666, LR: 0.0009248, Tokens/sec: 243761.1352976663\n",
      "Step: 269, Training Loss: 5.67846, LR: 0.0009239, Tokens/sec: 245224.01478549122\n",
      "Step: 270, Training Loss: 5.81125, LR: 0.0009231, Tokens/sec: 244276.78194592614\n",
      "Step: 271, Training Loss: 5.71297, LR: 0.0009222, Tokens/sec: 244872.49924904542\n",
      "Step: 272, Training Loss: 5.80445, LR: 0.0009213, Tokens/sec: 244226.05965308356\n",
      "Step: 273, Training Loss: 5.73202, LR: 0.0009204, Tokens/sec: 244569.03233287417\n",
      "Step: 274, Training Loss: 5.60590, LR: 0.0009195, Tokens/sec: 245155.65480182934\n",
      "Step: 275, Training Loss: 5.57526, LR: 0.0009186, Tokens/sec: 244861.7817659044\n",
      "Step: 276, Training Loss: 5.59859, LR: 0.0009177, Tokens/sec: 244658.05208448105\n",
      "Step: 277, Training Loss: 5.73176, LR: 0.0009168, Tokens/sec: 244462.95993038494\n",
      "Step: 278, Training Loss: 5.57875, LR: 0.0009159, Tokens/sec: 244771.15779771344\n",
      "Step: 279, Training Loss: 5.76612, LR: 0.0009150, Tokens/sec: 244571.451480221\n",
      "Step: 280, Training Loss: 5.69316, LR: 0.0009141, Tokens/sec: 244467.42887645774\n",
      "Step: 281, Training Loss: 5.74981, LR: 0.0009131, Tokens/sec: 245338.10127870046\n",
      "Step: 282, Training Loss: 5.68239, LR: 0.0009122, Tokens/sec: 244186.50227599623\n",
      "Step: 283, Training Loss: 5.65253, LR: 0.0009113, Tokens/sec: 244850.23881297326\n",
      "Step: 284, Training Loss: 5.61765, LR: 0.0009103, Tokens/sec: 244740.1157822748\n",
      "Step: 285, Training Loss: 5.66738, LR: 0.0009094, Tokens/sec: 245726.49889872965\n",
      "Step: 286, Training Loss: 5.75448, LR: 0.0009084, Tokens/sec: 244540.0317913414\n",
      "Step: 287, Training Loss: 5.61707, LR: 0.0009075, Tokens/sec: 244665.2332894349\n",
      "Step: 288, Training Loss: 5.55357, LR: 0.0009065, Tokens/sec: 245076.41567351308\n",
      "Step: 289, Training Loss: 5.54371, LR: 0.0009056, Tokens/sec: 243412.37860222763\n",
      "Step: 290, Training Loss: 5.66770, LR: 0.0009046, Tokens/sec: 245265.32126355002\n",
      "Step: 291, Training Loss: 5.65654, LR: 0.0009036, Tokens/sec: 244057.78123976447\n",
      "Step: 292, Training Loss: 5.63486, LR: 0.0009027, Tokens/sec: 244768.5133350297\n",
      "Step: 293, Training Loss: 5.62809, LR: 0.0009017, Tokens/sec: 244956.21787277298\n",
      "Step: 294, Training Loss: 5.57429, LR: 0.0009007, Tokens/sec: 244690.46697324238\n",
      "Step: 295, Training Loss: 5.84038, LR: 0.0008997, Tokens/sec: 245032.625012392\n",
      "Step: 296, Training Loss: 5.55550, LR: 0.0008987, Tokens/sec: 244860.58209331174\n",
      "Step: 297, Training Loss: 5.59549, LR: 0.0008977, Tokens/sec: 243711.10631960514\n",
      "Step: 298, Training Loss: 5.68545, LR: 0.0008967, Tokens/sec: 245171.2224487502\n",
      "Step: 299, Training Loss: 5.60370, LR: 0.0008957, Tokens/sec: 244495.84133421714\n",
      "Step: 300, Training Loss: 5.67590, LR: 0.0008947, Tokens/sec: 244847.37624727207\n",
      "Computing Eval loss, steps: 78\n",
      "Step: 300, Eval Loss: 5.58351\n",
      "Step: 301, Training Loss: 5.46457, LR: 0.0008937, Tokens/sec: 245019.70261557854\n",
      "Step: 302, Training Loss: 5.52605, LR: 0.0008927, Tokens/sec: 245073.95238545703\n",
      "Step: 303, Training Loss: 5.67946, LR: 0.0008917, Tokens/sec: 244986.48740734102\n",
      "Step: 304, Training Loss: 5.70601, LR: 0.0008906, Tokens/sec: 245286.87405368887\n",
      "Step: 305, Training Loss: 5.46175, LR: 0.0008896, Tokens/sec: 245432.35907747204\n",
      "Step: 306, Training Loss: 5.49019, LR: 0.0008886, Tokens/sec: 245727.71908444987\n",
      "Step: 307, Training Loss: 5.58955, LR: 0.0008875, Tokens/sec: 244513.1638572308\n",
      "Step: 308, Training Loss: 5.58732, LR: 0.0008865, Tokens/sec: 245530.79802738078\n",
      "Step: 309, Training Loss: 5.50397, LR: 0.0008855, Tokens/sec: 244694.08995699463\n",
      "Step: 310, Training Loss: 5.53313, LR: 0.0008844, Tokens/sec: 244504.93131732865\n",
      "Step: 311, Training Loss: 5.48631, LR: 0.0008834, Tokens/sec: 244170.12746263432\n",
      "Step: 312, Training Loss: 5.56027, LR: 0.0008823, Tokens/sec: 245260.85900126447\n",
      "Step: 313, Training Loss: 5.43848, LR: 0.0008812, Tokens/sec: 243730.52620050855\n",
      "Step: 314, Training Loss: 5.57875, LR: 0.0008802, Tokens/sec: 244179.55588752817\n",
      "Step: 315, Training Loss: 5.59259, LR: 0.0008791, Tokens/sec: 244859.41992169758\n",
      "Step: 316, Training Loss: 5.47775, LR: 0.0008780, Tokens/sec: 245093.34593044824\n",
      "Step: 317, Training Loss: 5.54818, LR: 0.0008770, Tokens/sec: 245382.2373679524\n",
      "Step: 318, Training Loss: 5.54775, LR: 0.0008759, Tokens/sec: 244736.89829667314\n",
      "Step: 319, Training Loss: 5.60166, LR: 0.0008748, Tokens/sec: 245503.68625518915\n",
      "Step: 320, Training Loss: 5.73700, LR: 0.0008737, Tokens/sec: 245153.50763643289\n",
      "Step: 321, Training Loss: 5.47298, LR: 0.0008726, Tokens/sec: 244514.98715519035\n",
      "Step: 322, Training Loss: 5.49292, LR: 0.0008715, Tokens/sec: 245476.3115829463\n",
      "Step: 323, Training Loss: 5.43178, LR: 0.0008704, Tokens/sec: 244471.12849248978\n",
      "Step: 324, Training Loss: 5.40672, LR: 0.0008693, Tokens/sec: 245124.3085544567\n",
      "Step: 325, Training Loss: 5.50097, LR: 0.0008682, Tokens/sec: 244502.06832449927\n",
      "Step: 326, Training Loss: 5.59980, LR: 0.0008671, Tokens/sec: 245176.88079515184\n",
      "Step: 327, Training Loss: 5.46025, LR: 0.0008660, Tokens/sec: 244739.04327767444\n",
      "Step: 328, Training Loss: 5.37314, LR: 0.0008648, Tokens/sec: 244876.97125100013\n",
      "Step: 329, Training Loss: 5.59838, LR: 0.0008637, Tokens/sec: 245196.55378182206\n",
      "Step: 330, Training Loss: 5.57617, LR: 0.0008626, Tokens/sec: 244686.05453154663\n",
      "Step: 331, Training Loss: 5.52240, LR: 0.0008615, Tokens/sec: 244722.71513052576\n",
      "Step: 332, Training Loss: 5.52731, LR: 0.0008603, Tokens/sec: 245105.9294758756\n",
      "Step: 333, Training Loss: 5.54166, LR: 0.0008592, Tokens/sec: 244843.77257669493\n",
      "Step: 334, Training Loss: 5.53650, LR: 0.0008580, Tokens/sec: 244142.27346575077\n",
      "Step: 335, Training Loss: 5.58043, LR: 0.0008569, Tokens/sec: 243745.48953743043\n",
      "Step: 336, Training Loss: 5.54879, LR: 0.0008557, Tokens/sec: 245112.31906503846\n",
      "Step: 337, Training Loss: 5.25478, LR: 0.0008546, Tokens/sec: 244742.0122657968\n",
      "Step: 338, Training Loss: 5.38484, LR: 0.0008534, Tokens/sec: 244611.20341135026\n",
      "Step: 339, Training Loss: 5.43725, LR: 0.0008523, Tokens/sec: 244464.83513529517\n",
      "Step: 340, Training Loss: 5.50849, LR: 0.0008511, Tokens/sec: 245146.99633538938\n",
      "Step: 341, Training Loss: 5.51968, LR: 0.0008499, Tokens/sec: 244694.65323935184\n",
      "Step: 342, Training Loss: 5.34795, LR: 0.0008488, Tokens/sec: 245138.79962526434\n",
      "Step: 343, Training Loss: 5.38798, LR: 0.0008476, Tokens/sec: 244799.29174291342\n",
      "Step: 344, Training Loss: 5.41609, LR: 0.0008464, Tokens/sec: 244705.60798015608\n",
      "Step: 345, Training Loss: 5.32617, LR: 0.0008452, Tokens/sec: 244682.64618668242\n",
      "Step: 346, Training Loss: 5.14097, LR: 0.0008440, Tokens/sec: 245181.44080601516\n",
      "Step: 347, Training Loss: 5.27668, LR: 0.0008428, Tokens/sec: 245580.7164781557\n",
      "Step: 348, Training Loss: 5.42955, LR: 0.0008417, Tokens/sec: 244549.26783889814\n",
      "Step: 349, Training Loss: 5.56256, LR: 0.0008405, Tokens/sec: 245045.71431725804\n",
      "Step: 350, Training Loss: 5.38415, LR: 0.0008393, Tokens/sec: 243995.04096164208\n",
      "Step: 351, Training Loss: 5.48008, LR: 0.0008380, Tokens/sec: 245145.61452192036\n",
      "Step: 352, Training Loss: 5.48417, LR: 0.0008368, Tokens/sec: 244829.66220294946\n",
      "Step: 353, Training Loss: 5.37643, LR: 0.0008356, Tokens/sec: 244164.0156430422\n",
      "Step: 354, Training Loss: 5.46294, LR: 0.0008344, Tokens/sec: 245303.8658963939\n",
      "Step: 355, Training Loss: 5.41319, LR: 0.0008332, Tokens/sec: 245113.12162918248\n",
      "Step: 356, Training Loss: 5.35500, LR: 0.0008320, Tokens/sec: 245159.46239762972\n",
      "Step: 357, Training Loss: 5.23485, LR: 0.0008307, Tokens/sec: 244675.63070629167\n",
      "Step: 358, Training Loss: 5.33236, LR: 0.0008295, Tokens/sec: 244915.1773948912\n",
      "Step: 359, Training Loss: 5.46417, LR: 0.0008283, Tokens/sec: 244705.64542230678\n",
      "Step: 360, Training Loss: 5.42640, LR: 0.0008270, Tokens/sec: 244937.87251675487\n",
      "Step: 361, Training Loss: 5.35769, LR: 0.0008258, Tokens/sec: 245468.32402938412\n",
      "Step: 362, Training Loss: 5.38021, LR: 0.0008246, Tokens/sec: 244946.4463009607\n",
      "Step: 363, Training Loss: 5.32952, LR: 0.0008233, Tokens/sec: 244926.15530030773\n",
      "Step: 364, Training Loss: 5.54115, LR: 0.0008221, Tokens/sec: 244698.73924127515\n",
      "Step: 365, Training Loss: 5.36886, LR: 0.0008208, Tokens/sec: 245319.32921931832\n",
      "Step: 366, Training Loss: 5.37089, LR: 0.0008196, Tokens/sec: 244251.0180043599\n",
      "Step: 367, Training Loss: 5.35748, LR: 0.0008183, Tokens/sec: 244797.9070336484\n",
      "Step: 368, Training Loss: 5.33636, LR: 0.0008170, Tokens/sec: 244961.77590864425\n",
      "Step: 369, Training Loss: 5.37677, LR: 0.0008158, Tokens/sec: 244425.46866463756\n",
      "Step: 370, Training Loss: 5.31875, LR: 0.0008145, Tokens/sec: 245012.918604274\n",
      "Step: 371, Training Loss: 5.28073, LR: 0.0008132, Tokens/sec: 245515.6592405129\n",
      "Step: 372, Training Loss: 5.53829, LR: 0.0008120, Tokens/sec: 244836.34577178909\n",
      "Step: 373, Training Loss: 5.43766, LR: 0.0008107, Tokens/sec: 244505.50902134064\n",
      "Step: 374, Training Loss: 5.37285, LR: 0.0008094, Tokens/sec: 243410.0496877509\n",
      "Step: 375, Training Loss: 5.21171, LR: 0.0008081, Tokens/sec: 244550.81461362774\n",
      "Step: 376, Training Loss: 5.30225, LR: 0.0008068, Tokens/sec: 244843.43010571998\n",
      "Step: 377, Training Loss: 5.40562, LR: 0.0008055, Tokens/sec: 244553.98471309096\n",
      "Step: 378, Training Loss: 5.38226, LR: 0.0008042, Tokens/sec: 245140.53320339203\n",
      "Step: 379, Training Loss: 5.31276, LR: 0.0008029, Tokens/sec: 245237.22539619292\n",
      "Step: 380, Training Loss: 5.38781, LR: 0.0008016, Tokens/sec: 244851.84221979286\n",
      "Step: 381, Training Loss: 5.40841, LR: 0.0008003, Tokens/sec: 244846.69980551163\n",
      "Step: 382, Training Loss: 5.35520, LR: 0.0007990, Tokens/sec: 245000.51862094968\n",
      "Step: 383, Training Loss: 5.33092, LR: 0.0007977, Tokens/sec: 244731.48664265653\n",
      "Step: 384, Training Loss: 5.23820, LR: 0.0007964, Tokens/sec: 244902.6287693787\n",
      "Step: 385, Training Loss: 5.30946, LR: 0.0007951, Tokens/sec: 244818.80878283476\n",
      "Step: 386, Training Loss: 5.42290, LR: 0.0007938, Tokens/sec: 244667.39913240095\n",
      "Step: 387, Training Loss: 5.42146, LR: 0.0007924, Tokens/sec: 244980.29712509358\n",
      "Step: 388, Training Loss: 5.51313, LR: 0.0007911, Tokens/sec: 244687.83786714688\n",
      "Step: 389, Training Loss: 5.34243, LR: 0.0007898, Tokens/sec: 245188.9739943132\n",
      "Step: 390, Training Loss: 5.41244, LR: 0.0007885, Tokens/sec: 245021.25705336238\n",
      "Step: 391, Training Loss: 5.42397, LR: 0.0007871, Tokens/sec: 245055.357268162\n",
      "Step: 392, Training Loss: 5.39388, LR: 0.0007858, Tokens/sec: 245404.17700381423\n",
      "Step: 393, Training Loss: 5.30440, LR: 0.0007845, Tokens/sec: 244447.9661200258\n",
      "Step: 394, Training Loss: 5.35140, LR: 0.0007831, Tokens/sec: 244893.25521753574\n",
      "Step: 395, Training Loss: 5.30591, LR: 0.0007818, Tokens/sec: 244668.02864618978\n",
      "Step: 396, Training Loss: 5.47322, LR: 0.0007804, Tokens/sec: 245210.9611125552\n",
      "Step: 397, Training Loss: 5.33410, LR: 0.0007791, Tokens/sec: 244652.02639034836\n",
      "Step: 398, Training Loss: 5.29972, LR: 0.0007777, Tokens/sec: 244749.71256321477\n",
      "Step: 399, Training Loss: 5.33994, LR: 0.0007764, Tokens/sec: 244854.9400407994\n",
      "Step: 400, Training Loss: 5.47993, LR: 0.0007750, Tokens/sec: 245061.95075268063\n",
      "Computing Eval loss, steps: 78\n",
      "Step: 400, Eval Loss: 5.28742\n",
      "Step: 401, Training Loss: 5.24597, LR: 0.0007736, Tokens/sec: 245617.51707981116\n",
      "Step: 402, Training Loss: 5.30338, LR: 0.0007723, Tokens/sec: 244618.53833106026\n",
      "Step: 403, Training Loss: 5.24364, LR: 0.0007709, Tokens/sec: 244485.94166733022\n",
      "Step: 404, Training Loss: 5.22944, LR: 0.0007695, Tokens/sec: 244974.91563763472\n",
      "Step: 405, Training Loss: 5.11154, LR: 0.0007682, Tokens/sec: 244285.30951026868\n",
      "Step: 406, Training Loss: 5.23535, LR: 0.0007668, Tokens/sec: 245172.17403040515\n",
      "Step: 407, Training Loss: 5.38262, LR: 0.0007654, Tokens/sec: 245886.40523811892\n",
      "Step: 408, Training Loss: 5.25093, LR: 0.0007640, Tokens/sec: 245353.7297468126\n",
      "Step: 409, Training Loss: 5.18389, LR: 0.0007626, Tokens/sec: 245087.38925893864\n",
      "Step: 410, Training Loss: 5.41943, LR: 0.0007613, Tokens/sec: 245006.84982600305\n",
      "Step: 411, Training Loss: 5.25956, LR: 0.0007599, Tokens/sec: 244460.6006781167\n",
      "Step: 412, Training Loss: 5.32269, LR: 0.0007585, Tokens/sec: 245065.48062305764\n",
      "Step: 413, Training Loss: 5.33287, LR: 0.0007571, Tokens/sec: 244243.20663303777\n",
      "Step: 414, Training Loss: 5.33437, LR: 0.0007557, Tokens/sec: 244365.43798605003\n",
      "Step: 415, Training Loss: 5.34988, LR: 0.0007543, Tokens/sec: 245187.98469233117\n",
      "Step: 416, Training Loss: 5.20407, LR: 0.0007529, Tokens/sec: 244896.71545590486\n",
      "Step: 417, Training Loss: 5.16379, LR: 0.0007515, Tokens/sec: 244997.4290570911\n",
      "Step: 418, Training Loss: 5.33494, LR: 0.0007501, Tokens/sec: 244909.955605751\n",
      "Step: 419, Training Loss: 5.28903, LR: 0.0007487, Tokens/sec: 244732.39395895213\n",
      "Step: 420, Training Loss: 5.26171, LR: 0.0007473, Tokens/sec: 243999.35744763614\n",
      "Step: 421, Training Loss: 5.30247, LR: 0.0007459, Tokens/sec: 245327.7791985203\n",
      "Step: 422, Training Loss: 5.22845, LR: 0.0007444, Tokens/sec: 244458.37395262986\n",
      "Step: 423, Training Loss: 5.17214, LR: 0.0007430, Tokens/sec: 244347.09107240773\n",
      "Step: 424, Training Loss: 5.22054, LR: 0.0007416, Tokens/sec: 244669.33532298915\n",
      "Step: 425, Training Loss: 5.35438, LR: 0.0007402, Tokens/sec: 245253.50431641671\n",
      "Step: 426, Training Loss: 5.13699, LR: 0.0007388, Tokens/sec: 244494.88140349585\n",
      "Step: 427, Training Loss: 5.20199, LR: 0.0007373, Tokens/sec: 244829.7899761437\n",
      "Step: 428, Training Loss: 5.26089, LR: 0.0007359, Tokens/sec: 245450.25804605326\n",
      "Step: 429, Training Loss: 5.10739, LR: 0.0007345, Tokens/sec: 245086.09689236776\n",
      "Step: 430, Training Loss: 5.17265, LR: 0.0007330, Tokens/sec: 243850.98066009243\n",
      "Step: 431, Training Loss: 5.30545, LR: 0.0007316, Tokens/sec: 244669.4237964019\n",
      "Step: 432, Training Loss: 5.31147, LR: 0.0007302, Tokens/sec: 245074.7034867547\n",
      "Step: 433, Training Loss: 5.28604, LR: 0.0007287, Tokens/sec: 244146.40541819428\n",
      "Step: 434, Training Loss: 5.22609, LR: 0.0007273, Tokens/sec: 245037.83838150933\n",
      "Step: 435, Training Loss: 5.23044, LR: 0.0007258, Tokens/sec: 245313.05882154356\n",
      "Step: 436, Training Loss: 5.12794, LR: 0.0007244, Tokens/sec: 244440.14044843926\n",
      "Step: 437, Training Loss: 5.19282, LR: 0.0007229, Tokens/sec: 245028.8998504827\n",
      "Step: 438, Training Loss: 5.14515, LR: 0.0007215, Tokens/sec: 244784.53934847593\n",
      "Step: 439, Training Loss: 5.17839, LR: 0.0007200, Tokens/sec: 245295.61593961797\n",
      "Step: 440, Training Loss: 5.37915, LR: 0.0007186, Tokens/sec: 245055.43578025745\n",
      "Step: 441, Training Loss: 5.47615, LR: 0.0007171, Tokens/sec: 243154.66684629404\n",
      "Step: 442, Training Loss: 5.18366, LR: 0.0007157, Tokens/sec: 244925.25677658533\n",
      "Step: 443, Training Loss: 5.30535, LR: 0.0007142, Tokens/sec: 244431.95018342798\n",
      "Step: 444, Training Loss: 5.16594, LR: 0.0007127, Tokens/sec: 244455.66830645836\n",
      "Step: 445, Training Loss: 5.17653, LR: 0.0007113, Tokens/sec: 244564.8690634433\n",
      "Step: 446, Training Loss: 5.25998, LR: 0.0007098, Tokens/sec: 245092.1678898982\n",
      "Step: 447, Training Loss: 5.19342, LR: 0.0007083, Tokens/sec: 245373.42770313416\n",
      "Step: 448, Training Loss: 5.18235, LR: 0.0007069, Tokens/sec: 244382.942331813\n",
      "Step: 449, Training Loss: 5.12989, LR: 0.0007054, Tokens/sec: 244738.9445396282\n",
      "Step: 450, Training Loss: 5.22517, LR: 0.0007039, Tokens/sec: 245204.91498821424\n",
      "Step: 451, Training Loss: 5.43173, LR: 0.0007024, Tokens/sec: 244883.17676514192\n",
      "Step: 452, Training Loss: 5.25942, LR: 0.0007010, Tokens/sec: 245093.23666244248\n",
      "Step: 453, Training Loss: 5.19826, LR: 0.0006995, Tokens/sec: 245303.8693168891\n",
      "Step: 454, Training Loss: 5.17834, LR: 0.0006980, Tokens/sec: 244564.71776722337\n",
      "Step: 455, Training Loss: 5.21584, LR: 0.0006965, Tokens/sec: 244872.5486719549\n",
      "Step: 456, Training Loss: 5.26988, LR: 0.0006950, Tokens/sec: 244637.12504489816\n",
      "Step: 457, Training Loss: 5.21798, LR: 0.0006935, Tokens/sec: 244956.89833062986\n",
      "Step: 458, Training Loss: 5.31949, LR: 0.0006920, Tokens/sec: 244216.11571123722\n",
      "Step: 459, Training Loss: 5.12444, LR: 0.0006906, Tokens/sec: 244632.05627951797\n",
      "Step: 460, Training Loss: 5.09431, LR: 0.0006891, Tokens/sec: 245421.42819065112\n",
      "Step: 461, Training Loss: 5.23250, LR: 0.0006876, Tokens/sec: 244279.73633697216\n",
      "Step: 462, Training Loss: 5.24609, LR: 0.0006861, Tokens/sec: 245100.4929525402\n",
      "Step: 463, Training Loss: 5.13119, LR: 0.0006846, Tokens/sec: 244783.79683671772\n",
      "Step: 464, Training Loss: 5.18877, LR: 0.0006831, Tokens/sec: 245342.6159649775\n",
      "Step: 465, Training Loss: 5.23485, LR: 0.0006816, Tokens/sec: 244179.35592415888\n",
      "Step: 466, Training Loss: 5.23178, LR: 0.0006801, Tokens/sec: 244528.98480388388\n",
      "Step: 467, Training Loss: 5.13401, LR: 0.0006786, Tokens/sec: 244949.5959702876\n",
      "Step: 468, Training Loss: 5.23130, LR: 0.0006771, Tokens/sec: 244519.2234967201\n",
      "Step: 469, Training Loss: 5.11614, LR: 0.0006755, Tokens/sec: 244358.60869546857\n",
      "Step: 470, Training Loss: 5.16146, LR: 0.0006740, Tokens/sec: 244753.31857324488\n",
      "Step: 471, Training Loss: 5.12575, LR: 0.0006725, Tokens/sec: 245157.27929324322\n",
      "Step: 472, Training Loss: 4.99459, LR: 0.0006710, Tokens/sec: 244783.24676783817\n",
      "Step: 473, Training Loss: 5.10260, LR: 0.0006695, Tokens/sec: 244136.8745242299\n",
      "Step: 474, Training Loss: 5.17880, LR: 0.0006680, Tokens/sec: 245293.474872381\n",
      "Step: 475, Training Loss: 5.22102, LR: 0.0006665, Tokens/sec: 244590.24527797365\n",
      "Step: 476, Training Loss: 5.06391, LR: 0.0006650, Tokens/sec: 244791.20512594486\n",
      "Step: 477, Training Loss: 5.06437, LR: 0.0006634, Tokens/sec: 244472.45854710494\n",
      "Step: 478, Training Loss: 5.16716, LR: 0.0006619, Tokens/sec: 245182.98875384757\n",
      "Step: 479, Training Loss: 5.25647, LR: 0.0006604, Tokens/sec: 244611.09117161855\n",
      "Step: 480, Training Loss: 5.19464, LR: 0.0006589, Tokens/sec: 245198.62822374178\n",
      "Step: 481, Training Loss: 5.19716, LR: 0.0006573, Tokens/sec: 244767.33841501703\n",
      "Step: 482, Training Loss: 5.02539, LR: 0.0006558, Tokens/sec: 245284.63396021028\n",
      "Step: 483, Training Loss: 5.25180, LR: 0.0006543, Tokens/sec: 245182.3292505049\n",
      "Step: 484, Training Loss: 4.99108, LR: 0.0006528, Tokens/sec: 244936.93127868063\n",
      "Step: 485, Training Loss: 5.12731, LR: 0.0006512, Tokens/sec: 245084.1097132937\n",
      "Step: 486, Training Loss: 5.12520, LR: 0.0006497, Tokens/sec: 244335.42182561685\n",
      "Step: 487, Training Loss: 5.01107, LR: 0.0006482, Tokens/sec: 244835.2894608221\n",
      "Step: 488, Training Loss: 5.20090, LR: 0.0006466, Tokens/sec: 245475.65906389715\n",
      "Step: 489, Training Loss: 5.08557, LR: 0.0006451, Tokens/sec: 244922.72321388786\n",
      "Step: 490, Training Loss: 5.19087, LR: 0.0006436, Tokens/sec: 243893.19693263463\n",
      "Step: 491, Training Loss: 5.07730, LR: 0.0006420, Tokens/sec: 244849.54702012887\n",
      "Step: 492, Training Loss: 5.21735, LR: 0.0006405, Tokens/sec: 245139.07460498635\n",
      "Step: 493, Training Loss: 5.05253, LR: 0.0006389, Tokens/sec: 244916.81574942145\n",
      "Step: 494, Training Loss: 5.04208, LR: 0.0006374, Tokens/sec: 244241.63661957544\n",
      "Step: 495, Training Loss: 5.08616, LR: 0.0006359, Tokens/sec: 245472.33657975856\n",
      "Step: 496, Training Loss: 5.05730, LR: 0.0006343, Tokens/sec: 244183.21798647442\n",
      "Step: 497, Training Loss: 5.19425, LR: 0.0006328, Tokens/sec: 244956.8829819147\n",
      "Step: 498, Training Loss: 5.17922, LR: 0.0006312, Tokens/sec: 244992.2071718649\n",
      "Step: 499, Training Loss: 5.10265, LR: 0.0006297, Tokens/sec: 244310.7481866851\n",
      "Step: 500, Training Loss: 5.16162, LR: 0.0006281, Tokens/sec: 244776.26468421693\n",
      "Computing Eval loss, steps: 78\n",
      "Step: 500, Eval Loss: 5.09321\n",
      "Step: 501, Training Loss: 5.16144, LR: 0.0006266, Tokens/sec: 243131.36514524987\n",
      "Step: 502, Training Loss: 5.09711, LR: 0.0006250, Tokens/sec: 245300.73618328446\n",
      "Step: 503, Training Loss: 5.27211, LR: 0.0006235, Tokens/sec: 244988.58217826483\n",
      "Step: 504, Training Loss: 5.14560, LR: 0.0006219, Tokens/sec: 245382.271594768\n",
      "Step: 505, Training Loss: 5.05355, LR: 0.0006204, Tokens/sec: 244697.95981013437\n",
      "Step: 506, Training Loss: 4.94411, LR: 0.0006188, Tokens/sec: 245315.28233017324\n",
      "Step: 507, Training Loss: 5.11624, LR: 0.0006173, Tokens/sec: 244485.3878397309\n",
      "Step: 508, Training Loss: 4.98015, LR: 0.0006157, Tokens/sec: 245185.8164573608\n",
      "Step: 509, Training Loss: 5.01359, LR: 0.0006142, Tokens/sec: 244828.0045707981\n",
      "Step: 510, Training Loss: 5.00498, LR: 0.0006126, Tokens/sec: 244071.542040112\n",
      "Step: 511, Training Loss: 5.02920, LR: 0.0006111, Tokens/sec: 244997.82655044788\n",
      "Step: 512, Training Loss: 5.11443, LR: 0.0006095, Tokens/sec: 244760.18697126448\n",
      "Step: 513, Training Loss: 5.08784, LR: 0.0006080, Tokens/sec: 244566.490834682\n",
      "Step: 514, Training Loss: 5.07505, LR: 0.0006064, Tokens/sec: 245566.34964764945\n",
      "Step: 515, Training Loss: 5.18772, LR: 0.0006048, Tokens/sec: 244430.27076293493\n",
      "Step: 516, Training Loss: 5.06003, LR: 0.0006033, Tokens/sec: 245193.90182811502\n",
      "Step: 517, Training Loss: 4.97259, LR: 0.0006017, Tokens/sec: 245418.19276643248\n",
      "Step: 518, Training Loss: 5.13668, LR: 0.0006002, Tokens/sec: 244681.96384846917\n",
      "Step: 519, Training Loss: 4.99143, LR: 0.0005986, Tokens/sec: 245664.4261440657\n",
      "Step: 520, Training Loss: 5.02471, LR: 0.0005970, Tokens/sec: 245296.1922556341\n",
      "Step: 521, Training Loss: 5.05791, LR: 0.0005955, Tokens/sec: 244859.47104338414\n",
      "Step: 522, Training Loss: 5.06112, LR: 0.0005939, Tokens/sec: 244886.6060388855\n",
      "Step: 523, Training Loss: 5.02285, LR: 0.0005923, Tokens/sec: 245165.09115988863\n",
      "Step: 524, Training Loss: 5.14742, LR: 0.0005908, Tokens/sec: 244473.2501318171\n",
      "Step: 525, Training Loss: 5.07269, LR: 0.0005892, Tokens/sec: 244648.67172822522\n",
      "Step: 526, Training Loss: 5.03745, LR: 0.0005877, Tokens/sec: 244905.4380768745\n",
      "Step: 527, Training Loss: 5.03559, LR: 0.0005861, Tokens/sec: 244892.45238802765\n",
      "Step: 528, Training Loss: 5.13204, LR: 0.0005845, Tokens/sec: 244833.83960367454\n",
      "Step: 529, Training Loss: 5.04291, LR: 0.0005830, Tokens/sec: 245042.69358432374\n",
      "Step: 530, Training Loss: 4.99109, LR: 0.0005814, Tokens/sec: 243816.858283033\n",
      "Step: 531, Training Loss: 5.10941, LR: 0.0005798, Tokens/sec: 244805.357056269\n",
      "Step: 532, Training Loss: 5.04610, LR: 0.0005783, Tokens/sec: 245069.80604774397\n",
      "Step: 533, Training Loss: 4.98441, LR: 0.0005767, Tokens/sec: 245247.43387096675\n",
      "Step: 534, Training Loss: 5.01964, LR: 0.0005751, Tokens/sec: 245596.33644301235\n",
      "Step: 535, Training Loss: 5.11451, LR: 0.0005736, Tokens/sec: 245135.67753851824\n",
      "Step: 536, Training Loss: 4.98047, LR: 0.0005720, Tokens/sec: 244927.22944547722\n",
      "Step: 537, Training Loss: 4.99073, LR: 0.0005704, Tokens/sec: 243621.2138491933\n",
      "Step: 538, Training Loss: 5.02433, LR: 0.0005688, Tokens/sec: 245461.6350244822\n",
      "Step: 539, Training Loss: 5.03378, LR: 0.0005673, Tokens/sec: 244730.43634307594\n",
      "Step: 540, Training Loss: 4.86265, LR: 0.0005657, Tokens/sec: 244847.21948965482\n",
      "Step: 541, Training Loss: 4.99656, LR: 0.0005641, Tokens/sec: 245037.49366089192\n",
      "Step: 542, Training Loss: 5.00634, LR: 0.0005626, Tokens/sec: 244837.16356718465\n",
      "Step: 543, Training Loss: 5.14292, LR: 0.0005610, Tokens/sec: 244768.41457320313\n",
      "Step: 544, Training Loss: 5.02656, LR: 0.0005594, Tokens/sec: 245002.87465768357\n",
      "Step: 545, Training Loss: 4.98959, LR: 0.0005579, Tokens/sec: 245286.69792271958\n",
      "Step: 546, Training Loss: 4.94463, LR: 0.0005563, Tokens/sec: 244690.46186812726\n",
      "Step: 547, Training Loss: 4.90083, LR: 0.0005547, Tokens/sec: 245413.1857304511\n",
      "Step: 548, Training Loss: 4.98764, LR: 0.0005531, Tokens/sec: 245212.50088829352\n",
      "Step: 549, Training Loss: 5.13082, LR: 0.0005516, Tokens/sec: 244209.15916277457\n",
      "Step: 550, Training Loss: 5.16520, LR: 0.0005500, Tokens/sec: 244680.054682966\n",
      "Step: 551, Training Loss: 5.12024, LR: 0.0005484, Tokens/sec: 245438.93519827002\n",
      "Step: 552, Training Loss: 4.92859, LR: 0.0005469, Tokens/sec: 244511.46462918533\n",
      "Step: 553, Training Loss: 5.01700, LR: 0.0005453, Tokens/sec: 244385.92645088007\n",
      "Step: 554, Training Loss: 5.04489, LR: 0.0005437, Tokens/sec: 245032.70862932783\n",
      "Step: 555, Training Loss: 4.98988, LR: 0.0005421, Tokens/sec: 245083.7836414546\n",
      "Step: 556, Training Loss: 4.93920, LR: 0.0005406, Tokens/sec: 244227.05476991308\n",
      "Step: 557, Training Loss: 5.10256, LR: 0.0005390, Tokens/sec: 245006.91295203398\n",
      "Step: 558, Training Loss: 4.95440, LR: 0.0005374, Tokens/sec: 245464.9349565056\n",
      "Step: 559, Training Loss: 5.06054, LR: 0.0005359, Tokens/sec: 244360.19888137357\n",
      "Step: 560, Training Loss: 5.11081, LR: 0.0005343, Tokens/sec: 244846.31643337736\n",
      "Step: 561, Training Loss: 4.93914, LR: 0.0005327, Tokens/sec: 244738.7351471386\n",
      "Step: 562, Training Loss: 4.78807, LR: 0.0005312, Tokens/sec: 243770.60816466736\n",
      "Step: 563, Training Loss: 4.98026, LR: 0.0005296, Tokens/sec: 245068.0564001896\n",
      "Step: 564, Training Loss: 5.04872, LR: 0.0005280, Tokens/sec: 244456.97950980405\n",
      "Step: 565, Training Loss: 5.00535, LR: 0.0005264, Tokens/sec: 245288.23351968112\n",
      "Step: 566, Training Loss: 4.97548, LR: 0.0005249, Tokens/sec: 245000.77623029056\n",
      "Step: 567, Training Loss: 4.94182, LR: 0.0005233, Tokens/sec: 244986.41576271495\n",
      "Step: 568, Training Loss: 4.74826, LR: 0.0005217, Tokens/sec: 245387.64874608125\n",
      "Step: 569, Training Loss: 5.06417, LR: 0.0005202, Tokens/sec: 245079.39285281824\n",
      "Step: 570, Training Loss: 4.90596, LR: 0.0005186, Tokens/sec: 244308.48517316135\n",
      "Step: 571, Training Loss: 4.93302, LR: 0.0005170, Tokens/sec: 243953.25965458874\n",
      "Step: 572, Training Loss: 5.00745, LR: 0.0005155, Tokens/sec: 245446.11951223065\n",
      "Step: 573, Training Loss: 5.04419, LR: 0.0005139, Tokens/sec: 244941.85069162934\n",
      "Step: 574, Training Loss: 4.94096, LR: 0.0005123, Tokens/sec: 244356.0138687795\n",
      "Step: 575, Training Loss: 4.84871, LR: 0.0005108, Tokens/sec: 244896.2364712652\n",
      "Step: 576, Training Loss: 4.93823, LR: 0.0005092, Tokens/sec: 244409.4828406025\n",
      "Step: 577, Training Loss: 4.91715, LR: 0.0005077, Tokens/sec: 245068.56166169827\n",
      "Step: 578, Training Loss: 4.95178, LR: 0.0005061, Tokens/sec: 243942.82036900453\n",
      "Step: 579, Training Loss: 5.06762, LR: 0.0005045, Tokens/sec: 244758.86059466313\n",
      "Step: 580, Training Loss: 4.94697, LR: 0.0005030, Tokens/sec: 245353.7246139839\n",
      "Step: 581, Training Loss: 5.13007, LR: 0.0005014, Tokens/sec: 245109.50330176787\n",
      "Step: 582, Training Loss: 4.59328, LR: 0.0004998, Tokens/sec: 244690.21001604552\n",
      "Step: 583, Training Loss: 4.89616, LR: 0.0004983, Tokens/sec: 244787.5537199961\n",
      "Step: 584, Training Loss: 4.74833, LR: 0.0004967, Tokens/sec: 244425.88807659375\n",
      "Step: 585, Training Loss: 4.91165, LR: 0.0004952, Tokens/sec: 245073.77656013644\n",
      "Step: 586, Training Loss: 5.09234, LR: 0.0004936, Tokens/sec: 244468.86420547173\n",
      "Step: 587, Training Loss: 4.98507, LR: 0.0004920, Tokens/sec: 245564.32725101133\n",
      "Step: 588, Training Loss: 5.05801, LR: 0.0004905, Tokens/sec: 244523.2237710421\n",
      "Step: 589, Training Loss: 4.86954, LR: 0.0004889, Tokens/sec: 245126.95387961867\n",
      "Step: 590, Training Loss: 4.88581, LR: 0.0004874, Tokens/sec: 245233.65980972708\n",
      "Step: 591, Training Loss: 4.95109, LR: 0.0004858, Tokens/sec: 245348.76640177754\n",
      "Step: 592, Training Loss: 4.95674, LR: 0.0004843, Tokens/sec: 244804.95507731164\n",
      "Step: 593, Training Loss: 4.99383, LR: 0.0004827, Tokens/sec: 244630.569708299\n",
      "Step: 594, Training Loss: 5.00928, LR: 0.0004812, Tokens/sec: 245348.51832534058\n",
      "Step: 595, Training Loss: 5.00439, LR: 0.0004796, Tokens/sec: 244546.7981391509\n",
      "Step: 596, Training Loss: 5.02369, LR: 0.0004781, Tokens/sec: 244604.20052960864\n",
      "Step: 597, Training Loss: 4.86258, LR: 0.0004765, Tokens/sec: 245143.54439294385\n",
      "Step: 598, Training Loss: 4.85894, LR: 0.0004750, Tokens/sec: 244849.96959263322\n",
      "Step: 599, Training Loss: 4.99214, LR: 0.0004734, Tokens/sec: 244715.66841733706\n",
      "Step: 600, Training Loss: 4.93788, LR: 0.0004719, Tokens/sec: 244143.11034847266\n",
      "Computing Eval loss, steps: 78\n",
      "Step: 600, Eval Loss: 4.94189\n",
      "Step: 601, Training Loss: 4.91854, LR: 0.0004703, Tokens/sec: 244956.44810245174\n",
      "Step: 602, Training Loss: 4.80546, LR: 0.0004688, Tokens/sec: 244847.71361760746\n",
      "Step: 603, Training Loss: 4.86559, LR: 0.0004672, Tokens/sec: 245095.94619626872\n",
      "Step: 604, Training Loss: 4.89606, LR: 0.0004657, Tokens/sec: 245467.942133219\n",
      "Step: 605, Training Loss: 5.00758, LR: 0.0004641, Tokens/sec: 244873.0377894106\n",
      "Step: 606, Training Loss: 4.85638, LR: 0.0004626, Tokens/sec: 245399.00451687997\n",
      "Step: 607, Training Loss: 4.89999, LR: 0.0004611, Tokens/sec: 245473.32646511105\n",
      "Step: 608, Training Loss: 4.96000, LR: 0.0004595, Tokens/sec: 244408.40813849718\n",
      "Step: 609, Training Loss: 4.95491, LR: 0.0004580, Tokens/sec: 244544.28089845023\n",
      "Step: 610, Training Loss: 4.82921, LR: 0.0004564, Tokens/sec: 245717.1496921553\n",
      "Step: 611, Training Loss: 4.95256, LR: 0.0004549, Tokens/sec: 244788.63346209092\n",
      "Step: 612, Training Loss: 5.05456, LR: 0.0004534, Tokens/sec: 245141.12416272203\n",
      "Step: 613, Training Loss: 5.00094, LR: 0.0004518, Tokens/sec: 244601.88446317692\n",
      "Step: 614, Training Loss: 4.89043, LR: 0.0004503, Tokens/sec: 245584.60071312453\n",
      "Step: 615, Training Loss: 4.82558, LR: 0.0004488, Tokens/sec: 244938.4966025098\n",
      "Step: 616, Training Loss: 4.80831, LR: 0.0004472, Tokens/sec: 242583.34192231353\n",
      "Step: 617, Training Loss: 4.98455, LR: 0.0004457, Tokens/sec: 244778.96210540461\n",
      "Step: 618, Training Loss: 4.89163, LR: 0.0004442, Tokens/sec: 244873.33944121975\n",
      "Step: 619, Training Loss: 5.05219, LR: 0.0004427, Tokens/sec: 244685.58658042268\n",
      "Step: 620, Training Loss: 4.97385, LR: 0.0004411, Tokens/sec: 244000.31179914193\n",
      "Step: 621, Training Loss: 4.94289, LR: 0.0004396, Tokens/sec: 245129.78199858905\n",
      "Step: 622, Training Loss: 4.98656, LR: 0.0004381, Tokens/sec: 244876.89626181594\n",
      "Step: 623, Training Loss: 5.00336, LR: 0.0004366, Tokens/sec: 244635.0141677524\n",
      "Step: 624, Training Loss: 5.06258, LR: 0.0004350, Tokens/sec: 244542.7664997412\n",
      "Step: 625, Training Loss: 4.83568, LR: 0.0004335, Tokens/sec: 245194.23844534895\n",
      "Step: 626, Training Loss: 5.01523, LR: 0.0004320, Tokens/sec: 245219.72493342552\n",
      "Step: 627, Training Loss: 4.85758, LR: 0.0004305, Tokens/sec: 244369.74722043637\n",
      "Step: 628, Training Loss: 4.71070, LR: 0.0004290, Tokens/sec: 245640.92735612934\n",
      "Step: 629, Training Loss: 4.93946, LR: 0.0004275, Tokens/sec: 243800.91142099912\n",
      "Step: 630, Training Loss: 4.77551, LR: 0.0004260, Tokens/sec: 245576.86664717604\n",
      "Step: 631, Training Loss: 4.96165, LR: 0.0004245, Tokens/sec: 244576.4888323833\n",
      "Step: 632, Training Loss: 4.83821, LR: 0.0004229, Tokens/sec: 244051.32801307898\n",
      "Step: 633, Training Loss: 5.01350, LR: 0.0004214, Tokens/sec: 244413.02450084756\n",
      "Step: 634, Training Loss: 4.80086, LR: 0.0004199, Tokens/sec: 244991.83699112988\n",
      "Step: 635, Training Loss: 4.90426, LR: 0.0004184, Tokens/sec: 244162.59236330166\n",
      "Step: 636, Training Loss: 4.95817, LR: 0.0004169, Tokens/sec: 244685.31091550743\n",
      "Step: 637, Training Loss: 4.86047, LR: 0.0004154, Tokens/sec: 245296.85921226756\n",
      "Step: 638, Training Loss: 4.84826, LR: 0.0004139, Tokens/sec: 244547.23496472478\n",
      "Step: 639, Training Loss: 4.81130, LR: 0.0004124, Tokens/sec: 245175.02198588496\n",
      "Step: 640, Training Loss: 4.88408, LR: 0.0004109, Tokens/sec: 245299.4142027765\n",
      "Step: 641, Training Loss: 5.04559, LR: 0.0004094, Tokens/sec: 244215.30205937178\n",
      "Step: 642, Training Loss: 4.85785, LR: 0.0004080, Tokens/sec: 244486.8709481111\n",
      "Step: 643, Training Loss: 4.84998, LR: 0.0004065, Tokens/sec: 245246.20990316363\n",
      "Step: 644, Training Loss: 4.87276, LR: 0.0004050, Tokens/sec: 244832.52094710243\n",
      "Step: 645, Training Loss: 4.90148, LR: 0.0004035, Tokens/sec: 245121.91602741723\n",
      "Step: 646, Training Loss: 5.15949, LR: 0.0004020, Tokens/sec: 245165.034785571\n",
      "Step: 647, Training Loss: 4.93672, LR: 0.0004005, Tokens/sec: 244560.81812924964\n",
      "Step: 648, Training Loss: 4.92907, LR: 0.0003990, Tokens/sec: 244709.82539989767\n",
      "Step: 649, Training Loss: 4.96311, LR: 0.0003976, Tokens/sec: 245251.41185070254\n",
      "Step: 650, Training Loss: 4.96130, LR: 0.0003961, Tokens/sec: 245133.69467898438\n",
      "Step: 651, Training Loss: 4.81029, LR: 0.0003946, Tokens/sec: 243841.85135904737\n",
      "Step: 652, Training Loss: 4.91550, LR: 0.0003931, Tokens/sec: 245452.8624659432\n",
      "Step: 653, Training Loss: 4.98184, LR: 0.0003917, Tokens/sec: 245335.49245086045\n",
      "Step: 654, Training Loss: 4.82656, LR: 0.0003902, Tokens/sec: 244781.5046155338\n",
      "Step: 655, Training Loss: 4.79509, LR: 0.0003887, Tokens/sec: 244944.1322750045\n",
      "Step: 656, Training Loss: 4.71131, LR: 0.0003873, Tokens/sec: 245113.01917389274\n",
      "Step: 657, Training Loss: 4.79420, LR: 0.0003858, Tokens/sec: 244455.31333284255\n",
      "Step: 658, Training Loss: 4.94065, LR: 0.0003843, Tokens/sec: 245015.46768317485\n",
      "Step: 659, Training Loss: 4.89946, LR: 0.0003829, Tokens/sec: 245458.11941870267\n",
      "Step: 660, Training Loss: 4.88602, LR: 0.0003814, Tokens/sec: 245166.86952667127\n",
      "Step: 661, Training Loss: 4.92160, LR: 0.0003800, Tokens/sec: 245025.11849184678\n",
      "Step: 662, Training Loss: 4.77952, LR: 0.0003785, Tokens/sec: 244427.21933703416\n",
      "Step: 663, Training Loss: 4.84422, LR: 0.0003771, Tokens/sec: 244939.43103193553\n",
      "Step: 664, Training Loss: 4.88557, LR: 0.0003756, Tokens/sec: 244615.90906975293\n",
      "Step: 665, Training Loss: 4.74347, LR: 0.0003742, Tokens/sec: 245240.2252867865\n",
      "Step: 666, Training Loss: 4.87060, LR: 0.0003727, Tokens/sec: 244395.12879225906\n",
      "Step: 667, Training Loss: 4.90201, LR: 0.0003713, Tokens/sec: 245228.63124416137\n",
      "Step: 668, Training Loss: 4.73923, LR: 0.0003698, Tokens/sec: 245161.0510661083\n",
      "Step: 669, Training Loss: 4.86507, LR: 0.0003684, Tokens/sec: 245019.68043384294\n",
      "Step: 670, Training Loss: 4.79024, LR: 0.0003670, Tokens/sec: 244371.6006283223\n",
      "Step: 671, Training Loss: 4.89599, LR: 0.0003655, Tokens/sec: 244922.16229087565\n",
      "Step: 672, Training Loss: 4.87075, LR: 0.0003641, Tokens/sec: 244809.46889989302\n",
      "Step: 673, Training Loss: 4.85155, LR: 0.0003627, Tokens/sec: 244623.76640924453\n",
      "Step: 674, Training Loss: 4.95566, LR: 0.0003612, Tokens/sec: 244641.33330030137\n",
      "Step: 675, Training Loss: 4.96693, LR: 0.0003598, Tokens/sec: 245332.05743626834\n",
      "Step: 676, Training Loss: 4.87568, LR: 0.0003584, Tokens/sec: 244298.49889680787\n",
      "Step: 677, Training Loss: 4.87514, LR: 0.0003570, Tokens/sec: 244513.76199112268\n",
      "Step: 678, Training Loss: 4.83113, LR: 0.0003556, Tokens/sec: 244853.30251398854\n",
      "Step: 679, Training Loss: 4.87394, LR: 0.0003541, Tokens/sec: 245425.5761635246\n",
      "Step: 680, Training Loss: 4.98365, LR: 0.0003527, Tokens/sec: 244880.67986431788\n",
      "Step: 681, Training Loss: 4.82529, LR: 0.0003513, Tokens/sec: 245309.45340245124\n",
      "Step: 682, Training Loss: 4.70399, LR: 0.0003499, Tokens/sec: 244216.16825977323\n",
      "Step: 683, Training Loss: 4.78684, LR: 0.0003485, Tokens/sec: 244737.31196865413\n",
      "Step: 684, Training Loss: 4.93959, LR: 0.0003471, Tokens/sec: 244567.43092785764\n",
      "Step: 685, Training Loss: 4.65179, LR: 0.0003457, Tokens/sec: 245063.59960597445\n",
      "Step: 686, Training Loss: 4.90270, LR: 0.0003443, Tokens/sec: 245271.55331536796\n",
      "Step: 687, Training Loss: 4.81848, LR: 0.0003429, Tokens/sec: 244567.61452736842\n",
      "Step: 688, Training Loss: 4.92154, LR: 0.0003415, Tokens/sec: 245138.9277213314\n",
      "Step: 689, Training Loss: 4.87047, LR: 0.0003401, Tokens/sec: 245370.51011170846\n",
      "Step: 690, Training Loss: 4.91258, LR: 0.0003387, Tokens/sec: 243796.8046688978\n",
      "Step: 691, Training Loss: 4.86627, LR: 0.0003374, Tokens/sec: 244779.5666492425\n",
      "Step: 692, Training Loss: 4.91225, LR: 0.0003360, Tokens/sec: 245483.22747049946\n",
      "Step: 693, Training Loss: 4.86689, LR: 0.0003346, Tokens/sec: 245625.65323964064\n",
      "Step: 694, Training Loss: 4.92742, LR: 0.0003332, Tokens/sec: 245020.95162440336\n",
      "Step: 695, Training Loss: 4.88465, LR: 0.0003318, Tokens/sec: 245116.08433169266\n",
      "Step: 696, Training Loss: 4.97901, LR: 0.0003305, Tokens/sec: 245282.29302074618\n",
      "Step: 697, Training Loss: 4.87624, LR: 0.0003291, Tokens/sec: 245007.27976610075\n",
      "Step: 698, Training Loss: 4.73431, LR: 0.0003277, Tokens/sec: 244349.09855992786\n",
      "Step: 699, Training Loss: 4.88849, LR: 0.0003264, Tokens/sec: 245338.5323821508\n",
      "Step: 700, Training Loss: 4.86279, LR: 0.0003250, Tokens/sec: 244334.4903022615\n",
      "Computing Eval loss, steps: 78\n",
      "Step: 700, Eval Loss: 4.83797\n",
      "Step: 701, Training Loss: 4.84265, LR: 0.0003236, Tokens/sec: 243397.28102317313\n",
      "Step: 702, Training Loss: 4.78408, LR: 0.0003223, Tokens/sec: 245434.35190789847\n",
      "Step: 703, Training Loss: 5.03827, LR: 0.0003209, Tokens/sec: 245116.0279799105\n",
      "Step: 704, Training Loss: 4.73685, LR: 0.0003196, Tokens/sec: 245314.65803329588\n",
      "Step: 705, Training Loss: 4.89796, LR: 0.0003182, Tokens/sec: 244453.9613853534\n",
      "Step: 706, Training Loss: 4.82646, LR: 0.0003169, Tokens/sec: 245179.54434028023\n",
      "Step: 707, Training Loss: 4.87197, LR: 0.0003155, Tokens/sec: 245168.46512459024\n",
      "Step: 708, Training Loss: 4.83716, LR: 0.0003142, Tokens/sec: 244720.6283079485\n",
      "Step: 709, Training Loss: 4.78673, LR: 0.0003129, Tokens/sec: 245090.89596027965\n",
      "Step: 710, Training Loss: 4.83355, LR: 0.0003115, Tokens/sec: 244119.36306523642\n",
      "Step: 711, Training Loss: 4.87369, LR: 0.0003102, Tokens/sec: 245224.1617711298\n",
      "Step: 712, Training Loss: 4.88559, LR: 0.0003089, Tokens/sec: 245705.60660012582\n",
      "Step: 713, Training Loss: 4.66955, LR: 0.0003076, Tokens/sec: 245001.70431081802\n",
      "Step: 714, Training Loss: 4.81932, LR: 0.0003062, Tokens/sec: 244698.2031691059\n",
      "Step: 715, Training Loss: 4.94778, LR: 0.0003049, Tokens/sec: 245413.7831404202\n",
      "Step: 716, Training Loss: 4.82764, LR: 0.0003036, Tokens/sec: 245021.0625342725\n",
      "Step: 717, Training Loss: 4.82042, LR: 0.0003023, Tokens/sec: 244211.50508901855\n",
      "Step: 718, Training Loss: 4.70746, LR: 0.0003010, Tokens/sec: 245259.75457367417\n",
      "Step: 719, Training Loss: 4.58454, LR: 0.0002997, Tokens/sec: 244883.44435412134\n",
      "Step: 720, Training Loss: 4.81417, LR: 0.0002984, Tokens/sec: 244344.32171012103\n",
      "Step: 721, Training Loss: 4.77091, LR: 0.0002971, Tokens/sec: 245463.8252652593\n",
      "Step: 722, Training Loss: 4.88619, LR: 0.0002958, Tokens/sec: 245316.95511938768\n",
      "Step: 723, Training Loss: 4.69899, LR: 0.0002945, Tokens/sec: 244855.9862972846\n",
      "Step: 724, Training Loss: 4.87049, LR: 0.0002932, Tokens/sec: 244493.40329482369\n",
      "Step: 725, Training Loss: 4.85339, LR: 0.0002919, Tokens/sec: 244959.19042701475\n",
      "Step: 726, Training Loss: 4.86844, LR: 0.0002906, Tokens/sec: 245095.77375408268\n",
      "Step: 727, Training Loss: 4.79688, LR: 0.0002893, Tokens/sec: 244600.51218900134\n",
      "Step: 728, Training Loss: 4.79555, LR: 0.0002880, Tokens/sec: 245187.1201275333\n",
      "Step: 729, Training Loss: 4.77499, LR: 0.0002868, Tokens/sec: 244944.88769650468\n",
      "Step: 730, Training Loss: 4.76647, LR: 0.0002855, Tokens/sec: 244969.00565519842\n",
      "Step: 731, Training Loss: 4.71778, LR: 0.0002842, Tokens/sec: 245519.2090465168\n",
      "Step: 732, Training Loss: 4.74668, LR: 0.0002830, Tokens/sec: 244389.8035380843\n",
      "Step: 733, Training Loss: 4.94199, LR: 0.0002817, Tokens/sec: 244667.36850748843\n",
      "Step: 734, Training Loss: 4.79840, LR: 0.0002804, Tokens/sec: 244515.17407450484\n",
      "Step: 735, Training Loss: 4.66738, LR: 0.0002792, Tokens/sec: 245359.47522771385\n",
      "Step: 736, Training Loss: 4.76500, LR: 0.0002779, Tokens/sec: 244726.54503617057\n",
      "Step: 737, Training Loss: 4.75714, LR: 0.0002767, Tokens/sec: 245047.00455131134\n",
      "Step: 738, Training Loss: 4.88497, LR: 0.0002754, Tokens/sec: 244996.36623784123\n",
      "Step: 739, Training Loss: 4.91843, LR: 0.0002742, Tokens/sec: 245504.9470554057\n",
      "Step: 740, Training Loss: 4.79085, LR: 0.0002730, Tokens/sec: 244436.1853462793\n",
      "Step: 741, Training Loss: 4.88794, LR: 0.0002717, Tokens/sec: 245642.7212077105\n",
      "Step: 742, Training Loss: 4.76146, LR: 0.0002705, Tokens/sec: 244810.8895109461\n",
      "Step: 743, Training Loss: 4.82208, LR: 0.0002693, Tokens/sec: 245079.07362198792\n",
      "Step: 744, Training Loss: 4.84456, LR: 0.0002680, Tokens/sec: 244622.81567833785\n",
      "Step: 745, Training Loss: 4.85137, LR: 0.0002668, Tokens/sec: 244872.42085412648\n",
      "Step: 746, Training Loss: 4.81289, LR: 0.0002656, Tokens/sec: 244857.8982092694\n",
      "Step: 747, Training Loss: 4.43261, LR: 0.0002644, Tokens/sec: 244396.2560036489\n",
      "Step: 748, Training Loss: 4.75293, LR: 0.0002632, Tokens/sec: 244678.9384645306\n",
      "Step: 749, Training Loss: 4.81136, LR: 0.0002620, Tokens/sec: 245605.86504869696\n",
      "Step: 750, Training Loss: 4.79195, LR: 0.0002607, Tokens/sec: 244753.99449913146\n",
      "Step: 751, Training Loss: 4.66383, LR: 0.0002595, Tokens/sec: 245137.22149270255\n",
      "Step: 752, Training Loss: 4.86651, LR: 0.0002583, Tokens/sec: 245252.01360189082\n",
      "Step: 753, Training Loss: 4.85437, LR: 0.0002572, Tokens/sec: 245097.11915104228\n",
      "Step: 754, Training Loss: 4.91328, LR: 0.0002560, Tokens/sec: 244493.09408354582\n",
      "Step: 755, Training Loss: 4.68742, LR: 0.0002548, Tokens/sec: 245192.5382814649\n",
      "Step: 756, Training Loss: 4.81346, LR: 0.0002536, Tokens/sec: 244838.9405872625\n",
      "Step: 757, Training Loss: 4.55526, LR: 0.0002524, Tokens/sec: 243932.0217566149\n",
      "Step: 758, Training Loss: 4.73774, LR: 0.0002512, Tokens/sec: 245476.79626469532\n",
      "Step: 759, Training Loss: 4.64333, LR: 0.0002501, Tokens/sec: 244337.70569981812\n",
      "Step: 760, Training Loss: 4.65421, LR: 0.0002489, Tokens/sec: 244994.5169884465\n",
      "Step: 761, Training Loss: 4.73086, LR: 0.0002477, Tokens/sec: 244899.83486789814\n",
      "Step: 762, Training Loss: 4.81685, LR: 0.0002466, Tokens/sec: 245494.33172764315\n",
      "Step: 763, Training Loss: 4.76465, LR: 0.0002454, Tokens/sec: 245043.63222217534\n",
      "Step: 764, Training Loss: 4.79477, LR: 0.0002443, Tokens/sec: 244291.9057216796\n",
      "Step: 765, Training Loss: 4.74530, LR: 0.0002431, Tokens/sec: 244171.21362616413\n",
      "Step: 766, Training Loss: 4.83076, LR: 0.0002420, Tokens/sec: 245308.078308477\n",
      "Step: 767, Training Loss: 4.81604, LR: 0.0002408, Tokens/sec: 244247.5539462612\n",
      "Step: 768, Training Loss: 4.66538, LR: 0.0002397, Tokens/sec: 245208.07130818392\n",
      "Step: 769, Training Loss: 4.80398, LR: 0.0002385, Tokens/sec: 244814.65233619898\n",
      "Step: 770, Training Loss: 4.76910, LR: 0.0002374, Tokens/sec: 245032.22569964878\n",
      "Step: 771, Training Loss: 4.97022, LR: 0.0002363, Tokens/sec: 244048.88359555282\n",
      "Step: 772, Training Loss: 4.75254, LR: 0.0002352, Tokens/sec: 244845.66725930094\n",
      "Step: 773, Training Loss: 4.80975, LR: 0.0002340, Tokens/sec: 244611.9703856069\n",
      "Step: 774, Training Loss: 5.34457, LR: 0.0002329, Tokens/sec: 244442.32608362884\n",
      "Step: 775, Training Loss: 4.95509, LR: 0.0002318, Tokens/sec: 245681.62988061446\n",
      "Step: 776, Training Loss: 4.84578, LR: 0.0002307, Tokens/sec: 245119.46548604083\n",
      "Step: 777, Training Loss: 4.79910, LR: 0.0002296, Tokens/sec: 244957.20018908512\n",
      "Step: 778, Training Loss: 4.78124, LR: 0.0002285, Tokens/sec: 245213.8219321693\n",
      "Step: 779, Training Loss: 4.90523, LR: 0.0002274, Tokens/sec: 245125.8284567065\n",
      "Step: 780, Training Loss: 4.82846, LR: 0.0002263, Tokens/sec: 245109.04055971824\n",
      "Step: 781, Training Loss: 4.85966, LR: 0.0002252, Tokens/sec: 245532.8061642486\n",
      "Step: 782, Training Loss: 4.79581, LR: 0.0002241, Tokens/sec: 244534.18526133464\n",
      "Step: 783, Training Loss: 4.80241, LR: 0.0002230, Tokens/sec: 245261.953180822\n",
      "Step: 784, Training Loss: 4.85139, LR: 0.0002220, Tokens/sec: 245130.0159702913\n",
      "Step: 785, Training Loss: 4.85030, LR: 0.0002209, Tokens/sec: 245506.50080391992\n",
      "Step: 786, Training Loss: 4.72194, LR: 0.0002198, Tokens/sec: 244915.87637801399\n",
      "Step: 787, Training Loss: 4.79845, LR: 0.0002188, Tokens/sec: 245709.11385573706\n",
      "Step: 788, Training Loss: 4.82521, LR: 0.0002177, Tokens/sec: 244332.61709647047\n",
      "Step: 789, Training Loss: 4.76197, LR: 0.0002166, Tokens/sec: 243171.40488804327\n",
      "Step: 790, Training Loss: 4.83671, LR: 0.0002156, Tokens/sec: 245137.24881950044\n",
      "Step: 791, Training Loss: 4.79314, LR: 0.0002145, Tokens/sec: 244710.8414828007\n",
      "Step: 792, Training Loss: 4.73476, LR: 0.0002135, Tokens/sec: 243886.23341050654\n",
      "Step: 793, Training Loss: 4.83405, LR: 0.0002125, Tokens/sec: 244743.6261725888\n",
      "Step: 794, Training Loss: 4.86951, LR: 0.0002114, Tokens/sec: 244085.6650653686\n",
      "Step: 795, Training Loss: 4.76949, LR: 0.0002104, Tokens/sec: 244780.87111175328\n",
      "Step: 796, Training Loss: 4.79434, LR: 0.0002094, Tokens/sec: 245155.50448194554\n",
      "Step: 797, Training Loss: 4.85842, LR: 0.0002083, Tokens/sec: 244394.78757519874\n",
      "Step: 798, Training Loss: 4.71412, LR: 0.0002073, Tokens/sec: 244861.97432808302\n",
      "Step: 799, Training Loss: 4.82908, LR: 0.0002063, Tokens/sec: 245266.73862144875\n",
      "Step: 800, Training Loss: 4.85656, LR: 0.0002053, Tokens/sec: 243808.7925154375\n",
      "Computing Eval loss, steps: 78\n",
      "Step: 800, Eval Loss: 4.75032\n",
      "Step: 801, Training Loss: 4.82111, LR: 0.0002043, Tokens/sec: 244862.85364241162\n",
      "Step: 802, Training Loss: 4.67241, LR: 0.0002033, Tokens/sec: 244889.4371369498\n",
      "Step: 803, Training Loss: 4.79185, LR: 0.0002023, Tokens/sec: 243975.65842121473\n",
      "Step: 804, Training Loss: 4.86929, LR: 0.0002013, Tokens/sec: 245011.1066406806\n",
      "Step: 805, Training Loss: 4.68526, LR: 0.0002003, Tokens/sec: 244929.53633194763\n",
      "Step: 806, Training Loss: 4.76708, LR: 0.0001993, Tokens/sec: 244360.96258526942\n",
      "Step: 807, Training Loss: 4.78367, LR: 0.0001983, Tokens/sec: 245080.26348658235\n",
      "Step: 808, Training Loss: 4.79359, LR: 0.0001973, Tokens/sec: 245052.85856166485\n",
      "Step: 809, Training Loss: 4.78689, LR: 0.0001964, Tokens/sec: 245063.91879649422\n",
      "Step: 810, Training Loss: 4.72237, LR: 0.0001954, Tokens/sec: 244820.12218117755\n",
      "Step: 811, Training Loss: 4.82935, LR: 0.0001944, Tokens/sec: 244860.11688252914\n",
      "Step: 812, Training Loss: 4.79526, LR: 0.0001935, Tokens/sec: 245103.06434338115\n",
      "Step: 813, Training Loss: 4.72034, LR: 0.0001925, Tokens/sec: 244171.28987825703\n",
      "Step: 814, Training Loss: 4.62739, LR: 0.0001916, Tokens/sec: 245158.6287819764\n",
      "Step: 815, Training Loss: 4.66580, LR: 0.0001906, Tokens/sec: 245096.38669325155\n",
      "Step: 816, Training Loss: 4.71323, LR: 0.0001897, Tokens/sec: 244700.39512347538\n",
      "Step: 817, Training Loss: 4.74240, LR: 0.0001887, Tokens/sec: 244996.0625768921\n",
      "Step: 818, Training Loss: 4.79132, LR: 0.0001878, Tokens/sec: 245192.82021655864\n",
      "Step: 819, Training Loss: 4.73978, LR: 0.0001869, Tokens/sec: 245089.3867347441\n",
      "Step: 820, Training Loss: 4.70379, LR: 0.0001859, Tokens/sec: 244557.76174317684\n",
      "Step: 821, Training Loss: 4.90901, LR: 0.0001850, Tokens/sec: 245361.22048365162\n",
      "Step: 822, Training Loss: 4.78883, LR: 0.0001841, Tokens/sec: 244677.0803917949\n",
      "Step: 823, Training Loss: 4.92424, LR: 0.0001832, Tokens/sec: 244882.88020224284\n",
      "Step: 824, Training Loss: 4.72463, LR: 0.0001823, Tokens/sec: 245289.55195958444\n",
      "Step: 825, Training Loss: 4.85387, LR: 0.0001814, Tokens/sec: 245063.18995091072\n",
      "Step: 826, Training Loss: 4.67774, LR: 0.0001805, Tokens/sec: 245102.50942379367\n",
      "Step: 827, Training Loss: 4.80848, LR: 0.0001796, Tokens/sec: 50622.84809141464\n",
      "Step: 828, Training Loss: 4.75469, LR: 0.0001787, Tokens/sec: 245923.57941865074\n",
      "Step: 829, Training Loss: 4.87059, LR: 0.0001778, Tokens/sec: 244893.36771635863\n",
      "Step: 830, Training Loss: 4.78366, LR: 0.0001769, Tokens/sec: 245973.29660946483\n",
      "Step: 831, Training Loss: 4.63810, LR: 0.0001761, Tokens/sec: 244499.06607703617\n",
      "Step: 832, Training Loss: 4.81400, LR: 0.0001752, Tokens/sec: 245388.03039244798\n",
      "Step: 833, Training Loss: 4.69079, LR: 0.0001743, Tokens/sec: 245608.92368557563\n",
      "Step: 834, Training Loss: 4.80091, LR: 0.0001735, Tokens/sec: 245073.22177315748\n",
      "Step: 835, Training Loss: 4.66879, LR: 0.0001726, Tokens/sec: 245252.36234540836\n",
      "Step: 836, Training Loss: 4.58928, LR: 0.0001717, Tokens/sec: 245289.0731470856\n",
      "Step: 837, Training Loss: 4.78279, LR: 0.0001709, Tokens/sec: 245238.49029837758\n",
      "Step: 838, Training Loss: 4.73925, LR: 0.0001701, Tokens/sec: 244998.17286545387\n",
      "Step: 839, Training Loss: 4.70044, LR: 0.0001692, Tokens/sec: 245270.08631829137\n",
      "Step: 840, Training Loss: 4.55840, LR: 0.0001684, Tokens/sec: 244948.2487818315\n",
      "Step: 841, Training Loss: 4.66741, LR: 0.0001675, Tokens/sec: 244885.14193859466\n",
      "Step: 842, Training Loss: 4.80626, LR: 0.0001667, Tokens/sec: 244881.3309284307\n",
      "Step: 843, Training Loss: 4.77511, LR: 0.0001659, Tokens/sec: 243966.48256627115\n",
      "Step: 844, Training Loss: 4.70350, LR: 0.0001651, Tokens/sec: 244898.38595080483\n",
      "Step: 845, Training Loss: 4.80930, LR: 0.0001643, Tokens/sec: 245587.36226293573\n",
      "Step: 846, Training Loss: 4.75593, LR: 0.0001635, Tokens/sec: 245275.21746513963\n",
      "Step: 847, Training Loss: 4.70007, LR: 0.0001627, Tokens/sec: 244312.51248418252\n",
      "Step: 848, Training Loss: 4.68535, LR: 0.0001619, Tokens/sec: 245401.40930251218\n",
      "Step: 849, Training Loss: 4.75647, LR: 0.0001611, Tokens/sec: 244886.31628540118\n",
      "Step: 850, Training Loss: 4.65629, LR: 0.0001603, Tokens/sec: 242978.31308090274\n",
      "Step: 851, Training Loss: 4.78049, LR: 0.0001595, Tokens/sec: 245327.248919549\n",
      "Step: 852, Training Loss: 4.68611, LR: 0.0001587, Tokens/sec: 244365.26996462903\n",
      "Step: 853, Training Loss: 4.56677, LR: 0.0001580, Tokens/sec: 245335.77984688012\n",
      "Step: 854, Training Loss: 4.64458, LR: 0.0001572, Tokens/sec: 244361.88412765917\n",
      "Step: 855, Training Loss: 4.65451, LR: 0.0001564, Tokens/sec: 244699.07790395146\n",
      "Step: 856, Training Loss: 4.63609, LR: 0.0001557, Tokens/sec: 244815.01516792638\n",
      "Step: 857, Training Loss: 4.81488, LR: 0.0001549, Tokens/sec: 243920.7184855584\n",
      "Step: 858, Training Loss: 4.61517, LR: 0.0001542, Tokens/sec: 245194.4417831106\n",
      "Step: 859, Training Loss: 4.63757, LR: 0.0001534, Tokens/sec: 245500.63537781226\n",
      "Step: 860, Training Loss: 4.65359, LR: 0.0001527, Tokens/sec: 244112.55768372712\n",
      "Step: 861, Training Loss: 4.71558, LR: 0.0001519, Tokens/sec: 245280.42577309243\n",
      "Step: 862, Training Loss: 4.81382, LR: 0.0001512, Tokens/sec: 245397.26556958706\n",
      "Step: 863, Training Loss: 4.64991, LR: 0.0001505, Tokens/sec: 243946.24872670925\n",
      "Step: 864, Training Loss: 4.69953, LR: 0.0001498, Tokens/sec: 245492.82438099766\n",
      "Step: 865, Training Loss: 4.77611, LR: 0.0001490, Tokens/sec: 245137.74582670012\n",
      "Step: 866, Training Loss: 4.73958, LR: 0.0001483, Tokens/sec: 244773.79380321733\n",
      "Step: 867, Training Loss: 4.75342, LR: 0.0001476, Tokens/sec: 244156.4944752093\n",
      "Step: 868, Training Loss: 4.95900, LR: 0.0001469, Tokens/sec: 245631.09593700137\n",
      "Step: 869, Training Loss: 4.72638, LR: 0.0001462, Tokens/sec: 244763.22287836307\n",
      "Step: 870, Training Loss: 4.71978, LR: 0.0001455, Tokens/sec: 245165.3234906833\n",
      "Step: 871, Training Loss: 4.57458, LR: 0.0001449, Tokens/sec: 245550.23995458093\n",
      "Step: 872, Training Loss: 4.76863, LR: 0.0001442, Tokens/sec: 245419.47494286267\n",
      "Step: 873, Training Loss: 4.64136, LR: 0.0001435, Tokens/sec: 245201.54514667514\n",
      "Step: 874, Training Loss: 4.90048, LR: 0.0001428, Tokens/sec: 244766.77309520417\n",
      "Step: 875, Training Loss: 4.72675, LR: 0.0001422, Tokens/sec: 245506.63271070828\n",
      "Step: 876, Training Loss: 4.60737, LR: 0.0001415, Tokens/sec: 244791.74501190783\n",
      "Step: 877, Training Loss: 4.73438, LR: 0.0001408, Tokens/sec: 244478.07789031992\n",
      "Step: 878, Training Loss: 4.73759, LR: 0.0001402, Tokens/sec: 245048.77437858848\n",
      "Step: 879, Training Loss: 4.59647, LR: 0.0001395, Tokens/sec: 244962.04708039435\n",
      "Step: 880, Training Loss: 4.75698, LR: 0.0001389, Tokens/sec: 244405.25199848186\n",
      "Step: 881, Training Loss: 4.69335, LR: 0.0001383, Tokens/sec: 243320.254607998\n",
      "Step: 882, Training Loss: 4.71681, LR: 0.0001376, Tokens/sec: 245329.88665244586\n",
      "Step: 883, Training Loss: 4.70409, LR: 0.0001370, Tokens/sec: 244914.58240996185\n",
      "Step: 884, Training Loss: 4.39310, LR: 0.0001364, Tokens/sec: 244562.0607641184\n",
      "Step: 885, Training Loss: 4.62286, LR: 0.0001358, Tokens/sec: 245526.61908335186\n",
      "Step: 886, Training Loss: 4.72142, LR: 0.0001352, Tokens/sec: 245368.95295054754\n",
      "Step: 887, Training Loss: 4.72105, LR: 0.0001346, Tokens/sec: 244267.78692665193\n",
      "Step: 888, Training Loss: 4.79320, LR: 0.0001340, Tokens/sec: 245513.85182563966\n",
      "Step: 889, Training Loss: 4.76125, LR: 0.0001334, Tokens/sec: 244739.61357656945\n",
      "Step: 890, Training Loss: 4.55157, LR: 0.0001328, Tokens/sec: 244434.54832126532\n",
      "Step: 891, Training Loss: 4.79475, LR: 0.0001322, Tokens/sec: 245658.13637746236\n",
      "Step: 892, Training Loss: 4.60705, LR: 0.0001316, Tokens/sec: 245480.24732258494\n",
      "Step: 893, Training Loss: 4.66356, LR: 0.0001310, Tokens/sec: 245115.63181053675\n",
      "Step: 894, Training Loss: 4.70582, LR: 0.0001305, Tokens/sec: 245330.36904553097\n",
      "Step: 895, Training Loss: 4.80381, LR: 0.0001299, Tokens/sec: 244650.04114054554\n",
      "Step: 896, Training Loss: 4.71407, LR: 0.0001293, Tokens/sec: 244949.69828900366\n",
      "Step: 897, Training Loss: 4.68177, LR: 0.0001288, Tokens/sec: 245307.6421811456\n",
      "Step: 898, Training Loss: 4.72311, LR: 0.0001282, Tokens/sec: 245839.07126211966\n",
      "Step: 899, Training Loss: 4.89949, LR: 0.0001277, Tokens/sec: 245053.00363544378\n",
      "Step: 900, Training Loss: 4.88247, LR: 0.0001271, Tokens/sec: 244045.25599414908\n",
      "Computing Eval loss, steps: 78\n",
      "Step: 900, Eval Loss: 4.69107\n",
      "Step: 901, Training Loss: 4.72999, LR: 0.0001266, Tokens/sec: 245161.93082148928\n",
      "Step: 902, Training Loss: 4.83282, LR: 0.0001261, Tokens/sec: 245441.71744345687\n",
      "Step: 903, Training Loss: 4.91867, LR: 0.0001255, Tokens/sec: 245251.9606066392\n",
      "Step: 904, Training Loss: 4.75427, LR: 0.0001250, Tokens/sec: 244429.03286310952\n",
      "Step: 905, Training Loss: 4.78942, LR: 0.0001245, Tokens/sec: 244864.00391778263\n",
      "Step: 906, Training Loss: 4.70029, LR: 0.0001240, Tokens/sec: 244447.7504318243\n",
      "Step: 907, Training Loss: 4.67613, LR: 0.0001235, Tokens/sec: 244469.29565677763\n",
      "Step: 908, Training Loss: 4.56963, LR: 0.0001230, Tokens/sec: 244992.717238397\n",
      "Step: 909, Training Loss: 4.67818, LR: 0.0001225, Tokens/sec: 244729.7401202837\n",
      "Step: 910, Training Loss: 4.75429, LR: 0.0001220, Tokens/sec: 244931.28741225632\n",
      "Step: 911, Training Loss: 4.69754, LR: 0.0001215, Tokens/sec: 244842.5918226\n",
      "Step: 912, Training Loss: 4.61996, LR: 0.0001211, Tokens/sec: 245163.37602558735\n",
      "Step: 913, Training Loss: 4.87736, LR: 0.0001206, Tokens/sec: 245180.0534786507\n",
      "Step: 914, Training Loss: 4.61919, LR: 0.0001201, Tokens/sec: 245272.6253627909\n",
      "Step: 915, Training Loss: 4.71599, LR: 0.0001197, Tokens/sec: 244758.58476462096\n",
      "Step: 916, Training Loss: 4.70600, LR: 0.0001192, Tokens/sec: 245188.84755458208\n",
      "Step: 917, Training Loss: 4.79013, LR: 0.0001188, Tokens/sec: 244591.41169713938\n",
      "Step: 918, Training Loss: 4.67891, LR: 0.0001183, Tokens/sec: 245151.17261540418\n",
      "Step: 919, Training Loss: 4.75382, LR: 0.0001179, Tokens/sec: 244537.11359011062\n",
      "Step: 920, Training Loss: 4.68392, LR: 0.0001174, Tokens/sec: 244765.22529016345\n",
      "Step: 921, Training Loss: 4.71610, LR: 0.0001170, Tokens/sec: 244943.79293368023\n",
      "Step: 922, Training Loss: 4.47180, LR: 0.0001166, Tokens/sec: 245082.6791008868\n",
      "Step: 923, Training Loss: 4.78176, LR: 0.0001162, Tokens/sec: 244697.33354591584\n",
      "Step: 924, Training Loss: 4.68911, LR: 0.0001157, Tokens/sec: 244825.70300366383\n",
      "Step: 925, Training Loss: 4.33576, LR: 0.0001153, Tokens/sec: 245310.69681537457\n",
      "Step: 926, Training Loss: 4.49685, LR: 0.0001149, Tokens/sec: 244868.5625227548\n",
      "Step: 927, Training Loss: 4.69467, LR: 0.0001145, Tokens/sec: 244770.95005292323\n",
      "Step: 928, Training Loss: 4.68721, LR: 0.0001141, Tokens/sec: 245131.29855303207\n",
      "Step: 929, Training Loss: 4.69629, LR: 0.0001137, Tokens/sec: 244922.54760549532\n",
      "Step: 930, Training Loss: 4.70498, LR: 0.0001134, Tokens/sec: 245066.9485874844\n",
      "Step: 931, Training Loss: 4.43423, LR: 0.0001130, Tokens/sec: 245312.56623431644\n",
      "Step: 932, Training Loss: 4.68986, LR: 0.0001126, Tokens/sec: 245166.67990131865\n",
      "Step: 933, Training Loss: 4.70029, LR: 0.0001123, Tokens/sec: 244984.80547550847\n",
      "Step: 934, Training Loss: 4.60704, LR: 0.0001119, Tokens/sec: 245350.75274223497\n",
      "Step: 935, Training Loss: 4.80763, LR: 0.0001115, Tokens/sec: 244392.6367413326\n",
      "Step: 936, Training Loss: 4.76060, LR: 0.0001112, Tokens/sec: 244728.50769649592\n",
      "Step: 937, Training Loss: 4.72769, LR: 0.0001108, Tokens/sec: 245501.86188649788\n",
      "Step: 938, Training Loss: 4.73964, LR: 0.0001105, Tokens/sec: 244518.7340923652\n",
      "Step: 939, Training Loss: 4.69489, LR: 0.0001102, Tokens/sec: 244559.4259172211\n",
      "Step: 940, Training Loss: 4.66869, LR: 0.0001098, Tokens/sec: 244946.12400585238\n",
      "Step: 941, Training Loss: 4.64944, LR: 0.0001095, Tokens/sec: 245077.65843222616\n",
      "Step: 942, Training Loss: 5.24042, LR: 0.0001092, Tokens/sec: 245963.8615548212\n",
      "Step: 943, Training Loss: 4.80654, LR: 0.0001089, Tokens/sec: 244553.22830119578\n",
      "Step: 944, Training Loss: 4.68030, LR: 0.0001086, Tokens/sec: 245232.58297513938\n",
      "Step: 945, Training Loss: 4.73668, LR: 0.0001083, Tokens/sec: 244451.03674609654\n",
      "Step: 946, Training Loss: 4.72368, LR: 0.0001080, Tokens/sec: 244950.00183503094\n",
      "Step: 947, Training Loss: 4.71813, LR: 0.0001077, Tokens/sec: 245004.25656852397\n",
      "Step: 948, Training Loss: 4.88476, LR: 0.0001074, Tokens/sec: 244852.79984365887\n",
      "Step: 949, Training Loss: 4.85581, LR: 0.0001071, Tokens/sec: 244939.02861254968\n",
      "Step: 950, Training Loss: 4.60631, LR: 0.0001068, Tokens/sec: 244843.62775054385\n",
      "Step: 951, Training Loss: 4.83680, LR: 0.0001066, Tokens/sec: 244487.97522147803\n",
      "Step: 952, Training Loss: 4.50762, LR: 0.0001063, Tokens/sec: 244936.14862426862\n",
      "Step: 953, Training Loss: 4.68257, LR: 0.0001060, Tokens/sec: 244967.15511326244\n",
      "Step: 954, Training Loss: 4.71886, LR: 0.0001058, Tokens/sec: 245574.1464698329\n",
      "Step: 955, Training Loss: 4.67138, LR: 0.0001055, Tokens/sec: 245176.79878826634\n",
      "Step: 956, Training Loss: 4.84262, LR: 0.0001053, Tokens/sec: 245104.85205526158\n",
      "Step: 957, Training Loss: 4.70725, LR: 0.0001051, Tokens/sec: 244688.08801267346\n",
      "Step: 958, Training Loss: 4.61549, LR: 0.0001048, Tokens/sec: 244965.93053037688\n",
      "Step: 959, Training Loss: 4.68237, LR: 0.0001046, Tokens/sec: 245137.9183279519\n",
      "Step: 960, Training Loss: 4.80788, LR: 0.0001044, Tokens/sec: 244525.03702260612\n",
      "Step: 961, Training Loss: 4.66123, LR: 0.0001042, Tokens/sec: 245313.15973375438\n",
      "Step: 962, Training Loss: 4.73619, LR: 0.0001040, Tokens/sec: 245643.7124677199\n",
      "Step: 963, Training Loss: 4.73831, LR: 0.0001037, Tokens/sec: 244012.13520900652\n",
      "Step: 964, Training Loss: 4.77980, LR: 0.0001035, Tokens/sec: 244710.53172128962\n",
      "Step: 965, Training Loss: 4.59466, LR: 0.0001034, Tokens/sec: 244513.31508967138\n",
      "Step: 966, Training Loss: 4.65513, LR: 0.0001032, Tokens/sec: 244553.01922637376\n",
      "Step: 967, Training Loss: 4.66186, LR: 0.0001030, Tokens/sec: 243936.55925597585\n",
      "Step: 968, Training Loss: 4.73060, LR: 0.0001028, Tokens/sec: 245271.85424001428\n",
      "Step: 969, Training Loss: 4.64951, LR: 0.0001026, Tokens/sec: 244569.49473822472\n",
      "Step: 970, Training Loss: 4.76888, LR: 0.0001025, Tokens/sec: 245117.18917338355\n",
      "Step: 971, Training Loss: 4.60645, LR: 0.0001023, Tokens/sec: 245387.9995868676\n",
      "Step: 972, Training Loss: 4.76987, LR: 0.0001021, Tokens/sec: 245028.44594477126\n",
      "Step: 973, Training Loss: 4.56865, LR: 0.0001020, Tokens/sec: 244856.14306612755\n",
      "Step: 974, Training Loss: 4.42554, LR: 0.0001019, Tokens/sec: 245627.79839496245\n",
      "Step: 975, Training Loss: 4.65428, LR: 0.0001017, Tokens/sec: 244402.376060218\n",
      "Step: 976, Training Loss: 4.70348, LR: 0.0001016, Tokens/sec: 244508.29053445964\n",
      "Step: 977, Training Loss: 4.64017, LR: 0.0001014, Tokens/sec: 245183.42443805374\n",
      "Step: 978, Training Loss: 4.63402, LR: 0.0001013, Tokens/sec: 245342.32513137363\n",
      "Step: 979, Training Loss: 4.69889, LR: 0.0001012, Tokens/sec: 244095.98449168826\n",
      "Step: 980, Training Loss: 4.68706, LR: 0.0001011, Tokens/sec: 245303.21258356477\n",
      "Step: 981, Training Loss: 4.64113, LR: 0.0001010, Tokens/sec: 245101.6283873949\n",
      "Step: 982, Training Loss: 4.64983, LR: 0.0001009, Tokens/sec: 245096.75548264093\n",
      "Step: 983, Training Loss: 4.76753, LR: 0.0001008, Tokens/sec: 244943.87137430478\n",
      "Step: 984, Training Loss: 4.68257, LR: 0.0001007, Tokens/sec: 245286.7081827691\n",
      "Step: 985, Training Loss: 4.67091, LR: 0.0001006, Tokens/sec: 244807.5304907846\n",
      "Step: 986, Training Loss: 4.68268, LR: 0.0001005, Tokens/sec: 244480.59886462532\n",
      "Step: 987, Training Loss: 4.88776, LR: 0.0001005, Tokens/sec: 245348.05125865169\n",
      "Step: 988, Training Loss: 4.59425, LR: 0.0001004, Tokens/sec: 244318.39422539194\n",
      "Step: 989, Training Loss: 4.78710, LR: 0.0001003, Tokens/sec: 244552.99202967522\n",
      "Step: 990, Training Loss: 4.80467, LR: 0.0001003, Tokens/sec: 245131.08678078896\n",
      "Step: 991, Training Loss: 4.62811, LR: 0.0001002, Tokens/sec: 244801.20617611104\n",
      "Step: 992, Training Loss: 4.66692, LR: 0.0001002, Tokens/sec: 245182.43176374497\n",
      "Step: 993, Training Loss: 4.60700, LR: 0.0001001, Tokens/sec: 245230.4293343344\n",
      "Step: 994, Training Loss: 4.58376, LR: 0.0001001, Tokens/sec: 244897.0921675605\n",
      "Step: 995, Training Loss: 4.49192, LR: 0.0001001, Tokens/sec: 245049.99808317822\n",
      "Step: 996, Training Loss: 4.68426, LR: 0.0001000, Tokens/sec: 244780.04347465772\n",
      "Step: 997, Training Loss: 4.35374, LR: 0.0001000, Tokens/sec: 245245.7876712948\n",
      "Step: 998, Training Loss: 4.70975, LR: 0.0001000, Tokens/sec: 243557.9947397318\n",
      "Step: 999, Training Loss: 4.57086, LR: 0.0001000, Tokens/sec: 244903.7487343624\n"
     ]
    }
   ],
   "source": [
    "trainer.train(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b5596eda083de0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:22:17.417599499Z",
     "start_time": "2024-12-16T06:27:52.251011Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The world is a very important part of the world.\n",
      "The first time the world is the worlds largest and most important. The world is the worlds largest and most important. The world is the worlds largest and most important.\n",
      "The world is the worlds largest and most important. The world is the worlds largest and most important and most important. The world is the worlds largest and most important.\n",
      "The world is the worlds largest and most important and most important.\n",
      "The world is the worlds most important and most important.\n",
      "The world is the worlds most important and most important.\n",
      "The world is the worlds most important and most important.\n",
      "The world is the worlds largest nation.\n",
      "The world is the worlds largest and most important.\n",
      "The world is the worlds largest.\n",
      "The world is the worlds largest.\n",
      "The world is the worlds largest.\n",
      "The world is the worlds largest.\n",
      "The world is the worlds largest.\n",
      "The world is the worlds largest.\n",
      "The world is the worlds largest.\n",
      "The world is the worlds largest.\n",
      "The world is the world\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer([\"The world is\"], return_tensors=\"pt\")['input_ids'].to(trainer.device)\n",
    "idx = model.generate(input_ids, temperature=0.25, top_k=50, max_new_tokens=256)\n",
    "print(tokenizer.batch_decode(idx)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3bcc343073d67a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
