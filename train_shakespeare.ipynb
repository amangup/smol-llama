{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T06:24:07.687622Z",
     "start_time": "2025-02-24T06:24:05.902425Z"
    }
   },
   "source": [
    "from model import ModelConfig, LlamaModel\n",
    "from train import TrainerConfig, SimpleDataLoader, Trainer\n",
    "\n",
    "from transformers import AutoTokenizer"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "2f28fa23c987e72b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T06:24:07.691769Z",
     "start_time": "2025-02-24T06:24:07.690164Z"
    }
   },
   "source": [
    "tokenizer_id = \"HuggingFaceTB/SmolLM2-135M\""
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "9bb4e51aa142abee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T06:24:07.881169Z",
     "start_time": "2025-02-24T06:24:07.733902Z"
    }
   },
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "cde027092af8291e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T06:24:07.888505Z",
     "start_time": "2025-02-24T06:24:07.886658Z"
    }
   },
   "source": [
    "model_config = ModelConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=960,\n",
    "    d_head=64,\n",
    "    d_mlp_proj=2560,\n",
    "    n_layers=32,\n",
    "    n_kv_heads=5,\n",
    "    n_attn_heads=15,\n",
    "    rms_norm_eps=1e-5,\n",
    "    initializer_range=0.02,\n",
    "    rope_theta=100000.0,\n",
    "    padding_idx=tokenizer.pad_token_id\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T06:24:07.932266Z",
     "start_time": "2025-02-24T06:24:07.930444Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_config = TrainerConfig(\n",
    "    per_device_train_batch_size=8,\n",
    "    max_seq_len=512,\n",
    "    num_epochs=12,\n",
    "    eval_interval_steps=25,\n",
    "    learning_rate=1e-3,\n",
    "    grad_clip_norm=1.0,\n",
    "    val_size=0.05,\n",
    "    log_dir=\"runs/think_exp1_base_nothink_32layer_big_lr1e-3_12epochs\",\n",
    "    warmup_ratio=0.1\n",
    ")"
   ],
   "id": "809773e662327a12",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T06:24:08.009358Z",
     "start_time": "2025-02-24T06:24:07.978210Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"data/complete_shakespeare.txt\") as f:\n",
    "    text = f.read()"
   ],
   "id": "374f398bb34f7ac1",
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "9a912a0ec92039d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T06:24:14.206588Z",
     "start_time": "2025-02-24T06:24:08.021935Z"
    }
   },
   "source": [
    "model = LlamaModel(model_config)\n",
    "dataloader = SimpleDataLoader(train_config, tokenizer, text=text)\n",
    "trainer = Trainer(train_config, model)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens                   | 1,596,416\n",
      "Num Trainable Params           | 409,007,040\n",
      "Train device                   | cuda, NVIDIA GeForce RTX 3090, N=1\n",
      "Training precision             | torch.bfloat16\n",
      "Flash Attention                | True\n",
      "torch.compile()                | True\n",
      "DistributedDataParallel        | False\n",
      "Batch size                     | 4,096\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "ee8c2059258a0195",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T06:45:55.184669Z",
     "start_time": "2025-02-24T06:24:14.213685Z"
    }
   },
   "source": [
    "trainer.train(dataloader)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training steps                 | 4,452 \n",
      "Step: 0, Training Loss: 10.96608, LR: 0.0000500, Tokens/sec: 365.53\n",
      "Step: 1, Training Loss: 10.72938, LR: 0.0000521, Tokens/sec: 393.28\n",
      "Step: 2, Training Loss: 10.25065, LR: 0.0000543, Tokens/sec: 41905.32\n",
      "Step: 3, Training Loss: 10.04979, LR: 0.0000564, Tokens/sec: 39745.76\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 3, Eval Loss: 9.58651\n",
      "Step: 4, Training Loss: 9.70757, LR: 0.0000585, Tokens/sec: 36094.66\n",
      "Step: 5, Training Loss: 9.45795, LR: 0.0000607, Tokens/sec: 40488.38\n",
      "Step: 6, Training Loss: 9.29785, LR: 0.0000628, Tokens/sec: 41442.04\n",
      "Step: 7, Training Loss: 9.23244, LR: 0.0000649, Tokens/sec: 40687.52\n",
      "Step: 8, Training Loss: 9.21922, LR: 0.0000671, Tokens/sec: 37934.86\n",
      "Step: 9, Training Loss: 9.06639, LR: 0.0000692, Tokens/sec: 40421.01\n",
      "Step: 10, Training Loss: 8.92146, LR: 0.0000713, Tokens/sec: 40734.68\n",
      "Step: 11, Training Loss: 8.85211, LR: 0.0000735, Tokens/sec: 40871.76\n",
      "Step: 12, Training Loss: 8.72774, LR: 0.0000756, Tokens/sec: 38833.94\n",
      "Step: 13, Training Loss: 8.68555, LR: 0.0000778, Tokens/sec: 38759.11\n",
      "Step: 14, Training Loss: 8.74289, LR: 0.0000799, Tokens/sec: 31278.76\n",
      "Step: 15, Training Loss: 8.58986, LR: 0.0000820, Tokens/sec: 36220.67\n",
      "Step: 16, Training Loss: 8.54354, LR: 0.0000842, Tokens/sec: 40681.07\n",
      "Step: 17, Training Loss: 8.36365, LR: 0.0000863, Tokens/sec: 40286.31\n",
      "Step: 18, Training Loss: 8.30064, LR: 0.0000884, Tokens/sec: 40280.91\n",
      "Step: 19, Training Loss: 8.39965, LR: 0.0000906, Tokens/sec: 40608.08\n",
      "Step: 20, Training Loss: 8.13339, LR: 0.0000927, Tokens/sec: 39875.24\n",
      "Step: 21, Training Loss: 8.06235, LR: 0.0000948, Tokens/sec: 40961.57\n",
      "Step: 22, Training Loss: 8.03637, LR: 0.0000970, Tokens/sec: 40922.61\n",
      "Step: 23, Training Loss: 7.80057, LR: 0.0000991, Tokens/sec: 37369.70\n",
      "Step: 24, Training Loss: 7.79857, LR: 0.0001012, Tokens/sec: 37747.30\n",
      "Step: 25, Training Loss: 7.78814, LR: 0.0001034, Tokens/sec: 39566.87\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 25, Eval Loss: 7.65947\n",
      "Step: 26, Training Loss: 7.63267, LR: 0.0001055, Tokens/sec: 40082.38\n",
      "Step: 27, Training Loss: 7.50934, LR: 0.0001076, Tokens/sec: 40393.18\n",
      "Step: 28, Training Loss: 7.43245, LR: 0.0001098, Tokens/sec: 40010.08\n",
      "Step: 29, Training Loss: 7.49279, LR: 0.0001119, Tokens/sec: 39948.52\n",
      "Step: 30, Training Loss: 7.52645, LR: 0.0001140, Tokens/sec: 39295.11\n",
      "Step: 31, Training Loss: 7.35244, LR: 0.0001162, Tokens/sec: 40362.18\n",
      "Step: 32, Training Loss: 7.33504, LR: 0.0001183, Tokens/sec: 41289.25\n",
      "Step: 33, Training Loss: 7.24662, LR: 0.0001204, Tokens/sec: 40475.49\n",
      "Step: 34, Training Loss: 7.03681, LR: 0.0001226, Tokens/sec: 41624.38\n",
      "Step: 35, Training Loss: 7.02667, LR: 0.0001247, Tokens/sec: 41650.66\n",
      "Step: 36, Training Loss: 7.07456, LR: 0.0001269, Tokens/sec: 39982.81\n",
      "Step: 37, Training Loss: 7.09159, LR: 0.0001290, Tokens/sec: 41769.01\n",
      "Step: 38, Training Loss: 6.97869, LR: 0.0001311, Tokens/sec: 40117.72\n",
      "Step: 39, Training Loss: 6.91989, LR: 0.0001333, Tokens/sec: 41770.62\n",
      "Step: 40, Training Loss: 6.90107, LR: 0.0001354, Tokens/sec: 42205.43\n",
      "Step: 41, Training Loss: 6.80017, LR: 0.0001375, Tokens/sec: 40099.13\n",
      "Step: 42, Training Loss: 6.76992, LR: 0.0001397, Tokens/sec: 41505.57\n",
      "Step: 43, Training Loss: 6.83448, LR: 0.0001418, Tokens/sec: 38594.16\n",
      "Step: 44, Training Loss: 6.94768, LR: 0.0001439, Tokens/sec: 38489.16\n",
      "Step: 45, Training Loss: 6.85485, LR: 0.0001461, Tokens/sec: 41128.97\n",
      "Step: 46, Training Loss: 6.76133, LR: 0.0001482, Tokens/sec: 40559.46\n",
      "Step: 47, Training Loss: 6.62288, LR: 0.0001503, Tokens/sec: 38733.03\n",
      "Step: 48, Training Loss: 6.71480, LR: 0.0001525, Tokens/sec: 36216.58\n",
      "Step: 49, Training Loss: 6.88186, LR: 0.0001546, Tokens/sec: 38734.55\n",
      "Step: 50, Training Loss: 6.80023, LR: 0.0001567, Tokens/sec: 38687.25\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 50, Eval Loss: 6.71246\n",
      "Step: 51, Training Loss: 6.56744, LR: 0.0001589, Tokens/sec: 38136.06\n",
      "Step: 52, Training Loss: 6.66346, LR: 0.0001610, Tokens/sec: 29438.82\n",
      "Step: 53, Training Loss: 6.73210, LR: 0.0001631, Tokens/sec: 40215.47\n",
      "Step: 54, Training Loss: 6.73604, LR: 0.0001653, Tokens/sec: 41708.70\n",
      "Step: 55, Training Loss: 6.76687, LR: 0.0001674, Tokens/sec: 41652.89\n",
      "Step: 56, Training Loss: 6.54634, LR: 0.0001696, Tokens/sec: 38985.00\n",
      "Step: 57, Training Loss: 6.53237, LR: 0.0001717, Tokens/sec: 41566.11\n",
      "Step: 58, Training Loss: 6.55946, LR: 0.0001738, Tokens/sec: 35171.21\n",
      "Step: 59, Training Loss: 6.48702, LR: 0.0001760, Tokens/sec: 36673.82\n",
      "Step: 60, Training Loss: 6.42710, LR: 0.0001781, Tokens/sec: 36559.28\n",
      "Step: 61, Training Loss: 6.53466, LR: 0.0001802, Tokens/sec: 38873.84\n",
      "Step: 62, Training Loss: 6.70701, LR: 0.0001824, Tokens/sec: 28381.93\n",
      "Step: 63, Training Loss: 6.57069, LR: 0.0001845, Tokens/sec: 38072.53\n",
      "Step: 64, Training Loss: 6.48953, LR: 0.0001866, Tokens/sec: 36896.27\n",
      "Step: 65, Training Loss: 6.44356, LR: 0.0001888, Tokens/sec: 40091.74\n",
      "Step: 66, Training Loss: 6.56166, LR: 0.0001909, Tokens/sec: 40389.95\n",
      "Step: 67, Training Loss: 6.55703, LR: 0.0001930, Tokens/sec: 41123.09\n",
      "Step: 68, Training Loss: 6.42582, LR: 0.0001952, Tokens/sec: 38660.77\n",
      "Step: 69, Training Loss: 6.43022, LR: 0.0001973, Tokens/sec: 40432.77\n",
      "Step: 70, Training Loss: 6.74714, LR: 0.0001994, Tokens/sec: 41782.75\n",
      "Step: 71, Training Loss: 6.43442, LR: 0.0002016, Tokens/sec: 37112.29\n",
      "Step: 72, Training Loss: 6.65567, LR: 0.0002037, Tokens/sec: 37061.98\n",
      "Step: 73, Training Loss: 6.41573, LR: 0.0002058, Tokens/sec: 39972.91\n",
      "Step: 74, Training Loss: 6.49939, LR: 0.0002080, Tokens/sec: 42218.40\n",
      "Step: 75, Training Loss: 6.48116, LR: 0.0002101, Tokens/sec: 42287.64\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 75, Eval Loss: 6.47170\n",
      "Step: 76, Training Loss: 6.26906, LR: 0.0002122, Tokens/sec: 36607.59\n",
      "Step: 77, Training Loss: 6.44520, LR: 0.0002144, Tokens/sec: 40179.47\n",
      "Step: 78, Training Loss: 6.25799, LR: 0.0002165, Tokens/sec: 40938.63\n",
      "Step: 79, Training Loss: 6.40568, LR: 0.0002187, Tokens/sec: 40701.60\n",
      "Step: 80, Training Loss: 6.45369, LR: 0.0002208, Tokens/sec: 42030.37\n",
      "Step: 81, Training Loss: 6.35434, LR: 0.0002229, Tokens/sec: 38615.35\n",
      "Step: 82, Training Loss: 6.62971, LR: 0.0002251, Tokens/sec: 40561.34\n",
      "Step: 83, Training Loss: 6.43266, LR: 0.0002272, Tokens/sec: 40590.15\n",
      "Step: 84, Training Loss: 6.24406, LR: 0.0002293, Tokens/sec: 40435.78\n",
      "Step: 85, Training Loss: 6.19349, LR: 0.0002315, Tokens/sec: 40484.54\n",
      "Step: 86, Training Loss: 6.34013, LR: 0.0002336, Tokens/sec: 39220.16\n",
      "Step: 87, Training Loss: 6.40596, LR: 0.0002357, Tokens/sec: 40498.72\n",
      "Step: 88, Training Loss: 6.51393, LR: 0.0002379, Tokens/sec: 42144.73\n",
      "Step: 89, Training Loss: 6.07395, LR: 0.0002400, Tokens/sec: 40116.91\n",
      "Step: 90, Training Loss: 6.42607, LR: 0.0002421, Tokens/sec: 40503.23\n",
      "Step: 91, Training Loss: 6.36912, LR: 0.0002443, Tokens/sec: 41581.30\n",
      "Step: 92, Training Loss: 6.13680, LR: 0.0002464, Tokens/sec: 41616.24\n",
      "Step: 93, Training Loss: 6.17243, LR: 0.0002485, Tokens/sec: 40649.18\n",
      "Step: 94, Training Loss: 6.22468, LR: 0.0002507, Tokens/sec: 40555.82\n",
      "Step: 95, Training Loss: 6.21504, LR: 0.0002528, Tokens/sec: 42229.96\n",
      "Step: 96, Training Loss: 6.29085, LR: 0.0002549, Tokens/sec: 42153.28\n",
      "Step: 97, Training Loss: 6.24149, LR: 0.0002571, Tokens/sec: 38958.88\n",
      "Step: 98, Training Loss: 6.29528, LR: 0.0002592, Tokens/sec: 38272.72\n",
      "Step: 99, Training Loss: 6.07734, LR: 0.0002613, Tokens/sec: 40028.44\n",
      "Step: 100, Training Loss: 5.92323, LR: 0.0002635, Tokens/sec: 40812.29\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 100, Eval Loss: 6.12786\n",
      "Step: 101, Training Loss: 6.12835, LR: 0.0002656, Tokens/sec: 38470.87\n",
      "Step: 102, Training Loss: 6.27061, LR: 0.0002678, Tokens/sec: 25440.07\n",
      "Step: 103, Training Loss: 5.95858, LR: 0.0002699, Tokens/sec: 30551.10\n",
      "Step: 104, Training Loss: 5.99937, LR: 0.0002720, Tokens/sec: 40089.92\n",
      "Step: 105, Training Loss: 5.98396, LR: 0.0002742, Tokens/sec: 40839.69\n",
      "Step: 106, Training Loss: 5.99084, LR: 0.0002763, Tokens/sec: 40787.74\n",
      "Step: 107, Training Loss: 6.06336, LR: 0.0002784, Tokens/sec: 42073.38\n",
      "Step: 108, Training Loss: 6.11510, LR: 0.0002806, Tokens/sec: 39843.64\n",
      "Step: 109, Training Loss: 5.94481, LR: 0.0002827, Tokens/sec: 41676.90\n",
      "Step: 110, Training Loss: 6.00232, LR: 0.0002848, Tokens/sec: 41270.63\n",
      "Step: 111, Training Loss: 6.02716, LR: 0.0002870, Tokens/sec: 41672.47\n",
      "Step: 112, Training Loss: 5.99057, LR: 0.0002891, Tokens/sec: 41576.85\n",
      "Step: 113, Training Loss: 6.03023, LR: 0.0002912, Tokens/sec: 37718.53\n",
      "Step: 114, Training Loss: 5.71745, LR: 0.0002934, Tokens/sec: 41081.27\n",
      "Step: 115, Training Loss: 6.19870, LR: 0.0002955, Tokens/sec: 41251.31\n",
      "Step: 116, Training Loss: 5.83745, LR: 0.0002976, Tokens/sec: 40938.27\n",
      "Step: 117, Training Loss: 5.96591, LR: 0.0002998, Tokens/sec: 41756.20\n",
      "Step: 118, Training Loss: 5.73744, LR: 0.0003019, Tokens/sec: 41482.58\n",
      "Step: 119, Training Loss: 5.92866, LR: 0.0003040, Tokens/sec: 41192.12\n",
      "Step: 120, Training Loss: 5.89580, LR: 0.0003062, Tokens/sec: 32680.12\n",
      "Step: 121, Training Loss: 6.06541, LR: 0.0003083, Tokens/sec: 38465.28\n",
      "Step: 122, Training Loss: 5.95107, LR: 0.0003104, Tokens/sec: 41633.14\n",
      "Step: 123, Training Loss: 5.82068, LR: 0.0003126, Tokens/sec: 41340.30\n",
      "Step: 124, Training Loss: 5.62134, LR: 0.0003147, Tokens/sec: 41691.54\n",
      "Step: 125, Training Loss: 5.80969, LR: 0.0003169, Tokens/sec: 41646.20\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 125, Eval Loss: 5.82563\n",
      "Step: 126, Training Loss: 5.74569, LR: 0.0003190, Tokens/sec: 39805.35\n",
      "Step: 127, Training Loss: 5.86594, LR: 0.0003211, Tokens/sec: 41135.70\n",
      "Step: 128, Training Loss: 5.79007, LR: 0.0003233, Tokens/sec: 41486.10\n",
      "Step: 129, Training Loss: 5.92223, LR: 0.0003254, Tokens/sec: 41249.59\n",
      "Step: 130, Training Loss: 5.98903, LR: 0.0003275, Tokens/sec: 41242.82\n",
      "Step: 131, Training Loss: 5.74566, LR: 0.0003297, Tokens/sec: 31289.56\n",
      "Step: 132, Training Loss: 5.69384, LR: 0.0003318, Tokens/sec: 41483.62\n",
      "Step: 133, Training Loss: 5.70969, LR: 0.0003339, Tokens/sec: 41326.30\n",
      "Step: 134, Training Loss: 5.75878, LR: 0.0003361, Tokens/sec: 40944.27\n",
      "Step: 135, Training Loss: 5.73481, LR: 0.0003382, Tokens/sec: 41089.37\n",
      "Step: 136, Training Loss: 5.83708, LR: 0.0003403, Tokens/sec: 39310.22\n",
      "Step: 137, Training Loss: 5.69436, LR: 0.0003425, Tokens/sec: 41285.34\n",
      "Step: 138, Training Loss: 5.75202, LR: 0.0003446, Tokens/sec: 41447.35\n",
      "Step: 139, Training Loss: 5.55764, LR: 0.0003467, Tokens/sec: 40360.56\n",
      "Step: 140, Training Loss: 5.70238, LR: 0.0003489, Tokens/sec: 41632.31\n",
      "Step: 141, Training Loss: 5.57730, LR: 0.0003510, Tokens/sec: 41598.62\n",
      "Step: 142, Training Loss: 5.53014, LR: 0.0003531, Tokens/sec: 41374.35\n",
      "Step: 143, Training Loss: 5.66918, LR: 0.0003553, Tokens/sec: 40866.84\n",
      "Step: 144, Training Loss: 5.78740, LR: 0.0003574, Tokens/sec: 39537.69\n",
      "Step: 145, Training Loss: 5.52919, LR: 0.0003596, Tokens/sec: 41576.22\n",
      "Step: 146, Training Loss: 5.59788, LR: 0.0003617, Tokens/sec: 41654.94\n",
      "Step: 147, Training Loss: 5.54584, LR: 0.0003638, Tokens/sec: 40617.21\n",
      "Step: 148, Training Loss: 5.61853, LR: 0.0003660, Tokens/sec: 42047.62\n",
      "Step: 149, Training Loss: 5.64968, LR: 0.0003681, Tokens/sec: 41516.48\n",
      "Step: 150, Training Loss: 5.47451, LR: 0.0003702, Tokens/sec: 41379.20\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 150, Eval Loss: 5.59679\n",
      "Step: 151, Training Loss: 5.35749, LR: 0.0003724, Tokens/sec: 40707.88\n",
      "Step: 152, Training Loss: 5.46834, LR: 0.0003745, Tokens/sec: 40550.58\n",
      "Step: 153, Training Loss: 5.56868, LR: 0.0003766, Tokens/sec: 41193.04\n",
      "Step: 154, Training Loss: 5.55335, LR: 0.0003788, Tokens/sec: 39391.10\n",
      "Step: 155, Training Loss: 5.49688, LR: 0.0003809, Tokens/sec: 41606.54\n",
      "Step: 156, Training Loss: 5.47256, LR: 0.0003830, Tokens/sec: 40728.87\n",
      "Step: 157, Training Loss: 5.53283, LR: 0.0003852, Tokens/sec: 39301.03\n",
      "Step: 158, Training Loss: 5.69551, LR: 0.0003873, Tokens/sec: 41401.26\n",
      "Step: 159, Training Loss: 5.63089, LR: 0.0003894, Tokens/sec: 40395.66\n",
      "Step: 160, Training Loss: 5.52366, LR: 0.0003916, Tokens/sec: 41406.15\n",
      "Step: 161, Training Loss: 5.56656, LR: 0.0003937, Tokens/sec: 40775.39\n",
      "Step: 162, Training Loss: 5.38581, LR: 0.0003958, Tokens/sec: 41193.85\n",
      "Step: 163, Training Loss: 5.59355, LR: 0.0003980, Tokens/sec: 41042.86\n",
      "Step: 164, Training Loss: 5.24580, LR: 0.0004001, Tokens/sec: 37317.13\n",
      "Step: 165, Training Loss: 5.66110, LR: 0.0004022, Tokens/sec: 36494.88\n",
      "Step: 166, Training Loss: 5.16063, LR: 0.0004044, Tokens/sec: 39985.19\n",
      "Step: 167, Training Loss: 5.53916, LR: 0.0004065, Tokens/sec: 37381.39\n",
      "Step: 168, Training Loss: 5.30580, LR: 0.0004087, Tokens/sec: 38167.18\n",
      "Step: 169, Training Loss: 5.62641, LR: 0.0004108, Tokens/sec: 35725.46\n",
      "Step: 170, Training Loss: 5.47245, LR: 0.0004129, Tokens/sec: 28200.70\n",
      "Step: 171, Training Loss: 5.42952, LR: 0.0004151, Tokens/sec: 35957.19\n",
      "Step: 172, Training Loss: 5.48489, LR: 0.0004172, Tokens/sec: 31356.26\n",
      "Step: 173, Training Loss: 5.12201, LR: 0.0004193, Tokens/sec: 34526.17\n",
      "Step: 174, Training Loss: 5.35589, LR: 0.0004215, Tokens/sec: 32444.79\n",
      "Step: 175, Training Loss: 5.53009, LR: 0.0004236, Tokens/sec: 34746.54\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 175, Eval Loss: 5.34062\n",
      "Step: 176, Training Loss: 5.16666, LR: 0.0004257, Tokens/sec: 38720.78\n",
      "Step: 177, Training Loss: 5.41226, LR: 0.0004279, Tokens/sec: 35198.00\n",
      "Step: 178, Training Loss: 5.30543, LR: 0.0004300, Tokens/sec: 36490.81\n",
      "Step: 179, Training Loss: 5.25250, LR: 0.0004321, Tokens/sec: 36423.49\n",
      "Step: 180, Training Loss: 5.30164, LR: 0.0004343, Tokens/sec: 33598.91\n",
      "Step: 181, Training Loss: 5.38514, LR: 0.0004364, Tokens/sec: 34131.01\n",
      "Step: 182, Training Loss: 5.46258, LR: 0.0004385, Tokens/sec: 29491.44\n",
      "Step: 183, Training Loss: 5.19982, LR: 0.0004407, Tokens/sec: 39593.15\n",
      "Step: 184, Training Loss: 5.31359, LR: 0.0004428, Tokens/sec: 28863.38\n",
      "Step: 185, Training Loss: 5.36065, LR: 0.0004449, Tokens/sec: 33625.08\n",
      "Step: 186, Training Loss: 5.29377, LR: 0.0004471, Tokens/sec: 38356.46\n",
      "Step: 187, Training Loss: 5.59347, LR: 0.0004492, Tokens/sec: 35351.75\n",
      "Step: 188, Training Loss: 5.47310, LR: 0.0004513, Tokens/sec: 34774.61\n",
      "Step: 189, Training Loss: 5.33120, LR: 0.0004535, Tokens/sec: 33572.46\n",
      "Step: 190, Training Loss: 5.33180, LR: 0.0004556, Tokens/sec: 40470.68\n",
      "Step: 191, Training Loss: 5.14956, LR: 0.0004578, Tokens/sec: 35358.55\n",
      "Step: 192, Training Loss: 5.46957, LR: 0.0004599, Tokens/sec: 37659.02\n",
      "Step: 193, Training Loss: 5.40910, LR: 0.0004620, Tokens/sec: 39237.26\n",
      "Step: 194, Training Loss: 5.13057, LR: 0.0004642, Tokens/sec: 40119.29\n",
      "Step: 195, Training Loss: 5.39773, LR: 0.0004663, Tokens/sec: 34870.10\n",
      "Step: 196, Training Loss: 5.45345, LR: 0.0004684, Tokens/sec: 34663.65\n",
      "Step: 197, Training Loss: 5.23090, LR: 0.0004706, Tokens/sec: 41274.73\n",
      "Step: 198, Training Loss: 5.07993, LR: 0.0004727, Tokens/sec: 39907.60\n",
      "Step: 199, Training Loss: 4.85576, LR: 0.0004748, Tokens/sec: 38279.39\n",
      "Step: 200, Training Loss: 5.10719, LR: 0.0004770, Tokens/sec: 31774.49\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 200, Eval Loss: 5.25297\n",
      "Step: 201, Training Loss: 5.11587, LR: 0.0004791, Tokens/sec: 40363.85\n",
      "Step: 202, Training Loss: 5.09226, LR: 0.0004812, Tokens/sec: 36877.20\n",
      "Step: 203, Training Loss: 6.26016, LR: 0.0004834, Tokens/sec: 38866.43\n",
      "Step: 204, Training Loss: 5.32732, LR: 0.0004855, Tokens/sec: 39875.10\n",
      "Step: 205, Training Loss: 5.42279, LR: 0.0004876, Tokens/sec: 40425.27\n",
      "Step: 206, Training Loss: 4.97310, LR: 0.0004898, Tokens/sec: 41668.41\n",
      "Step: 207, Training Loss: 5.06586, LR: 0.0004919, Tokens/sec: 41297.38\n",
      "Step: 208, Training Loss: 5.24851, LR: 0.0004940, Tokens/sec: 41157.85\n",
      "Step: 209, Training Loss: 5.08261, LR: 0.0004962, Tokens/sec: 39936.32\n",
      "Step: 210, Training Loss: 5.32916, LR: 0.0004983, Tokens/sec: 37354.69\n",
      "Step: 211, Training Loss: 5.31059, LR: 0.0005004, Tokens/sec: 40078.43\n",
      "Step: 212, Training Loss: 5.36968, LR: 0.0005026, Tokens/sec: 40658.24\n",
      "Step: 213, Training Loss: 5.34034, LR: 0.0005047, Tokens/sec: 34489.60\n",
      "Step: 214, Training Loss: 5.05602, LR: 0.0005069, Tokens/sec: 34825.14\n",
      "Step: 215, Training Loss: 5.17738, LR: 0.0005090, Tokens/sec: 32707.15\n",
      "Step: 216, Training Loss: 5.37230, LR: 0.0005111, Tokens/sec: 39107.95\n",
      "Step: 217, Training Loss: 5.25965, LR: 0.0005133, Tokens/sec: 39951.08\n",
      "Step: 218, Training Loss: 5.27706, LR: 0.0005154, Tokens/sec: 39951.43\n",
      "Step: 219, Training Loss: 5.26194, LR: 0.0005175, Tokens/sec: 41403.29\n",
      "Step: 220, Training Loss: 5.09407, LR: 0.0005197, Tokens/sec: 37300.27\n",
      "Step: 221, Training Loss: 5.13284, LR: 0.0005218, Tokens/sec: 40987.52\n",
      "Step: 222, Training Loss: 5.13201, LR: 0.0005239, Tokens/sec: 39160.79\n",
      "Step: 223, Training Loss: 5.23644, LR: 0.0005261, Tokens/sec: 38954.44\n",
      "Step: 224, Training Loss: 4.95291, LR: 0.0005282, Tokens/sec: 36979.38\n",
      "Step: 225, Training Loss: 5.31982, LR: 0.0005303, Tokens/sec: 37470.01\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 225, Eval Loss: 5.08178\n",
      "Step: 226, Training Loss: 5.10986, LR: 0.0005325, Tokens/sec: 37010.34\n",
      "Step: 227, Training Loss: 5.05275, LR: 0.0005346, Tokens/sec: 35735.74\n",
      "Step: 228, Training Loss: 5.28200, LR: 0.0005367, Tokens/sec: 38249.04\n",
      "Step: 229, Training Loss: 5.11047, LR: 0.0005389, Tokens/sec: 37708.52\n",
      "Step: 230, Training Loss: 5.03483, LR: 0.0005410, Tokens/sec: 33547.21\n",
      "Step: 231, Training Loss: 4.93059, LR: 0.0005431, Tokens/sec: 33873.61\n",
      "Step: 232, Training Loss: 5.23700, LR: 0.0005453, Tokens/sec: 30611.22\n",
      "Step: 233, Training Loss: 5.18007, LR: 0.0005474, Tokens/sec: 37792.16\n",
      "Step: 234, Training Loss: 5.55362, LR: 0.0005496, Tokens/sec: 37950.44\n",
      "Step: 235, Training Loss: 4.77859, LR: 0.0005517, Tokens/sec: 37458.45\n",
      "Step: 236, Training Loss: 5.23780, LR: 0.0005538, Tokens/sec: 39615.06\n",
      "Step: 237, Training Loss: 4.88378, LR: 0.0005560, Tokens/sec: 39904.72\n",
      "Step: 238, Training Loss: 4.79031, LR: 0.0005581, Tokens/sec: 35270.89\n",
      "Step: 239, Training Loss: 5.20331, LR: 0.0005602, Tokens/sec: 35461.96\n",
      "Step: 240, Training Loss: 4.86936, LR: 0.0005624, Tokens/sec: 41908.71\n",
      "Step: 241, Training Loss: 4.79795, LR: 0.0005645, Tokens/sec: 39640.85\n",
      "Step: 242, Training Loss: 4.97770, LR: 0.0005666, Tokens/sec: 41773.89\n",
      "Step: 243, Training Loss: 5.00781, LR: 0.0005688, Tokens/sec: 39625.03\n",
      "Step: 244, Training Loss: 5.16557, LR: 0.0005709, Tokens/sec: 41422.80\n",
      "Step: 245, Training Loss: 4.93431, LR: 0.0005730, Tokens/sec: 41323.94\n",
      "Step: 246, Training Loss: 4.84427, LR: 0.0005752, Tokens/sec: 39858.02\n",
      "Step: 247, Training Loss: 5.08121, LR: 0.0005773, Tokens/sec: 42060.87\n",
      "Step: 248, Training Loss: 5.13163, LR: 0.0005794, Tokens/sec: 41859.10\n",
      "Step: 249, Training Loss: 5.43052, LR: 0.0005816, Tokens/sec: 41316.12\n",
      "Step: 250, Training Loss: 4.86663, LR: 0.0005837, Tokens/sec: 41305.55\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 250, Eval Loss: 5.01008\n",
      "Step: 251, Training Loss: 5.00128, LR: 0.0005858, Tokens/sec: 41108.77\n",
      "Step: 252, Training Loss: 5.10267, LR: 0.0005880, Tokens/sec: 39737.69\n",
      "Step: 253, Training Loss: 4.62395, LR: 0.0005901, Tokens/sec: 39452.65\n",
      "Step: 254, Training Loss: 4.98879, LR: 0.0005922, Tokens/sec: 39286.45\n",
      "Step: 255, Training Loss: 4.95363, LR: 0.0005944, Tokens/sec: 39230.92\n",
      "Step: 256, Training Loss: 4.81940, LR: 0.0005965, Tokens/sec: 38633.39\n",
      "Step: 257, Training Loss: 5.24725, LR: 0.0005987, Tokens/sec: 40518.54\n",
      "Step: 258, Training Loss: 5.00717, LR: 0.0006008, Tokens/sec: 41937.24\n",
      "Step: 259, Training Loss: 5.16008, LR: 0.0006029, Tokens/sec: 41805.46\n",
      "Step: 260, Training Loss: 4.76161, LR: 0.0006051, Tokens/sec: 41915.98\n",
      "Step: 261, Training Loss: 4.94390, LR: 0.0006072, Tokens/sec: 38439.86\n",
      "Step: 262, Training Loss: 5.09747, LR: 0.0006093, Tokens/sec: 36950.65\n",
      "Step: 263, Training Loss: 5.17943, LR: 0.0006115, Tokens/sec: 38229.14\n",
      "Step: 264, Training Loss: 4.83956, LR: 0.0006136, Tokens/sec: 31736.81\n",
      "Step: 265, Training Loss: 5.09781, LR: 0.0006157, Tokens/sec: 38955.57\n",
      "Step: 266, Training Loss: 5.16708, LR: 0.0006179, Tokens/sec: 38008.14\n",
      "Step: 267, Training Loss: 4.89702, LR: 0.0006200, Tokens/sec: 37591.25\n",
      "Step: 268, Training Loss: 4.80309, LR: 0.0006221, Tokens/sec: 36359.20\n",
      "Step: 269, Training Loss: 4.74430, LR: 0.0006243, Tokens/sec: 39978.29\n",
      "Step: 270, Training Loss: 5.11363, LR: 0.0006264, Tokens/sec: 26995.89\n",
      "Step: 271, Training Loss: 4.87732, LR: 0.0006285, Tokens/sec: 33903.30\n",
      "Step: 272, Training Loss: 5.15734, LR: 0.0006307, Tokens/sec: 40032.00\n",
      "Step: 273, Training Loss: 5.02854, LR: 0.0006328, Tokens/sec: 38142.18\n",
      "Step: 274, Training Loss: 4.98559, LR: 0.0006349, Tokens/sec: 40668.69\n",
      "Step: 275, Training Loss: 4.76796, LR: 0.0006371, Tokens/sec: 39235.80\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 275, Eval Loss: 4.90556\n",
      "Step: 276, Training Loss: 4.68545, LR: 0.0006392, Tokens/sec: 36256.43\n",
      "Step: 277, Training Loss: 4.76756, LR: 0.0006413, Tokens/sec: 33151.33\n",
      "Step: 278, Training Loss: 5.06233, LR: 0.0006435, Tokens/sec: 38029.89\n",
      "Step: 279, Training Loss: 4.95318, LR: 0.0006456, Tokens/sec: 40372.62\n",
      "Step: 280, Training Loss: 4.88126, LR: 0.0006478, Tokens/sec: 39916.03\n",
      "Step: 281, Training Loss: 4.93957, LR: 0.0006499, Tokens/sec: 27871.26\n",
      "Step: 282, Training Loss: 5.25641, LR: 0.0006520, Tokens/sec: 32940.28\n",
      "Step: 283, Training Loss: 4.86554, LR: 0.0006542, Tokens/sec: 38500.50\n",
      "Step: 284, Training Loss: 5.24221, LR: 0.0006563, Tokens/sec: 38990.24\n",
      "Step: 285, Training Loss: 4.94993, LR: 0.0006584, Tokens/sec: 33620.50\n",
      "Step: 286, Training Loss: 5.05910, LR: 0.0006606, Tokens/sec: 37397.07\n",
      "Step: 287, Training Loss: 4.99284, LR: 0.0006627, Tokens/sec: 35489.43\n",
      "Step: 288, Training Loss: 5.03957, LR: 0.0006648, Tokens/sec: 36347.63\n",
      "Step: 289, Training Loss: 4.79661, LR: 0.0006670, Tokens/sec: 33836.83\n",
      "Step: 290, Training Loss: 4.89452, LR: 0.0006691, Tokens/sec: 29333.84\n",
      "Step: 291, Training Loss: 5.07487, LR: 0.0006712, Tokens/sec: 38054.12\n",
      "Step: 292, Training Loss: 4.74365, LR: 0.0006734, Tokens/sec: 35108.99\n",
      "Step: 293, Training Loss: 4.90936, LR: 0.0006755, Tokens/sec: 34651.14\n",
      "Step: 294, Training Loss: 5.00507, LR: 0.0006776, Tokens/sec: 40096.63\n",
      "Step: 295, Training Loss: 4.79969, LR: 0.0006798, Tokens/sec: 41835.59\n",
      "Step: 296, Training Loss: 4.96884, LR: 0.0006819, Tokens/sec: 27141.50\n",
      "Step: 297, Training Loss: 4.88461, LR: 0.0006840, Tokens/sec: 38424.84\n",
      "Step: 298, Training Loss: 5.10291, LR: 0.0006862, Tokens/sec: 35733.16\n",
      "Step: 299, Training Loss: 4.99587, LR: 0.0006883, Tokens/sec: 33364.65\n",
      "Step: 300, Training Loss: 4.93448, LR: 0.0006904, Tokens/sec: 36236.92\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 300, Eval Loss: 4.88009\n",
      "Step: 301, Training Loss: 4.81028, LR: 0.0006926, Tokens/sec: 27730.10\n",
      "Step: 302, Training Loss: 4.88187, LR: 0.0006947, Tokens/sec: 37655.68\n",
      "Step: 303, Training Loss: 5.02361, LR: 0.0006969, Tokens/sec: 27105.04\n",
      "Step: 304, Training Loss: 4.92804, LR: 0.0006990, Tokens/sec: 40416.35\n",
      "Step: 305, Training Loss: 4.86748, LR: 0.0007011, Tokens/sec: 38442.89\n",
      "Step: 306, Training Loss: 4.99045, LR: 0.0007033, Tokens/sec: 37264.13\n",
      "Step: 307, Training Loss: 5.00894, LR: 0.0007054, Tokens/sec: 39454.56\n",
      "Step: 308, Training Loss: 5.15053, LR: 0.0007075, Tokens/sec: 39004.57\n",
      "Step: 309, Training Loss: 4.90554, LR: 0.0007097, Tokens/sec: 41897.32\n",
      "Step: 310, Training Loss: 5.31262, LR: 0.0007118, Tokens/sec: 40678.28\n",
      "Step: 311, Training Loss: 4.82952, LR: 0.0007139, Tokens/sec: 38716.66\n",
      "Step: 312, Training Loss: 4.99152, LR: 0.0007161, Tokens/sec: 40825.54\n",
      "Step: 313, Training Loss: 4.81534, LR: 0.0007182, Tokens/sec: 35922.80\n",
      "Step: 314, Training Loss: 4.71147, LR: 0.0007203, Tokens/sec: 34918.90\n",
      "Step: 315, Training Loss: 4.67410, LR: 0.0007225, Tokens/sec: 38421.68\n",
      "Step: 316, Training Loss: 5.02982, LR: 0.0007246, Tokens/sec: 36892.57\n",
      "Step: 317, Training Loss: 4.69247, LR: 0.0007267, Tokens/sec: 36742.27\n",
      "Step: 318, Training Loss: 4.78909, LR: 0.0007289, Tokens/sec: 36567.97\n",
      "Step: 319, Training Loss: 4.77559, LR: 0.0007310, Tokens/sec: 28173.11\n",
      "Step: 320, Training Loss: 4.86092, LR: 0.0007331, Tokens/sec: 33756.25\n",
      "Step: 321, Training Loss: 4.77649, LR: 0.0007353, Tokens/sec: 36428.27\n",
      "Step: 322, Training Loss: 4.94866, LR: 0.0007374, Tokens/sec: 31951.14\n",
      "Step: 323, Training Loss: 4.94446, LR: 0.0007396, Tokens/sec: 35839.49\n",
      "Step: 324, Training Loss: 4.83450, LR: 0.0007417, Tokens/sec: 35879.60\n",
      "Step: 325, Training Loss: 5.00003, LR: 0.0007438, Tokens/sec: 41793.76\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 325, Eval Loss: 4.85711\n",
      "Step: 326, Training Loss: 4.64388, LR: 0.0007460, Tokens/sec: 41655.27\n",
      "Step: 327, Training Loss: 4.82053, LR: 0.0007481, Tokens/sec: 41873.39\n",
      "Step: 328, Training Loss: 4.75518, LR: 0.0007502, Tokens/sec: 34340.61\n",
      "Step: 329, Training Loss: 4.90903, LR: 0.0007524, Tokens/sec: 27071.21\n",
      "Step: 330, Training Loss: 4.48537, LR: 0.0007545, Tokens/sec: 38515.43\n",
      "Step: 331, Training Loss: 4.87627, LR: 0.0007566, Tokens/sec: 34947.12\n",
      "Step: 332, Training Loss: 4.69754, LR: 0.0007588, Tokens/sec: 36700.10\n",
      "Step: 333, Training Loss: 4.78021, LR: 0.0007609, Tokens/sec: 29759.08\n",
      "Step: 334, Training Loss: 4.90037, LR: 0.0007630, Tokens/sec: 41470.89\n",
      "Step: 335, Training Loss: 4.75476, LR: 0.0007652, Tokens/sec: 41236.44\n",
      "Step: 336, Training Loss: 4.81882, LR: 0.0007673, Tokens/sec: 30513.47\n",
      "Step: 337, Training Loss: 4.70743, LR: 0.0007694, Tokens/sec: 38193.05\n",
      "Step: 338, Training Loss: 4.65857, LR: 0.0007716, Tokens/sec: 40593.27\n",
      "Step: 339, Training Loss: 4.76179, LR: 0.0007737, Tokens/sec: 41722.98\n",
      "Step: 340, Training Loss: 4.56039, LR: 0.0007758, Tokens/sec: 41992.98\n",
      "Step: 341, Training Loss: 4.80694, LR: 0.0007780, Tokens/sec: 41809.85\n",
      "Step: 342, Training Loss: 4.91104, LR: 0.0007801, Tokens/sec: 41895.82\n",
      "Step: 343, Training Loss: 5.01239, LR: 0.0007822, Tokens/sec: 39109.67\n",
      "Step: 344, Training Loss: 4.55911, LR: 0.0007844, Tokens/sec: 29934.76\n",
      "Step: 345, Training Loss: 4.81274, LR: 0.0007865, Tokens/sec: 33016.09\n",
      "Step: 346, Training Loss: 4.58021, LR: 0.0007887, Tokens/sec: 36151.06\n",
      "Step: 347, Training Loss: 4.90487, LR: 0.0007908, Tokens/sec: 29169.98\n",
      "Step: 348, Training Loss: 4.81598, LR: 0.0007929, Tokens/sec: 32641.84\n",
      "Step: 349, Training Loss: 4.71958, LR: 0.0007951, Tokens/sec: 41824.62\n",
      "Step: 350, Training Loss: 4.80216, LR: 0.0007972, Tokens/sec: 29407.23\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 350, Eval Loss: 4.76739\n",
      "Step: 351, Training Loss: 4.88103, LR: 0.0007993, Tokens/sec: 41135.39\n",
      "Step: 352, Training Loss: 4.82463, LR: 0.0008015, Tokens/sec: 41986.90\n",
      "Step: 353, Training Loss: 4.96314, LR: 0.0008036, Tokens/sec: 9485.90\n",
      "Step: 354, Training Loss: 5.04379, LR: 0.0008057, Tokens/sec: 31681.76\n",
      "Step: 355, Training Loss: 4.61730, LR: 0.0008079, Tokens/sec: 41891.38\n",
      "Step: 356, Training Loss: 4.70444, LR: 0.0008100, Tokens/sec: 41756.45\n",
      "Step: 357, Training Loss: 4.64799, LR: 0.0008121, Tokens/sec: 30406.78\n",
      "Step: 358, Training Loss: 4.36652, LR: 0.0008143, Tokens/sec: 40214.65\n",
      "Step: 359, Training Loss: 4.59492, LR: 0.0008164, Tokens/sec: 37300.75\n",
      "Step: 360, Training Loss: 4.55125, LR: 0.0008185, Tokens/sec: 36390.00\n",
      "Step: 361, Training Loss: 4.82343, LR: 0.0008207, Tokens/sec: 31434.23\n",
      "Step: 362, Training Loss: 4.63387, LR: 0.0008228, Tokens/sec: 33603.92\n",
      "Step: 363, Training Loss: 4.57453, LR: 0.0008249, Tokens/sec: 26652.51\n",
      "Step: 364, Training Loss: 4.71653, LR: 0.0008271, Tokens/sec: 38920.51\n",
      "Step: 365, Training Loss: 4.59403, LR: 0.0008292, Tokens/sec: 32412.61\n",
      "Step: 366, Training Loss: 4.42388, LR: 0.0008313, Tokens/sec: 41753.49\n",
      "Step: 367, Training Loss: 4.80840, LR: 0.0008335, Tokens/sec: 41412.65\n",
      "Step: 368, Training Loss: 4.72720, LR: 0.0008356, Tokens/sec: 39142.69\n",
      "Step: 369, Training Loss: 4.75957, LR: 0.0008378, Tokens/sec: 32913.49\n",
      "Step: 370, Training Loss: 4.72075, LR: 0.0008399, Tokens/sec: 36559.01\n",
      "Step: 371, Training Loss: 4.87875, LR: 0.0008420, Tokens/sec: 41915.43\n",
      "Step: 372, Training Loss: 4.73987, LR: 0.0008442, Tokens/sec: 41923.67\n",
      "Step: 373, Training Loss: 4.74110, LR: 0.0008463, Tokens/sec: 39787.32\n",
      "Step: 374, Training Loss: 4.69688, LR: 0.0008484, Tokens/sec: 41426.97\n",
      "Step: 375, Training Loss: 4.68207, LR: 0.0008506, Tokens/sec: 40926.81\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 375, Eval Loss: 4.70052\n",
      "Step: 376, Training Loss: 4.75024, LR: 0.0008527, Tokens/sec: 41335.59\n",
      "Step: 377, Training Loss: 4.59858, LR: 0.0008548, Tokens/sec: 40256.91\n",
      "Step: 378, Training Loss: 4.61739, LR: 0.0008570, Tokens/sec: 39060.99\n",
      "Step: 379, Training Loss: 4.79996, LR: 0.0008591, Tokens/sec: 40701.65\n",
      "Step: 380, Training Loss: 4.60123, LR: 0.0008612, Tokens/sec: 38410.94\n",
      "Step: 381, Training Loss: 4.72240, LR: 0.0008634, Tokens/sec: 41239.44\n",
      "Step: 382, Training Loss: 4.75058, LR: 0.0008655, Tokens/sec: 40029.71\n",
      "Step: 383, Training Loss: 4.54604, LR: 0.0008676, Tokens/sec: 41733.91\n",
      "Step: 384, Training Loss: 4.66356, LR: 0.0008698, Tokens/sec: 40079.05\n",
      "Step: 385, Training Loss: 4.64240, LR: 0.0008719, Tokens/sec: 41011.15\n",
      "Step: 386, Training Loss: 4.59217, LR: 0.0008740, Tokens/sec: 41772.68\n",
      "Step: 387, Training Loss: 4.64957, LR: 0.0008762, Tokens/sec: 41829.11\n",
      "Step: 388, Training Loss: 4.66368, LR: 0.0008783, Tokens/sec: 39016.50\n",
      "Step: 389, Training Loss: 4.56985, LR: 0.0008804, Tokens/sec: 41340.30\n",
      "Step: 390, Training Loss: 4.34422, LR: 0.0008826, Tokens/sec: 40453.17\n",
      "Step: 391, Training Loss: 4.48732, LR: 0.0008847, Tokens/sec: 41652.25\n",
      "Step: 392, Training Loss: 4.74161, LR: 0.0008869, Tokens/sec: 41799.04\n",
      "Step: 393, Training Loss: 4.95026, LR: 0.0008890, Tokens/sec: 41345.70\n",
      "Step: 394, Training Loss: 4.77090, LR: 0.0008911, Tokens/sec: 39943.35\n",
      "Step: 395, Training Loss: 4.52501, LR: 0.0008933, Tokens/sec: 40746.30\n",
      "Step: 396, Training Loss: 4.46381, LR: 0.0008954, Tokens/sec: 38168.92\n",
      "Step: 397, Training Loss: 4.61522, LR: 0.0008975, Tokens/sec: 41525.99\n",
      "Step: 398, Training Loss: 4.57444, LR: 0.0008997, Tokens/sec: 40279.01\n",
      "Step: 399, Training Loss: 4.46255, LR: 0.0009018, Tokens/sec: 39897.54\n",
      "Step: 400, Training Loss: 4.72094, LR: 0.0009039, Tokens/sec: 40365.12\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 400, Eval Loss: 4.61299\n",
      "Step: 401, Training Loss: 4.77494, LR: 0.0009061, Tokens/sec: 39588.76\n",
      "Step: 402, Training Loss: 4.90844, LR: 0.0009082, Tokens/sec: 41622.82\n",
      "Step: 403, Training Loss: 4.62934, LR: 0.0009103, Tokens/sec: 41625.46\n",
      "Step: 404, Training Loss: 4.66822, LR: 0.0009125, Tokens/sec: 41561.62\n",
      "Step: 405, Training Loss: 4.36767, LR: 0.0009146, Tokens/sec: 41146.80\n",
      "Step: 406, Training Loss: 4.45299, LR: 0.0009167, Tokens/sec: 41510.84\n",
      "Step: 407, Training Loss: 4.79948, LR: 0.0009189, Tokens/sec: 40525.69\n",
      "Step: 408, Training Loss: 4.55988, LR: 0.0009210, Tokens/sec: 40648.59\n",
      "Step: 409, Training Loss: 4.94812, LR: 0.0009231, Tokens/sec: 38830.48\n",
      "Step: 410, Training Loss: 4.32115, LR: 0.0009253, Tokens/sec: 40776.40\n",
      "Step: 411, Training Loss: 4.73735, LR: 0.0009274, Tokens/sec: 41559.81\n",
      "Step: 412, Training Loss: 4.70774, LR: 0.0009296, Tokens/sec: 39307.88\n",
      "Step: 413, Training Loss: 4.67166, LR: 0.0009317, Tokens/sec: 40292.45\n",
      "Step: 414, Training Loss: 4.35178, LR: 0.0009338, Tokens/sec: 40393.26\n",
      "Step: 415, Training Loss: 4.57562, LR: 0.0009360, Tokens/sec: 39991.23\n",
      "Step: 416, Training Loss: 4.32798, LR: 0.0009381, Tokens/sec: 40287.74\n",
      "Step: 417, Training Loss: 4.64030, LR: 0.0009402, Tokens/sec: 38141.46\n",
      "Step: 418, Training Loss: 4.45533, LR: 0.0009424, Tokens/sec: 28375.94\n",
      "Step: 419, Training Loss: 4.47805, LR: 0.0009445, Tokens/sec: 41251.40\n",
      "Step: 420, Training Loss: 4.57230, LR: 0.0009466, Tokens/sec: 39734.34\n",
      "Step: 421, Training Loss: 4.65074, LR: 0.0009488, Tokens/sec: 40496.68\n",
      "Step: 422, Training Loss: 4.81888, LR: 0.0009509, Tokens/sec: 36625.72\n",
      "Step: 423, Training Loss: 4.79952, LR: 0.0009530, Tokens/sec: 41516.87\n",
      "Step: 424, Training Loss: 4.88824, LR: 0.0009552, Tokens/sec: 41465.88\n",
      "Step: 425, Training Loss: 4.45564, LR: 0.0009573, Tokens/sec: 41809.33\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 425, Eval Loss: 4.64485\n",
      "Step: 426, Training Loss: 3.98597, LR: 0.0009594, Tokens/sec: 41604.15\n",
      "Step: 427, Training Loss: 4.57251, LR: 0.0009616, Tokens/sec: 39052.70\n",
      "Step: 428, Training Loss: 4.62407, LR: 0.0009637, Tokens/sec: 41256.30\n",
      "Step: 429, Training Loss: 4.32548, LR: 0.0009658, Tokens/sec: 39673.05\n",
      "Step: 430, Training Loss: 4.79039, LR: 0.0009680, Tokens/sec: 34897.36\n",
      "Step: 431, Training Loss: 4.75678, LR: 0.0009701, Tokens/sec: 40536.44\n",
      "Step: 432, Training Loss: 4.45655, LR: 0.0009722, Tokens/sec: 38576.10\n",
      "Step: 433, Training Loss: 4.56799, LR: 0.0009744, Tokens/sec: 40517.95\n",
      "Step: 434, Training Loss: 4.61369, LR: 0.0009765, Tokens/sec: 41704.29\n",
      "Step: 435, Training Loss: 4.73848, LR: 0.0009787, Tokens/sec: 35845.37\n",
      "Step: 436, Training Loss: 4.48996, LR: 0.0009808, Tokens/sec: 41362.07\n",
      "Step: 437, Training Loss: 4.70298, LR: 0.0009829, Tokens/sec: 41804.92\n",
      "Step: 438, Training Loss: 4.64916, LR: 0.0009851, Tokens/sec: 40651.03\n",
      "Step: 439, Training Loss: 4.61603, LR: 0.0009872, Tokens/sec: 40953.40\n",
      "Step: 440, Training Loss: 4.60132, LR: 0.0009893, Tokens/sec: 38549.76\n",
      "Step: 441, Training Loss: 4.79485, LR: 0.0009915, Tokens/sec: 39376.48\n",
      "Step: 442, Training Loss: 4.60277, LR: 0.0009936, Tokens/sec: 41189.70\n",
      "Step: 443, Training Loss: 4.14201, LR: 0.0009957, Tokens/sec: 40489.66\n",
      "Step: 444, Training Loss: 4.31368, LR: 0.0009979, Tokens/sec: 40288.83\n",
      "Step: 445, Training Loss: 4.61592, LR: 0.0010000, Tokens/sec: 39184.08\n",
      "Step: 446, Training Loss: 4.58663, LR: 0.0010000, Tokens/sec: 40789.60\n",
      "Step: 447, Training Loss: 4.66237, LR: 0.0010000, Tokens/sec: 40636.10\n",
      "Step: 448, Training Loss: 4.47278, LR: 0.0010000, Tokens/sec: 39826.98\n",
      "Step: 449, Training Loss: 4.52896, LR: 0.0010000, Tokens/sec: 41732.89\n",
      "Step: 450, Training Loss: 4.66061, LR: 0.0010000, Tokens/sec: 39363.20\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 450, Eval Loss: 4.52563\n",
      "Step: 451, Training Loss: 4.54579, LR: 0.0010000, Tokens/sec: 41320.76\n",
      "Step: 452, Training Loss: 4.66371, LR: 0.0010000, Tokens/sec: 40815.63\n",
      "Step: 453, Training Loss: 4.41188, LR: 0.0010000, Tokens/sec: 38293.30\n",
      "Step: 454, Training Loss: 4.22111, LR: 0.0010000, Tokens/sec: 40676.15\n",
      "Step: 455, Training Loss: 4.43201, LR: 0.0010000, Tokens/sec: 41455.42\n",
      "Step: 456, Training Loss: 4.56347, LR: 0.0010000, Tokens/sec: 41681.56\n",
      "Step: 457, Training Loss: 4.75843, LR: 0.0010000, Tokens/sec: 40580.73\n",
      "Step: 458, Training Loss: 4.78761, LR: 0.0010000, Tokens/sec: 39004.53\n",
      "Step: 459, Training Loss: 4.66538, LR: 0.0010000, Tokens/sec: 40227.12\n",
      "Step: 460, Training Loss: 4.55281, LR: 0.0010000, Tokens/sec: 40303.13\n",
      "Step: 461, Training Loss: 4.55357, LR: 0.0010000, Tokens/sec: 38867.66\n",
      "Step: 462, Training Loss: 4.24086, LR: 0.0010000, Tokens/sec: 39310.31\n",
      "Step: 463, Training Loss: 4.50752, LR: 0.0010000, Tokens/sec: 41231.05\n",
      "Step: 464, Training Loss: 4.27164, LR: 0.0010000, Tokens/sec: 32508.65\n",
      "Step: 465, Training Loss: 4.75052, LR: 0.0009999, Tokens/sec: 39375.87\n",
      "Step: 466, Training Loss: 4.15344, LR: 0.0009999, Tokens/sec: 37844.63\n",
      "Step: 467, Training Loss: 4.38164, LR: 0.0009999, Tokens/sec: 38858.27\n",
      "Step: 468, Training Loss: 4.24338, LR: 0.0009999, Tokens/sec: 40406.24\n",
      "Step: 469, Training Loss: 4.43502, LR: 0.0009999, Tokens/sec: 40512.03\n",
      "Step: 470, Training Loss: 4.54941, LR: 0.0009999, Tokens/sec: 40406.10\n",
      "Step: 471, Training Loss: 4.21277, LR: 0.0009999, Tokens/sec: 38198.09\n",
      "Step: 472, Training Loss: 4.51440, LR: 0.0009999, Tokens/sec: 38634.59\n",
      "Step: 473, Training Loss: 4.35738, LR: 0.0009999, Tokens/sec: 41104.97\n",
      "Step: 474, Training Loss: 4.62276, LR: 0.0009999, Tokens/sec: 40947.63\n",
      "Step: 475, Training Loss: 4.41362, LR: 0.0009999, Tokens/sec: 41558.90\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 475, Eval Loss: 4.48528\n",
      "Step: 476, Training Loss: 4.65884, LR: 0.0009999, Tokens/sec: 39092.16\n",
      "Step: 477, Training Loss: 4.51132, LR: 0.0009999, Tokens/sec: 41625.62\n",
      "Step: 478, Training Loss: 4.69279, LR: 0.0009998, Tokens/sec: 41564.56\n",
      "Step: 479, Training Loss: 4.26983, LR: 0.0009998, Tokens/sec: 36599.04\n",
      "Step: 480, Training Loss: 4.12154, LR: 0.0009998, Tokens/sec: 30384.90\n",
      "Step: 481, Training Loss: 4.50819, LR: 0.0009998, Tokens/sec: 40790.52\n",
      "Step: 482, Training Loss: 4.53693, LR: 0.0009998, Tokens/sec: 40553.05\n",
      "Step: 483, Training Loss: 4.45717, LR: 0.0009998, Tokens/sec: 37707.94\n",
      "Step: 484, Training Loss: 4.39492, LR: 0.0009998, Tokens/sec: 39550.01\n",
      "Step: 485, Training Loss: 4.65522, LR: 0.0009998, Tokens/sec: 40488.61\n",
      "Step: 486, Training Loss: 4.57216, LR: 0.0009998, Tokens/sec: 39489.19\n",
      "Step: 487, Training Loss: 4.32624, LR: 0.0009998, Tokens/sec: 41315.36\n",
      "Step: 488, Training Loss: 4.48943, LR: 0.0009997, Tokens/sec: 40911.48\n",
      "Step: 489, Training Loss: 4.43035, LR: 0.0009997, Tokens/sec: 41062.38\n",
      "Step: 490, Training Loss: 4.43028, LR: 0.0009997, Tokens/sec: 37234.42\n",
      "Step: 491, Training Loss: 4.45798, LR: 0.0009997, Tokens/sec: 36426.49\n",
      "Step: 492, Training Loss: 4.48028, LR: 0.0009997, Tokens/sec: 39983.07\n",
      "Step: 493, Training Loss: 4.43013, LR: 0.0009997, Tokens/sec: 41179.52\n",
      "Step: 494, Training Loss: 4.06201, LR: 0.0009997, Tokens/sec: 41394.26\n",
      "Step: 495, Training Loss: 4.97298, LR: 0.0009997, Tokens/sec: 40841.32\n",
      "Step: 496, Training Loss: 4.23037, LR: 0.0009996, Tokens/sec: 40710.48\n",
      "Step: 497, Training Loss: 4.21253, LR: 0.0009996, Tokens/sec: 40845.94\n",
      "Step: 498, Training Loss: 4.35543, LR: 0.0009996, Tokens/sec: 41279.83\n",
      "Step: 499, Training Loss: 4.47210, LR: 0.0009996, Tokens/sec: 39396.71\n",
      "Step: 500, Training Loss: 4.57175, LR: 0.0009996, Tokens/sec: 41391.50\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 500, Eval Loss: 4.51459\n",
      "Step: 501, Training Loss: 4.61483, LR: 0.0009996, Tokens/sec: 39396.17\n",
      "Step: 502, Training Loss: 4.20640, LR: 0.0009996, Tokens/sec: 39706.99\n",
      "Step: 503, Training Loss: 4.27765, LR: 0.0009995, Tokens/sec: 41246.74\n",
      "Step: 504, Training Loss: 4.86043, LR: 0.0009995, Tokens/sec: 41362.62\n",
      "Step: 505, Training Loss: 4.60298, LR: 0.0009995, Tokens/sec: 40998.71\n",
      "Step: 506, Training Loss: 4.54040, LR: 0.0009995, Tokens/sec: 39257.65\n",
      "Step: 507, Training Loss: 4.44286, LR: 0.0009995, Tokens/sec: 41047.84\n",
      "Step: 508, Training Loss: 4.21683, LR: 0.0009995, Tokens/sec: 41277.62\n",
      "Step: 509, Training Loss: 4.62436, LR: 0.0009994, Tokens/sec: 41130.78\n",
      "Step: 510, Training Loss: 4.60969, LR: 0.0009994, Tokens/sec: 41177.50\n",
      "Step: 511, Training Loss: 4.16575, LR: 0.0009994, Tokens/sec: 38599.04\n",
      "Step: 512, Training Loss: 4.42134, LR: 0.0009994, Tokens/sec: 41043.94\n",
      "Step: 513, Training Loss: 4.48706, LR: 0.0009994, Tokens/sec: 40930.86\n",
      "Step: 514, Training Loss: 4.32791, LR: 0.0009993, Tokens/sec: 40136.12\n",
      "Step: 515, Training Loss: 4.45105, LR: 0.0009993, Tokens/sec: 41292.70\n",
      "Step: 516, Training Loss: 4.43718, LR: 0.0009993, Tokens/sec: 40921.68\n",
      "Step: 517, Training Loss: 4.29064, LR: 0.0009993, Tokens/sec: 40861.45\n",
      "Step: 518, Training Loss: 4.45128, LR: 0.0009993, Tokens/sec: 41180.50\n",
      "Step: 519, Training Loss: 4.42039, LR: 0.0009992, Tokens/sec: 38840.78\n",
      "Step: 520, Training Loss: 4.23969, LR: 0.0009992, Tokens/sec: 41035.87\n",
      "Step: 521, Training Loss: 4.43254, LR: 0.0009992, Tokens/sec: 41196.98\n",
      "Step: 522, Training Loss: 4.37972, LR: 0.0009992, Tokens/sec: 41277.46\n",
      "Step: 523, Training Loss: 4.47405, LR: 0.0009992, Tokens/sec: 41040.82\n",
      "Step: 524, Training Loss: 4.98781, LR: 0.0009991, Tokens/sec: 38498.01\n",
      "Step: 525, Training Loss: 4.50529, LR: 0.0009991, Tokens/sec: 40104.63\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 525, Eval Loss: 4.42255\n",
      "Step: 526, Training Loss: 4.62364, LR: 0.0009991, Tokens/sec: 40367.73\n",
      "Step: 527, Training Loss: 4.65455, LR: 0.0009991, Tokens/sec: 40523.14\n",
      "Step: 528, Training Loss: 4.51412, LR: 0.0009990, Tokens/sec: 40483.15\n",
      "Step: 529, Training Loss: 4.43600, LR: 0.0009990, Tokens/sec: 38134.63\n",
      "Step: 530, Training Loss: 4.19481, LR: 0.0009990, Tokens/sec: 39375.18\n",
      "Step: 531, Training Loss: 4.25565, LR: 0.0009990, Tokens/sec: 40012.37\n",
      "Step: 532, Training Loss: 4.35381, LR: 0.0009990, Tokens/sec: 41087.73\n",
      "Step: 533, Training Loss: 4.37860, LR: 0.0009989, Tokens/sec: 40179.83\n",
      "Step: 534, Training Loss: 4.47307, LR: 0.0009989, Tokens/sec: 38684.21\n",
      "Step: 535, Training Loss: 4.13240, LR: 0.0009989, Tokens/sec: 40580.33\n",
      "Step: 536, Training Loss: 4.18669, LR: 0.0009989, Tokens/sec: 40835.59\n",
      "Step: 537, Training Loss: 4.16404, LR: 0.0009988, Tokens/sec: 40741.99\n",
      "Step: 538, Training Loss: 4.32073, LR: 0.0009988, Tokens/sec: 41017.62\n",
      "Step: 539, Training Loss: 4.40250, LR: 0.0009988, Tokens/sec: 34412.81\n",
      "Step: 540, Training Loss: 4.37473, LR: 0.0009988, Tokens/sec: 40302.36\n",
      "Step: 541, Training Loss: 4.55742, LR: 0.0009987, Tokens/sec: 40648.16\n",
      "Step: 542, Training Loss: 4.40050, LR: 0.0009987, Tokens/sec: 41227.86\n",
      "Step: 543, Training Loss: 4.52993, LR: 0.0009987, Tokens/sec: 40985.03\n",
      "Step: 544, Training Loss: 4.61474, LR: 0.0009986, Tokens/sec: 41204.37\n",
      "Step: 545, Training Loss: 4.41418, LR: 0.0009986, Tokens/sec: 39054.02\n",
      "Step: 546, Training Loss: 4.19577, LR: 0.0009986, Tokens/sec: 41067.44\n",
      "Step: 547, Training Loss: 4.65919, LR: 0.0009986, Tokens/sec: 40994.29\n",
      "Step: 548, Training Loss: 4.59256, LR: 0.0009985, Tokens/sec: 41123.79\n",
      "Step: 549, Training Loss: 4.19452, LR: 0.0009985, Tokens/sec: 40446.65\n",
      "Step: 550, Training Loss: 4.29614, LR: 0.0009985, Tokens/sec: 40789.26\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 550, Eval Loss: 4.35240\n",
      "Step: 551, Training Loss: 4.29997, LR: 0.0009984, Tokens/sec: 41293.35\n",
      "Step: 552, Training Loss: 4.50982, LR: 0.0009984, Tokens/sec: 40485.31\n",
      "Step: 553, Training Loss: 4.43490, LR: 0.0009984, Tokens/sec: 39908.36\n",
      "Step: 554, Training Loss: 4.13859, LR: 0.0009984, Tokens/sec: 41099.53\n",
      "Step: 555, Training Loss: 4.55221, LR: 0.0009983, Tokens/sec: 38576.01\n",
      "Step: 556, Training Loss: 4.51295, LR: 0.0009983, Tokens/sec: 41162.24\n",
      "Step: 557, Training Loss: 4.45442, LR: 0.0009983, Tokens/sec: 40381.18\n",
      "Step: 558, Training Loss: 4.72091, LR: 0.0009982, Tokens/sec: 40188.67\n",
      "Step: 559, Training Loss: 4.23403, LR: 0.0009982, Tokens/sec: 41032.08\n",
      "Step: 560, Training Loss: 4.57426, LR: 0.0009982, Tokens/sec: 41316.14\n",
      "Step: 561, Training Loss: 4.34551, LR: 0.0009981, Tokens/sec: 41033.13\n",
      "Step: 562, Training Loss: 4.44349, LR: 0.0009981, Tokens/sec: 40818.02\n",
      "Step: 563, Training Loss: 4.29311, LR: 0.0009981, Tokens/sec: 38717.34\n",
      "Step: 564, Training Loss: 4.09304, LR: 0.0009980, Tokens/sec: 41309.83\n",
      "Step: 565, Training Loss: 4.53890, LR: 0.0009980, Tokens/sec: 39808.64\n",
      "Step: 566, Training Loss: 4.39015, LR: 0.0009980, Tokens/sec: 40665.14\n",
      "Step: 567, Training Loss: 4.21822, LR: 0.0009979, Tokens/sec: 37550.86\n",
      "Step: 568, Training Loss: 4.51768, LR: 0.0009979, Tokens/sec: 39016.79\n",
      "Step: 569, Training Loss: 4.25271, LR: 0.0009979, Tokens/sec: 40785.97\n",
      "Step: 570, Training Loss: 4.34296, LR: 0.0009978, Tokens/sec: 38584.93\n",
      "Step: 571, Training Loss: 4.61343, LR: 0.0009978, Tokens/sec: 39883.49\n",
      "Step: 572, Training Loss: 4.13680, LR: 0.0009978, Tokens/sec: 40804.95\n",
      "Step: 573, Training Loss: 4.38835, LR: 0.0009977, Tokens/sec: 40426.39\n",
      "Step: 574, Training Loss: 4.19672, LR: 0.0009977, Tokens/sec: 40376.57\n",
      "Step: 575, Training Loss: 4.21941, LR: 0.0009977, Tokens/sec: 39569.03\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 575, Eval Loss: 4.31595\n",
      "Step: 576, Training Loss: 4.11301, LR: 0.0009976, Tokens/sec: 40479.01\n",
      "Step: 577, Training Loss: 4.52755, LR: 0.0009976, Tokens/sec: 41242.70\n",
      "Step: 578, Training Loss: 4.30715, LR: 0.0009976, Tokens/sec: 38966.66\n",
      "Step: 579, Training Loss: 3.98565, LR: 0.0009975, Tokens/sec: 41114.62\n",
      "Step: 580, Training Loss: 4.69918, LR: 0.0009975, Tokens/sec: 40980.48\n",
      "Step: 581, Training Loss: 4.12445, LR: 0.0009974, Tokens/sec: 41027.48\n",
      "Step: 582, Training Loss: 4.37027, LR: 0.0009974, Tokens/sec: 40680.42\n",
      "Step: 583, Training Loss: 4.27596, LR: 0.0009974, Tokens/sec: 39027.10\n",
      "Step: 584, Training Loss: 4.04212, LR: 0.0009973, Tokens/sec: 41025.85\n",
      "Step: 585, Training Loss: 4.09872, LR: 0.0009973, Tokens/sec: 41315.12\n",
      "Step: 586, Training Loss: 3.94355, LR: 0.0009973, Tokens/sec: 40881.53\n",
      "Step: 587, Training Loss: 4.59876, LR: 0.0009972, Tokens/sec: 40758.93\n",
      "Step: 588, Training Loss: 4.53602, LR: 0.0009972, Tokens/sec: 40925.48\n",
      "Step: 589, Training Loss: 4.49802, LR: 0.0009971, Tokens/sec: 40529.77\n",
      "Step: 590, Training Loss: 3.93650, LR: 0.0009971, Tokens/sec: 40734.64\n",
      "Step: 591, Training Loss: 4.22009, LR: 0.0009971, Tokens/sec: 39134.40\n",
      "Step: 592, Training Loss: 4.43502, LR: 0.0009970, Tokens/sec: 40281.73\n",
      "Step: 593, Training Loss: 5.02865, LR: 0.0009970, Tokens/sec: 41092.40\n",
      "Step: 594, Training Loss: 4.15104, LR: 0.0009969, Tokens/sec: 40207.24\n",
      "Step: 595, Training Loss: 4.27528, LR: 0.0009969, Tokens/sec: 40355.20\n",
      "Step: 596, Training Loss: 4.25574, LR: 0.0009969, Tokens/sec: 39043.37\n",
      "Step: 597, Training Loss: 4.19033, LR: 0.0009968, Tokens/sec: 41161.68\n",
      "Step: 598, Training Loss: 4.47672, LR: 0.0009968, Tokens/sec: 40735.99\n",
      "Step: 599, Training Loss: 4.48405, LR: 0.0009967, Tokens/sec: 40763.74\n",
      "Step: 600, Training Loss: 4.21163, LR: 0.0009967, Tokens/sec: 41154.82\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 600, Eval Loss: 4.29754\n",
      "Step: 601, Training Loss: 4.21796, LR: 0.0009966, Tokens/sec: 33327.31\n",
      "Step: 602, Training Loss: 4.27025, LR: 0.0009966, Tokens/sec: 39303.06\n",
      "Step: 603, Training Loss: 4.26445, LR: 0.0009966, Tokens/sec: 41689.42\n",
      "Step: 604, Training Loss: 4.29994, LR: 0.0009965, Tokens/sec: 40416.33\n",
      "Step: 605, Training Loss: 4.17377, LR: 0.0009965, Tokens/sec: 35329.85\n",
      "Step: 606, Training Loss: 4.07899, LR: 0.0009964, Tokens/sec: 40710.94\n",
      "Step: 607, Training Loss: 4.48674, LR: 0.0009964, Tokens/sec: 38584.68\n",
      "Step: 608, Training Loss: 3.92933, LR: 0.0009963, Tokens/sec: 35151.96\n",
      "Step: 609, Training Loss: 4.37381, LR: 0.0009963, Tokens/sec: 37607.14\n",
      "Step: 610, Training Loss: 4.31053, LR: 0.0009962, Tokens/sec: 37111.11\n",
      "Step: 611, Training Loss: 3.86891, LR: 0.0009962, Tokens/sec: 41578.57\n",
      "Step: 612, Training Loss: 4.27677, LR: 0.0009961, Tokens/sec: 41665.08\n",
      "Step: 613, Training Loss: 4.28919, LR: 0.0009961, Tokens/sec: 32082.37\n",
      "Step: 614, Training Loss: 4.37847, LR: 0.0009961, Tokens/sec: 28710.12\n",
      "Step: 615, Training Loss: 4.22594, LR: 0.0009960, Tokens/sec: 38836.21\n",
      "Step: 616, Training Loss: 3.96549, LR: 0.0009960, Tokens/sec: 35438.02\n",
      "Step: 617, Training Loss: 4.38204, LR: 0.0009959, Tokens/sec: 36840.28\n",
      "Step: 618, Training Loss: 4.43678, LR: 0.0009959, Tokens/sec: 37324.55\n",
      "Step: 619, Training Loss: 3.95735, LR: 0.0009958, Tokens/sec: 35716.39\n",
      "Step: 620, Training Loss: 4.30638, LR: 0.0009958, Tokens/sec: 36600.71\n",
      "Step: 621, Training Loss: 4.57660, LR: 0.0009957, Tokens/sec: 40718.42\n",
      "Step: 622, Training Loss: 4.31497, LR: 0.0009957, Tokens/sec: 29127.08\n",
      "Step: 623, Training Loss: 4.02706, LR: 0.0009956, Tokens/sec: 32996.71\n",
      "Step: 624, Training Loss: 4.24443, LR: 0.0009956, Tokens/sec: 41600.61\n",
      "Step: 625, Training Loss: 4.22458, LR: 0.0009955, Tokens/sec: 41712.47\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 625, Eval Loss: 4.32814\n",
      "Step: 626, Training Loss: 4.26392, LR: 0.0009955, Tokens/sec: 34593.67\n",
      "Step: 627, Training Loss: 4.35965, LR: 0.0009954, Tokens/sec: 41238.96\n",
      "Step: 628, Training Loss: 4.15681, LR: 0.0009954, Tokens/sec: 30814.57\n",
      "Step: 629, Training Loss: 4.55559, LR: 0.0009953, Tokens/sec: 40113.29\n",
      "Step: 630, Training Loss: 4.33304, LR: 0.0009953, Tokens/sec: 36419.97\n",
      "Step: 631, Training Loss: 4.27085, LR: 0.0009952, Tokens/sec: 36454.54\n",
      "Step: 632, Training Loss: 4.54172, LR: 0.0009952, Tokens/sec: 41603.26\n",
      "Step: 633, Training Loss: 4.23489, LR: 0.0009951, Tokens/sec: 41594.39\n",
      "Step: 634, Training Loss: 4.28296, LR: 0.0009951, Tokens/sec: 33136.44\n",
      "Step: 635, Training Loss: 4.43477, LR: 0.0009950, Tokens/sec: 28005.56\n",
      "Step: 636, Training Loss: 3.91479, LR: 0.0009950, Tokens/sec: 30401.68\n",
      "Step: 637, Training Loss: 4.40355, LR: 0.0009949, Tokens/sec: 38261.82\n",
      "Step: 638, Training Loss: 3.89653, LR: 0.0009949, Tokens/sec: 32031.95\n",
      "Step: 639, Training Loss: 4.48522, LR: 0.0009948, Tokens/sec: 35934.25\n",
      "Step: 640, Training Loss: 4.43927, LR: 0.0009948, Tokens/sec: 40442.80\n",
      "Step: 641, Training Loss: 4.27573, LR: 0.0009947, Tokens/sec: 41593.21\n",
      "Step: 642, Training Loss: 4.27272, LR: 0.0009946, Tokens/sec: 38796.03\n",
      "Step: 643, Training Loss: 4.13497, LR: 0.0009946, Tokens/sec: 37957.99\n",
      "Step: 644, Training Loss: 4.11116, LR: 0.0009945, Tokens/sec: 37103.20\n",
      "Step: 645, Training Loss: 4.12160, LR: 0.0009945, Tokens/sec: 40910.89\n",
      "Step: 646, Training Loss: 4.11999, LR: 0.0009944, Tokens/sec: 40901.44\n",
      "Step: 647, Training Loss: 4.06118, LR: 0.0009944, Tokens/sec: 38377.80\n",
      "Step: 648, Training Loss: 4.23682, LR: 0.0009943, Tokens/sec: 27034.79\n",
      "Step: 649, Training Loss: 4.20228, LR: 0.0009943, Tokens/sec: 29058.62\n",
      "Step: 650, Training Loss: 4.28380, LR: 0.0009942, Tokens/sec: 38674.76\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 650, Eval Loss: 4.21147\n",
      "Step: 651, Training Loss: 4.33306, LR: 0.0009941, Tokens/sec: 41665.68\n",
      "Step: 652, Training Loss: 4.31209, LR: 0.0009941, Tokens/sec: 40203.22\n",
      "Step: 653, Training Loss: 4.24677, LR: 0.0009940, Tokens/sec: 41586.25\n",
      "Step: 654, Training Loss: 4.55394, LR: 0.0009940, Tokens/sec: 41642.79\n",
      "Step: 655, Training Loss: 4.25501, LR: 0.0009939, Tokens/sec: 40544.46\n",
      "Step: 656, Training Loss: 4.19761, LR: 0.0009939, Tokens/sec: 41742.67\n",
      "Step: 657, Training Loss: 4.42377, LR: 0.0009938, Tokens/sec: 41739.47\n",
      "Step: 658, Training Loss: 3.96533, LR: 0.0009937, Tokens/sec: 41757.63\n",
      "Step: 659, Training Loss: 4.15101, LR: 0.0009937, Tokens/sec: 41755.78\n",
      "Step: 660, Training Loss: 4.10086, LR: 0.0009936, Tokens/sec: 41831.38\n",
      "Step: 661, Training Loss: 4.30891, LR: 0.0009936, Tokens/sec: 41831.00\n",
      "Step: 662, Training Loss: 4.17780, LR: 0.0009935, Tokens/sec: 41434.38\n",
      "Step: 663, Training Loss: 4.15000, LR: 0.0009934, Tokens/sec: 41637.13\n",
      "Step: 664, Training Loss: 4.12313, LR: 0.0009934, Tokens/sec: 41427.75\n",
      "Step: 665, Training Loss: 4.40156, LR: 0.0009933, Tokens/sec: 41516.28\n",
      "Step: 666, Training Loss: 4.37166, LR: 0.0009933, Tokens/sec: 40073.28\n",
      "Step: 667, Training Loss: 4.14418, LR: 0.0009932, Tokens/sec: 41295.79\n",
      "Step: 668, Training Loss: 4.27593, LR: 0.0009931, Tokens/sec: 39936.19\n",
      "Step: 669, Training Loss: 4.37681, LR: 0.0009931, Tokens/sec: 39189.15\n",
      "Step: 670, Training Loss: 4.13120, LR: 0.0009930, Tokens/sec: 38174.30\n",
      "Step: 671, Training Loss: 4.40893, LR: 0.0009930, Tokens/sec: 41763.69\n",
      "Step: 672, Training Loss: 4.22924, LR: 0.0009929, Tokens/sec: 41690.87\n",
      "Step: 673, Training Loss: 4.14206, LR: 0.0009928, Tokens/sec: 41482.87\n",
      "Step: 674, Training Loss: 4.36576, LR: 0.0009928, Tokens/sec: 41671.29\n",
      "Step: 675, Training Loss: 4.19469, LR: 0.0009927, Tokens/sec: 40125.21\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 675, Eval Loss: 4.27181\n",
      "Step: 676, Training Loss: 4.15626, LR: 0.0009926, Tokens/sec: 41601.67\n",
      "Step: 677, Training Loss: 3.86374, LR: 0.0009926, Tokens/sec: 41574.16\n",
      "Step: 678, Training Loss: 4.09039, LR: 0.0009925, Tokens/sec: 41124.72\n",
      "Step: 679, Training Loss: 4.35947, LR: 0.0009924, Tokens/sec: 41722.38\n",
      "Step: 680, Training Loss: 4.03614, LR: 0.0009924, Tokens/sec: 41718.39\n",
      "Step: 681, Training Loss: 4.12719, LR: 0.0009923, Tokens/sec: 41653.12\n",
      "Step: 682, Training Loss: 4.16920, LR: 0.0009923, Tokens/sec: 41549.07\n",
      "Step: 683, Training Loss: 4.46025, LR: 0.0009922, Tokens/sec: 41564.85\n",
      "Step: 684, Training Loss: 4.56699, LR: 0.0009921, Tokens/sec: 41675.95\n",
      "Step: 685, Training Loss: 4.24168, LR: 0.0009921, Tokens/sec: 41290.08\n",
      "Step: 686, Training Loss: 4.12700, LR: 0.0009920, Tokens/sec: 41581.66\n",
      "Step: 687, Training Loss: 4.48258, LR: 0.0009919, Tokens/sec: 41549.41\n",
      "Step: 688, Training Loss: 4.00334, LR: 0.0009919, Tokens/sec: 41673.02\n",
      "Step: 689, Training Loss: 4.17760, LR: 0.0009918, Tokens/sec: 41707.03\n",
      "Step: 690, Training Loss: 4.09156, LR: 0.0009917, Tokens/sec: 41674.40\n",
      "Step: 691, Training Loss: 4.29809, LR: 0.0009917, Tokens/sec: 40007.83\n",
      "Step: 692, Training Loss: 4.15965, LR: 0.0009916, Tokens/sec: 41148.85\n",
      "Step: 693, Training Loss: 4.53289, LR: 0.0009915, Tokens/sec: 39875.17\n",
      "Step: 694, Training Loss: 4.05305, LR: 0.0009915, Tokens/sec: 40659.95\n",
      "Step: 695, Training Loss: 4.39859, LR: 0.0009914, Tokens/sec: 41703.35\n",
      "Step: 696, Training Loss: 3.97426, LR: 0.0009913, Tokens/sec: 41435.65\n",
      "Step: 697, Training Loss: 4.18028, LR: 0.0009912, Tokens/sec: 39643.31\n",
      "Step: 698, Training Loss: 4.33618, LR: 0.0009912, Tokens/sec: 41690.55\n",
      "Step: 699, Training Loss: 4.29028, LR: 0.0009911, Tokens/sec: 41568.78\n",
      "Step: 700, Training Loss: 4.26675, LR: 0.0009910, Tokens/sec: 41658.64\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 700, Eval Loss: 4.13411\n",
      "Step: 701, Training Loss: 4.51292, LR: 0.0009910, Tokens/sec: 41614.41\n",
      "Step: 702, Training Loss: 4.36371, LR: 0.0009909, Tokens/sec: 41292.06\n",
      "Step: 703, Training Loss: 3.94989, LR: 0.0009908, Tokens/sec: 41695.20\n",
      "Step: 704, Training Loss: 4.44457, LR: 0.0009908, Tokens/sec: 41266.23\n",
      "Step: 705, Training Loss: 4.01707, LR: 0.0009907, Tokens/sec: 41705.52\n",
      "Step: 706, Training Loss: 4.03450, LR: 0.0009906, Tokens/sec: 41659.00\n",
      "Step: 707, Training Loss: 3.75615, LR: 0.0009905, Tokens/sec: 41558.24\n",
      "Step: 708, Training Loss: 3.87293, LR: 0.0009905, Tokens/sec: 41630.43\n",
      "Step: 709, Training Loss: 4.17497, LR: 0.0009904, Tokens/sec: 41496.36\n",
      "Step: 710, Training Loss: 4.47078, LR: 0.0009903, Tokens/sec: 41652.78\n",
      "Step: 711, Training Loss: 3.86353, LR: 0.0009902, Tokens/sec: 41288.64\n",
      "Step: 712, Training Loss: 4.00516, LR: 0.0009902, Tokens/sec: 41703.40\n",
      "Step: 713, Training Loss: 4.78148, LR: 0.0009901, Tokens/sec: 41670.49\n",
      "Step: 714, Training Loss: 4.15302, LR: 0.0009900, Tokens/sec: 41755.33\n",
      "Step: 715, Training Loss: 4.21183, LR: 0.0009900, Tokens/sec: 41613.76\n",
      "Step: 716, Training Loss: 4.09626, LR: 0.0009899, Tokens/sec: 41140.69\n",
      "Step: 717, Training Loss: 4.32803, LR: 0.0009898, Tokens/sec: 41788.37\n",
      "Step: 718, Training Loss: 3.91023, LR: 0.0009897, Tokens/sec: 41679.19\n",
      "Step: 719, Training Loss: 4.19671, LR: 0.0009897, Tokens/sec: 41712.15\n",
      "Step: 720, Training Loss: 4.08001, LR: 0.0009896, Tokens/sec: 35637.89\n",
      "Step: 721, Training Loss: 4.18938, LR: 0.0009895, Tokens/sec: 37342.49\n",
      "Step: 722, Training Loss: 4.13972, LR: 0.0009894, Tokens/sec: 34825.24\n",
      "Step: 723, Training Loss: 4.33598, LR: 0.0009894, Tokens/sec: 41531.14\n",
      "Step: 724, Training Loss: 4.16222, LR: 0.0009893, Tokens/sec: 41550.67\n",
      "Step: 725, Training Loss: 3.77480, LR: 0.0009892, Tokens/sec: 41228.38\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 725, Eval Loss: 4.14919\n",
      "Step: 726, Training Loss: 3.84913, LR: 0.0009891, Tokens/sec: 40321.10\n",
      "Step: 727, Training Loss: 3.86311, LR: 0.0009890, Tokens/sec: 41042.37\n",
      "Step: 728, Training Loss: 4.18317, LR: 0.0009890, Tokens/sec: 41674.92\n",
      "Step: 729, Training Loss: 3.76940, LR: 0.0009889, Tokens/sec: 40287.60\n",
      "Step: 730, Training Loss: 4.10072, LR: 0.0009888, Tokens/sec: 36993.87\n",
      "Step: 731, Training Loss: 3.83456, LR: 0.0009887, Tokens/sec: 38185.59\n",
      "Step: 732, Training Loss: 4.18061, LR: 0.0009887, Tokens/sec: 39763.01\n",
      "Step: 733, Training Loss: 4.30603, LR: 0.0009886, Tokens/sec: 40513.84\n",
      "Step: 734, Training Loss: 4.31564, LR: 0.0009885, Tokens/sec: 30537.94\n",
      "Step: 735, Training Loss: 4.09873, LR: 0.0009884, Tokens/sec: 30668.54\n",
      "Step: 736, Training Loss: 4.21269, LR: 0.0009883, Tokens/sec: 40135.82\n",
      "Step: 737, Training Loss: 4.40360, LR: 0.0009883, Tokens/sec: 39589.29\n",
      "Step: 738, Training Loss: 4.36860, LR: 0.0009882, Tokens/sec: 37940.00\n",
      "Step: 739, Training Loss: 4.11190, LR: 0.0009881, Tokens/sec: 40677.65\n",
      "Step: 740, Training Loss: 4.01457, LR: 0.0009880, Tokens/sec: 41706.09\n",
      "Step: 741, Training Loss: 4.29229, LR: 0.0009879, Tokens/sec: 41509.17\n",
      "Step: 742, Training Loss: 4.15617, LR: 0.0009879, Tokens/sec: 41798.71\n",
      "Step: 743, Training Loss: 4.04976, LR: 0.0009878, Tokens/sec: 41806.10\n",
      "Step: 744, Training Loss: 4.18336, LR: 0.0009877, Tokens/sec: 41418.47\n",
      "Step: 745, Training Loss: 4.06238, LR: 0.0009876, Tokens/sec: 41830.84\n",
      "Step: 746, Training Loss: 3.85777, LR: 0.0009875, Tokens/sec: 41517.97\n",
      "Step: 747, Training Loss: 4.03244, LR: 0.0009874, Tokens/sec: 41817.05\n",
      "Step: 748, Training Loss: 3.87676, LR: 0.0009874, Tokens/sec: 41671.56\n",
      "Step: 749, Training Loss: 4.09960, LR: 0.0009873, Tokens/sec: 41358.18\n",
      "Step: 750, Training Loss: 3.91196, LR: 0.0009872, Tokens/sec: 41744.20\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 750, Eval Loss: 4.12126\n",
      "Step: 751, Training Loss: 4.09853, LR: 0.0009871, Tokens/sec: 41533.11\n",
      "Step: 752, Training Loss: 3.76863, LR: 0.0009870, Tokens/sec: 41681.84\n",
      "Step: 753, Training Loss: 3.72340, LR: 0.0009869, Tokens/sec: 41737.46\n",
      "Step: 754, Training Loss: 4.04887, LR: 0.0009869, Tokens/sec: 41303.60\n",
      "Step: 755, Training Loss: 3.90263, LR: 0.0009868, Tokens/sec: 41419.27\n",
      "Step: 756, Training Loss: 4.06291, LR: 0.0009867, Tokens/sec: 41559.59\n",
      "Step: 757, Training Loss: 4.17191, LR: 0.0009866, Tokens/sec: 41616.96\n",
      "Step: 758, Training Loss: 4.06888, LR: 0.0009865, Tokens/sec: 41278.28\n",
      "Step: 759, Training Loss: 4.02777, LR: 0.0009864, Tokens/sec: 41776.02\n",
      "Step: 760, Training Loss: 4.14232, LR: 0.0009863, Tokens/sec: 41795.80\n",
      "Step: 761, Training Loss: 4.21314, LR: 0.0009863, Tokens/sec: 41734.87\n",
      "Step: 762, Training Loss: 4.21879, LR: 0.0009862, Tokens/sec: 41780.54\n",
      "Step: 763, Training Loss: 4.10675, LR: 0.0009861, Tokens/sec: 41816.13\n",
      "Step: 764, Training Loss: 4.06417, LR: 0.0009860, Tokens/sec: 41712.65\n",
      "Step: 765, Training Loss: 4.43702, LR: 0.0009859, Tokens/sec: 41710.45\n",
      "Step: 766, Training Loss: 3.83289, LR: 0.0009858, Tokens/sec: 41404.05\n",
      "Step: 767, Training Loss: 4.13978, LR: 0.0009857, Tokens/sec: 41850.44\n",
      "Step: 768, Training Loss: 4.20346, LR: 0.0009856, Tokens/sec: 41776.10\n",
      "Step: 769, Training Loss: 4.23292, LR: 0.0009856, Tokens/sec: 41765.24\n",
      "Step: 770, Training Loss: 4.17527, LR: 0.0009855, Tokens/sec: 41753.28\n",
      "Step: 771, Training Loss: 4.11456, LR: 0.0009854, Tokens/sec: 41845.52\n",
      "Step: 772, Training Loss: 4.31809, LR: 0.0009853, Tokens/sec: 41481.58\n",
      "Step: 773, Training Loss: 4.34318, LR: 0.0009852, Tokens/sec: 41186.48\n",
      "Step: 774, Training Loss: 3.97141, LR: 0.0009851, Tokens/sec: 37442.01\n",
      "Step: 775, Training Loss: 3.87814, LR: 0.0009850, Tokens/sec: 41069.00\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 775, Eval Loss: 4.10879\n",
      "Step: 776, Training Loss: 3.97262, LR: 0.0009849, Tokens/sec: 41544.06\n",
      "Step: 777, Training Loss: 3.95844, LR: 0.0009848, Tokens/sec: 40468.37\n",
      "Step: 778, Training Loss: 3.85249, LR: 0.0009848, Tokens/sec: 41211.24\n",
      "Step: 779, Training Loss: 4.21234, LR: 0.0009847, Tokens/sec: 41553.90\n",
      "Step: 780, Training Loss: 4.70622, LR: 0.0009846, Tokens/sec: 41092.56\n",
      "Step: 781, Training Loss: 3.70121, LR: 0.0009845, Tokens/sec: 38069.94\n",
      "Step: 782, Training Loss: 4.15418, LR: 0.0009844, Tokens/sec: 41272.73\n",
      "Step: 783, Training Loss: 4.05699, LR: 0.0009843, Tokens/sec: 41763.23\n",
      "Step: 784, Training Loss: 3.94548, LR: 0.0009842, Tokens/sec: 41772.03\n",
      "Step: 785, Training Loss: 4.28519, LR: 0.0009841, Tokens/sec: 40876.56\n",
      "Step: 786, Training Loss: 4.26387, LR: 0.0009840, Tokens/sec: 37248.64\n",
      "Step: 787, Training Loss: 4.24051, LR: 0.0009839, Tokens/sec: 38600.63\n",
      "Step: 788, Training Loss: 3.96139, LR: 0.0009838, Tokens/sec: 41198.80\n",
      "Step: 789, Training Loss: 4.38582, LR: 0.0009837, Tokens/sec: 40540.29\n",
      "Step: 790, Training Loss: 4.03741, LR: 0.0009836, Tokens/sec: 41398.58\n",
      "Step: 791, Training Loss: 4.15173, LR: 0.0009835, Tokens/sec: 41329.07\n",
      "Step: 792, Training Loss: 3.89672, LR: 0.0009834, Tokens/sec: 41692.56\n",
      "Step: 793, Training Loss: 4.27700, LR: 0.0009834, Tokens/sec: 41428.78\n",
      "Step: 794, Training Loss: 4.05997, LR: 0.0009833, Tokens/sec: 38374.08\n",
      "Step: 795, Training Loss: 3.86478, LR: 0.0009832, Tokens/sec: 41545.87\n",
      "Step: 796, Training Loss: 4.00113, LR: 0.0009831, Tokens/sec: 41192.33\n",
      "Step: 797, Training Loss: 4.16955, LR: 0.0009830, Tokens/sec: 41611.04\n",
      "Step: 798, Training Loss: 3.92046, LR: 0.0009829, Tokens/sec: 39427.23\n",
      "Step: 799, Training Loss: 4.17860, LR: 0.0009828, Tokens/sec: 41765.47\n",
      "Step: 800, Training Loss: 3.93337, LR: 0.0009827, Tokens/sec: 41319.10\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 800, Eval Loss: 4.08469\n",
      "Step: 801, Training Loss: 4.29452, LR: 0.0009826, Tokens/sec: 39364.35\n",
      "Step: 802, Training Loss: 4.14810, LR: 0.0009825, Tokens/sec: 41721.64\n",
      "Step: 803, Training Loss: 4.00438, LR: 0.0009824, Tokens/sec: 41487.47\n",
      "Step: 804, Training Loss: 3.93188, LR: 0.0009823, Tokens/sec: 41742.15\n",
      "Step: 805, Training Loss: 3.97358, LR: 0.0009822, Tokens/sec: 41661.25\n",
      "Step: 806, Training Loss: 3.81137, LR: 0.0009821, Tokens/sec: 41739.29\n",
      "Step: 807, Training Loss: 4.15904, LR: 0.0009820, Tokens/sec: 41805.91\n",
      "Step: 808, Training Loss: 4.13968, LR: 0.0009819, Tokens/sec: 41417.31\n",
      "Step: 809, Training Loss: 4.15383, LR: 0.0009818, Tokens/sec: 40818.13\n",
      "Step: 810, Training Loss: 4.19172, LR: 0.0009817, Tokens/sec: 28340.66\n",
      "Step: 811, Training Loss: 3.84286, LR: 0.0009816, Tokens/sec: 40220.04\n",
      "Step: 812, Training Loss: 4.09633, LR: 0.0009815, Tokens/sec: 39877.50\n",
      "Step: 813, Training Loss: 4.10088, LR: 0.0009814, Tokens/sec: 39236.75\n",
      "Step: 814, Training Loss: 4.30653, LR: 0.0009813, Tokens/sec: 41296.81\n",
      "Step: 815, Training Loss: 4.36530, LR: 0.0009812, Tokens/sec: 41277.24\n",
      "Step: 816, Training Loss: 4.29323, LR: 0.0009811, Tokens/sec: 41558.11\n",
      "Step: 817, Training Loss: 4.22413, LR: 0.0009810, Tokens/sec: 41233.31\n",
      "Step: 818, Training Loss: 3.82086, LR: 0.0009809, Tokens/sec: 41680.86\n",
      "Step: 819, Training Loss: 4.10693, LR: 0.0009808, Tokens/sec: 41607.99\n",
      "Step: 820, Training Loss: 4.09227, LR: 0.0009807, Tokens/sec: 41740.33\n",
      "Step: 821, Training Loss: 3.76291, LR: 0.0009806, Tokens/sec: 39593.89\n",
      "Step: 822, Training Loss: 3.94898, LR: 0.0009805, Tokens/sec: 41312.25\n",
      "Step: 823, Training Loss: 4.08154, LR: 0.0009804, Tokens/sec: 41175.88\n",
      "Step: 824, Training Loss: 3.82217, LR: 0.0009803, Tokens/sec: 40679.51\n",
      "Step: 825, Training Loss: 3.95530, LR: 0.0009802, Tokens/sec: 26477.19\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 825, Eval Loss: 4.09549\n",
      "Step: 826, Training Loss: 4.26485, LR: 0.0009801, Tokens/sec: 41053.26\n",
      "Step: 827, Training Loss: 4.06147, LR: 0.0009800, Tokens/sec: 41454.68\n",
      "Step: 828, Training Loss: 4.31415, LR: 0.0009799, Tokens/sec: 39954.90\n",
      "Step: 829, Training Loss: 4.32214, LR: 0.0009798, Tokens/sec: 41587.19\n",
      "Step: 830, Training Loss: 3.90091, LR: 0.0009797, Tokens/sec: 41615.19\n",
      "Step: 831, Training Loss: 4.30325, LR: 0.0009795, Tokens/sec: 41244.89\n",
      "Step: 832, Training Loss: 4.32073, LR: 0.0009794, Tokens/sec: 41666.77\n",
      "Step: 833, Training Loss: 3.69060, LR: 0.0009793, Tokens/sec: 41216.91\n",
      "Step: 834, Training Loss: 3.95173, LR: 0.0009792, Tokens/sec: 41573.09\n",
      "Step: 835, Training Loss: 4.16242, LR: 0.0009791, Tokens/sec: 41295.67\n",
      "Step: 836, Training Loss: 4.03945, LR: 0.0009790, Tokens/sec: 40214.17\n",
      "Step: 837, Training Loss: 4.25549, LR: 0.0009789, Tokens/sec: 38266.12\n",
      "Step: 838, Training Loss: 3.93581, LR: 0.0009788, Tokens/sec: 40622.48\n",
      "Step: 839, Training Loss: 4.02093, LR: 0.0009787, Tokens/sec: 41183.30\n",
      "Step: 840, Training Loss: 3.89805, LR: 0.0009786, Tokens/sec: 39931.35\n",
      "Step: 841, Training Loss: 4.16910, LR: 0.0009785, Tokens/sec: 41492.20\n",
      "Step: 842, Training Loss: 4.10032, LR: 0.0009784, Tokens/sec: 40710.66\n",
      "Step: 843, Training Loss: 3.90776, LR: 0.0009783, Tokens/sec: 41580.12\n",
      "Step: 844, Training Loss: 4.01295, LR: 0.0009782, Tokens/sec: 39989.05\n",
      "Step: 845, Training Loss: 4.06300, LR: 0.0009781, Tokens/sec: 40797.93\n",
      "Step: 846, Training Loss: 3.98459, LR: 0.0009779, Tokens/sec: 41570.32\n",
      "Step: 847, Training Loss: 4.07113, LR: 0.0009778, Tokens/sec: 41610.23\n",
      "Step: 848, Training Loss: 4.24020, LR: 0.0009777, Tokens/sec: 41727.89\n",
      "Step: 849, Training Loss: 3.74807, LR: 0.0009776, Tokens/sec: 41648.45\n",
      "Step: 850, Training Loss: 4.00879, LR: 0.0009775, Tokens/sec: 39531.47\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 850, Eval Loss: 4.02842\n",
      "Step: 851, Training Loss: 4.12027, LR: 0.0009774, Tokens/sec: 37811.69\n",
      "Step: 852, Training Loss: 4.26353, LR: 0.0009773, Tokens/sec: 41090.93\n",
      "Step: 853, Training Loss: 4.19242, LR: 0.0009772, Tokens/sec: 41547.24\n",
      "Step: 854, Training Loss: 3.86523, LR: 0.0009771, Tokens/sec: 41252.97\n",
      "Step: 855, Training Loss: 3.88920, LR: 0.0009770, Tokens/sec: 41654.60\n",
      "Step: 856, Training Loss: 4.25238, LR: 0.0009768, Tokens/sec: 41606.42\n",
      "Step: 857, Training Loss: 3.87698, LR: 0.0009767, Tokens/sec: 41579.50\n",
      "Step: 858, Training Loss: 4.34399, LR: 0.0009766, Tokens/sec: 41659.29\n",
      "Step: 859, Training Loss: 3.93018, LR: 0.0009765, Tokens/sec: 41655.66\n",
      "Step: 860, Training Loss: 4.06872, LR: 0.0009764, Tokens/sec: 41583.49\n",
      "Step: 861, Training Loss: 4.15256, LR: 0.0009763, Tokens/sec: 41235.42\n",
      "Step: 862, Training Loss: 4.13162, LR: 0.0009762, Tokens/sec: 41612.85\n",
      "Step: 863, Training Loss: 4.04503, LR: 0.0009760, Tokens/sec: 41323.97\n",
      "Step: 864, Training Loss: 4.07967, LR: 0.0009759, Tokens/sec: 40442.53\n",
      "Step: 865, Training Loss: 3.97732, LR: 0.0009758, Tokens/sec: 41593.84\n",
      "Step: 866, Training Loss: 3.94577, LR: 0.0009757, Tokens/sec: 41574.47\n",
      "Step: 867, Training Loss: 4.04716, LR: 0.0009756, Tokens/sec: 41582.72\n",
      "Step: 868, Training Loss: 3.96881, LR: 0.0009755, Tokens/sec: 41513.60\n",
      "Step: 869, Training Loss: 3.83885, LR: 0.0009754, Tokens/sec: 41500.26\n",
      "Step: 870, Training Loss: 3.90152, LR: 0.0009752, Tokens/sec: 34668.51\n",
      "Step: 871, Training Loss: 3.93911, LR: 0.0009751, Tokens/sec: 39849.65\n",
      "Step: 872, Training Loss: 3.88156, LR: 0.0009750, Tokens/sec: 40739.39\n",
      "Step: 873, Training Loss: 3.89846, LR: 0.0009749, Tokens/sec: 41641.54\n",
      "Step: 874, Training Loss: 4.13212, LR: 0.0009748, Tokens/sec: 38363.61\n",
      "Step: 875, Training Loss: 4.24807, LR: 0.0009747, Tokens/sec: 41620.86\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 875, Eval Loss: 4.05218\n",
      "Step: 876, Training Loss: 4.05795, LR: 0.0009746, Tokens/sec: 40013.25\n",
      "Step: 877, Training Loss: 3.96742, LR: 0.0009744, Tokens/sec: 41107.11\n",
      "Step: 878, Training Loss: 4.23716, LR: 0.0009743, Tokens/sec: 41473.97\n",
      "Step: 879, Training Loss: 4.00170, LR: 0.0009742, Tokens/sec: 41530.43\n",
      "Step: 880, Training Loss: 3.90105, LR: 0.0009741, Tokens/sec: 41285.94\n",
      "Step: 881, Training Loss: 3.98392, LR: 0.0009740, Tokens/sec: 41560.14\n",
      "Step: 882, Training Loss: 3.85705, LR: 0.0009738, Tokens/sec: 41588.27\n",
      "Step: 883, Training Loss: 3.72316, LR: 0.0009737, Tokens/sec: 41023.85\n",
      "Step: 884, Training Loss: 4.12533, LR: 0.0009736, Tokens/sec: 41602.72\n",
      "Step: 885, Training Loss: 4.28469, LR: 0.0009735, Tokens/sec: 41622.85\n",
      "Step: 886, Training Loss: 4.14835, LR: 0.0009734, Tokens/sec: 41594.30\n",
      "Step: 887, Training Loss: 3.93444, LR: 0.0009732, Tokens/sec: 41589.42\n",
      "Step: 888, Training Loss: 4.15075, LR: 0.0009731, Tokens/sec: 41662.55\n",
      "Step: 889, Training Loss: 4.10378, LR: 0.0009730, Tokens/sec: 41583.68\n",
      "Step: 890, Training Loss: 3.93228, LR: 0.0009729, Tokens/sec: 41594.24\n",
      "Step: 891, Training Loss: 4.06711, LR: 0.0009728, Tokens/sec: 41218.89\n",
      "Step: 892, Training Loss: 3.89108, LR: 0.0009726, Tokens/sec: 41561.62\n",
      "Step: 893, Training Loss: 4.00949, LR: 0.0009725, Tokens/sec: 41594.15\n",
      "Step: 894, Training Loss: 3.83629, LR: 0.0009724, Tokens/sec: 41490.09\n",
      "Step: 895, Training Loss: 4.07661, LR: 0.0009723, Tokens/sec: 41565.57\n",
      "Step: 896, Training Loss: 3.82477, LR: 0.0009722, Tokens/sec: 41380.02\n",
      "Step: 897, Training Loss: 4.04697, LR: 0.0009720, Tokens/sec: 41531.11\n",
      "Step: 898, Training Loss: 4.04871, LR: 0.0009719, Tokens/sec: 40013.25\n",
      "Step: 899, Training Loss: 3.97719, LR: 0.0009718, Tokens/sec: 41530.01\n",
      "Step: 900, Training Loss: 3.62566, LR: 0.0009717, Tokens/sec: 41150.98\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 900, Eval Loss: 4.02271\n",
      "Step: 901, Training Loss: 3.47247, LR: 0.0009715, Tokens/sec: 40791.68\n",
      "Step: 902, Training Loss: 3.95049, LR: 0.0009714, Tokens/sec: 41388.39\n",
      "Step: 903, Training Loss: 4.12089, LR: 0.0009713, Tokens/sec: 38618.63\n",
      "Step: 904, Training Loss: 3.76363, LR: 0.0009712, Tokens/sec: 39515.87\n",
      "Step: 905, Training Loss: 4.30057, LR: 0.0009711, Tokens/sec: 41530.91\n",
      "Step: 906, Training Loss: 4.15315, LR: 0.0009709, Tokens/sec: 41606.45\n",
      "Step: 907, Training Loss: 4.10709, LR: 0.0009708, Tokens/sec: 41218.03\n",
      "Step: 908, Training Loss: 4.10353, LR: 0.0009707, Tokens/sec: 41110.73\n",
      "Step: 909, Training Loss: 4.06828, LR: 0.0009706, Tokens/sec: 41561.09\n",
      "Step: 910, Training Loss: 4.14404, LR: 0.0009704, Tokens/sec: 41278.38\n",
      "Step: 911, Training Loss: 3.60988, LR: 0.0009703, Tokens/sec: 41605.88\n",
      "Step: 912, Training Loss: 3.79046, LR: 0.0009702, Tokens/sec: 39968.72\n",
      "Step: 913, Training Loss: 3.90358, LR: 0.0009700, Tokens/sec: 39735.40\n",
      "Step: 914, Training Loss: 3.67704, LR: 0.0009699, Tokens/sec: 40479.04\n",
      "Step: 915, Training Loss: 3.78747, LR: 0.0009698, Tokens/sec: 41596.94\n",
      "Step: 916, Training Loss: 4.23474, LR: 0.0009697, Tokens/sec: 41599.60\n",
      "Step: 917, Training Loss: 4.26683, LR: 0.0009695, Tokens/sec: 41252.45\n",
      "Step: 918, Training Loss: 4.15786, LR: 0.0009694, Tokens/sec: 41597.15\n",
      "Step: 919, Training Loss: 3.92896, LR: 0.0009693, Tokens/sec: 41226.72\n",
      "Step: 920, Training Loss: 3.87176, LR: 0.0009692, Tokens/sec: 41225.47\n",
      "Step: 921, Training Loss: 4.17579, LR: 0.0009690, Tokens/sec: 41454.94\n",
      "Step: 922, Training Loss: 3.95748, LR: 0.0009689, Tokens/sec: 41152.36\n",
      "Step: 923, Training Loss: 3.64960, LR: 0.0009688, Tokens/sec: 39958.21\n",
      "Step: 924, Training Loss: 3.92789, LR: 0.0009686, Tokens/sec: 39509.54\n",
      "Step: 925, Training Loss: 3.84890, LR: 0.0009685, Tokens/sec: 41438.23\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 925, Eval Loss: 4.01241\n",
      "Step: 926, Training Loss: 4.00676, LR: 0.0009684, Tokens/sec: 41537.14\n",
      "Step: 927, Training Loss: 4.44265, LR: 0.0009682, Tokens/sec: 41581.88\n",
      "Step: 928, Training Loss: 3.93714, LR: 0.0009681, Tokens/sec: 40455.97\n",
      "Step: 929, Training Loss: 4.19309, LR: 0.0009680, Tokens/sec: 41190.90\n",
      "Step: 930, Training Loss: 3.79332, LR: 0.0009679, Tokens/sec: 41596.03\n",
      "Step: 931, Training Loss: 3.89396, LR: 0.0009677, Tokens/sec: 41623.45\n",
      "Step: 932, Training Loss: 4.11812, LR: 0.0009676, Tokens/sec: 41526.48\n",
      "Step: 933, Training Loss: 4.09932, LR: 0.0009675, Tokens/sec: 41597.51\n",
      "Step: 934, Training Loss: 3.90684, LR: 0.0009673, Tokens/sec: 40836.35\n",
      "Step: 935, Training Loss: 4.16921, LR: 0.0009672, Tokens/sec: 41596.94\n",
      "Step: 936, Training Loss: 4.04498, LR: 0.0009671, Tokens/sec: 41494.35\n",
      "Step: 937, Training Loss: 4.20317, LR: 0.0009669, Tokens/sec: 41509.62\n",
      "Step: 938, Training Loss: 3.98008, LR: 0.0009668, Tokens/sec: 39961.75\n",
      "Step: 939, Training Loss: 3.78368, LR: 0.0009667, Tokens/sec: 39799.85\n",
      "Step: 940, Training Loss: 4.12244, LR: 0.0009665, Tokens/sec: 40325.43\n",
      "Step: 941, Training Loss: 3.97803, LR: 0.0009664, Tokens/sec: 41109.61\n",
      "Step: 942, Training Loss: 4.00759, LR: 0.0009663, Tokens/sec: 39594.28\n",
      "Step: 943, Training Loss: 3.86948, LR: 0.0009661, Tokens/sec: 40309.87\n",
      "Step: 944, Training Loss: 3.78522, LR: 0.0009660, Tokens/sec: 41501.94\n",
      "Step: 945, Training Loss: 4.16958, LR: 0.0009659, Tokens/sec: 41383.60\n",
      "Step: 946, Training Loss: 3.88698, LR: 0.0009657, Tokens/sec: 39970.22\n",
      "Step: 947, Training Loss: 3.85541, LR: 0.0009656, Tokens/sec: 40189.32\n",
      "Step: 948, Training Loss: 3.94739, LR: 0.0009655, Tokens/sec: 41532.23\n",
      "Step: 949, Training Loss: 3.86614, LR: 0.0009653, Tokens/sec: 39761.27\n",
      "Step: 950, Training Loss: 3.87457, LR: 0.0009652, Tokens/sec: 41214.74\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 950, Eval Loss: 3.92802\n",
      "Step: 951, Training Loss: 3.70354, LR: 0.0009651, Tokens/sec: 40139.30\n",
      "Step: 952, Training Loss: 4.36742, LR: 0.0009649, Tokens/sec: 40866.89\n",
      "Step: 953, Training Loss: 4.14248, LR: 0.0009648, Tokens/sec: 41109.81\n",
      "Step: 954, Training Loss: 4.26546, LR: 0.0009646, Tokens/sec: 41726.38\n",
      "Step: 955, Training Loss: 4.17018, LR: 0.0009645, Tokens/sec: 41192.18\n",
      "Step: 956, Training Loss: 3.69737, LR: 0.0009644, Tokens/sec: 39212.44\n",
      "Step: 957, Training Loss: 4.20407, LR: 0.0009642, Tokens/sec: 41452.03\n",
      "Step: 958, Training Loss: 3.90288, LR: 0.0009641, Tokens/sec: 41705.27\n",
      "Step: 959, Training Loss: 4.13775, LR: 0.0009640, Tokens/sec: 41678.94\n",
      "Step: 960, Training Loss: 3.94522, LR: 0.0009638, Tokens/sec: 40756.37\n",
      "Step: 961, Training Loss: 3.70003, LR: 0.0009637, Tokens/sec: 41752.66\n",
      "Step: 962, Training Loss: 3.96608, LR: 0.0009635, Tokens/sec: 41753.22\n",
      "Step: 963, Training Loss: 3.97919, LR: 0.0009634, Tokens/sec: 41794.21\n",
      "Step: 964, Training Loss: 3.96479, LR: 0.0009633, Tokens/sec: 39677.95\n",
      "Step: 965, Training Loss: 4.01818, LR: 0.0009631, Tokens/sec: 41706.35\n",
      "Step: 966, Training Loss: 3.75493, LR: 0.0009630, Tokens/sec: 38127.37\n",
      "Step: 967, Training Loss: 3.70422, LR: 0.0009628, Tokens/sec: 36885.15\n",
      "Step: 968, Training Loss: 4.16223, LR: 0.0009627, Tokens/sec: 41208.66\n",
      "Step: 969, Training Loss: 4.10057, LR: 0.0009626, Tokens/sec: 40628.86\n",
      "Step: 970, Training Loss: 4.02141, LR: 0.0009624, Tokens/sec: 41261.86\n",
      "Step: 971, Training Loss: 3.73287, LR: 0.0009623, Tokens/sec: 41744.32\n",
      "Step: 972, Training Loss: 3.71723, LR: 0.0009621, Tokens/sec: 41646.54\n",
      "Step: 973, Training Loss: 3.93635, LR: 0.0009620, Tokens/sec: 41601.92\n",
      "Step: 974, Training Loss: 3.84259, LR: 0.0009618, Tokens/sec: 41723.34\n",
      "Step: 975, Training Loss: 3.98139, LR: 0.0009617, Tokens/sec: 40928.01\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 975, Eval Loss: 3.94873\n",
      "Step: 976, Training Loss: 3.98160, LR: 0.0009616, Tokens/sec: 41444.53\n",
      "Step: 977, Training Loss: 3.58941, LR: 0.0009614, Tokens/sec: 40610.55\n",
      "Step: 978, Training Loss: 3.70547, LR: 0.0009613, Tokens/sec: 39397.20\n",
      "Step: 979, Training Loss: 3.95582, LR: 0.0009611, Tokens/sec: 41608.81\n",
      "Step: 980, Training Loss: 3.91856, LR: 0.0009610, Tokens/sec: 39887.17\n",
      "Step: 981, Training Loss: 4.02465, LR: 0.0009608, Tokens/sec: 41713.90\n",
      "Step: 982, Training Loss: 3.65590, LR: 0.0009607, Tokens/sec: 40157.35\n",
      "Step: 983, Training Loss: 3.82981, LR: 0.0009606, Tokens/sec: 39685.74\n",
      "Step: 984, Training Loss: 3.84932, LR: 0.0009604, Tokens/sec: 40161.65\n",
      "Step: 985, Training Loss: 4.06204, LR: 0.0009603, Tokens/sec: 39097.95\n",
      "Step: 986, Training Loss: 3.64027, LR: 0.0009601, Tokens/sec: 41475.20\n",
      "Step: 987, Training Loss: 3.89910, LR: 0.0009600, Tokens/sec: 41144.27\n",
      "Step: 988, Training Loss: 4.04031, LR: 0.0009598, Tokens/sec: 38694.02\n",
      "Step: 989, Training Loss: 4.48180, LR: 0.0009597, Tokens/sec: 39731.82\n",
      "Step: 990, Training Loss: 3.69971, LR: 0.0009595, Tokens/sec: 38597.01\n",
      "Step: 991, Training Loss: 3.70237, LR: 0.0009594, Tokens/sec: 39599.62\n",
      "Step: 992, Training Loss: 3.83163, LR: 0.0009592, Tokens/sec: 41504.60\n",
      "Step: 993, Training Loss: 3.82219, LR: 0.0009591, Tokens/sec: 41636.73\n",
      "Step: 994, Training Loss: 4.06182, LR: 0.0009590, Tokens/sec: 40729.11\n",
      "Step: 995, Training Loss: 3.95813, LR: 0.0009588, Tokens/sec: 41732.66\n",
      "Step: 996, Training Loss: 4.11431, LR: 0.0009587, Tokens/sec: 41405.93\n",
      "Step: 997, Training Loss: 3.99544, LR: 0.0009585, Tokens/sec: 41367.45\n",
      "Step: 998, Training Loss: 3.86490, LR: 0.0009584, Tokens/sec: 41511.27\n",
      "Step: 999, Training Loss: 3.97395, LR: 0.0009582, Tokens/sec: 41591.44\n",
      "Step: 1000, Training Loss: 3.86772, LR: 0.0009581, Tokens/sec: 41656.87\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 1000, Eval Loss: 3.90445\n",
      "Step: 1001, Training Loss: 3.97412, LR: 0.0009579, Tokens/sec: 41527.89\n",
      "Step: 1002, Training Loss: 4.21206, LR: 0.0009578, Tokens/sec: 41069.41\n",
      "Step: 1003, Training Loss: 3.85785, LR: 0.0009576, Tokens/sec: 41399.45\n",
      "Step: 1004, Training Loss: 3.91003, LR: 0.0009575, Tokens/sec: 41073.46\n",
      "Step: 1005, Training Loss: 3.77254, LR: 0.0009573, Tokens/sec: 39790.31\n",
      "Step: 1006, Training Loss: 3.66081, LR: 0.0009572, Tokens/sec: 41068.08\n",
      "Step: 1007, Training Loss: 4.03995, LR: 0.0009570, Tokens/sec: 41439.81\n",
      "Step: 1008, Training Loss: 3.73723, LR: 0.0009569, Tokens/sec: 41537.46\n",
      "Step: 1009, Training Loss: 4.03357, LR: 0.0009567, Tokens/sec: 41401.60\n",
      "Step: 1010, Training Loss: 3.42838, LR: 0.0009566, Tokens/sec: 41442.72\n",
      "Step: 1011, Training Loss: 3.93678, LR: 0.0009564, Tokens/sec: 40838.29\n",
      "Step: 1012, Training Loss: 4.11167, LR: 0.0009563, Tokens/sec: 39472.64\n",
      "Step: 1013, Training Loss: 3.86395, LR: 0.0009561, Tokens/sec: 41491.93\n",
      "Step: 1014, Training Loss: 4.42169, LR: 0.0009560, Tokens/sec: 41502.82\n",
      "Step: 1015, Training Loss: 4.01672, LR: 0.0009558, Tokens/sec: 41402.50\n",
      "Step: 1016, Training Loss: 4.01259, LR: 0.0009557, Tokens/sec: 41429.38\n",
      "Step: 1017, Training Loss: 4.31753, LR: 0.0009555, Tokens/sec: 41554.86\n",
      "Step: 1018, Training Loss: 3.58023, LR: 0.0009553, Tokens/sec: 40191.79\n",
      "Step: 1019, Training Loss: 3.80240, LR: 0.0009552, Tokens/sec: 41523.62\n",
      "Step: 1020, Training Loss: 3.86848, LR: 0.0009550, Tokens/sec: 41189.84\n",
      "Step: 1021, Training Loss: 3.81306, LR: 0.0009549, Tokens/sec: 41584.54\n",
      "Step: 1022, Training Loss: 3.93977, LR: 0.0009547, Tokens/sec: 41383.68\n",
      "Step: 1023, Training Loss: 4.04435, LR: 0.0009546, Tokens/sec: 41007.55\n",
      "Step: 1024, Training Loss: 3.66463, LR: 0.0009544, Tokens/sec: 41507.97\n",
      "Step: 1025, Training Loss: 4.20829, LR: 0.0009543, Tokens/sec: 41473.87\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 1025, Eval Loss: 3.86050\n",
      "Step: 1026, Training Loss: 3.94264, LR: 0.0009541, Tokens/sec: 41374.06\n",
      "Step: 1027, Training Loss: 3.45029, LR: 0.0009540, Tokens/sec: 41170.21\n",
      "Step: 1028, Training Loss: 4.19595, LR: 0.0009538, Tokens/sec: 41204.09\n",
      "Step: 1029, Training Loss: 4.09297, LR: 0.0009536, Tokens/sec: 41335.14\n",
      "Step: 1030, Training Loss: 3.84883, LR: 0.0009535, Tokens/sec: 41366.61\n",
      "Step: 1031, Training Loss: 3.84239, LR: 0.0009533, Tokens/sec: 41485.56\n",
      "Step: 1032, Training Loss: 3.88448, LR: 0.0009532, Tokens/sec: 41517.02\n",
      "Step: 1033, Training Loss: 3.47273, LR: 0.0009530, Tokens/sec: 41543.74\n",
      "Step: 1034, Training Loss: 3.57028, LR: 0.0009529, Tokens/sec: 41510.59\n",
      "Step: 1035, Training Loss: 3.52971, LR: 0.0009527, Tokens/sec: 41135.08\n",
      "Step: 1036, Training Loss: 3.87749, LR: 0.0009526, Tokens/sec: 41426.49\n",
      "Step: 1037, Training Loss: 3.99097, LR: 0.0009524, Tokens/sec: 41506.40\n",
      "Step: 1038, Training Loss: 3.84621, LR: 0.0009522, Tokens/sec: 39432.28\n",
      "Step: 1039, Training Loss: 3.90908, LR: 0.0009521, Tokens/sec: 41184.34\n",
      "Step: 1040, Training Loss: 3.34913, LR: 0.0009519, Tokens/sec: 41536.67\n",
      "Step: 1041, Training Loss: 3.82957, LR: 0.0009518, Tokens/sec: 41526.91\n",
      "Step: 1042, Training Loss: 4.16223, LR: 0.0009516, Tokens/sec: 41457.00\n",
      "Step: 1043, Training Loss: 3.74066, LR: 0.0009514, Tokens/sec: 41398.54\n",
      "Step: 1044, Training Loss: 4.25392, LR: 0.0009513, Tokens/sec: 41464.89\n",
      "Step: 1045, Training Loss: 3.59532, LR: 0.0009511, Tokens/sec: 41082.14\n",
      "Step: 1046, Training Loss: 4.08322, LR: 0.0009510, Tokens/sec: 39110.90\n",
      "Step: 1047, Training Loss: 3.54187, LR: 0.0009508, Tokens/sec: 41570.80\n",
      "Step: 1048, Training Loss: 3.88466, LR: 0.0009506, Tokens/sec: 41477.42\n",
      "Step: 1049, Training Loss: 3.78941, LR: 0.0009505, Tokens/sec: 41422.63\n",
      "Step: 1050, Training Loss: 3.90360, LR: 0.0009503, Tokens/sec: 41383.59\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 1050, Eval Loss: 3.84801\n",
      "Step: 1051, Training Loss: 3.89942, LR: 0.0009502, Tokens/sec: 41342.31\n",
      "Step: 1052, Training Loss: 4.10748, LR: 0.0009500, Tokens/sec: 41035.23\n",
      "Step: 1053, Training Loss: 3.88416, LR: 0.0009498, Tokens/sec: 41521.05\n",
      "Step: 1054, Training Loss: 3.93456, LR: 0.0009497, Tokens/sec: 40960.85\n",
      "Step: 1055, Training Loss: 4.15851, LR: 0.0009495, Tokens/sec: 41568.85\n",
      "Step: 1056, Training Loss: 3.65190, LR: 0.0009493, Tokens/sec: 35749.70\n",
      "Step: 1057, Training Loss: 3.72395, LR: 0.0009492, Tokens/sec: 36716.29\n",
      "Step: 1058, Training Loss: 3.72212, LR: 0.0009490, Tokens/sec: 38985.86\n",
      "Step: 1059, Training Loss: 3.89794, LR: 0.0009489, Tokens/sec: 38499.13\n",
      "Step: 1060, Training Loss: 3.68581, LR: 0.0009487, Tokens/sec: 38510.88\n",
      "Step: 1061, Training Loss: 3.85080, LR: 0.0009485, Tokens/sec: 36341.34\n",
      "Step: 1062, Training Loss: 3.61652, LR: 0.0009484, Tokens/sec: 40834.99\n",
      "Step: 1063, Training Loss: 3.85504, LR: 0.0009482, Tokens/sec: 41334.10\n",
      "Step: 1064, Training Loss: 3.59263, LR: 0.0009480, Tokens/sec: 41381.93\n",
      "Step: 1065, Training Loss: 3.71327, LR: 0.0009479, Tokens/sec: 41373.28\n",
      "Step: 1066, Training Loss: 3.77240, LR: 0.0009477, Tokens/sec: 41379.17\n",
      "Step: 1067, Training Loss: 3.63941, LR: 0.0009475, Tokens/sec: 41377.68\n",
      "Step: 1068, Training Loss: 3.82997, LR: 0.0009474, Tokens/sec: 41396.63\n",
      "Step: 1069, Training Loss: 4.00920, LR: 0.0009472, Tokens/sec: 41084.14\n",
      "Step: 1070, Training Loss: 3.77378, LR: 0.0009470, Tokens/sec: 41318.41\n",
      "Step: 1071, Training Loss: 3.78564, LR: 0.0009469, Tokens/sec: 37556.36\n",
      "Step: 1072, Training Loss: 4.00388, LR: 0.0009467, Tokens/sec: 36822.95\n",
      "Step: 1073, Training Loss: 3.88511, LR: 0.0009465, Tokens/sec: 41332.07\n",
      "Step: 1074, Training Loss: 4.06345, LR: 0.0009464, Tokens/sec: 41289.18\n",
      "Step: 1075, Training Loss: 3.56844, LR: 0.0009462, Tokens/sec: 41365.03\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 1075, Eval Loss: 3.79185\n",
      "Step: 1076, Training Loss: 3.75517, LR: 0.0009460, Tokens/sec: 41267.79\n",
      "Step: 1077, Training Loss: 3.55723, LR: 0.0009459, Tokens/sec: 40919.35\n",
      "Step: 1078, Training Loss: 3.87383, LR: 0.0009457, Tokens/sec: 41349.66\n",
      "Step: 1079, Training Loss: 4.23696, LR: 0.0009455, Tokens/sec: 41407.03\n",
      "Step: 1080, Training Loss: 3.73612, LR: 0.0009454, Tokens/sec: 40164.57\n",
      "Step: 1081, Training Loss: 3.92851, LR: 0.0009452, Tokens/sec: 41348.45\n",
      "Step: 1082, Training Loss: 3.82228, LR: 0.0009450, Tokens/sec: 41323.07\n",
      "Step: 1083, Training Loss: 3.89434, LR: 0.0009449, Tokens/sec: 41338.84\n",
      "Step: 1084, Training Loss: 3.89032, LR: 0.0009447, Tokens/sec: 41315.14\n",
      "Step: 1085, Training Loss: 3.57903, LR: 0.0009445, Tokens/sec: 41356.06\n",
      "Step: 1086, Training Loss: 3.73136, LR: 0.0009444, Tokens/sec: 41022.31\n",
      "Step: 1087, Training Loss: 4.12582, LR: 0.0009442, Tokens/sec: 41386.14\n",
      "Step: 1088, Training Loss: 4.20103, LR: 0.0009440, Tokens/sec: 40289.77\n",
      "Step: 1089, Training Loss: 3.59040, LR: 0.0009438, Tokens/sec: 41347.01\n",
      "Step: 1090, Training Loss: 3.79134, LR: 0.0009437, Tokens/sec: 41389.93\n",
      "Step: 1091, Training Loss: 3.86492, LR: 0.0009435, Tokens/sec: 41341.65\n",
      "Step: 1092, Training Loss: 4.32683, LR: 0.0009433, Tokens/sec: 41379.82\n",
      "Step: 1093, Training Loss: 3.84999, LR: 0.0009432, Tokens/sec: 41377.86\n",
      "Step: 1094, Training Loss: 3.84761, LR: 0.0009430, Tokens/sec: 41396.82\n",
      "Step: 1095, Training Loss: 3.72337, LR: 0.0009428, Tokens/sec: 40891.43\n",
      "Step: 1096, Training Loss: 4.02378, LR: 0.0009426, Tokens/sec: 41333.32\n",
      "Step: 1097, Training Loss: 4.13004, LR: 0.0009425, Tokens/sec: 41234.34\n",
      "Step: 1098, Training Loss: 3.54998, LR: 0.0009423, Tokens/sec: 41368.25\n",
      "Step: 1099, Training Loss: 4.07528, LR: 0.0009421, Tokens/sec: 39437.94\n",
      "Step: 1100, Training Loss: 3.85401, LR: 0.0009420, Tokens/sec: 41395.64\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 1100, Eval Loss: 3.81183\n",
      "Step: 1101, Training Loss: 3.89020, LR: 0.0009418, Tokens/sec: 41059.17\n",
      "Step: 1102, Training Loss: 3.79659, LR: 0.0009416, Tokens/sec: 41411.29\n",
      "Step: 1103, Training Loss: 3.74930, LR: 0.0009414, Tokens/sec: 38122.16\n",
      "Step: 1104, Training Loss: 3.81282, LR: 0.0009413, Tokens/sec: 37516.15\n",
      "Step: 1105, Training Loss: 3.97739, LR: 0.0009411, Tokens/sec: 39143.68\n",
      "Step: 1106, Training Loss: 3.91371, LR: 0.0009409, Tokens/sec: 39912.32\n",
      "Step: 1107, Training Loss: 3.90527, LR: 0.0009407, Tokens/sec: 39686.39\n",
      "Step: 1108, Training Loss: 3.98615, LR: 0.0009406, Tokens/sec: 29873.53\n",
      "Step: 1109, Training Loss: 3.90315, LR: 0.0009404, Tokens/sec: 41106.90\n",
      "Step: 1110, Training Loss: 3.61988, LR: 0.0009402, Tokens/sec: 41398.11\n",
      "Step: 1111, Training Loss: 3.71305, LR: 0.0009400, Tokens/sec: 40709.50\n",
      "Step: 1112, Training Loss: 3.98308, LR: 0.0009399, Tokens/sec: 40476.73\n",
      "Step: 1113, Training Loss: 4.13231, LR: 0.0009397, Tokens/sec: 41345.60\n",
      "Step: 1114, Training Loss: 3.95834, LR: 0.0009395, Tokens/sec: 41463.39\n",
      "Step: 1115, Training Loss: 3.84346, LR: 0.0009393, Tokens/sec: 39913.03\n",
      "Step: 1116, Training Loss: 3.95050, LR: 0.0009392, Tokens/sec: 41337.10\n",
      "Step: 1117, Training Loss: 4.01934, LR: 0.0009390, Tokens/sec: 39719.66\n",
      "Step: 1118, Training Loss: 3.71115, LR: 0.0009388, Tokens/sec: 40853.99\n",
      "Step: 1119, Training Loss: 3.92351, LR: 0.0009386, Tokens/sec: 40514.73\n",
      "Step: 1120, Training Loss: 3.90987, LR: 0.0009384, Tokens/sec: 37478.93\n",
      "Step: 1121, Training Loss: 3.85716, LR: 0.0009383, Tokens/sec: 38344.10\n",
      "Step: 1122, Training Loss: 3.88162, LR: 0.0009381, Tokens/sec: 40474.09\n",
      "Step: 1123, Training Loss: 3.62916, LR: 0.0009379, Tokens/sec: 41246.47\n",
      "Step: 1124, Training Loss: 3.89728, LR: 0.0009377, Tokens/sec: 40725.08\n",
      "Step: 1125, Training Loss: 3.89823, LR: 0.0009375, Tokens/sec: 39302.74\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 1125, Eval Loss: 3.79470\n",
      "Step: 1126, Training Loss: 3.70144, LR: 0.0009374, Tokens/sec: 39601.39\n",
      "Step: 1127, Training Loss: 3.78831, LR: 0.0009372, Tokens/sec: 40990.65\n",
      "Step: 1128, Training Loss: 3.72900, LR: 0.0009370, Tokens/sec: 41417.67\n",
      "Step: 1129, Training Loss: 3.91587, LR: 0.0009368, Tokens/sec: 41255.98\n",
      "Step: 1130, Training Loss: 3.74774, LR: 0.0009366, Tokens/sec: 40924.61\n",
      "Step: 1131, Training Loss: 3.96986, LR: 0.0009365, Tokens/sec: 41390.86\n",
      "Step: 1132, Training Loss: 3.93248, LR: 0.0009363, Tokens/sec: 40905.82\n",
      "Step: 1133, Training Loss: 4.05908, LR: 0.0009361, Tokens/sec: 41412.18\n",
      "Step: 1134, Training Loss: 3.58019, LR: 0.0009359, Tokens/sec: 40990.78\n",
      "Step: 1135, Training Loss: 3.91867, LR: 0.0009357, Tokens/sec: 41320.87\n",
      "Step: 1136, Training Loss: 3.47643, LR: 0.0009356, Tokens/sec: 41415.69\n",
      "Step: 1137, Training Loss: 3.51406, LR: 0.0009354, Tokens/sec: 40859.66\n",
      "Step: 1138, Training Loss: 3.96976, LR: 0.0009352, Tokens/sec: 41368.46\n",
      "Step: 1139, Training Loss: 3.91063, LR: 0.0009350, Tokens/sec: 40989.42\n",
      "Step: 1140, Training Loss: 3.87737, LR: 0.0009348, Tokens/sec: 41392.28\n",
      "Step: 1141, Training Loss: 3.68859, LR: 0.0009346, Tokens/sec: 41414.75\n",
      "Step: 1142, Training Loss: 3.71982, LR: 0.0009345, Tokens/sec: 41305.59\n",
      "Step: 1143, Training Loss: 3.92818, LR: 0.0009343, Tokens/sec: 40764.93\n",
      "Step: 1144, Training Loss: 3.84829, LR: 0.0009341, Tokens/sec: 41364.01\n",
      "Step: 1145, Training Loss: 4.06945, LR: 0.0009339, Tokens/sec: 41384.31\n",
      "Step: 1146, Training Loss: 3.85027, LR: 0.0009337, Tokens/sec: 41370.95\n",
      "Step: 1147, Training Loss: 3.99843, LR: 0.0009335, Tokens/sec: 41393.57\n",
      "Step: 1148, Training Loss: 3.75152, LR: 0.0009334, Tokens/sec: 41456.36\n",
      "Step: 1149, Training Loss: 3.71713, LR: 0.0009332, Tokens/sec: 41464.93\n",
      "Step: 1150, Training Loss: 3.72098, LR: 0.0009330, Tokens/sec: 41399.80\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 1150, Eval Loss: 3.79320\n",
      "Step: 1151, Training Loss: 3.53831, LR: 0.0009328, Tokens/sec: 40999.62\n",
      "Step: 1152, Training Loss: 3.92610, LR: 0.0009326, Tokens/sec: 41385.23\n",
      "Step: 1153, Training Loss: 3.87112, LR: 0.0009324, Tokens/sec: 41415.09\n",
      "Step: 1154, Training Loss: 3.83696, LR: 0.0009322, Tokens/sec: 41378.38\n",
      "Step: 1155, Training Loss: 3.48955, LR: 0.0009321, Tokens/sec: 41369.73\n",
      "Step: 1156, Training Loss: 3.94709, LR: 0.0009319, Tokens/sec: 41390.17\n",
      "Step: 1157, Training Loss: 3.77575, LR: 0.0009317, Tokens/sec: 41466.35\n",
      "Step: 1158, Training Loss: 3.80560, LR: 0.0009315, Tokens/sec: 41024.85\n",
      "Step: 1159, Training Loss: 3.92084, LR: 0.0009313, Tokens/sec: 41323.49\n",
      "Step: 1160, Training Loss: 3.93464, LR: 0.0009311, Tokens/sec: 40618.83\n",
      "Step: 1161, Training Loss: 4.01076, LR: 0.0009309, Tokens/sec: 41400.68\n",
      "Step: 1162, Training Loss: 3.98889, LR: 0.0009308, Tokens/sec: 41318.08\n",
      "Step: 1163, Training Loss: 3.96598, LR: 0.0009306, Tokens/sec: 41442.26\n",
      "Step: 1164, Training Loss: 3.69583, LR: 0.0009304, Tokens/sec: 41415.38\n",
      "Step: 1165, Training Loss: 3.63410, LR: 0.0009302, Tokens/sec: 41378.93\n",
      "Step: 1166, Training Loss: 3.62149, LR: 0.0009300, Tokens/sec: 41401.35\n",
      "Step: 1167, Training Loss: 3.54432, LR: 0.0009298, Tokens/sec: 40599.96\n",
      "Step: 1168, Training Loss: 3.86371, LR: 0.0009296, Tokens/sec: 41412.78\n",
      "Step: 1169, Training Loss: 3.56116, LR: 0.0009294, Tokens/sec: 40921.00\n",
      "Step: 1170, Training Loss: 3.99005, LR: 0.0009292, Tokens/sec: 41422.90\n",
      "Step: 1171, Training Loss: 3.86885, LR: 0.0009290, Tokens/sec: 41460.98\n",
      "Step: 1172, Training Loss: 3.95742, LR: 0.0009289, Tokens/sec: 41416.92\n",
      "Step: 1173, Training Loss: 3.61434, LR: 0.0009287, Tokens/sec: 41423.53\n",
      "Step: 1174, Training Loss: 3.55469, LR: 0.0009285, Tokens/sec: 41433.17\n",
      "Step: 1175, Training Loss: 3.71844, LR: 0.0009283, Tokens/sec: 41412.37\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 1175, Eval Loss: 3.78740\n",
      "Step: 1176, Training Loss: 3.89441, LR: 0.0009281, Tokens/sec: 41262.04\n",
      "Step: 1177, Training Loss: 3.76377, LR: 0.0009279, Tokens/sec: 40798.80\n",
      "Step: 1178, Training Loss: 3.67888, LR: 0.0009277, Tokens/sec: 41484.08\n",
      "Step: 1179, Training Loss: 3.35121, LR: 0.0009275, Tokens/sec: 41520.30\n",
      "Step: 1180, Training Loss: 3.88975, LR: 0.0009273, Tokens/sec: 41416.81\n",
      "Step: 1181, Training Loss: 3.93888, LR: 0.0009271, Tokens/sec: 41422.09\n",
      "Step: 1182, Training Loss: 3.81090, LR: 0.0009269, Tokens/sec: 40933.26\n",
      "Step: 1183, Training Loss: 3.62245, LR: 0.0009268, Tokens/sec: 41411.24\n",
      "Step: 1184, Training Loss: 3.74866, LR: 0.0009266, Tokens/sec: 40961.22\n",
      "Step: 1185, Training Loss: 3.55783, LR: 0.0009264, Tokens/sec: 41340.69\n",
      "Step: 1186, Training Loss: 3.92601, LR: 0.0009262, Tokens/sec: 41064.98\n",
      "Step: 1187, Training Loss: 3.91145, LR: 0.0009260, Tokens/sec: 41342.58\n",
      "Step: 1188, Training Loss: 3.66399, LR: 0.0009258, Tokens/sec: 41355.85\n",
      "Step: 1189, Training Loss: 3.69784, LR: 0.0009256, Tokens/sec: 41369.95\n",
      "Step: 1190, Training Loss: 4.05466, LR: 0.0009254, Tokens/sec: 41365.56\n",
      "Step: 1191, Training Loss: 3.63511, LR: 0.0009252, Tokens/sec: 41384.02\n",
      "Step: 1192, Training Loss: 3.76667, LR: 0.0009250, Tokens/sec: 41507.73\n",
      "Step: 1193, Training Loss: 3.99550, LR: 0.0009248, Tokens/sec: 41000.56\n",
      "Step: 1194, Training Loss: 3.71126, LR: 0.0009246, Tokens/sec: 41373.34\n",
      "Step: 1195, Training Loss: 4.23197, LR: 0.0009244, Tokens/sec: 41420.82\n",
      "Step: 1196, Training Loss: 3.85024, LR: 0.0009242, Tokens/sec: 41367.26\n",
      "Step: 1197, Training Loss: 3.81003, LR: 0.0009240, Tokens/sec: 41384.31\n",
      "Step: 1198, Training Loss: 3.78693, LR: 0.0009238, Tokens/sec: 41125.84\n",
      "Step: 1199, Training Loss: 3.78996, LR: 0.0009236, Tokens/sec: 41409.76\n",
      "Step: 1200, Training Loss: 3.71273, LR: 0.0009234, Tokens/sec: 41434.80\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 1200, Eval Loss: 3.71554\n",
      "Step: 1201, Training Loss: 3.94451, LR: 0.0009232, Tokens/sec: 40111.78\n",
      "Step: 1202, Training Loss: 3.55980, LR: 0.0009230, Tokens/sec: 41399.36\n",
      "Step: 1203, Training Loss: 3.79897, LR: 0.0009228, Tokens/sec: 40978.93\n",
      "Step: 1204, Training Loss: 3.95489, LR: 0.0009226, Tokens/sec: 38500.81\n",
      "Step: 1205, Training Loss: 3.91679, LR: 0.0009224, Tokens/sec: 39708.83\n",
      "Step: 1206, Training Loss: 3.72806, LR: 0.0009223, Tokens/sec: 39770.20\n",
      "Step: 1207, Training Loss: 3.90869, LR: 0.0009221, Tokens/sec: 40209.49\n",
      "Step: 1208, Training Loss: 3.55071, LR: 0.0009219, Tokens/sec: 41280.77\n",
      "Step: 1209, Training Loss: 3.65195, LR: 0.0009217, Tokens/sec: 41247.20\n",
      "Step: 1210, Training Loss: 3.96694, LR: 0.0009215, Tokens/sec: 41316.45\n",
      "Step: 1211, Training Loss: 3.97217, LR: 0.0009213, Tokens/sec: 41361.98\n",
      "Step: 1212, Training Loss: 3.55699, LR: 0.0009211, Tokens/sec: 40467.46\n",
      "Step: 1213, Training Loss: 3.92176, LR: 0.0009209, Tokens/sec: 41343.59\n",
      "Step: 1214, Training Loss: 3.68529, LR: 0.0009207, Tokens/sec: 41012.49\n",
      "Step: 1215, Training Loss: 3.70811, LR: 0.0009205, Tokens/sec: 41238.18\n",
      "Step: 1216, Training Loss: 3.37208, LR: 0.0009203, Tokens/sec: 41424.44\n",
      "Step: 1217, Training Loss: 3.66664, LR: 0.0009201, Tokens/sec: 41072.85\n",
      "Step: 1218, Training Loss: 4.01313, LR: 0.0009199, Tokens/sec: 41373.50\n",
      "Step: 1219, Training Loss: 3.85441, LR: 0.0009197, Tokens/sec: 40589.72\n",
      "Step: 1220, Training Loss: 3.64627, LR: 0.0009195, Tokens/sec: 41366.95\n",
      "Step: 1221, Training Loss: 3.80586, LR: 0.0009193, Tokens/sec: 41224.59\n",
      "Step: 1222, Training Loss: 3.54864, LR: 0.0009191, Tokens/sec: 41374.75\n",
      "Step: 1223, Training Loss: 3.66055, LR: 0.0009188, Tokens/sec: 40958.98\n",
      "Step: 1224, Training Loss: 3.90652, LR: 0.0009186, Tokens/sec: 41332.36\n",
      "Step: 1225, Training Loss: 3.86120, LR: 0.0009184, Tokens/sec: 40977.04\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 1225, Eval Loss: 3.73488\n",
      "Step: 1226, Training Loss: 3.55431, LR: 0.0009182, Tokens/sec: 40191.49\n",
      "Step: 1227, Training Loss: 3.75350, LR: 0.0009180, Tokens/sec: 40773.09\n",
      "Step: 1228, Training Loss: 3.50451, LR: 0.0009178, Tokens/sec: 41373.52\n",
      "Step: 1229, Training Loss: 4.06850, LR: 0.0009176, Tokens/sec: 41367.41\n",
      "Step: 1230, Training Loss: 3.98787, LR: 0.0009174, Tokens/sec: 41319.89\n",
      "Step: 1231, Training Loss: 3.63690, LR: 0.0009172, Tokens/sec: 34337.90\n",
      "Step: 1232, Training Loss: 3.64436, LR: 0.0009170, Tokens/sec: 36476.34\n",
      "Step: 1233, Training Loss: 3.77672, LR: 0.0009168, Tokens/sec: 39250.60\n",
      "Step: 1234, Training Loss: 3.91772, LR: 0.0009166, Tokens/sec: 40199.14\n",
      "Step: 1235, Training Loss: 3.64237, LR: 0.0009164, Tokens/sec: 38132.40\n",
      "Step: 1236, Training Loss: 4.06024, LR: 0.0009162, Tokens/sec: 41324.26\n",
      "Step: 1237, Training Loss: 4.06957, LR: 0.0009160, Tokens/sec: 41425.83\n",
      "Step: 1238, Training Loss: 3.92399, LR: 0.0009158, Tokens/sec: 41280.44\n",
      "Step: 1239, Training Loss: 3.74706, LR: 0.0009156, Tokens/sec: 41391.76\n",
      "Step: 1240, Training Loss: 3.65229, LR: 0.0009154, Tokens/sec: 40550.06\n",
      "Step: 1241, Training Loss: 3.75680, LR: 0.0009152, Tokens/sec: 41349.01\n",
      "Step: 1242, Training Loss: 3.32718, LR: 0.0009150, Tokens/sec: 41222.05\n",
      "Step: 1243, Training Loss: 4.02359, LR: 0.0009148, Tokens/sec: 41394.23\n",
      "Step: 1244, Training Loss: 3.84052, LR: 0.0009146, Tokens/sec: 41432.44\n",
      "Step: 1245, Training Loss: 3.86213, LR: 0.0009143, Tokens/sec: 41383.57\n",
      "Step: 1246, Training Loss: 3.46020, LR: 0.0009141, Tokens/sec: 40979.42\n",
      "Step: 1247, Training Loss: 3.81480, LR: 0.0009139, Tokens/sec: 40947.16\n",
      "Step: 1248, Training Loss: 3.78017, LR: 0.0009137, Tokens/sec: 41285.89\n",
      "Step: 1249, Training Loss: 3.68773, LR: 0.0009135, Tokens/sec: 40891.39\n",
      "Step: 1250, Training Loss: 3.81446, LR: 0.0009133, Tokens/sec: 41350.47\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 1250, Eval Loss: 3.71718\n",
      "Step: 1251, Training Loss: 3.41794, LR: 0.0009131, Tokens/sec: 41384.08\n",
      "Step: 1252, Training Loss: 3.52683, LR: 0.0009129, Tokens/sec: 41462.24\n",
      "Step: 1253, Training Loss: 3.78663, LR: 0.0009127, Tokens/sec: 41278.71\n",
      "Step: 1254, Training Loss: 3.74140, LR: 0.0009125, Tokens/sec: 41357.59\n",
      "Step: 1255, Training Loss: 3.79873, LR: 0.0009123, Tokens/sec: 40952.70\n",
      "Step: 1256, Training Loss: 3.86993, LR: 0.0009121, Tokens/sec: 41395.61\n",
      "Step: 1257, Training Loss: 3.40777, LR: 0.0009118, Tokens/sec: 40976.51\n",
      "Step: 1258, Training Loss: 3.86235, LR: 0.0009116, Tokens/sec: 41321.29\n",
      "Step: 1259, Training Loss: 3.87459, LR: 0.0009114, Tokens/sec: 41212.01\n",
      "Step: 1260, Training Loss: 3.66622, LR: 0.0009112, Tokens/sec: 41346.08\n",
      "Step: 1261, Training Loss: 4.28825, LR: 0.0009110, Tokens/sec: 41329.20\n",
      "Step: 1262, Training Loss: 3.44987, LR: 0.0009108, Tokens/sec: 41257.55\n",
      "Step: 1263, Training Loss: 3.48659, LR: 0.0009106, Tokens/sec: 41329.82\n",
      "Step: 1264, Training Loss: 3.60087, LR: 0.0009104, Tokens/sec: 41287.66\n",
      "Step: 1265, Training Loss: 3.83454, LR: 0.0009102, Tokens/sec: 41158.52\n",
      "Step: 1266, Training Loss: 3.72117, LR: 0.0009100, Tokens/sec: 40972.25\n",
      "Step: 1267, Training Loss: 3.49343, LR: 0.0009097, Tokens/sec: 41340.75\n",
      "Step: 1268, Training Loss: 3.65140, LR: 0.0009095, Tokens/sec: 40984.37\n",
      "Step: 1269, Training Loss: 3.60556, LR: 0.0009093, Tokens/sec: 41292.47\n",
      "Step: 1270, Training Loss: 3.59398, LR: 0.0009091, Tokens/sec: 41338.71\n",
      "Step: 1271, Training Loss: 3.70290, LR: 0.0009089, Tokens/sec: 41340.45\n",
      "Step: 1272, Training Loss: 3.78002, LR: 0.0009087, Tokens/sec: 41363.20\n",
      "Step: 1273, Training Loss: 3.86968, LR: 0.0009085, Tokens/sec: 41370.04\n",
      "Step: 1274, Training Loss: 3.45944, LR: 0.0009082, Tokens/sec: 41411.40\n",
      "Step: 1275, Training Loss: 3.99281, LR: 0.0009080, Tokens/sec: 41034.91\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 1275, Eval Loss: 3.75915\n",
      "Step: 1276, Training Loss: 3.82266, LR: 0.0009078, Tokens/sec: 41240.14\n",
      "Step: 1277, Training Loss: 3.98956, LR: 0.0009076, Tokens/sec: 41322.47\n",
      "Step: 1278, Training Loss: 4.11470, LR: 0.0009074, Tokens/sec: 40976.06\n",
      "Step: 1279, Training Loss: 3.38744, LR: 0.0009072, Tokens/sec: 41322.84\n",
      "Step: 1280, Training Loss: 3.66238, LR: 0.0009070, Tokens/sec: 41334.32\n",
      "Step: 1281, Training Loss: 3.69988, LR: 0.0009067, Tokens/sec: 41321.71\n",
      "Step: 1282, Training Loss: 3.52689, LR: 0.0009065, Tokens/sec: 41000.41\n",
      "Step: 1283, Training Loss: 3.88936, LR: 0.0009063, Tokens/sec: 40964.43\n",
      "Step: 1284, Training Loss: 3.54391, LR: 0.0009061, Tokens/sec: 41301.25\n",
      "Step: 1285, Training Loss: 3.82089, LR: 0.0009059, Tokens/sec: 41102.17\n",
      "Step: 1286, Training Loss: 3.79029, LR: 0.0009057, Tokens/sec: 41335.14\n",
      "Step: 1287, Training Loss: 3.82714, LR: 0.0009055, Tokens/sec: 41469.81\n",
      "Step: 1288, Training Loss: 3.83298, LR: 0.0009052, Tokens/sec: 41426.67\n",
      "Step: 1289, Training Loss: 3.86960, LR: 0.0009050, Tokens/sec: 41024.56\n",
      "Step: 1290, Training Loss: 4.07412, LR: 0.0009048, Tokens/sec: 40945.33\n",
      "Step: 1291, Training Loss: 3.67643, LR: 0.0009046, Tokens/sec: 41247.23\n",
      "Step: 1292, Training Loss: 3.59341, LR: 0.0009044, Tokens/sec: 41352.63\n",
      "Step: 1293, Training Loss: 3.49831, LR: 0.0009042, Tokens/sec: 41378.21\n",
      "Step: 1294, Training Loss: 3.55768, LR: 0.0009039, Tokens/sec: 41418.09\n",
      "Step: 1295, Training Loss: 3.48688, LR: 0.0009037, Tokens/sec: 41421.80\n",
      "Step: 1296, Training Loss: 3.67564, LR: 0.0009035, Tokens/sec: 40904.77\n",
      "Step: 1297, Training Loss: 3.77163, LR: 0.0009033, Tokens/sec: 41311.11\n",
      "Step: 1298, Training Loss: 3.58817, LR: 0.0009031, Tokens/sec: 39924.78\n",
      "Step: 1299, Training Loss: 3.71088, LR: 0.0009028, Tokens/sec: 41230.92\n",
      "Step: 1300, Training Loss: 3.70505, LR: 0.0009026, Tokens/sec: 41286.04\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 1300, Eval Loss: 3.75964\n",
      "Step: 1301, Training Loss: 3.58443, LR: 0.0009024, Tokens/sec: 39431.70\n",
      "Step: 1302, Training Loss: 3.87012, LR: 0.0009022, Tokens/sec: 39247.12\n",
      "Step: 1303, Training Loss: 3.88268, LR: 0.0009020, Tokens/sec: 41280.87\n",
      "Step: 1304, Training Loss: 3.23322, LR: 0.0009017, Tokens/sec: 40869.25\n",
      "Step: 1305, Training Loss: 3.80610, LR: 0.0009015, Tokens/sec: 40943.70\n",
      "Step: 1306, Training Loss: 3.77306, LR: 0.0009013, Tokens/sec: 41087.44\n",
      "Step: 1307, Training Loss: 3.97909, LR: 0.0009011, Tokens/sec: 39279.46\n",
      "Step: 1308, Training Loss: 3.59828, LR: 0.0009009, Tokens/sec: 40039.88\n",
      "Step: 1309, Training Loss: 3.59754, LR: 0.0009006, Tokens/sec: 39173.62\n",
      "Step: 1310, Training Loss: 3.46851, LR: 0.0009004, Tokens/sec: 38340.31\n",
      "Step: 1311, Training Loss: 3.81839, LR: 0.0009002, Tokens/sec: 40722.03\n",
      "Step: 1312, Training Loss: 3.88335, LR: 0.0009000, Tokens/sec: 40006.34\n",
      "Step: 1313, Training Loss: 3.85719, LR: 0.0008998, Tokens/sec: 40764.48\n",
      "Step: 1314, Training Loss: 3.81259, LR: 0.0008995, Tokens/sec: 40780.82\n",
      "Step: 1315, Training Loss: 3.40226, LR: 0.0008993, Tokens/sec: 37921.48\n",
      "Step: 1316, Training Loss: 3.53632, LR: 0.0008991, Tokens/sec: 41299.02\n",
      "Step: 1317, Training Loss: 3.98462, LR: 0.0008989, Tokens/sec: 41310.28\n",
      "Step: 1318, Training Loss: 3.74176, LR: 0.0008986, Tokens/sec: 40434.86\n",
      "Step: 1319, Training Loss: 4.11427, LR: 0.0008984, Tokens/sec: 41365.76\n",
      "Step: 1320, Training Loss: 3.52605, LR: 0.0008982, Tokens/sec: 41343.90\n",
      "Step: 1321, Training Loss: 3.75190, LR: 0.0008980, Tokens/sec: 41378.15\n",
      "Step: 1322, Training Loss: 4.00071, LR: 0.0008977, Tokens/sec: 41395.77\n",
      "Step: 1323, Training Loss: 3.79281, LR: 0.0008975, Tokens/sec: 41396.89\n",
      "Step: 1324, Training Loss: 3.50562, LR: 0.0008973, Tokens/sec: 41238.96\n",
      "Step: 1325, Training Loss: 3.77730, LR: 0.0008971, Tokens/sec: 41368.61\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 1325, Eval Loss: 3.70544\n",
      "Step: 1326, Training Loss: 3.85349, LR: 0.0008969, Tokens/sec: 40323.60\n",
      "Step: 1327, Training Loss: 3.50145, LR: 0.0008966, Tokens/sec: 38280.20\n",
      "Step: 1328, Training Loss: 3.38780, LR: 0.0008964, Tokens/sec: 40892.48\n",
      "Step: 1329, Training Loss: 3.46492, LR: 0.0008962, Tokens/sec: 39444.83\n",
      "Step: 1330, Training Loss: 3.77195, LR: 0.0008960, Tokens/sec: 41315.27\n",
      "Step: 1331, Training Loss: 3.88342, LR: 0.0008957, Tokens/sec: 40771.40\n",
      "Step: 1332, Training Loss: 3.56941, LR: 0.0008955, Tokens/sec: 41409.39\n",
      "Step: 1333, Training Loss: 3.45264, LR: 0.0008953, Tokens/sec: 39697.10\n",
      "Step: 1334, Training Loss: 3.62889, LR: 0.0008950, Tokens/sec: 39759.82\n",
      "Step: 1335, Training Loss: 3.48626, LR: 0.0008948, Tokens/sec: 40906.41\n",
      "Step: 1336, Training Loss: 3.52629, LR: 0.0008946, Tokens/sec: 41334.05\n",
      "Step: 1337, Training Loss: 3.59502, LR: 0.0008944, Tokens/sec: 41348.19\n",
      "Step: 1338, Training Loss: 3.89035, LR: 0.0008941, Tokens/sec: 41316.41\n",
      "Step: 1339, Training Loss: 3.37004, LR: 0.0008939, Tokens/sec: 40221.01\n",
      "Step: 1340, Training Loss: 3.53668, LR: 0.0008937, Tokens/sec: 41355.78\n",
      "Step: 1341, Training Loss: 3.80415, LR: 0.0008935, Tokens/sec: 39261.31\n",
      "Step: 1342, Training Loss: 3.62654, LR: 0.0008932, Tokens/sec: 39821.78\n",
      "Step: 1343, Training Loss: 3.49748, LR: 0.0008930, Tokens/sec: 38876.28\n",
      "Step: 1344, Training Loss: 3.61627, LR: 0.0008928, Tokens/sec: 40917.42\n",
      "Step: 1345, Training Loss: 3.38838, LR: 0.0008925, Tokens/sec: 40231.66\n",
      "Step: 1346, Training Loss: 3.76587, LR: 0.0008923, Tokens/sec: 36387.22\n",
      "Step: 1347, Training Loss: 3.80923, LR: 0.0008921, Tokens/sec: 37895.34\n",
      "Step: 1348, Training Loss: 3.70353, LR: 0.0008919, Tokens/sec: 39436.32\n",
      "Step: 1349, Training Loss: 3.62943, LR: 0.0008916, Tokens/sec: 40948.40\n",
      "Step: 1350, Training Loss: 3.46674, LR: 0.0008914, Tokens/sec: 40117.26\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 1350, Eval Loss: 3.64684\n",
      "Step: 1351, Training Loss: 3.58669, LR: 0.0008912, Tokens/sec: 40393.65\n",
      "Step: 1352, Training Loss: 3.68527, LR: 0.0008909, Tokens/sec: 41342.92\n",
      "Step: 1353, Training Loss: 3.84563, LR: 0.0008907, Tokens/sec: 38220.48\n",
      "Step: 1354, Training Loss: 3.49760, LR: 0.0008905, Tokens/sec: 40918.23\n",
      "Step: 1355, Training Loss: 3.25117, LR: 0.0008902, Tokens/sec: 40979.24\n",
      "Step: 1356, Training Loss: 3.73887, LR: 0.0008900, Tokens/sec: 41437.24\n",
      "Step: 1357, Training Loss: 3.65499, LR: 0.0008898, Tokens/sec: 41430.82\n",
      "Step: 1358, Training Loss: 3.51455, LR: 0.0008896, Tokens/sec: 40900.09\n",
      "Step: 1359, Training Loss: 3.71819, LR: 0.0008893, Tokens/sec: 41375.73\n",
      "Step: 1360, Training Loss: 3.56349, LR: 0.0008891, Tokens/sec: 41337.95\n",
      "Step: 1361, Training Loss: 3.70062, LR: 0.0008889, Tokens/sec: 38310.42\n",
      "Step: 1362, Training Loss: 3.84788, LR: 0.0008886, Tokens/sec: 40895.55\n",
      "Step: 1363, Training Loss: 3.76268, LR: 0.0008884, Tokens/sec: 40866.84\n",
      "Step: 1364, Training Loss: 3.80369, LR: 0.0008882, Tokens/sec: 37801.15\n",
      "Step: 1365, Training Loss: 3.91725, LR: 0.0008879, Tokens/sec: 40164.67\n",
      "Step: 1366, Training Loss: 3.70646, LR: 0.0008877, Tokens/sec: 41321.82\n",
      "Step: 1367, Training Loss: 3.48661, LR: 0.0008875, Tokens/sec: 41346.80\n",
      "Step: 1368, Training Loss: 3.68322, LR: 0.0008872, Tokens/sec: 40852.38\n",
      "Step: 1369, Training Loss: 3.70373, LR: 0.0008870, Tokens/sec: 38322.19\n",
      "Step: 1370, Training Loss: 3.41975, LR: 0.0008868, Tokens/sec: 41299.77\n",
      "Step: 1371, Training Loss: 3.91535, LR: 0.0008865, Tokens/sec: 39191.20\n",
      "Step: 1372, Training Loss: 3.80389, LR: 0.0008863, Tokens/sec: 38828.47\n",
      "Step: 1373, Training Loss: 3.51937, LR: 0.0008861, Tokens/sec: 41361.37\n",
      "Step: 1374, Training Loss: 3.61165, LR: 0.0008858, Tokens/sec: 38528.58\n",
      "Step: 1375, Training Loss: 3.34740, LR: 0.0008856, Tokens/sec: 39715.67\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 1375, Eval Loss: 3.58439\n",
      "Step: 1376, Training Loss: 3.59990, LR: 0.0008853, Tokens/sec: 41232.29\n",
      "Step: 1377, Training Loss: 3.42063, LR: 0.0008851, Tokens/sec: 39880.95\n",
      "Step: 1378, Training Loss: 3.63283, LR: 0.0008849, Tokens/sec: 38983.76\n",
      "Step: 1379, Training Loss: 3.86583, LR: 0.0008846, Tokens/sec: 40527.11\n",
      "Step: 1380, Training Loss: 4.03691, LR: 0.0008844, Tokens/sec: 38892.47\n",
      "Step: 1381, Training Loss: 3.41607, LR: 0.0008842, Tokens/sec: 41315.54\n",
      "Step: 1382, Training Loss: 3.66789, LR: 0.0008839, Tokens/sec: 41335.39\n",
      "Step: 1383, Training Loss: 3.56503, LR: 0.0008837, Tokens/sec: 40905.79\n",
      "Step: 1384, Training Loss: 3.59261, LR: 0.0008835, Tokens/sec: 40560.31\n",
      "Step: 1385, Training Loss: 3.86816, LR: 0.0008832, Tokens/sec: 40649.82\n",
      "Step: 1386, Training Loss: 3.91220, LR: 0.0008830, Tokens/sec: 41228.61\n",
      "Step: 1387, Training Loss: 3.51626, LR: 0.0008827, Tokens/sec: 41316.91\n",
      "Step: 1388, Training Loss: 3.69485, LR: 0.0008825, Tokens/sec: 40501.51\n",
      "Step: 1389, Training Loss: 3.44340, LR: 0.0008823, Tokens/sec: 39016.37\n",
      "Step: 1390, Training Loss: 4.08446, LR: 0.0008820, Tokens/sec: 38977.32\n",
      "Step: 1391, Training Loss: 3.58131, LR: 0.0008818, Tokens/sec: 41292.49\n",
      "Step: 1392, Training Loss: 3.72431, LR: 0.0008816, Tokens/sec: 41382.30\n",
      "Step: 1393, Training Loss: 3.85860, LR: 0.0008813, Tokens/sec: 41311.00\n",
      "Step: 1394, Training Loss: 3.72053, LR: 0.0008811, Tokens/sec: 39992.20\n",
      "Step: 1395, Training Loss: 3.66465, LR: 0.0008808, Tokens/sec: 41303.10\n",
      "Step: 1396, Training Loss: 3.47024, LR: 0.0008806, Tokens/sec: 39812.90\n",
      "Step: 1397, Training Loss: 3.31584, LR: 0.0008804, Tokens/sec: 41303.07\n",
      "Step: 1398, Training Loss: 3.82469, LR: 0.0008801, Tokens/sec: 41369.18\n",
      "Step: 1399, Training Loss: 3.93685, LR: 0.0008799, Tokens/sec: 38556.36\n",
      "Step: 1400, Training Loss: 3.25091, LR: 0.0008796, Tokens/sec: 41294.27\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 1400, Eval Loss: 3.58952\n",
      "Step: 1401, Training Loss: 3.87707, LR: 0.0008794, Tokens/sec: 40643.42\n",
      "Step: 1402, Training Loss: 3.47618, LR: 0.0008792, Tokens/sec: 41190.60\n",
      "Step: 1403, Training Loss: 3.49080, LR: 0.0008789, Tokens/sec: 39763.54\n",
      "Step: 1404, Training Loss: 3.64823, LR: 0.0008787, Tokens/sec: 41393.53\n",
      "Step: 1405, Training Loss: 3.22206, LR: 0.0008784, Tokens/sec: 40851.20\n",
      "Step: 1406, Training Loss: 3.55588, LR: 0.0008782, Tokens/sec: 41452.15\n",
      "Step: 1407, Training Loss: 3.26497, LR: 0.0008780, Tokens/sec: 39825.58\n",
      "Step: 1408, Training Loss: 3.48310, LR: 0.0008777, Tokens/sec: 35941.70\n",
      "Step: 1409, Training Loss: 3.53233, LR: 0.0008775, Tokens/sec: 40871.75\n",
      "Step: 1410, Training Loss: 3.66665, LR: 0.0008772, Tokens/sec: 39634.75\n",
      "Step: 1411, Training Loss: 3.54139, LR: 0.0008770, Tokens/sec: 38162.19\n",
      "Step: 1412, Training Loss: 3.59059, LR: 0.0008767, Tokens/sec: 40622.36\n",
      "Step: 1413, Training Loss: 4.05296, LR: 0.0008765, Tokens/sec: 39820.26\n",
      "Step: 1414, Training Loss: 3.69231, LR: 0.0008763, Tokens/sec: 41216.70\n",
      "Step: 1415, Training Loss: 3.65554, LR: 0.0008760, Tokens/sec: 40071.18\n",
      "Step: 1416, Training Loss: 3.64814, LR: 0.0008758, Tokens/sec: 41346.61\n",
      "Step: 1417, Training Loss: 3.59779, LR: 0.0008755, Tokens/sec: 41330.41\n",
      "Step: 1418, Training Loss: 3.57288, LR: 0.0008753, Tokens/sec: 41009.79\n",
      "Step: 1419, Training Loss: 3.67406, LR: 0.0008750, Tokens/sec: 41334.43\n",
      "Step: 1420, Training Loss: 3.77594, LR: 0.0008748, Tokens/sec: 41240.33\n",
      "Step: 1421, Training Loss: 3.53211, LR: 0.0008746, Tokens/sec: 39105.12\n",
      "Step: 1422, Training Loss: 3.79307, LR: 0.0008743, Tokens/sec: 40994.62\n",
      "Step: 1423, Training Loss: 3.74061, LR: 0.0008741, Tokens/sec: 41359.66\n",
      "Step: 1424, Training Loss: 3.10699, LR: 0.0008738, Tokens/sec: 41299.01\n",
      "Step: 1425, Training Loss: 3.51058, LR: 0.0008736, Tokens/sec: 41449.90\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 1425, Eval Loss: 3.61048\n",
      "Step: 1426, Training Loss: 3.54045, LR: 0.0008733, Tokens/sec: 41176.19\n",
      "Step: 1427, Training Loss: 3.99556, LR: 0.0008731, Tokens/sec: 41259.08\n",
      "Step: 1428, Training Loss: 3.37133, LR: 0.0008728, Tokens/sec: 41090.08\n",
      "Step: 1429, Training Loss: 3.60923, LR: 0.0008726, Tokens/sec: 32992.96\n",
      "Step: 1430, Training Loss: 3.33243, LR: 0.0008723, Tokens/sec: 39546.33\n",
      "Step: 1431, Training Loss: 3.57946, LR: 0.0008721, Tokens/sec: 40278.03\n",
      "Step: 1432, Training Loss: 3.62906, LR: 0.0008719, Tokens/sec: 37895.29\n",
      "Step: 1433, Training Loss: 3.59967, LR: 0.0008716, Tokens/sec: 40393.31\n",
      "Step: 1434, Training Loss: 3.27180, LR: 0.0008714, Tokens/sec: 40274.00\n",
      "Step: 1435, Training Loss: 3.41207, LR: 0.0008711, Tokens/sec: 41193.41\n",
      "Step: 1436, Training Loss: 3.77728, LR: 0.0008709, Tokens/sec: 40998.78\n",
      "Step: 1437, Training Loss: 3.84972, LR: 0.0008706, Tokens/sec: 39570.62\n",
      "Step: 1438, Training Loss: 3.63231, LR: 0.0008704, Tokens/sec: 41454.69\n",
      "Step: 1439, Training Loss: 3.43329, LR: 0.0008701, Tokens/sec: 40380.40\n",
      "Step: 1440, Training Loss: 3.69714, LR: 0.0008699, Tokens/sec: 41470.17\n",
      "Step: 1441, Training Loss: 3.70921, LR: 0.0008696, Tokens/sec: 41514.26\n",
      "Step: 1442, Training Loss: 3.52445, LR: 0.0008694, Tokens/sec: 40406.93\n",
      "Step: 1443, Training Loss: 3.51491, LR: 0.0008691, Tokens/sec: 41328.40\n",
      "Step: 1444, Training Loss: 3.35821, LR: 0.0008689, Tokens/sec: 41370.53\n",
      "Step: 1445, Training Loss: 3.69744, LR: 0.0008686, Tokens/sec: 41449.23\n",
      "Step: 1446, Training Loss: 3.59768, LR: 0.0008684, Tokens/sec: 41423.64\n",
      "Step: 1447, Training Loss: 3.68142, LR: 0.0008681, Tokens/sec: 29475.46\n",
      "Step: 1448, Training Loss: 3.43170, LR: 0.0008679, Tokens/sec: 41239.31\n",
      "Step: 1449, Training Loss: 3.86278, LR: 0.0008676, Tokens/sec: 37561.55\n",
      "Step: 1450, Training Loss: 3.92706, LR: 0.0008674, Tokens/sec: 41038.91\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 1450, Eval Loss: 3.58322\n",
      "Step: 1451, Training Loss: 3.80531, LR: 0.0008671, Tokens/sec: 41366.43\n",
      "Step: 1452, Training Loss: 3.83906, LR: 0.0008669, Tokens/sec: 41378.84\n",
      "Step: 1453, Training Loss: 3.26000, LR: 0.0008666, Tokens/sec: 41537.88\n",
      "Step: 1454, Training Loss: 3.72654, LR: 0.0008664, Tokens/sec: 40457.85\n",
      "Step: 1455, Training Loss: 3.93434, LR: 0.0008661, Tokens/sec: 41179.69\n",
      "Step: 1456, Training Loss: 3.39243, LR: 0.0008659, Tokens/sec: 41139.98\n",
      "Step: 1457, Training Loss: 3.61926, LR: 0.0008656, Tokens/sec: 40598.34\n",
      "Step: 1458, Training Loss: 3.71812, LR: 0.0008654, Tokens/sec: 40597.87\n",
      "Step: 1459, Training Loss: 3.80225, LR: 0.0008651, Tokens/sec: 40958.06\n",
      "Step: 1460, Training Loss: 3.39224, LR: 0.0008649, Tokens/sec: 37712.95\n",
      "Step: 1461, Training Loss: 3.72263, LR: 0.0008646, Tokens/sec: 41394.00\n",
      "Step: 1462, Training Loss: 3.76948, LR: 0.0008644, Tokens/sec: 41448.93\n",
      "Step: 1463, Training Loss: 4.05121, LR: 0.0008641, Tokens/sec: 41504.77\n",
      "Step: 1464, Training Loss: 3.74406, LR: 0.0008639, Tokens/sec: 39946.85\n",
      "Step: 1465, Training Loss: 3.77799, LR: 0.0008636, Tokens/sec: 41563.18\n",
      "Step: 1466, Training Loss: 3.69973, LR: 0.0008634, Tokens/sec: 41221.95\n",
      "Step: 1467, Training Loss: 3.44039, LR: 0.0008631, Tokens/sec: 41089.43\n",
      "Step: 1468, Training Loss: 3.39902, LR: 0.0008629, Tokens/sec: 41493.57\n",
      "Step: 1469, Training Loss: 3.35471, LR: 0.0008626, Tokens/sec: 41420.40\n",
      "Step: 1470, Training Loss: 3.68690, LR: 0.0008623, Tokens/sec: 41424.29\n",
      "Step: 1471, Training Loss: 3.83006, LR: 0.0008621, Tokens/sec: 27114.35\n",
      "Step: 1472, Training Loss: 3.68864, LR: 0.0008618, Tokens/sec: 40579.13\n",
      "Step: 1473, Training Loss: 3.52003, LR: 0.0008616, Tokens/sec: 40525.11\n",
      "Step: 1474, Training Loss: 3.51310, LR: 0.0008613, Tokens/sec: 40377.10\n",
      "Step: 1475, Training Loss: 3.84537, LR: 0.0008611, Tokens/sec: 40029.10\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 1475, Eval Loss: 3.50681\n",
      "Step: 1476, Training Loss: 3.38822, LR: 0.0008608, Tokens/sec: 40229.96\n",
      "Step: 1477, Training Loss: 3.61348, LR: 0.0008606, Tokens/sec: 41020.75\n",
      "Step: 1478, Training Loss: 3.52017, LR: 0.0008603, Tokens/sec: 39475.63\n",
      "Step: 1479, Training Loss: 3.29728, LR: 0.0008601, Tokens/sec: 40741.24\n",
      "Step: 1480, Training Loss: 3.19534, LR: 0.0008598, Tokens/sec: 39870.71\n",
      "Step: 1481, Training Loss: 3.58505, LR: 0.0008595, Tokens/sec: 36661.91\n",
      "Step: 1482, Training Loss: 3.39276, LR: 0.0008593, Tokens/sec: 41345.85\n",
      "Step: 1483, Training Loss: 3.82126, LR: 0.0008590, Tokens/sec: 40255.45\n",
      "Step: 1484, Training Loss: 3.52750, LR: 0.0008588, Tokens/sec: 41499.82\n",
      "Step: 1485, Training Loss: 3.80061, LR: 0.0008585, Tokens/sec: 41472.76\n",
      "Step: 1486, Training Loss: 3.39688, LR: 0.0008583, Tokens/sec: 41366.26\n",
      "Step: 1487, Training Loss: 3.23488, LR: 0.0008580, Tokens/sec: 41129.08\n",
      "Step: 1488, Training Loss: 3.24154, LR: 0.0008577, Tokens/sec: 41541.16\n",
      "Step: 1489, Training Loss: 3.46515, LR: 0.0008575, Tokens/sec: 41535.53\n",
      "Step: 1490, Training Loss: 3.25069, LR: 0.0008572, Tokens/sec: 37921.70\n",
      "Step: 1491, Training Loss: 3.70262, LR: 0.0008570, Tokens/sec: 31365.89\n",
      "Step: 1492, Training Loss: 3.28141, LR: 0.0008567, Tokens/sec: 37991.11\n",
      "Step: 1493, Training Loss: 3.47863, LR: 0.0008565, Tokens/sec: 41058.78\n",
      "Step: 1494, Training Loss: 3.40762, LR: 0.0008562, Tokens/sec: 41279.62\n",
      "Step: 1495, Training Loss: 3.62197, LR: 0.0008559, Tokens/sec: 38400.21\n",
      "Step: 1496, Training Loss: 3.73365, LR: 0.0008557, Tokens/sec: 41382.45\n",
      "Step: 1497, Training Loss: 3.64581, LR: 0.0008554, Tokens/sec: 40388.59\n",
      "Step: 1498, Training Loss: 3.32504, LR: 0.0008552, Tokens/sec: 40662.64\n",
      "Step: 1499, Training Loss: 3.83314, LR: 0.0008549, Tokens/sec: 40854.28\n",
      "Step: 1500, Training Loss: 3.34149, LR: 0.0008546, Tokens/sec: 41089.94\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 1500, Eval Loss: 3.55002\n",
      "Step: 1501, Training Loss: 3.75212, LR: 0.0008544, Tokens/sec: 37281.96\n",
      "Step: 1502, Training Loss: 3.56610, LR: 0.0008541, Tokens/sec: 40307.23\n",
      "Step: 1503, Training Loss: 3.38113, LR: 0.0008539, Tokens/sec: 41439.90\n",
      "Step: 1504, Training Loss: 3.72376, LR: 0.0008536, Tokens/sec: 41497.78\n",
      "Step: 1505, Training Loss: 4.02494, LR: 0.0008533, Tokens/sec: 41060.88\n",
      "Step: 1506, Training Loss: 3.51303, LR: 0.0008531, Tokens/sec: 41529.57\n",
      "Step: 1507, Training Loss: 3.63416, LR: 0.0008528, Tokens/sec: 40838.05\n",
      "Step: 1508, Training Loss: 3.53003, LR: 0.0008526, Tokens/sec: 31831.99\n",
      "Step: 1509, Training Loss: 3.78479, LR: 0.0008523, Tokens/sec: 40840.40\n",
      "Step: 1510, Training Loss: 3.24803, LR: 0.0008520, Tokens/sec: 40519.82\n",
      "Step: 1511, Training Loss: 3.87121, LR: 0.0008518, Tokens/sec: 41309.17\n",
      "Step: 1512, Training Loss: 3.67002, LR: 0.0008515, Tokens/sec: 41341.63\n",
      "Step: 1513, Training Loss: 3.61659, LR: 0.0008512, Tokens/sec: 40926.85\n",
      "Step: 1514, Training Loss: 3.88374, LR: 0.0008510, Tokens/sec: 40866.62\n",
      "Step: 1515, Training Loss: 3.74083, LR: 0.0008507, Tokens/sec: 41077.10\n",
      "Step: 1516, Training Loss: 3.34617, LR: 0.0008505, Tokens/sec: 39503.82\n",
      "Step: 1517, Training Loss: 3.28281, LR: 0.0008502, Tokens/sec: 36378.11\n",
      "Step: 1518, Training Loss: 3.26794, LR: 0.0008499, Tokens/sec: 36789.82\n",
      "Step: 1519, Training Loss: 3.45756, LR: 0.0008497, Tokens/sec: 39248.37\n",
      "Step: 1520, Training Loss: 3.50014, LR: 0.0008494, Tokens/sec: 41294.99\n",
      "Step: 1521, Training Loss: 3.67034, LR: 0.0008491, Tokens/sec: 41468.96\n",
      "Step: 1522, Training Loss: 3.56412, LR: 0.0008489, Tokens/sec: 41459.33\n",
      "Step: 1523, Training Loss: 3.72450, LR: 0.0008486, Tokens/sec: 36494.31\n",
      "Step: 1524, Training Loss: 3.63387, LR: 0.0008484, Tokens/sec: 36650.42\n",
      "Step: 1525, Training Loss: 3.42840, LR: 0.0008481, Tokens/sec: 40782.08\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 1525, Eval Loss: 3.53208\n",
      "Step: 1526, Training Loss: 3.16964, LR: 0.0008478, Tokens/sec: 39665.60\n",
      "Step: 1527, Training Loss: 3.27701, LR: 0.0008476, Tokens/sec: 40962.17\n",
      "Step: 1528, Training Loss: 3.69447, LR: 0.0008473, Tokens/sec: 39552.55\n",
      "Step: 1529, Training Loss: 3.57077, LR: 0.0008470, Tokens/sec: 40014.23\n",
      "Step: 1530, Training Loss: 3.67215, LR: 0.0008468, Tokens/sec: 37634.72\n",
      "Step: 1531, Training Loss: 3.53186, LR: 0.0008465, Tokens/sec: 33964.02\n",
      "Step: 1532, Training Loss: 3.42786, LR: 0.0008462, Tokens/sec: 36121.50\n",
      "Step: 1533, Training Loss: 3.61807, LR: 0.0008460, Tokens/sec: 31928.02\n",
      "Step: 1534, Training Loss: 3.54767, LR: 0.0008457, Tokens/sec: 37326.20\n",
      "Step: 1535, Training Loss: 3.35221, LR: 0.0008454, Tokens/sec: 37370.19\n",
      "Step: 1536, Training Loss: 3.83805, LR: 0.0008452, Tokens/sec: 39084.51\n",
      "Step: 1537, Training Loss: 3.78539, LR: 0.0008449, Tokens/sec: 39912.50\n",
      "Step: 1538, Training Loss: 3.09588, LR: 0.0008446, Tokens/sec: 40192.99\n",
      "Step: 1539, Training Loss: 3.81706, LR: 0.0008444, Tokens/sec: 27378.61\n",
      "Step: 1540, Training Loss: 3.12274, LR: 0.0008441, Tokens/sec: 38154.54\n",
      "Step: 1541, Training Loss: 3.37851, LR: 0.0008438, Tokens/sec: 39873.51\n",
      "Step: 1542, Training Loss: 3.38875, LR: 0.0008436, Tokens/sec: 39898.59\n",
      "Step: 1543, Training Loss: 3.54740, LR: 0.0008433, Tokens/sec: 35594.57\n",
      "Step: 1544, Training Loss: 3.50533, LR: 0.0008430, Tokens/sec: 34459.54\n",
      "Step: 1545, Training Loss: 3.27996, LR: 0.0008428, Tokens/sec: 38086.75\n",
      "Step: 1546, Training Loss: 3.59597, LR: 0.0008425, Tokens/sec: 41323.22\n",
      "Step: 1547, Training Loss: 3.15359, LR: 0.0008422, Tokens/sec: 40541.26\n",
      "Step: 1548, Training Loss: 3.50233, LR: 0.0008420, Tokens/sec: 29608.57\n",
      "Step: 1549, Training Loss: 3.59166, LR: 0.0008417, Tokens/sec: 38536.10\n",
      "Step: 1550, Training Loss: 3.79502, LR: 0.0008414, Tokens/sec: 40895.09\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 1550, Eval Loss: 3.50907\n",
      "Step: 1551, Training Loss: 3.65932, LR: 0.0008412, Tokens/sec: 41320.17\n",
      "Step: 1552, Training Loss: 3.67969, LR: 0.0008409, Tokens/sec: 40703.14\n",
      "Step: 1553, Training Loss: 3.36043, LR: 0.0008406, Tokens/sec: 41355.79\n",
      "Step: 1554, Training Loss: 3.73709, LR: 0.0008403, Tokens/sec: 41327.69\n",
      "Step: 1555, Training Loss: 3.48693, LR: 0.0008401, Tokens/sec: 40248.87\n",
      "Step: 1556, Training Loss: 3.40080, LR: 0.0008398, Tokens/sec: 41287.53\n",
      "Step: 1557, Training Loss: 3.41115, LR: 0.0008395, Tokens/sec: 39240.33\n",
      "Step: 1558, Training Loss: 3.42220, LR: 0.0008393, Tokens/sec: 39906.86\n",
      "Step: 1559, Training Loss: 3.50249, LR: 0.0008390, Tokens/sec: 40025.02\n",
      "Step: 1560, Training Loss: 3.30511, LR: 0.0008387, Tokens/sec: 40477.98\n",
      "Step: 1561, Training Loss: 3.73170, LR: 0.0008385, Tokens/sec: 41405.79\n",
      "Step: 1562, Training Loss: 3.42807, LR: 0.0008382, Tokens/sec: 41393.42\n",
      "Step: 1563, Training Loss: 3.76745, LR: 0.0008379, Tokens/sec: 40967.33\n",
      "Step: 1564, Training Loss: 3.16532, LR: 0.0008376, Tokens/sec: 35415.02\n",
      "Step: 1565, Training Loss: 3.19277, LR: 0.0008374, Tokens/sec: 38615.00\n",
      "Step: 1566, Training Loss: 3.22683, LR: 0.0008371, Tokens/sec: 39458.71\n",
      "Step: 1567, Training Loss: 3.29384, LR: 0.0008368, Tokens/sec: 39574.63\n",
      "Step: 1568, Training Loss: 3.40994, LR: 0.0008366, Tokens/sec: 40171.31\n",
      "Step: 1569, Training Loss: 3.08629, LR: 0.0008363, Tokens/sec: 41260.18\n",
      "Step: 1570, Training Loss: 3.81783, LR: 0.0008360, Tokens/sec: 41258.58\n",
      "Step: 1571, Training Loss: 3.40855, LR: 0.0008357, Tokens/sec: 40301.69\n",
      "Step: 1572, Training Loss: 3.33168, LR: 0.0008355, Tokens/sec: 40513.61\n",
      "Step: 1573, Training Loss: 3.60143, LR: 0.0008352, Tokens/sec: 40548.16\n",
      "Step: 1574, Training Loss: 3.37104, LR: 0.0008349, Tokens/sec: 41201.34\n",
      "Step: 1575, Training Loss: 3.37210, LR: 0.0008346, Tokens/sec: 28469.48\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 1575, Eval Loss: 3.51575\n",
      "Step: 1576, Training Loss: 3.82866, LR: 0.0008344, Tokens/sec: 41546.97\n",
      "Step: 1577, Training Loss: 3.57645, LR: 0.0008341, Tokens/sec: 41658.39\n",
      "Step: 1578, Training Loss: 3.16109, LR: 0.0008338, Tokens/sec: 41621.61\n",
      "Step: 1579, Training Loss: 3.37167, LR: 0.0008336, Tokens/sec: 41056.29\n",
      "Step: 1580, Training Loss: 4.05288, LR: 0.0008333, Tokens/sec: 39225.18\n",
      "Step: 1581, Training Loss: 3.53868, LR: 0.0008330, Tokens/sec: 39794.61\n",
      "Step: 1582, Training Loss: 2.89961, LR: 0.0008327, Tokens/sec: 40158.28\n",
      "Step: 1583, Training Loss: 3.14317, LR: 0.0008325, Tokens/sec: 39393.48\n",
      "Step: 1584, Training Loss: 3.62186, LR: 0.0008322, Tokens/sec: 40097.61\n",
      "Step: 1585, Training Loss: 3.33807, LR: 0.0008319, Tokens/sec: 40670.84\n",
      "Step: 1586, Training Loss: 3.26678, LR: 0.0008316, Tokens/sec: 41038.51\n",
      "Step: 1587, Training Loss: 3.65818, LR: 0.0008314, Tokens/sec: 41123.57\n",
      "Step: 1588, Training Loss: 3.12751, LR: 0.0008311, Tokens/sec: 39515.74\n",
      "Step: 1589, Training Loss: 3.39826, LR: 0.0008308, Tokens/sec: 39228.78\n",
      "Step: 1590, Training Loss: 3.34142, LR: 0.0008305, Tokens/sec: 39902.18\n",
      "Step: 1591, Training Loss: 2.97337, LR: 0.0008303, Tokens/sec: 40960.37\n",
      "Step: 1592, Training Loss: 3.31405, LR: 0.0008300, Tokens/sec: 39211.48\n",
      "Step: 1593, Training Loss: 3.61243, LR: 0.0008297, Tokens/sec: 39627.92\n",
      "Step: 1594, Training Loss: 3.51571, LR: 0.0008294, Tokens/sec: 41011.98\n",
      "Step: 1595, Training Loss: 3.76603, LR: 0.0008291, Tokens/sec: 39322.66\n",
      "Step: 1596, Training Loss: 3.71139, LR: 0.0008289, Tokens/sec: 41621.78\n",
      "Step: 1597, Training Loss: 3.49271, LR: 0.0008286, Tokens/sec: 41299.25\n",
      "Step: 1598, Training Loss: 3.28591, LR: 0.0008283, Tokens/sec: 41626.54\n",
      "Step: 1599, Training Loss: 3.50576, LR: 0.0008280, Tokens/sec: 41292.67\n",
      "Step: 1600, Training Loss: 3.56154, LR: 0.0008278, Tokens/sec: 39465.61\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 1600, Eval Loss: 3.45870\n",
      "Step: 1601, Training Loss: 3.51689, LR: 0.0008275, Tokens/sec: 41407.64\n",
      "Step: 1602, Training Loss: 3.30797, LR: 0.0008272, Tokens/sec: 41618.21\n",
      "Step: 1603, Training Loss: 3.82371, LR: 0.0008269, Tokens/sec: 41761.18\n",
      "Step: 1604, Training Loss: 3.54776, LR: 0.0008267, Tokens/sec: 41794.11\n",
      "Step: 1605, Training Loss: 3.44142, LR: 0.0008264, Tokens/sec: 41596.90\n",
      "Step: 1606, Training Loss: 3.16317, LR: 0.0008261, Tokens/sec: 41700.02\n",
      "Step: 1607, Training Loss: 3.23224, LR: 0.0008258, Tokens/sec: 41770.49\n",
      "Step: 1608, Training Loss: 3.54856, LR: 0.0008255, Tokens/sec: 41075.87\n",
      "Step: 1609, Training Loss: 3.69896, LR: 0.0008253, Tokens/sec: 41455.72\n",
      "Step: 1610, Training Loss: 3.82787, LR: 0.0008250, Tokens/sec: 41509.38\n",
      "Step: 1611, Training Loss: 3.62185, LR: 0.0008247, Tokens/sec: 40435.21\n",
      "Step: 1612, Training Loss: 3.65117, LR: 0.0008244, Tokens/sec: 41548.99\n",
      "Step: 1613, Training Loss: 3.52186, LR: 0.0008241, Tokens/sec: 41518.68\n",
      "Step: 1614, Training Loss: 3.10622, LR: 0.0008239, Tokens/sec: 40771.06\n",
      "Step: 1615, Training Loss: 3.41624, LR: 0.0008236, Tokens/sec: 41527.19\n",
      "Step: 1616, Training Loss: 3.21987, LR: 0.0008233, Tokens/sec: 39777.81\n",
      "Step: 1617, Training Loss: 3.70657, LR: 0.0008230, Tokens/sec: 40456.01\n",
      "Step: 1618, Training Loss: 3.51338, LR: 0.0008227, Tokens/sec: 41691.50\n",
      "Step: 1619, Training Loss: 3.50970, LR: 0.0008225, Tokens/sec: 41234.72\n",
      "Step: 1620, Training Loss: 3.72432, LR: 0.0008222, Tokens/sec: 39295.26\n",
      "Step: 1621, Training Loss: 3.53841, LR: 0.0008219, Tokens/sec: 39789.03\n",
      "Step: 1622, Training Loss: 3.01657, LR: 0.0008216, Tokens/sec: 41151.07\n",
      "Step: 1623, Training Loss: 3.49960, LR: 0.0008213, Tokens/sec: 41261.51\n",
      "Step: 1624, Training Loss: 3.65986, LR: 0.0008211, Tokens/sec: 39405.74\n",
      "Step: 1625, Training Loss: 3.73720, LR: 0.0008208, Tokens/sec: 41317.88\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 1625, Eval Loss: 3.58147\n",
      "Step: 1626, Training Loss: 3.25906, LR: 0.0008205, Tokens/sec: 41495.21\n",
      "Step: 1627, Training Loss: 3.48033, LR: 0.0008202, Tokens/sec: 40342.85\n",
      "Step: 1628, Training Loss: 3.74659, LR: 0.0008199, Tokens/sec: 41667.38\n",
      "Step: 1629, Training Loss: 3.24402, LR: 0.0008196, Tokens/sec: 40595.81\n",
      "Step: 1630, Training Loss: 3.97089, LR: 0.0008194, Tokens/sec: 37331.48\n",
      "Step: 1631, Training Loss: 3.67312, LR: 0.0008191, Tokens/sec: 41125.39\n",
      "Step: 1632, Training Loss: 3.70870, LR: 0.0008188, Tokens/sec: 41257.26\n",
      "Step: 1633, Training Loss: 3.23178, LR: 0.0008185, Tokens/sec: 40721.11\n",
      "Step: 1634, Training Loss: 3.31428, LR: 0.0008182, Tokens/sec: 39516.43\n",
      "Step: 1635, Training Loss: 3.58656, LR: 0.0008179, Tokens/sec: 40512.83\n",
      "Step: 1636, Training Loss: 3.19426, LR: 0.0008177, Tokens/sec: 40013.51\n",
      "Step: 1637, Training Loss: 3.69659, LR: 0.0008174, Tokens/sec: 40123.59\n",
      "Step: 1638, Training Loss: 3.42540, LR: 0.0008171, Tokens/sec: 41649.62\n",
      "Step: 1639, Training Loss: 3.32888, LR: 0.0008168, Tokens/sec: 41630.53\n",
      "Step: 1640, Training Loss: 3.18637, LR: 0.0008165, Tokens/sec: 39823.95\n",
      "Step: 1641, Training Loss: 3.37643, LR: 0.0008162, Tokens/sec: 41647.42\n",
      "Step: 1642, Training Loss: 3.57094, LR: 0.0008160, Tokens/sec: 41637.68\n",
      "Step: 1643, Training Loss: 3.16537, LR: 0.0008157, Tokens/sec: 39257.18\n",
      "Step: 1644, Training Loss: 3.38018, LR: 0.0008154, Tokens/sec: 41229.67\n",
      "Step: 1645, Training Loss: 3.36997, LR: 0.0008151, Tokens/sec: 41644.05\n",
      "Step: 1646, Training Loss: 3.62690, LR: 0.0008148, Tokens/sec: 41617.53\n",
      "Step: 1647, Training Loss: 3.29548, LR: 0.0008145, Tokens/sec: 41615.67\n",
      "Step: 1648, Training Loss: 3.43817, LR: 0.0008142, Tokens/sec: 40440.17\n",
      "Step: 1649, Training Loss: 3.32933, LR: 0.0008140, Tokens/sec: 39842.06\n",
      "Step: 1650, Training Loss: 3.24397, LR: 0.0008137, Tokens/sec: 41400.14\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 1650, Eval Loss: 3.41085\n",
      "Step: 1651, Training Loss: 3.13757, LR: 0.0008134, Tokens/sec: 41353.86\n",
      "Step: 1652, Training Loss: 3.39801, LR: 0.0008131, Tokens/sec: 41039.75\n",
      "Step: 1653, Training Loss: 3.37960, LR: 0.0008128, Tokens/sec: 40452.85\n",
      "Step: 1654, Training Loss: 3.29302, LR: 0.0008125, Tokens/sec: 40889.37\n",
      "Step: 1655, Training Loss: 3.51920, LR: 0.0008122, Tokens/sec: 41347.27\n",
      "Step: 1656, Training Loss: 3.60758, LR: 0.0008120, Tokens/sec: 41060.24\n",
      "Step: 1657, Training Loss: 3.78563, LR: 0.0008117, Tokens/sec: 41242.47\n",
      "Step: 1658, Training Loss: 3.61498, LR: 0.0008114, Tokens/sec: 41354.15\n",
      "Step: 1659, Training Loss: 3.49744, LR: 0.0008111, Tokens/sec: 41242.80\n",
      "Step: 1660, Training Loss: 3.58381, LR: 0.0008108, Tokens/sec: 41348.74\n",
      "Step: 1661, Training Loss: 3.70643, LR: 0.0008105, Tokens/sec: 40235.10\n",
      "Step: 1662, Training Loss: 3.29450, LR: 0.0008102, Tokens/sec: 37646.72\n",
      "Step: 1663, Training Loss: 3.50650, LR: 0.0008099, Tokens/sec: 41364.39\n",
      "Step: 1664, Training Loss: 3.14405, LR: 0.0008097, Tokens/sec: 41393.04\n",
      "Step: 1665, Training Loss: 3.36087, LR: 0.0008094, Tokens/sec: 39100.80\n",
      "Step: 1666, Training Loss: 3.41981, LR: 0.0008091, Tokens/sec: 40983.30\n",
      "Step: 1667, Training Loss: 3.10687, LR: 0.0008088, Tokens/sec: 41391.36\n",
      "Step: 1668, Training Loss: 3.35111, LR: 0.0008085, Tokens/sec: 41386.24\n",
      "Step: 1669, Training Loss: 3.49731, LR: 0.0008082, Tokens/sec: 40002.10\n",
      "Step: 1670, Training Loss: 3.72608, LR: 0.0008079, Tokens/sec: 41503.33\n",
      "Step: 1671, Training Loss: 3.33489, LR: 0.0008076, Tokens/sec: 39811.12\n",
      "Step: 1672, Training Loss: 3.42945, LR: 0.0008073, Tokens/sec: 40932.35\n",
      "Step: 1673, Training Loss: 3.56055, LR: 0.0008071, Tokens/sec: 37512.14\n",
      "Step: 1674, Training Loss: 3.58595, LR: 0.0008068, Tokens/sec: 40964.29\n",
      "Step: 1675, Training Loss: 3.43644, LR: 0.0008065, Tokens/sec: 41346.01\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 1675, Eval Loss: 3.37524\n",
      "Step: 1676, Training Loss: 3.33006, LR: 0.0008062, Tokens/sec: 40318.29\n",
      "Step: 1677, Training Loss: 3.13034, LR: 0.0008059, Tokens/sec: 41021.54\n",
      "Step: 1678, Training Loss: 3.38388, LR: 0.0008056, Tokens/sec: 41325.82\n",
      "Step: 1679, Training Loss: 3.24984, LR: 0.0008053, Tokens/sec: 41121.16\n",
      "Step: 1680, Training Loss: 3.09448, LR: 0.0008050, Tokens/sec: 41062.38\n",
      "Step: 1681, Training Loss: 3.54991, LR: 0.0008047, Tokens/sec: 40765.36\n",
      "Step: 1682, Training Loss: 3.41211, LR: 0.0008044, Tokens/sec: 41176.65\n",
      "Step: 1683, Training Loss: 3.26615, LR: 0.0008042, Tokens/sec: 40820.11\n",
      "Step: 1684, Training Loss: 3.65085, LR: 0.0008039, Tokens/sec: 39413.77\n",
      "Step: 1685, Training Loss: 3.51703, LR: 0.0008036, Tokens/sec: 41060.11\n",
      "Step: 1686, Training Loss: 3.86466, LR: 0.0008033, Tokens/sec: 41357.82\n",
      "Step: 1687, Training Loss: 3.05507, LR: 0.0008030, Tokens/sec: 39228.56\n",
      "Step: 1688, Training Loss: 3.50182, LR: 0.0008027, Tokens/sec: 41236.96\n",
      "Step: 1689, Training Loss: 3.45829, LR: 0.0008024, Tokens/sec: 41319.55\n",
      "Step: 1690, Training Loss: 3.38658, LR: 0.0008021, Tokens/sec: 37515.09\n",
      "Step: 1691, Training Loss: 3.23399, LR: 0.0008018, Tokens/sec: 41309.43\n",
      "Step: 1692, Training Loss: 3.17934, LR: 0.0008015, Tokens/sec: 40934.07\n",
      "Step: 1693, Training Loss: 3.54809, LR: 0.0008012, Tokens/sec: 40582.50\n",
      "Step: 1694, Training Loss: 3.30301, LR: 0.0008009, Tokens/sec: 40950.70\n",
      "Step: 1695, Training Loss: 3.41806, LR: 0.0008006, Tokens/sec: 41306.77\n",
      "Step: 1696, Training Loss: 3.14679, LR: 0.0008004, Tokens/sec: 41413.58\n",
      "Step: 1697, Training Loss: 3.49082, LR: 0.0008001, Tokens/sec: 41433.49\n",
      "Step: 1698, Training Loss: 3.71873, LR: 0.0007998, Tokens/sec: 40364.02\n",
      "Step: 1699, Training Loss: 3.25714, LR: 0.0007995, Tokens/sec: 41004.86\n",
      "Step: 1700, Training Loss: 3.30786, LR: 0.0007992, Tokens/sec: 41069.55\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 1700, Eval Loss: 3.31882\n",
      "Step: 1701, Training Loss: 3.08147, LR: 0.0007989, Tokens/sec: 39424.50\n",
      "Step: 1702, Training Loss: 3.37526, LR: 0.0007986, Tokens/sec: 40618.36\n",
      "Step: 1703, Training Loss: 3.31473, LR: 0.0007983, Tokens/sec: 39374.76\n",
      "Step: 1704, Training Loss: 3.37316, LR: 0.0007980, Tokens/sec: 40151.38\n",
      "Step: 1705, Training Loss: 3.20556, LR: 0.0007977, Tokens/sec: 36961.37\n",
      "Step: 1706, Training Loss: 3.66998, LR: 0.0007974, Tokens/sec: 40513.72\n",
      "Step: 1707, Training Loss: 3.26289, LR: 0.0007971, Tokens/sec: 39598.88\n",
      "Step: 1708, Training Loss: 3.17066, LR: 0.0007968, Tokens/sec: 39613.52\n",
      "Step: 1709, Training Loss: 3.69860, LR: 0.0007965, Tokens/sec: 41316.09\n",
      "Step: 1710, Training Loss: 3.42237, LR: 0.0007962, Tokens/sec: 37275.24\n",
      "Step: 1711, Training Loss: 2.98310, LR: 0.0007959, Tokens/sec: 40148.92\n",
      "Step: 1712, Training Loss: 3.48284, LR: 0.0007956, Tokens/sec: 40869.97\n",
      "Step: 1713, Training Loss: 3.81919, LR: 0.0007953, Tokens/sec: 39280.76\n",
      "Step: 1714, Training Loss: 3.17823, LR: 0.0007951, Tokens/sec: 40861.26\n",
      "Step: 1715, Training Loss: 3.09001, LR: 0.0007948, Tokens/sec: 40037.93\n",
      "Step: 1716, Training Loss: 3.39022, LR: 0.0007945, Tokens/sec: 38801.09\n",
      "Step: 1717, Training Loss: 3.13763, LR: 0.0007942, Tokens/sec: 40916.16\n",
      "Step: 1718, Training Loss: 3.43320, LR: 0.0007939, Tokens/sec: 39531.12\n",
      "Step: 1719, Training Loss: 3.45237, LR: 0.0007936, Tokens/sec: 41364.89\n",
      "Step: 1720, Training Loss: 3.11001, LR: 0.0007933, Tokens/sec: 40998.65\n",
      "Step: 1721, Training Loss: 3.31223, LR: 0.0007930, Tokens/sec: 38812.45\n",
      "Step: 1722, Training Loss: 3.00132, LR: 0.0007927, Tokens/sec: 40618.24\n",
      "Step: 1723, Training Loss: 3.18626, LR: 0.0007924, Tokens/sec: 40653.37\n",
      "Step: 1724, Training Loss: 3.53210, LR: 0.0007921, Tokens/sec: 40071.26\n",
      "Step: 1725, Training Loss: 3.47713, LR: 0.0007918, Tokens/sec: 39702.51\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 1725, Eval Loss: 3.47006\n",
      "Step: 1726, Training Loss: 3.53233, LR: 0.0007915, Tokens/sec: 39632.43\n",
      "Step: 1727, Training Loss: 3.38988, LR: 0.0007912, Tokens/sec: 41119.37\n",
      "Step: 1728, Training Loss: 3.17531, LR: 0.0007909, Tokens/sec: 38631.70\n",
      "Step: 1729, Training Loss: 3.13858, LR: 0.0007906, Tokens/sec: 38894.70\n",
      "Step: 1730, Training Loss: 2.92050, LR: 0.0007903, Tokens/sec: 40437.63\n",
      "Step: 1731, Training Loss: 3.23367, LR: 0.0007900, Tokens/sec: 40609.29\n",
      "Step: 1732, Training Loss: 3.38042, LR: 0.0007897, Tokens/sec: 40903.75\n",
      "Step: 1733, Training Loss: 3.26931, LR: 0.0007894, Tokens/sec: 41007.31\n",
      "Step: 1734, Training Loss: 3.18306, LR: 0.0007891, Tokens/sec: 37213.80\n",
      "Step: 1735, Training Loss: 3.49854, LR: 0.0007888, Tokens/sec: 38517.52\n",
      "Step: 1736, Training Loss: 3.34532, LR: 0.0007885, Tokens/sec: 40895.70\n",
      "Step: 1737, Training Loss: 3.43902, LR: 0.0007882, Tokens/sec: 40482.91\n",
      "Step: 1738, Training Loss: 3.28024, LR: 0.0007879, Tokens/sec: 39338.06\n",
      "Step: 1739, Training Loss: 3.23741, LR: 0.0007876, Tokens/sec: 40838.82\n",
      "Step: 1740, Training Loss: 3.51861, LR: 0.0007873, Tokens/sec: 40909.49\n",
      "Step: 1741, Training Loss: 3.88603, LR: 0.0007870, Tokens/sec: 38281.31\n",
      "Step: 1742, Training Loss: 2.98830, LR: 0.0007867, Tokens/sec: 37173.98\n",
      "Step: 1743, Training Loss: 3.57231, LR: 0.0007864, Tokens/sec: 41403.94\n",
      "Step: 1744, Training Loss: 3.56855, LR: 0.0007861, Tokens/sec: 40617.90\n",
      "Step: 1745, Training Loss: 3.48365, LR: 0.0007858, Tokens/sec: 41265.15\n",
      "Step: 1746, Training Loss: 3.45374, LR: 0.0007855, Tokens/sec: 41380.31\n",
      "Step: 1747, Training Loss: 3.26378, LR: 0.0007852, Tokens/sec: 35569.28\n",
      "Step: 1748, Training Loss: 2.98287, LR: 0.0007849, Tokens/sec: 41174.62\n",
      "Step: 1749, Training Loss: 3.21532, LR: 0.0007846, Tokens/sec: 41352.47\n",
      "Step: 1750, Training Loss: 3.47001, LR: 0.0007843, Tokens/sec: 41340.61\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 1750, Eval Loss: 3.39675\n",
      "Step: 1751, Training Loss: 3.18004, LR: 0.0007840, Tokens/sec: 40931.38\n",
      "Step: 1752, Training Loss: 3.23711, LR: 0.0007837, Tokens/sec: 38455.41\n",
      "Step: 1753, Training Loss: 3.34510, LR: 0.0007834, Tokens/sec: 41230.57\n",
      "Step: 1754, Training Loss: 3.00721, LR: 0.0007831, Tokens/sec: 40941.17\n",
      "Step: 1755, Training Loss: 3.23333, LR: 0.0007828, Tokens/sec: 40489.09\n",
      "Step: 1756, Training Loss: 3.15829, LR: 0.0007825, Tokens/sec: 41400.14\n",
      "Step: 1757, Training Loss: 3.12175, LR: 0.0007822, Tokens/sec: 37772.69\n",
      "Step: 1758, Training Loss: 3.29834, LR: 0.0007819, Tokens/sec: 40763.83\n",
      "Step: 1759, Training Loss: 3.56397, LR: 0.0007816, Tokens/sec: 41403.75\n",
      "Step: 1760, Training Loss: 2.73464, LR: 0.0007813, Tokens/sec: 41347.23\n",
      "Step: 1761, Training Loss: 3.38223, LR: 0.0007810, Tokens/sec: 40904.39\n",
      "Step: 1762, Training Loss: 3.40774, LR: 0.0007807, Tokens/sec: 40990.76\n",
      "Step: 1763, Training Loss: 3.37164, LR: 0.0007804, Tokens/sec: 37547.34\n",
      "Step: 1764, Training Loss: 3.53059, LR: 0.0007801, Tokens/sec: 41287.98\n",
      "Step: 1765, Training Loss: 3.57436, LR: 0.0007798, Tokens/sec: 41344.76\n",
      "Step: 1766, Training Loss: 3.05937, LR: 0.0007795, Tokens/sec: 38339.88\n",
      "Step: 1767, Training Loss: 3.16505, LR: 0.0007792, Tokens/sec: 41330.58\n",
      "Step: 1768, Training Loss: 3.26339, LR: 0.0007789, Tokens/sec: 41330.32\n",
      "Step: 1769, Training Loss: 3.44057, LR: 0.0007786, Tokens/sec: 41390.38\n",
      "Step: 1770, Training Loss: 3.24966, LR: 0.0007783, Tokens/sec: 40906.10\n",
      "Step: 1771, Training Loss: 2.95938, LR: 0.0007779, Tokens/sec: 39927.68\n",
      "Step: 1772, Training Loss: 3.10929, LR: 0.0007776, Tokens/sec: 41436.37\n",
      "Step: 1773, Training Loss: 3.63488, LR: 0.0007773, Tokens/sec: 41467.12\n",
      "Step: 1774, Training Loss: 3.31182, LR: 0.0007770, Tokens/sec: 40812.58\n",
      "Step: 1775, Training Loss: 3.49226, LR: 0.0007767, Tokens/sec: 38156.80\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 1775, Eval Loss: 3.41388\n",
      "Step: 1776, Training Loss: 3.16905, LR: 0.0007764, Tokens/sec: 40098.98\n",
      "Step: 1777, Training Loss: 3.51720, LR: 0.0007761, Tokens/sec: 39710.14\n",
      "Step: 1778, Training Loss: 3.30330, LR: 0.0007758, Tokens/sec: 41292.25\n",
      "Step: 1779, Training Loss: 3.37662, LR: 0.0007755, Tokens/sec: 41201.45\n",
      "Step: 1780, Training Loss: 3.15242, LR: 0.0007752, Tokens/sec: 41008.31\n",
      "Step: 1781, Training Loss: 3.37291, LR: 0.0007749, Tokens/sec: 41372.46\n",
      "Step: 1782, Training Loss: 3.52765, LR: 0.0007746, Tokens/sec: 41301.91\n",
      "Step: 1783, Training Loss: 3.37641, LR: 0.0007743, Tokens/sec: 41354.45\n",
      "Step: 1784, Training Loss: 3.17159, LR: 0.0007740, Tokens/sec: 40465.59\n",
      "Step: 1785, Training Loss: 3.49036, LR: 0.0007737, Tokens/sec: 40899.12\n",
      "Step: 1786, Training Loss: 3.13327, LR: 0.0007734, Tokens/sec: 41255.15\n",
      "Step: 1787, Training Loss: 3.67563, LR: 0.0007731, Tokens/sec: 39803.70\n",
      "Step: 1788, Training Loss: 3.53487, LR: 0.0007728, Tokens/sec: 41402.65\n",
      "Step: 1789, Training Loss: 3.00708, LR: 0.0007724, Tokens/sec: 41351.11\n",
      "Step: 1790, Training Loss: 3.65211, LR: 0.0007721, Tokens/sec: 41303.20\n",
      "Step: 1791, Training Loss: 3.27008, LR: 0.0007718, Tokens/sec: 41251.61\n",
      "Step: 1792, Training Loss: 3.57680, LR: 0.0007715, Tokens/sec: 41441.65\n",
      "Step: 1793, Training Loss: 3.48997, LR: 0.0007712, Tokens/sec: 41301.52\n",
      "Step: 1794, Training Loss: 3.08863, LR: 0.0007709, Tokens/sec: 40962.45\n",
      "Step: 1795, Training Loss: 3.25321, LR: 0.0007706, Tokens/sec: 40489.25\n",
      "Step: 1796, Training Loss: 3.15401, LR: 0.0007703, Tokens/sec: 40755.43\n",
      "Step: 1797, Training Loss: 3.65706, LR: 0.0007700, Tokens/sec: 41275.77\n",
      "Step: 1798, Training Loss: 3.19950, LR: 0.0007697, Tokens/sec: 41396.24\n",
      "Step: 1799, Training Loss: 3.31034, LR: 0.0007694, Tokens/sec: 41423.52\n",
      "Step: 1800, Training Loss: 3.36509, LR: 0.0007691, Tokens/sec: 41311.19\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 1800, Eval Loss: 3.38801\n",
      "Step: 1801, Training Loss: 3.02800, LR: 0.0007688, Tokens/sec: 41264.90\n",
      "Step: 1802, Training Loss: 3.44967, LR: 0.0007685, Tokens/sec: 31758.28\n",
      "Step: 1803, Training Loss: 3.04930, LR: 0.0007681, Tokens/sec: 34441.00\n",
      "Step: 1804, Training Loss: 2.90455, LR: 0.0007678, Tokens/sec: 37324.01\n",
      "Step: 1805, Training Loss: 3.25171, LR: 0.0007675, Tokens/sec: 41302.57\n",
      "Step: 1806, Training Loss: 3.58911, LR: 0.0007672, Tokens/sec: 31206.36\n",
      "Step: 1807, Training Loss: 3.50895, LR: 0.0007669, Tokens/sec: 29634.94\n",
      "Step: 1808, Training Loss: 3.18440, LR: 0.0007666, Tokens/sec: 41600.61\n",
      "Step: 1809, Training Loss: 2.95904, LR: 0.0007663, Tokens/sec: 38875.49\n",
      "Step: 1810, Training Loss: 3.36214, LR: 0.0007660, Tokens/sec: 39523.25\n",
      "Step: 1811, Training Loss: 3.17854, LR: 0.0007657, Tokens/sec: 38161.54\n",
      "Step: 1812, Training Loss: 2.95282, LR: 0.0007654, Tokens/sec: 41496.32\n",
      "Step: 1813, Training Loss: 3.48506, LR: 0.0007650, Tokens/sec: 41101.77\n",
      "Step: 1814, Training Loss: 3.25234, LR: 0.0007647, Tokens/sec: 40754.94\n",
      "Step: 1815, Training Loss: 3.13725, LR: 0.0007644, Tokens/sec: 40999.19\n",
      "Step: 1816, Training Loss: 3.18226, LR: 0.0007641, Tokens/sec: 41441.38\n",
      "Step: 1817, Training Loss: 3.19047, LR: 0.0007638, Tokens/sec: 40191.50\n",
      "Step: 1818, Training Loss: 3.08184, LR: 0.0007635, Tokens/sec: 41486.87\n",
      "Step: 1819, Training Loss: 2.71323, LR: 0.0007632, Tokens/sec: 41544.94\n",
      "Step: 1820, Training Loss: 3.10396, LR: 0.0007629, Tokens/sec: 38712.22\n",
      "Step: 1821, Training Loss: 3.09112, LR: 0.0007626, Tokens/sec: 40989.71\n",
      "Step: 1822, Training Loss: 2.97431, LR: 0.0007623, Tokens/sec: 29034.35\n",
      "Step: 1823, Training Loss: 2.96128, LR: 0.0007619, Tokens/sec: 38372.86\n",
      "Step: 1824, Training Loss: 3.21306, LR: 0.0007616, Tokens/sec: 41538.70\n",
      "Step: 1825, Training Loss: 3.20320, LR: 0.0007613, Tokens/sec: 38300.67\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 1825, Eval Loss: 3.31663\n",
      "Step: 1826, Training Loss: 2.98601, LR: 0.0007610, Tokens/sec: 41043.24\n",
      "Step: 1827, Training Loss: 2.85687, LR: 0.0007607, Tokens/sec: 40735.75\n",
      "Step: 1828, Training Loss: 3.18558, LR: 0.0007604, Tokens/sec: 40980.68\n",
      "Step: 1829, Training Loss: 3.36098, LR: 0.0007601, Tokens/sec: 41610.12\n",
      "Step: 1830, Training Loss: 3.18511, LR: 0.0007598, Tokens/sec: 39752.27\n",
      "Step: 1831, Training Loss: 3.10863, LR: 0.0007594, Tokens/sec: 41492.33\n",
      "Step: 1832, Training Loss: 2.98488, LR: 0.0007591, Tokens/sec: 41599.93\n",
      "Step: 1833, Training Loss: 3.18661, LR: 0.0007588, Tokens/sec: 39892.43\n",
      "Step: 1834, Training Loss: 3.85898, LR: 0.0007585, Tokens/sec: 40734.32\n",
      "Step: 1835, Training Loss: 3.11060, LR: 0.0007582, Tokens/sec: 40716.66\n",
      "Step: 1836, Training Loss: 3.08452, LR: 0.0007579, Tokens/sec: 30268.16\n",
      "Step: 1837, Training Loss: 3.04000, LR: 0.0007576, Tokens/sec: 41446.89\n",
      "Step: 1838, Training Loss: 3.11787, LR: 0.0007573, Tokens/sec: 39899.64\n",
      "Step: 1839, Training Loss: 3.38227, LR: 0.0007569, Tokens/sec: 41527.01\n",
      "Step: 1840, Training Loss: 3.30921, LR: 0.0007566, Tokens/sec: 40761.94\n",
      "Step: 1841, Training Loss: 3.69771, LR: 0.0007563, Tokens/sec: 41465.47\n",
      "Step: 1842, Training Loss: 3.04897, LR: 0.0007560, Tokens/sec: 38663.36\n",
      "Step: 1843, Training Loss: 3.21830, LR: 0.0007557, Tokens/sec: 40648.72\n",
      "Step: 1844, Training Loss: 2.70835, LR: 0.0007554, Tokens/sec: 41108.32\n",
      "Step: 1845, Training Loss: 3.32512, LR: 0.0007551, Tokens/sec: 41558.66\n",
      "Step: 1846, Training Loss: 2.96368, LR: 0.0007548, Tokens/sec: 41434.92\n",
      "Step: 1847, Training Loss: 3.57828, LR: 0.0007544, Tokens/sec: 38490.10\n",
      "Step: 1848, Training Loss: 3.44108, LR: 0.0007541, Tokens/sec: 38920.43\n",
      "Step: 1849, Training Loss: 3.25719, LR: 0.0007538, Tokens/sec: 41503.65\n",
      "Step: 1850, Training Loss: 2.98613, LR: 0.0007535, Tokens/sec: 41529.49\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 1850, Eval Loss: 3.32145\n",
      "Step: 1851, Training Loss: 3.12179, LR: 0.0007532, Tokens/sec: 36348.26\n",
      "Step: 1852, Training Loss: 2.94010, LR: 0.0007529, Tokens/sec: 38765.39\n",
      "Step: 1853, Training Loss: 2.71833, LR: 0.0007525, Tokens/sec: 36356.86\n",
      "Step: 1854, Training Loss: 3.40612, LR: 0.0007522, Tokens/sec: 39460.09\n",
      "Step: 1855, Training Loss: 3.20322, LR: 0.0007519, Tokens/sec: 40193.82\n",
      "Step: 1856, Training Loss: 2.90367, LR: 0.0007516, Tokens/sec: 40802.43\n",
      "Step: 1857, Training Loss: 3.23434, LR: 0.0007513, Tokens/sec: 40995.87\n",
      "Step: 1858, Training Loss: 3.19928, LR: 0.0007510, Tokens/sec: 41255.57\n",
      "Step: 1859, Training Loss: 3.60768, LR: 0.0007507, Tokens/sec: 41391.03\n",
      "Step: 1860, Training Loss: 3.20596, LR: 0.0007503, Tokens/sec: 40877.20\n",
      "Step: 1861, Training Loss: 3.62992, LR: 0.0007500, Tokens/sec: 40492.90\n",
      "Step: 1862, Training Loss: 3.26841, LR: 0.0007497, Tokens/sec: 40744.22\n",
      "Step: 1863, Training Loss: 2.99123, LR: 0.0007494, Tokens/sec: 41392.85\n",
      "Step: 1864, Training Loss: 2.94845, LR: 0.0007491, Tokens/sec: 40456.56\n",
      "Step: 1865, Training Loss: 2.82521, LR: 0.0007488, Tokens/sec: 41314.51\n",
      "Step: 1866, Training Loss: 3.48474, LR: 0.0007484, Tokens/sec: 38036.01\n",
      "Step: 1867, Training Loss: 2.96728, LR: 0.0007481, Tokens/sec: 39207.02\n",
      "Step: 1868, Training Loss: 2.77124, LR: 0.0007478, Tokens/sec: 40589.50\n",
      "Step: 1869, Training Loss: 3.30802, LR: 0.0007475, Tokens/sec: 40903.82\n",
      "Step: 1870, Training Loss: 3.55320, LR: 0.0007472, Tokens/sec: 41331.31\n",
      "Step: 1871, Training Loss: 3.00746, LR: 0.0007469, Tokens/sec: 34019.23\n",
      "Step: 1872, Training Loss: 3.38609, LR: 0.0007465, Tokens/sec: 35520.60\n",
      "Step: 1873, Training Loss: 3.36466, LR: 0.0007462, Tokens/sec: 40545.12\n",
      "Step: 1874, Training Loss: 3.49272, LR: 0.0007459, Tokens/sec: 40308.13\n",
      "Step: 1875, Training Loss: 3.13480, LR: 0.0007456, Tokens/sec: 41314.87\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 1875, Eval Loss: 3.20166\n",
      "Step: 1876, Training Loss: 2.86778, LR: 0.0007453, Tokens/sec: 40949.98\n",
      "Step: 1877, Training Loss: 3.45315, LR: 0.0007450, Tokens/sec: 40339.68\n",
      "Step: 1878, Training Loss: 3.04886, LR: 0.0007446, Tokens/sec: 41331.43\n",
      "Step: 1879, Training Loss: 3.17239, LR: 0.0007443, Tokens/sec: 40625.63\n",
      "Step: 1880, Training Loss: 3.57394, LR: 0.0007440, Tokens/sec: 40249.08\n",
      "Step: 1881, Training Loss: 3.53598, LR: 0.0007437, Tokens/sec: 40926.84\n",
      "Step: 1882, Training Loss: 2.99330, LR: 0.0007434, Tokens/sec: 37552.68\n",
      "Step: 1883, Training Loss: 3.40073, LR: 0.0007430, Tokens/sec: 39458.04\n",
      "Step: 1884, Training Loss: 3.31745, LR: 0.0007427, Tokens/sec: 38780.62\n",
      "Step: 1885, Training Loss: 3.33157, LR: 0.0007424, Tokens/sec: 38292.67\n",
      "Step: 1886, Training Loss: 3.13743, LR: 0.0007421, Tokens/sec: 40340.06\n",
      "Step: 1887, Training Loss: 2.90635, LR: 0.0007418, Tokens/sec: 39583.21\n",
      "Step: 1888, Training Loss: 3.31363, LR: 0.0007414, Tokens/sec: 38013.99\n",
      "Step: 1889, Training Loss: 2.99455, LR: 0.0007411, Tokens/sec: 37781.42\n",
      "Step: 1890, Training Loss: 3.07330, LR: 0.0007408, Tokens/sec: 37095.35\n",
      "Step: 1891, Training Loss: 2.98786, LR: 0.0007405, Tokens/sec: 40884.65\n",
      "Step: 1892, Training Loss: 3.32731, LR: 0.0007402, Tokens/sec: 41342.16\n",
      "Step: 1893, Training Loss: 3.26211, LR: 0.0007398, Tokens/sec: 41124.56\n",
      "Step: 1894, Training Loss: 3.41741, LR: 0.0007395, Tokens/sec: 37839.15\n",
      "Step: 1895, Training Loss: 3.32308, LR: 0.0007392, Tokens/sec: 38568.81\n",
      "Step: 1896, Training Loss: 3.32917, LR: 0.0007389, Tokens/sec: 39399.29\n",
      "Step: 1897, Training Loss: 3.32875, LR: 0.0007386, Tokens/sec: 41333.22\n",
      "Step: 1898, Training Loss: 2.99130, LR: 0.0007382, Tokens/sec: 40797.90\n",
      "Step: 1899, Training Loss: 3.51811, LR: 0.0007379, Tokens/sec: 41338.37\n",
      "Step: 1900, Training Loss: 3.49480, LR: 0.0007376, Tokens/sec: 41403.83\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 1900, Eval Loss: 3.27175\n",
      "Step: 1901, Training Loss: 3.32420, LR: 0.0007373, Tokens/sec: 36456.53\n",
      "Step: 1902, Training Loss: 3.29660, LR: 0.0007370, Tokens/sec: 41250.86\n",
      "Step: 1903, Training Loss: 3.41748, LR: 0.0007366, Tokens/sec: 41284.46\n",
      "Step: 1904, Training Loss: 3.34068, LR: 0.0007363, Tokens/sec: 41244.55\n",
      "Step: 1905, Training Loss: 2.87458, LR: 0.0007360, Tokens/sec: 37006.07\n",
      "Step: 1906, Training Loss: 3.18424, LR: 0.0007357, Tokens/sec: 39548.48\n",
      "Step: 1907, Training Loss: 3.33370, LR: 0.0007354, Tokens/sec: 38377.56\n",
      "Step: 1908, Training Loss: 3.33587, LR: 0.0007350, Tokens/sec: 41106.59\n",
      "Step: 1909, Training Loss: 3.01240, LR: 0.0007347, Tokens/sec: 41397.87\n",
      "Step: 1910, Training Loss: 3.33914, LR: 0.0007344, Tokens/sec: 39190.41\n",
      "Step: 1911, Training Loss: 3.41725, LR: 0.0007341, Tokens/sec: 38587.06\n",
      "Step: 1912, Training Loss: 3.41547, LR: 0.0007338, Tokens/sec: 40658.06\n",
      "Step: 1913, Training Loss: 3.10047, LR: 0.0007334, Tokens/sec: 40991.60\n",
      "Step: 1914, Training Loss: 3.14891, LR: 0.0007331, Tokens/sec: 40899.29\n",
      "Step: 1915, Training Loss: 3.27052, LR: 0.0007328, Tokens/sec: 41359.33\n",
      "Step: 1916, Training Loss: 3.23288, LR: 0.0007325, Tokens/sec: 41020.06\n",
      "Step: 1917, Training Loss: 2.98972, LR: 0.0007321, Tokens/sec: 41317.83\n",
      "Step: 1918, Training Loss: 2.96879, LR: 0.0007318, Tokens/sec: 39122.02\n",
      "Step: 1919, Training Loss: 3.09099, LR: 0.0007315, Tokens/sec: 41325.56\n",
      "Step: 1920, Training Loss: 3.08842, LR: 0.0007312, Tokens/sec: 41292.80\n",
      "Step: 1921, Training Loss: 3.23819, LR: 0.0007308, Tokens/sec: 41250.68\n",
      "Step: 1922, Training Loss: 3.04843, LR: 0.0007305, Tokens/sec: 41371.58\n",
      "Step: 1923, Training Loss: 3.49166, LR: 0.0007302, Tokens/sec: 26680.45\n",
      "Step: 1924, Training Loss: 2.93970, LR: 0.0007299, Tokens/sec: 35961.55\n",
      "Step: 1925, Training Loss: 3.28752, LR: 0.0007296, Tokens/sec: 41291.50\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 1925, Eval Loss: 3.17602\n",
      "Step: 1926, Training Loss: 3.11564, LR: 0.0007292, Tokens/sec: 41319.96\n",
      "Step: 1927, Training Loss: 2.86380, LR: 0.0007289, Tokens/sec: 40919.97\n",
      "Step: 1928, Training Loss: 3.01013, LR: 0.0007286, Tokens/sec: 40343.12\n",
      "Step: 1929, Training Loss: 2.78596, LR: 0.0007283, Tokens/sec: 39896.50\n",
      "Step: 1930, Training Loss: 3.12650, LR: 0.0007279, Tokens/sec: 39675.01\n",
      "Step: 1931, Training Loss: 3.32008, LR: 0.0007276, Tokens/sec: 33456.01\n",
      "Step: 1932, Training Loss: 3.18091, LR: 0.0007273, Tokens/sec: 38592.49\n",
      "Step: 1933, Training Loss: 3.00573, LR: 0.0007270, Tokens/sec: 39122.19\n",
      "Step: 1934, Training Loss: 3.29009, LR: 0.0007266, Tokens/sec: 28780.31\n",
      "Step: 1935, Training Loss: 3.04018, LR: 0.0007263, Tokens/sec: 40353.01\n",
      "Step: 1936, Training Loss: 3.30143, LR: 0.0007260, Tokens/sec: 38794.82\n",
      "Step: 1937, Training Loss: 2.95208, LR: 0.0007257, Tokens/sec: 41317.05\n",
      "Step: 1938, Training Loss: 3.07001, LR: 0.0007253, Tokens/sec: 39345.50\n",
      "Step: 1939, Training Loss: 2.95989, LR: 0.0007250, Tokens/sec: 41332.74\n",
      "Step: 1940, Training Loss: 3.27475, LR: 0.0007247, Tokens/sec: 41348.52\n",
      "Step: 1941, Training Loss: 3.31829, LR: 0.0007244, Tokens/sec: 40012.97\n",
      "Step: 1942, Training Loss: 3.22360, LR: 0.0007240, Tokens/sec: 41334.78\n",
      "Step: 1943, Training Loss: 2.89833, LR: 0.0007237, Tokens/sec: 37204.91\n",
      "Step: 1944, Training Loss: 3.38349, LR: 0.0007234, Tokens/sec: 41260.69\n",
      "Step: 1945, Training Loss: 3.35700, LR: 0.0007231, Tokens/sec: 40848.51\n",
      "Step: 1946, Training Loss: 2.83277, LR: 0.0007227, Tokens/sec: 29704.12\n",
      "Step: 1947, Training Loss: 3.14306, LR: 0.0007224, Tokens/sec: 29555.93\n",
      "Step: 1948, Training Loss: 3.22317, LR: 0.0007221, Tokens/sec: 31879.74\n",
      "Step: 1949, Training Loss: 3.04292, LR: 0.0007218, Tokens/sec: 41115.66\n",
      "Step: 1950, Training Loss: 2.98385, LR: 0.0007214, Tokens/sec: 38351.00\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 1950, Eval Loss: 3.17450\n",
      "Step: 1951, Training Loss: 3.38611, LR: 0.0007211, Tokens/sec: 41335.56\n",
      "Step: 1952, Training Loss: 3.04185, LR: 0.0007208, Tokens/sec: 41388.52\n",
      "Step: 1953, Training Loss: 3.19036, LR: 0.0007205, Tokens/sec: 41342.28\n",
      "Step: 1954, Training Loss: 3.17632, LR: 0.0007201, Tokens/sec: 40582.27\n",
      "Step: 1955, Training Loss: 3.02373, LR: 0.0007198, Tokens/sec: 40238.36\n",
      "Step: 1956, Training Loss: 3.34306, LR: 0.0007195, Tokens/sec: 34787.60\n",
      "Step: 1957, Training Loss: 3.05624, LR: 0.0007191, Tokens/sec: 39823.81\n",
      "Step: 1958, Training Loss: 3.30000, LR: 0.0007188, Tokens/sec: 40079.91\n",
      "Step: 1959, Training Loss: 3.14179, LR: 0.0007185, Tokens/sec: 37589.60\n",
      "Step: 1960, Training Loss: 3.10806, LR: 0.0007182, Tokens/sec: 41084.32\n",
      "Step: 1961, Training Loss: 3.15173, LR: 0.0007178, Tokens/sec: 39345.67\n",
      "Step: 1962, Training Loss: 3.15060, LR: 0.0007175, Tokens/sec: 37708.75\n",
      "Step: 1963, Training Loss: 3.13588, LR: 0.0007172, Tokens/sec: 39016.26\n",
      "Step: 1964, Training Loss: 3.15924, LR: 0.0007169, Tokens/sec: 40532.69\n",
      "Step: 1965, Training Loss: 3.49502, LR: 0.0007165, Tokens/sec: 40698.01\n",
      "Step: 1966, Training Loss: 3.47512, LR: 0.0007162, Tokens/sec: 41374.60\n",
      "Step: 1967, Training Loss: 3.66886, LR: 0.0007159, Tokens/sec: 40888.79\n",
      "Step: 1968, Training Loss: 3.19145, LR: 0.0007155, Tokens/sec: 29514.88\n",
      "Step: 1969, Training Loss: 3.30644, LR: 0.0007152, Tokens/sec: 37148.10\n",
      "Step: 1970, Training Loss: 3.02391, LR: 0.0007149, Tokens/sec: 40295.87\n",
      "Step: 1971, Training Loss: 2.94058, LR: 0.0007146, Tokens/sec: 39690.06\n",
      "Step: 1972, Training Loss: 3.27763, LR: 0.0007142, Tokens/sec: 40956.08\n",
      "Step: 1973, Training Loss: 3.05828, LR: 0.0007139, Tokens/sec: 40050.19\n",
      "Step: 1974, Training Loss: 2.92021, LR: 0.0007136, Tokens/sec: 38136.89\n",
      "Step: 1975, Training Loss: 3.08116, LR: 0.0007132, Tokens/sec: 41305.88\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 1975, Eval Loss: 3.21302\n",
      "Step: 1976, Training Loss: 3.12381, LR: 0.0007129, Tokens/sec: 37300.18\n",
      "Step: 1977, Training Loss: 2.70739, LR: 0.0007126, Tokens/sec: 41275.83\n",
      "Step: 1978, Training Loss: 3.10434, LR: 0.0007123, Tokens/sec: 38746.88\n",
      "Step: 1979, Training Loss: 3.06327, LR: 0.0007119, Tokens/sec: 41154.03\n",
      "Step: 1980, Training Loss: 3.32897, LR: 0.0007116, Tokens/sec: 41313.23\n",
      "Step: 1981, Training Loss: 3.23971, LR: 0.0007113, Tokens/sec: 38762.87\n",
      "Step: 1982, Training Loss: 2.94778, LR: 0.0007109, Tokens/sec: 41384.50\n",
      "Step: 1983, Training Loss: 3.11393, LR: 0.0007106, Tokens/sec: 37666.34\n",
      "Step: 1984, Training Loss: 3.28569, LR: 0.0007103, Tokens/sec: 41294.15\n",
      "Step: 1985, Training Loss: 2.81146, LR: 0.0007100, Tokens/sec: 40737.68\n",
      "Step: 1986, Training Loss: 3.15808, LR: 0.0007096, Tokens/sec: 39379.47\n",
      "Step: 1987, Training Loss: 2.93326, LR: 0.0007093, Tokens/sec: 39955.67\n",
      "Step: 1988, Training Loss: 2.94748, LR: 0.0007090, Tokens/sec: 37104.72\n",
      "Step: 1989, Training Loss: 3.08377, LR: 0.0007086, Tokens/sec: 38332.92\n",
      "Step: 1990, Training Loss: 3.07816, LR: 0.0007083, Tokens/sec: 38331.12\n",
      "Step: 1991, Training Loss: 3.09245, LR: 0.0007080, Tokens/sec: 37856.29\n",
      "Step: 1992, Training Loss: 2.86023, LR: 0.0007076, Tokens/sec: 38142.51\n",
      "Step: 1993, Training Loss: 2.92895, LR: 0.0007073, Tokens/sec: 39489.53\n",
      "Step: 1994, Training Loss: 2.99167, LR: 0.0007070, Tokens/sec: 39136.99\n",
      "Step: 1995, Training Loss: 3.37814, LR: 0.0007067, Tokens/sec: 39308.88\n",
      "Step: 1996, Training Loss: 3.28988, LR: 0.0007063, Tokens/sec: 36898.38\n",
      "Step: 1997, Training Loss: 2.73545, LR: 0.0007060, Tokens/sec: 40370.47\n",
      "Step: 1998, Training Loss: 2.92798, LR: 0.0007057, Tokens/sec: 41360.36\n",
      "Step: 1999, Training Loss: 3.22336, LR: 0.0007053, Tokens/sec: 41189.67\n",
      "Step: 2000, Training Loss: 3.26270, LR: 0.0007050, Tokens/sec: 41112.96\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 2000, Eval Loss: 3.18939\n",
      "Step: 2001, Training Loss: 2.80917, LR: 0.0007047, Tokens/sec: 38989.08\n",
      "Step: 2002, Training Loss: 2.65333, LR: 0.0007043, Tokens/sec: 40797.42\n",
      "Step: 2003, Training Loss: 3.06196, LR: 0.0007040, Tokens/sec: 41178.35\n",
      "Step: 2004, Training Loss: 3.22091, LR: 0.0007037, Tokens/sec: 41324.09\n",
      "Step: 2005, Training Loss: 2.73378, LR: 0.0007033, Tokens/sec: 39262.21\n",
      "Step: 2006, Training Loss: 3.27350, LR: 0.0007030, Tokens/sec: 39801.17\n",
      "Step: 2007, Training Loss: 2.80316, LR: 0.0007027, Tokens/sec: 41418.97\n",
      "Step: 2008, Training Loss: 3.19319, LR: 0.0007023, Tokens/sec: 40955.41\n",
      "Step: 2009, Training Loss: 3.16228, LR: 0.0007020, Tokens/sec: 35533.09\n",
      "Step: 2010, Training Loss: 3.35520, LR: 0.0007017, Tokens/sec: 40734.91\n",
      "Step: 2011, Training Loss: 3.19848, LR: 0.0007013, Tokens/sec: 41550.81\n",
      "Step: 2012, Training Loss: 3.09855, LR: 0.0007010, Tokens/sec: 41417.34\n",
      "Step: 2013, Training Loss: 3.52297, LR: 0.0007007, Tokens/sec: 41525.32\n",
      "Step: 2014, Training Loss: 2.98460, LR: 0.0007003, Tokens/sec: 40453.57\n",
      "Step: 2015, Training Loss: 3.31253, LR: 0.0007000, Tokens/sec: 41150.10\n",
      "Step: 2016, Training Loss: 3.43324, LR: 0.0006997, Tokens/sec: 41496.63\n",
      "Step: 2017, Training Loss: 2.60199, LR: 0.0006994, Tokens/sec: 29298.87\n",
      "Step: 2018, Training Loss: 3.06360, LR: 0.0006990, Tokens/sec: 41488.93\n",
      "Step: 2019, Training Loss: 3.34654, LR: 0.0006987, Tokens/sec: 39732.31\n",
      "Step: 2020, Training Loss: 3.03905, LR: 0.0006984, Tokens/sec: 41533.44\n",
      "Step: 2021, Training Loss: 2.91990, LR: 0.0006980, Tokens/sec: 41556.72\n",
      "Step: 2022, Training Loss: 2.87258, LR: 0.0006977, Tokens/sec: 41550.88\n",
      "Step: 2023, Training Loss: 3.16806, LR: 0.0006974, Tokens/sec: 41215.97\n",
      "Step: 2024, Training Loss: 3.13363, LR: 0.0006970, Tokens/sec: 41453.85\n",
      "Step: 2025, Training Loss: 2.99783, LR: 0.0006967, Tokens/sec: 41092.15\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 2025, Eval Loss: 3.10123\n",
      "Step: 2026, Training Loss: 3.39879, LR: 0.0006964, Tokens/sec: 40848.72\n",
      "Step: 2027, Training Loss: 3.20595, LR: 0.0006960, Tokens/sec: 40312.86\n",
      "Step: 2028, Training Loss: 2.60990, LR: 0.0006957, Tokens/sec: 41156.73\n",
      "Step: 2029, Training Loss: 2.94918, LR: 0.0006954, Tokens/sec: 37904.05\n",
      "Step: 2030, Training Loss: 3.16462, LR: 0.0006950, Tokens/sec: 41259.15\n",
      "Step: 2031, Training Loss: 2.93429, LR: 0.0006947, Tokens/sec: 40839.17\n",
      "Step: 2032, Training Loss: 3.18178, LR: 0.0006943, Tokens/sec: 40421.08\n",
      "Step: 2033, Training Loss: 3.46474, LR: 0.0006940, Tokens/sec: 41501.97\n",
      "Step: 2034, Training Loss: 2.93554, LR: 0.0006937, Tokens/sec: 41033.88\n",
      "Step: 2035, Training Loss: 2.56242, LR: 0.0006933, Tokens/sec: 40032.53\n",
      "Step: 2036, Training Loss: 2.96504, LR: 0.0006930, Tokens/sec: 41545.33\n",
      "Step: 2037, Training Loss: 3.20793, LR: 0.0006927, Tokens/sec: 41524.77\n",
      "Step: 2038, Training Loss: 3.34208, LR: 0.0006923, Tokens/sec: 41552.22\n",
      "Step: 2039, Training Loss: 3.06961, LR: 0.0006920, Tokens/sec: 41145.93\n",
      "Step: 2040, Training Loss: 2.85085, LR: 0.0006917, Tokens/sec: 41560.82\n",
      "Step: 2041, Training Loss: 2.64445, LR: 0.0006913, Tokens/sec: 41553.69\n",
      "Step: 2042, Training Loss: 2.59644, LR: 0.0006910, Tokens/sec: 41146.55\n",
      "Step: 2043, Training Loss: 2.59912, LR: 0.0006907, Tokens/sec: 36364.36\n",
      "Step: 2044, Training Loss: 2.97281, LR: 0.0006903, Tokens/sec: 34325.60\n",
      "Step: 2045, Training Loss: 2.98301, LR: 0.0006900, Tokens/sec: 41411.79\n",
      "Step: 2046, Training Loss: 3.11036, LR: 0.0006897, Tokens/sec: 41450.78\n",
      "Step: 2047, Training Loss: 2.80180, LR: 0.0006893, Tokens/sec: 41520.56\n",
      "Step: 2048, Training Loss: 3.12543, LR: 0.0006890, Tokens/sec: 40309.40\n",
      "Step: 2049, Training Loss: 2.49795, LR: 0.0006887, Tokens/sec: 41454.32\n",
      "Step: 2050, Training Loss: 2.95242, LR: 0.0006883, Tokens/sec: 41482.61\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 2050, Eval Loss: 3.14722\n",
      "Step: 2051, Training Loss: 2.74458, LR: 0.0006880, Tokens/sec: 41323.38\n",
      "Step: 2052, Training Loss: 2.89193, LR: 0.0006876, Tokens/sec: 40978.30\n",
      "Step: 2053, Training Loss: 3.17971, LR: 0.0006873, Tokens/sec: 41403.12\n",
      "Step: 2054, Training Loss: 2.57125, LR: 0.0006870, Tokens/sec: 41312.32\n",
      "Step: 2055, Training Loss: 2.88419, LR: 0.0006866, Tokens/sec: 40901.39\n",
      "Step: 2056, Training Loss: 2.88884, LR: 0.0006863, Tokens/sec: 40228.88\n",
      "Step: 2057, Training Loss: 3.25093, LR: 0.0006860, Tokens/sec: 41376.87\n",
      "Step: 2058, Training Loss: 2.95464, LR: 0.0006856, Tokens/sec: 41500.62\n",
      "Step: 2059, Training Loss: 3.31953, LR: 0.0006853, Tokens/sec: 39984.23\n",
      "Step: 2060, Training Loss: 2.81118, LR: 0.0006850, Tokens/sec: 41389.18\n",
      "Step: 2061, Training Loss: 2.59680, LR: 0.0006846, Tokens/sec: 41380.15\n",
      "Step: 2062, Training Loss: 3.55346, LR: 0.0006843, Tokens/sec: 41383.21\n",
      "Step: 2063, Training Loss: 3.51453, LR: 0.0006839, Tokens/sec: 41015.77\n",
      "Step: 2064, Training Loss: 3.10676, LR: 0.0006836, Tokens/sec: 39992.70\n",
      "Step: 2065, Training Loss: 2.63243, LR: 0.0006833, Tokens/sec: 40465.22\n",
      "Step: 2066, Training Loss: 2.75344, LR: 0.0006829, Tokens/sec: 41396.78\n",
      "Step: 2067, Training Loss: 3.06778, LR: 0.0006826, Tokens/sec: 40126.71\n",
      "Step: 2068, Training Loss: 2.97854, LR: 0.0006823, Tokens/sec: 41419.03\n",
      "Step: 2069, Training Loss: 2.88942, LR: 0.0006819, Tokens/sec: 41484.06\n",
      "Step: 2070, Training Loss: 3.08743, LR: 0.0006816, Tokens/sec: 41375.59\n",
      "Step: 2071, Training Loss: 2.96264, LR: 0.0006813, Tokens/sec: 41504.82\n",
      "Step: 2072, Training Loss: 3.23956, LR: 0.0006809, Tokens/sec: 40012.35\n",
      "Step: 2073, Training Loss: 2.80003, LR: 0.0006806, Tokens/sec: 41433.36\n",
      "Step: 2074, Training Loss: 3.14089, LR: 0.0006802, Tokens/sec: 40943.92\n",
      "Step: 2075, Training Loss: 3.26122, LR: 0.0006799, Tokens/sec: 40032.33\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 2075, Eval Loss: 2.98975\n",
      "Step: 2076, Training Loss: 3.52520, LR: 0.0006796, Tokens/sec: 41262.49\n",
      "Step: 2077, Training Loss: 2.73716, LR: 0.0006792, Tokens/sec: 41441.98\n",
      "Step: 2078, Training Loss: 2.87313, LR: 0.0006789, Tokens/sec: 41447.82\n",
      "Step: 2079, Training Loss: 3.39303, LR: 0.0006785, Tokens/sec: 41418.17\n",
      "Step: 2080, Training Loss: 3.08390, LR: 0.0006782, Tokens/sec: 40071.87\n",
      "Step: 2081, Training Loss: 2.76518, LR: 0.0006779, Tokens/sec: 41344.41\n",
      "Step: 2082, Training Loss: 3.08700, LR: 0.0006775, Tokens/sec: 39699.21\n",
      "Step: 2083, Training Loss: 2.90044, LR: 0.0006772, Tokens/sec: 41169.13\n",
      "Step: 2084, Training Loss: 2.90049, LR: 0.0006769, Tokens/sec: 41064.00\n",
      "Step: 2085, Training Loss: 2.73410, LR: 0.0006765, Tokens/sec: 41170.62\n",
      "Step: 2086, Training Loss: 3.14302, LR: 0.0006762, Tokens/sec: 41437.67\n",
      "Step: 2087, Training Loss: 3.73124, LR: 0.0006758, Tokens/sec: 41512.37\n",
      "Step: 2088, Training Loss: 2.97845, LR: 0.0006755, Tokens/sec: 41370.31\n",
      "Step: 2089, Training Loss: 2.92602, LR: 0.0006752, Tokens/sec: 41473.95\n",
      "Step: 2090, Training Loss: 2.26412, LR: 0.0006748, Tokens/sec: 40583.88\n",
      "Step: 2091, Training Loss: 2.56351, LR: 0.0006745, Tokens/sec: 41438.70\n",
      "Step: 2092, Training Loss: 2.31692, LR: 0.0006741, Tokens/sec: 41468.14\n",
      "Step: 2093, Training Loss: 3.11780, LR: 0.0006738, Tokens/sec: 41459.59\n",
      "Step: 2094, Training Loss: 2.78852, LR: 0.0006735, Tokens/sec: 41357.36\n",
      "Step: 2095, Training Loss: 2.87165, LR: 0.0006731, Tokens/sec: 41395.10\n",
      "Step: 2096, Training Loss: 2.89407, LR: 0.0006728, Tokens/sec: 41498.85\n",
      "Step: 2097, Training Loss: 3.22880, LR: 0.0006724, Tokens/sec: 41477.19\n",
      "Step: 2098, Training Loss: 2.84714, LR: 0.0006721, Tokens/sec: 40295.60\n",
      "Step: 2099, Training Loss: 2.94703, LR: 0.0006718, Tokens/sec: 40879.06\n",
      "Step: 2100, Training Loss: 3.02065, LR: 0.0006714, Tokens/sec: 41467.23\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 2100, Eval Loss: 3.07514\n",
      "Step: 2101, Training Loss: 3.26514, LR: 0.0006711, Tokens/sec: 41315.84\n",
      "Step: 2102, Training Loss: 3.34017, LR: 0.0006708, Tokens/sec: 40077.36\n",
      "Step: 2103, Training Loss: 2.77420, LR: 0.0006704, Tokens/sec: 39052.48\n",
      "Step: 2104, Training Loss: 2.75437, LR: 0.0006701, Tokens/sec: 40857.56\n",
      "Step: 2105, Training Loss: 3.10213, LR: 0.0006697, Tokens/sec: 39370.67\n",
      "Step: 2106, Training Loss: 2.93019, LR: 0.0006694, Tokens/sec: 40906.53\n",
      "Step: 2107, Training Loss: 3.26483, LR: 0.0006691, Tokens/sec: 41145.88\n",
      "Step: 2108, Training Loss: 2.84084, LR: 0.0006687, Tokens/sec: 41323.88\n",
      "Step: 2109, Training Loss: 2.92461, LR: 0.0006684, Tokens/sec: 41409.17\n",
      "Step: 2110, Training Loss: 2.66840, LR: 0.0006680, Tokens/sec: 41119.94\n",
      "Step: 2111, Training Loss: 2.79199, LR: 0.0006677, Tokens/sec: 35757.11\n",
      "Step: 2112, Training Loss: 3.41248, LR: 0.0006673, Tokens/sec: 39733.60\n",
      "Step: 2113, Training Loss: 2.64639, LR: 0.0006670, Tokens/sec: 36958.30\n",
      "Step: 2114, Training Loss: 3.07709, LR: 0.0006667, Tokens/sec: 40093.19\n",
      "Step: 2115, Training Loss: 3.08912, LR: 0.0006663, Tokens/sec: 39831.22\n",
      "Step: 2116, Training Loss: 3.03086, LR: 0.0006660, Tokens/sec: 40372.79\n",
      "Step: 2117, Training Loss: 3.28035, LR: 0.0006656, Tokens/sec: 40805.25\n",
      "Step: 2118, Training Loss: 2.88107, LR: 0.0006653, Tokens/sec: 41331.51\n",
      "Step: 2119, Training Loss: 3.32253, LR: 0.0006650, Tokens/sec: 40448.76\n",
      "Step: 2120, Training Loss: 3.16712, LR: 0.0006646, Tokens/sec: 41264.96\n",
      "Step: 2121, Training Loss: 2.35885, LR: 0.0006643, Tokens/sec: 41288.79\n",
      "Step: 2122, Training Loss: 3.18942, LR: 0.0006639, Tokens/sec: 41235.41\n",
      "Step: 2123, Training Loss: 2.83638, LR: 0.0006636, Tokens/sec: 39256.80\n",
      "Step: 2124, Training Loss: 2.97526, LR: 0.0006633, Tokens/sec: 40536.61\n",
      "Step: 2125, Training Loss: 3.26231, LR: 0.0006629, Tokens/sec: 41292.14\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 2125, Eval Loss: 3.03026\n",
      "Step: 2126, Training Loss: 2.97108, LR: 0.0006626, Tokens/sec: 41302.83\n",
      "Step: 2127, Training Loss: 3.09799, LR: 0.0006622, Tokens/sec: 37230.12\n",
      "Step: 2128, Training Loss: 2.68208, LR: 0.0006619, Tokens/sec: 38322.21\n",
      "Step: 2129, Training Loss: 2.87248, LR: 0.0006615, Tokens/sec: 26023.72\n",
      "Step: 2130, Training Loss: 2.88840, LR: 0.0006612, Tokens/sec: 41114.47\n",
      "Step: 2131, Training Loss: 2.95422, LR: 0.0006609, Tokens/sec: 33572.99\n",
      "Step: 2132, Training Loss: 2.62718, LR: 0.0006605, Tokens/sec: 27498.73\n",
      "Step: 2133, Training Loss: 3.36922, LR: 0.0006602, Tokens/sec: 30614.67\n",
      "Step: 2134, Training Loss: 3.07388, LR: 0.0006598, Tokens/sec: 37401.03\n",
      "Step: 2135, Training Loss: 3.23387, LR: 0.0006595, Tokens/sec: 41259.05\n",
      "Step: 2136, Training Loss: 2.82569, LR: 0.0006592, Tokens/sec: 41279.43\n",
      "Step: 2137, Training Loss: 2.89334, LR: 0.0006588, Tokens/sec: 39688.38\n",
      "Step: 2138, Training Loss: 3.09036, LR: 0.0006585, Tokens/sec: 38271.86\n",
      "Step: 2139, Training Loss: 3.30537, LR: 0.0006581, Tokens/sec: 38546.06\n",
      "Step: 2140, Training Loss: 3.45795, LR: 0.0006578, Tokens/sec: 40341.60\n",
      "Step: 2141, Training Loss: 3.15479, LR: 0.0006574, Tokens/sec: 40374.08\n",
      "Step: 2142, Training Loss: 3.03377, LR: 0.0006571, Tokens/sec: 38977.74\n",
      "Step: 2143, Training Loss: 3.19463, LR: 0.0006568, Tokens/sec: 41391.40\n",
      "Step: 2144, Training Loss: 2.79678, LR: 0.0006564, Tokens/sec: 33973.99\n",
      "Step: 2145, Training Loss: 3.40858, LR: 0.0006561, Tokens/sec: 35769.97\n",
      "Step: 2146, Training Loss: 2.57365, LR: 0.0006557, Tokens/sec: 36822.08\n",
      "Step: 2147, Training Loss: 3.11269, LR: 0.0006554, Tokens/sec: 34064.79\n",
      "Step: 2148, Training Loss: 2.79081, LR: 0.0006550, Tokens/sec: 33834.58\n",
      "Step: 2149, Training Loss: 3.00673, LR: 0.0006547, Tokens/sec: 35002.35\n",
      "Step: 2150, Training Loss: 2.85856, LR: 0.0006544, Tokens/sec: 34296.50\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 2150, Eval Loss: 2.93569\n",
      "Step: 2151, Training Loss: 2.78952, LR: 0.0006540, Tokens/sec: 36229.82\n",
      "Step: 2152, Training Loss: 2.67220, LR: 0.0006537, Tokens/sec: 36152.25\n",
      "Step: 2153, Training Loss: 3.59476, LR: 0.0006533, Tokens/sec: 32314.34\n",
      "Step: 2154, Training Loss: 3.29227, LR: 0.0006530, Tokens/sec: 35010.56\n",
      "Step: 2155, Training Loss: 3.08071, LR: 0.0006526, Tokens/sec: 36232.89\n",
      "Step: 2156, Training Loss: 3.15690, LR: 0.0006523, Tokens/sec: 41313.33\n",
      "Step: 2157, Training Loss: 2.95690, LR: 0.0006520, Tokens/sec: 41418.65\n",
      "Step: 2158, Training Loss: 2.97885, LR: 0.0006516, Tokens/sec: 37381.95\n",
      "Step: 2159, Training Loss: 2.89196, LR: 0.0006513, Tokens/sec: 37477.69\n",
      "Step: 2160, Training Loss: 3.13695, LR: 0.0006509, Tokens/sec: 41176.70\n",
      "Step: 2161, Training Loss: 2.68741, LR: 0.0006506, Tokens/sec: 41360.75\n",
      "Step: 2162, Training Loss: 2.91346, LR: 0.0006502, Tokens/sec: 39286.31\n",
      "Step: 2163, Training Loss: 3.16324, LR: 0.0006499, Tokens/sec: 41314.57\n",
      "Step: 2164, Training Loss: 3.14658, LR: 0.0006495, Tokens/sec: 41374.48\n",
      "Step: 2165, Training Loss: 3.06053, LR: 0.0006492, Tokens/sec: 41223.77\n",
      "Step: 2166, Training Loss: 2.88291, LR: 0.0006489, Tokens/sec: 41330.03\n",
      "Step: 2167, Training Loss: 3.08329, LR: 0.0006485, Tokens/sec: 40508.82\n",
      "Step: 2168, Training Loss: 2.99488, LR: 0.0006482, Tokens/sec: 41407.21\n",
      "Step: 2169, Training Loss: 2.71602, LR: 0.0006478, Tokens/sec: 41376.95\n",
      "Step: 2170, Training Loss: 2.87668, LR: 0.0006475, Tokens/sec: 40499.95\n",
      "Step: 2171, Training Loss: 3.09805, LR: 0.0006471, Tokens/sec: 41381.99\n",
      "Step: 2172, Training Loss: 3.15951, LR: 0.0006468, Tokens/sec: 41252.81\n",
      "Step: 2173, Training Loss: 2.95277, LR: 0.0006464, Tokens/sec: 41386.34\n",
      "Step: 2174, Training Loss: 2.94488, LR: 0.0006461, Tokens/sec: 41477.39\n",
      "Step: 2175, Training Loss: 3.01299, LR: 0.0006458, Tokens/sec: 41379.13\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 2175, Eval Loss: 3.00031\n",
      "Step: 2176, Training Loss: 3.17882, LR: 0.0006454, Tokens/sec: 40621.11\n",
      "Step: 2177, Training Loss: 2.65547, LR: 0.0006451, Tokens/sec: 40494.64\n",
      "Step: 2178, Training Loss: 3.14364, LR: 0.0006447, Tokens/sec: 41067.72\n",
      "Step: 2179, Training Loss: 2.64426, LR: 0.0006444, Tokens/sec: 26563.11\n",
      "Step: 2180, Training Loss: 3.24562, LR: 0.0006440, Tokens/sec: 32679.35\n",
      "Step: 2181, Training Loss: 3.10942, LR: 0.0006437, Tokens/sec: 41164.07\n",
      "Step: 2182, Training Loss: 2.80512, LR: 0.0006433, Tokens/sec: 41484.00\n",
      "Step: 2183, Training Loss: 3.29199, LR: 0.0006430, Tokens/sec: 40957.89\n",
      "Step: 2184, Training Loss: 3.06272, LR: 0.0006427, Tokens/sec: 41455.81\n",
      "Step: 2185, Training Loss: 2.90456, LR: 0.0006423, Tokens/sec: 41497.35\n",
      "Step: 2186, Training Loss: 2.90607, LR: 0.0006420, Tokens/sec: 41493.38\n",
      "Step: 2187, Training Loss: 2.73031, LR: 0.0006416, Tokens/sec: 39429.25\n",
      "Step: 2188, Training Loss: 3.16554, LR: 0.0006413, Tokens/sec: 27835.87\n",
      "Step: 2189, Training Loss: 3.20404, LR: 0.0006409, Tokens/sec: 40715.76\n",
      "Step: 2190, Training Loss: 2.99193, LR: 0.0006406, Tokens/sec: 41607.61\n",
      "Step: 2191, Training Loss: 3.13757, LR: 0.0006402, Tokens/sec: 37675.41\n",
      "Step: 2192, Training Loss: 3.10784, LR: 0.0006399, Tokens/sec: 27543.20\n",
      "Step: 2193, Training Loss: 2.79555, LR: 0.0006395, Tokens/sec: 38430.73\n",
      "Step: 2194, Training Loss: 2.99882, LR: 0.0006392, Tokens/sec: 36523.59\n",
      "Step: 2195, Training Loss: 2.68335, LR: 0.0006389, Tokens/sec: 27662.81\n",
      "Step: 2196, Training Loss: 3.37754, LR: 0.0006385, Tokens/sec: 27398.90\n",
      "Step: 2197, Training Loss: 3.10011, LR: 0.0006382, Tokens/sec: 40895.97\n",
      "Step: 2198, Training Loss: 2.94458, LR: 0.0006378, Tokens/sec: 30055.14\n",
      "Step: 2199, Training Loss: 2.79614, LR: 0.0006375, Tokens/sec: 26592.20\n",
      "Step: 2200, Training Loss: 3.29000, LR: 0.0006371, Tokens/sec: 41501.78\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 2200, Eval Loss: 2.87813\n",
      "Step: 2201, Training Loss: 2.35396, LR: 0.0006368, Tokens/sec: 41559.81\n",
      "Step: 2202, Training Loss: 3.07673, LR: 0.0006364, Tokens/sec: 36817.64\n",
      "Step: 2203, Training Loss: 2.82298, LR: 0.0006361, Tokens/sec: 37905.21\n",
      "Step: 2204, Training Loss: 2.86844, LR: 0.0006357, Tokens/sec: 41062.11\n",
      "Step: 2205, Training Loss: 2.88005, LR: 0.0006354, Tokens/sec: 27114.84\n",
      "Step: 2206, Training Loss: 2.41754, LR: 0.0006350, Tokens/sec: 39830.07\n",
      "Step: 2207, Training Loss: 2.78159, LR: 0.0006347, Tokens/sec: 41630.68\n",
      "Step: 2208, Training Loss: 3.02110, LR: 0.0006343, Tokens/sec: 41526.35\n",
      "Step: 2209, Training Loss: 2.67165, LR: 0.0006340, Tokens/sec: 40563.05\n",
      "Step: 2210, Training Loss: 2.52138, LR: 0.0006337, Tokens/sec: 41684.52\n",
      "Step: 2211, Training Loss: 3.13755, LR: 0.0006333, Tokens/sec: 41154.98\n",
      "Step: 2212, Training Loss: 3.28223, LR: 0.0006330, Tokens/sec: 35015.88\n",
      "Step: 2213, Training Loss: 3.22150, LR: 0.0006326, Tokens/sec: 39712.78\n",
      "Step: 2214, Training Loss: 2.91032, LR: 0.0006323, Tokens/sec: 41664.22\n",
      "Step: 2215, Training Loss: 3.25687, LR: 0.0006319, Tokens/sec: 40477.02\n",
      "Step: 2216, Training Loss: 3.05894, LR: 0.0006316, Tokens/sec: 36922.22\n",
      "Step: 2217, Training Loss: 2.82373, LR: 0.0006312, Tokens/sec: 40705.02\n",
      "Step: 2218, Training Loss: 3.13771, LR: 0.0006309, Tokens/sec: 41602.61\n",
      "Step: 2219, Training Loss: 2.44572, LR: 0.0006305, Tokens/sec: 41647.90\n",
      "Step: 2220, Training Loss: 2.95915, LR: 0.0006302, Tokens/sec: 41761.73\n",
      "Step: 2221, Training Loss: 3.06556, LR: 0.0006298, Tokens/sec: 41710.04\n",
      "Step: 2222, Training Loss: 3.16519, LR: 0.0006295, Tokens/sec: 41731.16\n",
      "Step: 2223, Training Loss: 3.13683, LR: 0.0006291, Tokens/sec: 35280.63\n",
      "Step: 2224, Training Loss: 3.00756, LR: 0.0006288, Tokens/sec: 35414.30\n",
      "Step: 2225, Training Loss: 2.90324, LR: 0.0006285, Tokens/sec: 38645.89\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 2225, Eval Loss: 2.95437\n",
      "Step: 2226, Training Loss: 2.78730, LR: 0.0006281, Tokens/sec: 39671.54\n",
      "Step: 2227, Training Loss: 2.71581, LR: 0.0006278, Tokens/sec: 38614.82\n",
      "Step: 2228, Training Loss: 2.69837, LR: 0.0006274, Tokens/sec: 41664.72\n",
      "Step: 2229, Training Loss: 2.85461, LR: 0.0006271, Tokens/sec: 41593.69\n",
      "Step: 2230, Training Loss: 3.26236, LR: 0.0006267, Tokens/sec: 41528.52\n",
      "Step: 2231, Training Loss: 3.15013, LR: 0.0006264, Tokens/sec: 41672.91\n",
      "Step: 2232, Training Loss: 2.68888, LR: 0.0006260, Tokens/sec: 40813.91\n",
      "Step: 2233, Training Loss: 2.82958, LR: 0.0006257, Tokens/sec: 40200.16\n",
      "Step: 2234, Training Loss: 2.58768, LR: 0.0006253, Tokens/sec: 39206.93\n",
      "Step: 2235, Training Loss: 2.87428, LR: 0.0006250, Tokens/sec: 41624.10\n",
      "Step: 2236, Training Loss: 3.00608, LR: 0.0006246, Tokens/sec: 40864.32\n",
      "Step: 2237, Training Loss: 2.53421, LR: 0.0006243, Tokens/sec: 38545.99\n",
      "Step: 2238, Training Loss: 2.92499, LR: 0.0006239, Tokens/sec: 41380.74\n",
      "Step: 2239, Training Loss: 2.55797, LR: 0.0006236, Tokens/sec: 41015.71\n",
      "Step: 2240, Training Loss: 2.73571, LR: 0.0006232, Tokens/sec: 40844.70\n",
      "Step: 2241, Training Loss: 2.83234, LR: 0.0006229, Tokens/sec: 41773.31\n",
      "Step: 2242, Training Loss: 3.02666, LR: 0.0006225, Tokens/sec: 38659.07\n",
      "Step: 2243, Training Loss: 2.28837, LR: 0.0006222, Tokens/sec: 35607.64\n",
      "Step: 2244, Training Loss: 2.94032, LR: 0.0006218, Tokens/sec: 35966.50\n",
      "Step: 2245, Training Loss: 2.87306, LR: 0.0006215, Tokens/sec: 37960.86\n",
      "Step: 2246, Training Loss: 2.77223, LR: 0.0006211, Tokens/sec: 41617.70\n",
      "Step: 2247, Training Loss: 3.28159, LR: 0.0006208, Tokens/sec: 41573.18\n",
      "Step: 2248, Training Loss: 2.97807, LR: 0.0006204, Tokens/sec: 41610.95\n",
      "Step: 2249, Training Loss: 3.13425, LR: 0.0006201, Tokens/sec: 38229.47\n",
      "Step: 2250, Training Loss: 2.75603, LR: 0.0006198, Tokens/sec: 36980.64\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 2250, Eval Loss: 2.90647\n",
      "Step: 2251, Training Loss: 2.96604, LR: 0.0006194, Tokens/sec: 41628.10\n",
      "Step: 2252, Training Loss: 2.79742, LR: 0.0006191, Tokens/sec: 41167.88\n",
      "Step: 2253, Training Loss: 2.81886, LR: 0.0006187, Tokens/sec: 41730.62\n",
      "Step: 2254, Training Loss: 2.79886, LR: 0.0006184, Tokens/sec: 41254.12\n",
      "Step: 2255, Training Loss: 2.79480, LR: 0.0006180, Tokens/sec: 41711.76\n",
      "Step: 2256, Training Loss: 2.83796, LR: 0.0006177, Tokens/sec: 41588.72\n",
      "Step: 2257, Training Loss: 3.05298, LR: 0.0006173, Tokens/sec: 41738.12\n",
      "Step: 2258, Training Loss: 2.75416, LR: 0.0006170, Tokens/sec: 40764.29\n",
      "Step: 2259, Training Loss: 2.53827, LR: 0.0006166, Tokens/sec: 41549.47\n",
      "Step: 2260, Training Loss: 2.60189, LR: 0.0006163, Tokens/sec: 41689.77\n",
      "Step: 2261, Training Loss: 3.20573, LR: 0.0006159, Tokens/sec: 41041.25\n",
      "Step: 2262, Training Loss: 2.72484, LR: 0.0006156, Tokens/sec: 41643.77\n",
      "Step: 2263, Training Loss: 3.05354, LR: 0.0006152, Tokens/sec: 41676.01\n",
      "Step: 2264, Training Loss: 2.71825, LR: 0.0006149, Tokens/sec: 41701.77\n",
      "Step: 2265, Training Loss: 2.84611, LR: 0.0006145, Tokens/sec: 41104.17\n",
      "Step: 2266, Training Loss: 2.64924, LR: 0.0006142, Tokens/sec: 40552.75\n",
      "Step: 2267, Training Loss: 2.84075, LR: 0.0006138, Tokens/sec: 41656.36\n",
      "Step: 2268, Training Loss: 3.19154, LR: 0.0006135, Tokens/sec: 41699.37\n",
      "Step: 2269, Training Loss: 2.99578, LR: 0.0006131, Tokens/sec: 40892.26\n",
      "Step: 2270, Training Loss: 2.41880, LR: 0.0006128, Tokens/sec: 41541.03\n",
      "Step: 2271, Training Loss: 2.72137, LR: 0.0006124, Tokens/sec: 41525.58\n",
      "Step: 2272, Training Loss: 3.04146, LR: 0.0006121, Tokens/sec: 40857.01\n",
      "Step: 2273, Training Loss: 2.80190, LR: 0.0006117, Tokens/sec: 26990.34\n",
      "Step: 2274, Training Loss: 2.79410, LR: 0.0006114, Tokens/sec: 26818.50\n",
      "Step: 2275, Training Loss: 2.58061, LR: 0.0006110, Tokens/sec: 28799.81\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 2275, Eval Loss: 3.01582\n",
      "Step: 2276, Training Loss: 2.53340, LR: 0.0006107, Tokens/sec: 41184.96\n",
      "Step: 2277, Training Loss: 2.65987, LR: 0.0006103, Tokens/sec: 41423.87\n",
      "Step: 2278, Training Loss: 2.63438, LR: 0.0006100, Tokens/sec: 40524.42\n",
      "Step: 2279, Training Loss: 2.87031, LR: 0.0006096, Tokens/sec: 38132.94\n",
      "Step: 2280, Training Loss: 2.60693, LR: 0.0006093, Tokens/sec: 36397.12\n",
      "Step: 2281, Training Loss: 2.59812, LR: 0.0006089, Tokens/sec: 38004.90\n",
      "Step: 2282, Training Loss: 2.04490, LR: 0.0006086, Tokens/sec: 41352.88\n",
      "Step: 2283, Training Loss: 2.91010, LR: 0.0006082, Tokens/sec: 39657.37\n",
      "Step: 2284, Training Loss: 2.76579, LR: 0.0006079, Tokens/sec: 41279.62\n",
      "Step: 2285, Training Loss: 3.00652, LR: 0.0006075, Tokens/sec: 41476.01\n",
      "Step: 2286, Training Loss: 2.42943, LR: 0.0006072, Tokens/sec: 40397.62\n",
      "Step: 2287, Training Loss: 2.62527, LR: 0.0006068, Tokens/sec: 41393.89\n",
      "Step: 2288, Training Loss: 2.41782, LR: 0.0006065, Tokens/sec: 40891.58\n",
      "Step: 2289, Training Loss: 2.78033, LR: 0.0006061, Tokens/sec: 41391.69\n",
      "Step: 2290, Training Loss: 2.68962, LR: 0.0006058, Tokens/sec: 41377.16\n",
      "Step: 2291, Training Loss: 2.99788, LR: 0.0006054, Tokens/sec: 41453.39\n",
      "Step: 2292, Training Loss: 2.88136, LR: 0.0006051, Tokens/sec: 41333.18\n",
      "Step: 2293, Training Loss: 2.53875, LR: 0.0006047, Tokens/sec: 41357.68\n",
      "Step: 2294, Training Loss: 2.89764, LR: 0.0006044, Tokens/sec: 40494.61\n",
      "Step: 2295, Training Loss: 2.50821, LR: 0.0006040, Tokens/sec: 40803.33\n",
      "Step: 2296, Training Loss: 2.71213, LR: 0.0006037, Tokens/sec: 41339.22\n",
      "Step: 2297, Training Loss: 2.81894, LR: 0.0006033, Tokens/sec: 40756.71\n",
      "Step: 2298, Training Loss: 3.02535, LR: 0.0006030, Tokens/sec: 41409.28\n",
      "Step: 2299, Training Loss: 3.11469, LR: 0.0006026, Tokens/sec: 41398.98\n",
      "Step: 2300, Training Loss: 2.44351, LR: 0.0006023, Tokens/sec: 41362.15\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 2300, Eval Loss: 2.71009\n",
      "Step: 2301, Training Loss: 2.76604, LR: 0.0006019, Tokens/sec: 29763.47\n",
      "Step: 2302, Training Loss: 3.25598, LR: 0.0006016, Tokens/sec: 41237.50\n",
      "Step: 2303, Training Loss: 2.79365, LR: 0.0006012, Tokens/sec: 37010.82\n",
      "Step: 2304, Training Loss: 2.63896, LR: 0.0006009, Tokens/sec: 36256.45\n",
      "Step: 2305, Training Loss: 2.96392, LR: 0.0006005, Tokens/sec: 41275.53\n",
      "Step: 2306, Training Loss: 2.55615, LR: 0.0006002, Tokens/sec: 40561.82\n",
      "Step: 2307, Training Loss: 2.68904, LR: 0.0005998, Tokens/sec: 31782.01\n",
      "Step: 2308, Training Loss: 2.94509, LR: 0.0005995, Tokens/sec: 40715.19\n",
      "Step: 2309, Training Loss: 2.88925, LR: 0.0005991, Tokens/sec: 41344.71\n",
      "Step: 2310, Training Loss: 2.84612, LR: 0.0005988, Tokens/sec: 40548.88\n",
      "Step: 2311, Training Loss: 2.87652, LR: 0.0005984, Tokens/sec: 31577.40\n",
      "Step: 2312, Training Loss: 2.78604, LR: 0.0005981, Tokens/sec: 41344.85\n",
      "Step: 2313, Training Loss: 2.84088, LR: 0.0005977, Tokens/sec: 26151.58\n",
      "Step: 2314, Training Loss: 2.62643, LR: 0.0005974, Tokens/sec: 28880.99\n",
      "Step: 2315, Training Loss: 3.06069, LR: 0.0005970, Tokens/sec: 41610.63\n",
      "Step: 2316, Training Loss: 3.20810, LR: 0.0005967, Tokens/sec: 33771.26\n",
      "Step: 2317, Training Loss: 2.59017, LR: 0.0005963, Tokens/sec: 41649.44\n",
      "Step: 2318, Training Loss: 2.90361, LR: 0.0005960, Tokens/sec: 41527.33\n",
      "Step: 2319, Training Loss: 2.50313, LR: 0.0005956, Tokens/sec: 41696.30\n",
      "Step: 2320, Training Loss: 2.89886, LR: 0.0005953, Tokens/sec: 41751.70\n",
      "Step: 2321, Training Loss: 2.60152, LR: 0.0005949, Tokens/sec: 40811.51\n",
      "Step: 2322, Training Loss: 2.70220, LR: 0.0005946, Tokens/sec: 38422.23\n",
      "Step: 2323, Training Loss: 2.55149, LR: 0.0005942, Tokens/sec: 29141.19\n",
      "Step: 2324, Training Loss: 2.95437, LR: 0.0005939, Tokens/sec: 40640.33\n",
      "Step: 2325, Training Loss: 2.57492, LR: 0.0005935, Tokens/sec: 41731.69\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 2325, Eval Loss: 2.76035\n",
      "Step: 2326, Training Loss: 2.98801, LR: 0.0005932, Tokens/sec: 41544.06\n",
      "Step: 2327, Training Loss: 2.70515, LR: 0.0005928, Tokens/sec: 41631.11\n",
      "Step: 2328, Training Loss: 3.12477, LR: 0.0005925, Tokens/sec: 41407.21\n",
      "Step: 2329, Training Loss: 2.47734, LR: 0.0005921, Tokens/sec: 41740.03\n",
      "Step: 2330, Training Loss: 2.54964, LR: 0.0005917, Tokens/sec: 41812.35\n",
      "Step: 2331, Training Loss: 2.68659, LR: 0.0005914, Tokens/sec: 41724.56\n",
      "Step: 2332, Training Loss: 2.71097, LR: 0.0005910, Tokens/sec: 41708.32\n",
      "Step: 2333, Training Loss: 2.90252, LR: 0.0005907, Tokens/sec: 40780.41\n",
      "Step: 2334, Training Loss: 2.75731, LR: 0.0005903, Tokens/sec: 41668.02\n",
      "Step: 2335, Training Loss: 2.35186, LR: 0.0005900, Tokens/sec: 38288.03\n",
      "Step: 2336, Training Loss: 2.99161, LR: 0.0005896, Tokens/sec: 40582.75\n",
      "Step: 2337, Training Loss: 2.37636, LR: 0.0005893, Tokens/sec: 41670.95\n",
      "Step: 2338, Training Loss: 2.76071, LR: 0.0005889, Tokens/sec: 41703.93\n",
      "Step: 2339, Training Loss: 2.60893, LR: 0.0005886, Tokens/sec: 41733.57\n",
      "Step: 2340, Training Loss: 2.63175, LR: 0.0005882, Tokens/sec: 41718.81\n",
      "Step: 2341, Training Loss: 2.71946, LR: 0.0005879, Tokens/sec: 40841.88\n",
      "Step: 2342, Training Loss: 2.80341, LR: 0.0005875, Tokens/sec: 41151.72\n",
      "Step: 2343, Training Loss: 2.91040, LR: 0.0005872, Tokens/sec: 41739.75\n",
      "Step: 2344, Training Loss: 2.66759, LR: 0.0005868, Tokens/sec: 40839.83\n",
      "Step: 2345, Training Loss: 2.87107, LR: 0.0005865, Tokens/sec: 41630.56\n",
      "Step: 2346, Training Loss: 3.03033, LR: 0.0005861, Tokens/sec: 41774.51\n",
      "Step: 2347, Training Loss: 2.56116, LR: 0.0005858, Tokens/sec: 41612.69\n",
      "Step: 2348, Training Loss: 2.93764, LR: 0.0005854, Tokens/sec: 41656.46\n",
      "Step: 2349, Training Loss: 2.53455, LR: 0.0005851, Tokens/sec: 41749.15\n",
      "Step: 2350, Training Loss: 2.89785, LR: 0.0005847, Tokens/sec: 41151.88\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 2350, Eval Loss: 2.80905\n",
      "Step: 2351, Training Loss: 3.29999, LR: 0.0005844, Tokens/sec: 40877.34\n",
      "Step: 2352, Training Loss: 2.77732, LR: 0.0005840, Tokens/sec: 40517.25\n",
      "Step: 2353, Training Loss: 2.63519, LR: 0.0005837, Tokens/sec: 41465.46\n",
      "Step: 2354, Training Loss: 2.72807, LR: 0.0005833, Tokens/sec: 41404.47\n",
      "Step: 2355, Training Loss: 2.55845, LR: 0.0005830, Tokens/sec: 41503.65\n",
      "Step: 2356, Training Loss: 2.85605, LR: 0.0005826, Tokens/sec: 40660.42\n",
      "Step: 2357, Training Loss: 2.52980, LR: 0.0005823, Tokens/sec: 41482.51\n",
      "Step: 2358, Training Loss: 2.44236, LR: 0.0005819, Tokens/sec: 41545.43\n",
      "Step: 2359, Training Loss: 2.85094, LR: 0.0005816, Tokens/sec: 41493.44\n",
      "Step: 2360, Training Loss: 2.48128, LR: 0.0005812, Tokens/sec: 40698.35\n",
      "Step: 2361, Training Loss: 2.68858, LR: 0.0005808, Tokens/sec: 41540.41\n",
      "Step: 2362, Training Loss: 2.97493, LR: 0.0005805, Tokens/sec: 41426.38\n",
      "Step: 2363, Training Loss: 2.60241, LR: 0.0005801, Tokens/sec: 41358.24\n",
      "Step: 2364, Training Loss: 2.42526, LR: 0.0005798, Tokens/sec: 40892.78\n",
      "Step: 2365, Training Loss: 2.33025, LR: 0.0005794, Tokens/sec: 41511.11\n",
      "Step: 2366, Training Loss: 2.77299, LR: 0.0005791, Tokens/sec: 41574.90\n",
      "Step: 2367, Training Loss: 2.60536, LR: 0.0005787, Tokens/sec: 41493.02\n",
      "Step: 2368, Training Loss: 2.74968, LR: 0.0005784, Tokens/sec: 40433.05\n",
      "Step: 2369, Training Loss: 2.10101, LR: 0.0005780, Tokens/sec: 41390.78\n",
      "Step: 2370, Training Loss: 2.86030, LR: 0.0005777, Tokens/sec: 41298.11\n",
      "Step: 2371, Training Loss: 2.05253, LR: 0.0005773, Tokens/sec: 41450.83\n",
      "Step: 2372, Training Loss: 2.28155, LR: 0.0005770, Tokens/sec: 40866.72\n",
      "Step: 2373, Training Loss: 2.38087, LR: 0.0005766, Tokens/sec: 41415.13\n",
      "Step: 2374, Training Loss: 2.57573, LR: 0.0005763, Tokens/sec: 41388.58\n",
      "Step: 2375, Training Loss: 2.85031, LR: 0.0005759, Tokens/sec: 41492.87\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 2375, Eval Loss: 2.63555\n",
      "Step: 2376, Training Loss: 2.54676, LR: 0.0005756, Tokens/sec: 37897.76\n",
      "Step: 2377, Training Loss: 2.65551, LR: 0.0005752, Tokens/sec: 38147.27\n",
      "Step: 2378, Training Loss: 3.12542, LR: 0.0005749, Tokens/sec: 39903.61\n",
      "Step: 2379, Training Loss: 1.58737, LR: 0.0005745, Tokens/sec: 26797.66\n",
      "Step: 2380, Training Loss: 2.80121, LR: 0.0005742, Tokens/sec: 39957.15\n",
      "Step: 2381, Training Loss: 3.00659, LR: 0.0005738, Tokens/sec: 40187.52\n",
      "Step: 2382, Training Loss: 2.61088, LR: 0.0005735, Tokens/sec: 41445.87\n",
      "Step: 2383, Training Loss: 2.53051, LR: 0.0005731, Tokens/sec: 36848.61\n",
      "Step: 2384, Training Loss: 2.55683, LR: 0.0005727, Tokens/sec: 41076.14\n",
      "Step: 2385, Training Loss: 2.86857, LR: 0.0005724, Tokens/sec: 40633.84\n",
      "Step: 2386, Training Loss: 2.40528, LR: 0.0005720, Tokens/sec: 41510.25\n",
      "Step: 2387, Training Loss: 3.10590, LR: 0.0005717, Tokens/sec: 32869.35\n",
      "Step: 2388, Training Loss: 1.78683, LR: 0.0005713, Tokens/sec: 38977.44\n",
      "Step: 2389, Training Loss: 3.12891, LR: 0.0005710, Tokens/sec: 37754.85\n",
      "Step: 2390, Training Loss: 3.07863, LR: 0.0005706, Tokens/sec: 40552.13\n",
      "Step: 2391, Training Loss: 2.46315, LR: 0.0005703, Tokens/sec: 41488.67\n",
      "Step: 2392, Training Loss: 3.04489, LR: 0.0005699, Tokens/sec: 41467.42\n",
      "Step: 2393, Training Loss: 3.59689, LR: 0.0005696, Tokens/sec: 41515.75\n",
      "Step: 2394, Training Loss: 2.67602, LR: 0.0005692, Tokens/sec: 41482.33\n",
      "Step: 2395, Training Loss: 2.58194, LR: 0.0005689, Tokens/sec: 41331.68\n",
      "Step: 2396, Training Loss: 2.64938, LR: 0.0005685, Tokens/sec: 41463.55\n",
      "Step: 2397, Training Loss: 2.97204, LR: 0.0005682, Tokens/sec: 28751.59\n",
      "Step: 2398, Training Loss: 2.39156, LR: 0.0005678, Tokens/sec: 40224.93\n",
      "Step: 2399, Training Loss: 2.44089, LR: 0.0005675, Tokens/sec: 39468.67\n",
      "Step: 2400, Training Loss: 2.83943, LR: 0.0005671, Tokens/sec: 40614.23\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 2400, Eval Loss: 2.74066\n",
      "Step: 2401, Training Loss: 2.52497, LR: 0.0005668, Tokens/sec: 37858.08\n",
      "Step: 2402, Training Loss: 2.55804, LR: 0.0005664, Tokens/sec: 37116.69\n",
      "Step: 2403, Training Loss: 2.04547, LR: 0.0005660, Tokens/sec: 36591.91\n",
      "Step: 2404, Training Loss: 2.37723, LR: 0.0005657, Tokens/sec: 31018.79\n",
      "Step: 2405, Training Loss: 2.49996, LR: 0.0005653, Tokens/sec: 38433.02\n",
      "Step: 2406, Training Loss: 3.21472, LR: 0.0005650, Tokens/sec: 37956.07\n",
      "Step: 2407, Training Loss: 2.95116, LR: 0.0005646, Tokens/sec: 35496.33\n",
      "Step: 2408, Training Loss: 2.65848, LR: 0.0005643, Tokens/sec: 38854.34\n",
      "Step: 2409, Training Loss: 2.50075, LR: 0.0005639, Tokens/sec: 36174.25\n",
      "Step: 2410, Training Loss: 2.22513, LR: 0.0005636, Tokens/sec: 39497.40\n",
      "Step: 2411, Training Loss: 2.75273, LR: 0.0005632, Tokens/sec: 33187.39\n",
      "Step: 2412, Training Loss: 2.72465, LR: 0.0005629, Tokens/sec: 32828.87\n",
      "Step: 2413, Training Loss: 2.54259, LR: 0.0005625, Tokens/sec: 34741.46\n",
      "Step: 2414, Training Loss: 2.41488, LR: 0.0005622, Tokens/sec: 35647.26\n",
      "Step: 2415, Training Loss: 2.49303, LR: 0.0005618, Tokens/sec: 34390.42\n",
      "Step: 2416, Training Loss: 1.99227, LR: 0.0005615, Tokens/sec: 40394.64\n",
      "Step: 2417, Training Loss: 2.35511, LR: 0.0005611, Tokens/sec: 27982.26\n",
      "Step: 2418, Training Loss: 2.93127, LR: 0.0005608, Tokens/sec: 38771.05\n",
      "Step: 2419, Training Loss: 2.41965, LR: 0.0005604, Tokens/sec: 41354.88\n",
      "Step: 2420, Training Loss: 2.93490, LR: 0.0005601, Tokens/sec: 33595.37\n",
      "Step: 2421, Training Loss: 2.49507, LR: 0.0005597, Tokens/sec: 41528.18\n",
      "Step: 2422, Training Loss: 2.66758, LR: 0.0005593, Tokens/sec: 41488.09\n",
      "Step: 2423, Training Loss: 2.82981, LR: 0.0005590, Tokens/sec: 41470.93\n",
      "Step: 2424, Training Loss: 2.41858, LR: 0.0005586, Tokens/sec: 41736.16\n",
      "Step: 2425, Training Loss: 2.71381, LR: 0.0005583, Tokens/sec: 39040.30\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 2425, Eval Loss: 2.62347\n",
      "Step: 2426, Training Loss: 2.30944, LR: 0.0005579, Tokens/sec: 41470.59\n",
      "Step: 2427, Training Loss: 2.64690, LR: 0.0005576, Tokens/sec: 41803.05\n",
      "Step: 2428, Training Loss: 2.49779, LR: 0.0005572, Tokens/sec: 41673.13\n",
      "Step: 2429, Training Loss: 2.42898, LR: 0.0005569, Tokens/sec: 38387.86\n",
      "Step: 2430, Training Loss: 2.35329, LR: 0.0005565, Tokens/sec: 38204.88\n",
      "Step: 2431, Training Loss: 2.46157, LR: 0.0005562, Tokens/sec: 41625.52\n",
      "Step: 2432, Training Loss: 2.58364, LR: 0.0005558, Tokens/sec: 41705.66\n",
      "Step: 2433, Training Loss: 2.44872, LR: 0.0005555, Tokens/sec: 40737.01\n",
      "Step: 2434, Training Loss: 2.80480, LR: 0.0005551, Tokens/sec: 41565.62\n",
      "Step: 2435, Training Loss: 2.35151, LR: 0.0005548, Tokens/sec: 32418.22\n",
      "Step: 2436, Training Loss: 2.90790, LR: 0.0005544, Tokens/sec: 38333.70\n",
      "Step: 2437, Training Loss: 2.17273, LR: 0.0005541, Tokens/sec: 41722.59\n",
      "Step: 2438, Training Loss: 2.21487, LR: 0.0005537, Tokens/sec: 41056.25\n",
      "Step: 2439, Training Loss: 2.76427, LR: 0.0005534, Tokens/sec: 41552.17\n",
      "Step: 2440, Training Loss: 2.78034, LR: 0.0005530, Tokens/sec: 36092.23\n",
      "Step: 2441, Training Loss: 2.95010, LR: 0.0005526, Tokens/sec: 29537.59\n",
      "Step: 2442, Training Loss: 2.68267, LR: 0.0005523, Tokens/sec: 40930.11\n",
      "Step: 2443, Training Loss: 2.19097, LR: 0.0005519, Tokens/sec: 40750.21\n",
      "Step: 2444, Training Loss: 2.82204, LR: 0.0005516, Tokens/sec: 40890.90\n",
      "Step: 2445, Training Loss: 2.44681, LR: 0.0005512, Tokens/sec: 41561.58\n",
      "Step: 2446, Training Loss: 2.36265, LR: 0.0005509, Tokens/sec: 41020.77\n",
      "Step: 2447, Training Loss: 2.44173, LR: 0.0005505, Tokens/sec: 40790.94\n",
      "Step: 2448, Training Loss: 2.08705, LR: 0.0005502, Tokens/sec: 41738.62\n",
      "Step: 2449, Training Loss: 2.28198, LR: 0.0005498, Tokens/sec: 41700.92\n",
      "Step: 2450, Training Loss: 2.25745, LR: 0.0005495, Tokens/sec: 41139.87\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 2450, Eval Loss: 2.51909\n",
      "Step: 2451, Training Loss: 2.09531, LR: 0.0005491, Tokens/sec: 40177.63\n",
      "Step: 2452, Training Loss: 2.54122, LR: 0.0005488, Tokens/sec: 41410.19\n",
      "Step: 2453, Training Loss: 2.38961, LR: 0.0005484, Tokens/sec: 41384.25\n",
      "Step: 2454, Training Loss: 2.63111, LR: 0.0005481, Tokens/sec: 41529.58\n",
      "Step: 2455, Training Loss: 2.53105, LR: 0.0005477, Tokens/sec: 39709.30\n",
      "Step: 2456, Training Loss: 2.30308, LR: 0.0005474, Tokens/sec: 40677.10\n",
      "Step: 2457, Training Loss: 2.91018, LR: 0.0005470, Tokens/sec: 40677.52\n",
      "Step: 2458, Training Loss: 2.50465, LR: 0.0005466, Tokens/sec: 41386.68\n",
      "Step: 2459, Training Loss: 2.37985, LR: 0.0005463, Tokens/sec: 40633.51\n",
      "Step: 2460, Training Loss: 1.90671, LR: 0.0005459, Tokens/sec: 41519.83\n",
      "Step: 2461, Training Loss: 2.61744, LR: 0.0005456, Tokens/sec: 41544.54\n",
      "Step: 2462, Training Loss: 2.64409, LR: 0.0005452, Tokens/sec: 41379.94\n",
      "Step: 2463, Training Loss: 2.58380, LR: 0.0005449, Tokens/sec: 41472.91\n",
      "Step: 2464, Training Loss: 2.66281, LR: 0.0005445, Tokens/sec: 41528.45\n",
      "Step: 2465, Training Loss: 2.53700, LR: 0.0005442, Tokens/sec: 41521.85\n",
      "Step: 2466, Training Loss: 2.47522, LR: 0.0005438, Tokens/sec: 41374.33\n",
      "Step: 2467, Training Loss: 2.35076, LR: 0.0005435, Tokens/sec: 40671.02\n",
      "Step: 2468, Training Loss: 2.49506, LR: 0.0005431, Tokens/sec: 41497.01\n",
      "Step: 2469, Training Loss: 2.35057, LR: 0.0005428, Tokens/sec: 38384.52\n",
      "Step: 2470, Training Loss: 2.52546, LR: 0.0005424, Tokens/sec: 38093.94\n",
      "Step: 2471, Training Loss: 2.75119, LR: 0.0005421, Tokens/sec: 40550.07\n",
      "Step: 2472, Training Loss: 2.27924, LR: 0.0005417, Tokens/sec: 40519.89\n",
      "Step: 2473, Training Loss: 2.52143, LR: 0.0005414, Tokens/sec: 37772.82\n",
      "Step: 2474, Training Loss: 2.10617, LR: 0.0005410, Tokens/sec: 38759.65\n",
      "Step: 2475, Training Loss: 2.27450, LR: 0.0005407, Tokens/sec: 40903.43\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 2475, Eval Loss: 2.55583\n",
      "Step: 2476, Training Loss: 2.47243, LR: 0.0005403, Tokens/sec: 40761.62\n",
      "Step: 2477, Training Loss: 2.53333, LR: 0.0005399, Tokens/sec: 40592.33\n",
      "Step: 2478, Training Loss: 1.98924, LR: 0.0005396, Tokens/sec: 40879.22\n",
      "Step: 2479, Training Loss: 2.74713, LR: 0.0005392, Tokens/sec: 41456.22\n",
      "Step: 2480, Training Loss: 2.49457, LR: 0.0005389, Tokens/sec: 40783.76\n",
      "Step: 2481, Training Loss: 2.53482, LR: 0.0005385, Tokens/sec: 41471.87\n",
      "Step: 2482, Training Loss: 2.28021, LR: 0.0005382, Tokens/sec: 41461.71\n",
      "Step: 2483, Training Loss: 2.71243, LR: 0.0005378, Tokens/sec: 41324.71\n",
      "Step: 2484, Training Loss: 2.85780, LR: 0.0005375, Tokens/sec: 41518.01\n",
      "Step: 2485, Training Loss: 1.95657, LR: 0.0005371, Tokens/sec: 40493.27\n",
      "Step: 2486, Training Loss: 2.66380, LR: 0.0005368, Tokens/sec: 41348.51\n",
      "Step: 2487, Training Loss: 3.14266, LR: 0.0005364, Tokens/sec: 40819.25\n",
      "Step: 2488, Training Loss: 2.50402, LR: 0.0005361, Tokens/sec: 40653.17\n",
      "Step: 2489, Training Loss: 3.11244, LR: 0.0005357, Tokens/sec: 41525.93\n",
      "Step: 2490, Training Loss: 2.31351, LR: 0.0005354, Tokens/sec: 41473.90\n",
      "Step: 2491, Training Loss: 2.63145, LR: 0.0005350, Tokens/sec: 41402.88\n",
      "Step: 2492, Training Loss: 2.81257, LR: 0.0005347, Tokens/sec: 41487.36\n",
      "Step: 2493, Training Loss: 2.34197, LR: 0.0005343, Tokens/sec: 41530.14\n",
      "Step: 2494, Training Loss: 2.58046, LR: 0.0005340, Tokens/sec: 39029.16\n",
      "Step: 2495, Training Loss: 2.18030, LR: 0.0005336, Tokens/sec: 37357.00\n",
      "Step: 2496, Training Loss: 1.82002, LR: 0.0005332, Tokens/sec: 39709.79\n",
      "Step: 2497, Training Loss: 2.40399, LR: 0.0005329, Tokens/sec: 41398.48\n",
      "Step: 2498, Training Loss: 2.42952, LR: 0.0005325, Tokens/sec: 41508.82\n",
      "Step: 2499, Training Loss: 2.26310, LR: 0.0005322, Tokens/sec: 41498.36\n",
      "Step: 2500, Training Loss: 2.89780, LR: 0.0005318, Tokens/sec: 41420.93\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 2500, Eval Loss: 2.50527\n",
      "Step: 2501, Training Loss: 1.98111, LR: 0.0005315, Tokens/sec: 40503.59\n",
      "Step: 2502, Training Loss: 2.66546, LR: 0.0005311, Tokens/sec: 41466.12\n",
      "Step: 2503, Training Loss: 2.63915, LR: 0.0005308, Tokens/sec: 41479.04\n",
      "Step: 2504, Training Loss: 2.79092, LR: 0.0005304, Tokens/sec: 40610.88\n",
      "Step: 2505, Training Loss: 2.84683, LR: 0.0005301, Tokens/sec: 41410.91\n",
      "Step: 2506, Training Loss: 3.22475, LR: 0.0005297, Tokens/sec: 41399.49\n",
      "Step: 2507, Training Loss: 2.66819, LR: 0.0005294, Tokens/sec: 40883.07\n",
      "Step: 2508, Training Loss: 2.40937, LR: 0.0005290, Tokens/sec: 41307.17\n",
      "Step: 2509, Training Loss: 2.25430, LR: 0.0005287, Tokens/sec: 40461.37\n",
      "Step: 2510, Training Loss: 2.69532, LR: 0.0005283, Tokens/sec: 41440.40\n",
      "Step: 2511, Training Loss: 2.55646, LR: 0.0005280, Tokens/sec: 41286.27\n",
      "Step: 2512, Training Loss: 2.58448, LR: 0.0005276, Tokens/sec: 40328.90\n",
      "Step: 2513, Training Loss: 2.53766, LR: 0.0005273, Tokens/sec: 41245.34\n",
      "Step: 2514, Training Loss: 2.43964, LR: 0.0005269, Tokens/sec: 41348.84\n",
      "Step: 2515, Training Loss: 2.43144, LR: 0.0005265, Tokens/sec: 41376.09\n",
      "Step: 2516, Training Loss: 2.03808, LR: 0.0005262, Tokens/sec: 40761.24\n",
      "Step: 2517, Training Loss: 2.24220, LR: 0.0005258, Tokens/sec: 41425.28\n",
      "Step: 2518, Training Loss: 2.17417, LR: 0.0005255, Tokens/sec: 40749.94\n",
      "Step: 2519, Training Loss: 1.88189, LR: 0.0005251, Tokens/sec: 41414.22\n",
      "Step: 2520, Training Loss: 2.60405, LR: 0.0005248, Tokens/sec: 40420.15\n",
      "Step: 2521, Training Loss: 2.40060, LR: 0.0005244, Tokens/sec: 41347.45\n",
      "Step: 2522, Training Loss: 2.52972, LR: 0.0005241, Tokens/sec: 41363.44\n",
      "Step: 2523, Training Loss: 1.82835, LR: 0.0005237, Tokens/sec: 41363.53\n",
      "Step: 2524, Training Loss: 2.00382, LR: 0.0005234, Tokens/sec: 41432.22\n",
      "Step: 2525, Training Loss: 1.80048, LR: 0.0005230, Tokens/sec: 41321.84\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 2525, Eval Loss: 2.39784\n",
      "Step: 2526, Training Loss: 2.43987, LR: 0.0005227, Tokens/sec: 41305.51\n",
      "Step: 2527, Training Loss: 2.43117, LR: 0.0005223, Tokens/sec: 41395.52\n",
      "Step: 2528, Training Loss: 2.36033, LR: 0.0005220, Tokens/sec: 40565.16\n",
      "Step: 2529, Training Loss: 2.01526, LR: 0.0005216, Tokens/sec: 41340.32\n",
      "Step: 2530, Training Loss: 2.81352, LR: 0.0005213, Tokens/sec: 41367.94\n",
      "Step: 2531, Training Loss: 2.08654, LR: 0.0005209, Tokens/sec: 41298.13\n",
      "Step: 2532, Training Loss: 2.32827, LR: 0.0005206, Tokens/sec: 41459.53\n",
      "Step: 2533, Training Loss: 2.40241, LR: 0.0005202, Tokens/sec: 41394.49\n",
      "Step: 2534, Training Loss: 2.50198, LR: 0.0005199, Tokens/sec: 40820.12\n",
      "Step: 2535, Training Loss: 2.60094, LR: 0.0005195, Tokens/sec: 41421.59\n",
      "Step: 2536, Training Loss: 1.79785, LR: 0.0005192, Tokens/sec: 40051.53\n",
      "Step: 2537, Training Loss: 2.42387, LR: 0.0005188, Tokens/sec: 41364.00\n",
      "Step: 2538, Training Loss: 2.57573, LR: 0.0005184, Tokens/sec: 41396.33\n",
      "Step: 2539, Training Loss: 2.16837, LR: 0.0005181, Tokens/sec: 41367.69\n",
      "Step: 2540, Training Loss: 2.41722, LR: 0.0005177, Tokens/sec: 41421.48\n",
      "Step: 2541, Training Loss: 2.26271, LR: 0.0005174, Tokens/sec: 41373.45\n",
      "Step: 2542, Training Loss: 2.21865, LR: 0.0005170, Tokens/sec: 41326.38\n",
      "Step: 2543, Training Loss: 2.76264, LR: 0.0005167, Tokens/sec: 40761.98\n",
      "Step: 2544, Training Loss: 2.38430, LR: 0.0005163, Tokens/sec: 37122.43\n",
      "Step: 2545, Training Loss: 2.35962, LR: 0.0005160, Tokens/sec: 38729.65\n",
      "Step: 2546, Training Loss: 2.20849, LR: 0.0005156, Tokens/sec: 41361.46\n",
      "Step: 2547, Training Loss: 3.02229, LR: 0.0005153, Tokens/sec: 41440.60\n",
      "Step: 2548, Training Loss: 2.06589, LR: 0.0005149, Tokens/sec: 40211.47\n",
      "Step: 2549, Training Loss: 2.39295, LR: 0.0005146, Tokens/sec: 40994.29\n",
      "Step: 2550, Training Loss: 2.60496, LR: 0.0005142, Tokens/sec: 40507.36\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 2550, Eval Loss: 2.39937\n",
      "Step: 2551, Training Loss: 2.17024, LR: 0.0005139, Tokens/sec: 41295.10\n",
      "Step: 2552, Training Loss: 2.18534, LR: 0.0005135, Tokens/sec: 41302.92\n",
      "Step: 2553, Training Loss: 2.28338, LR: 0.0005132, Tokens/sec: 40793.66\n",
      "Step: 2554, Training Loss: 2.32726, LR: 0.0005128, Tokens/sec: 28581.65\n",
      "Step: 2555, Training Loss: 2.30603, LR: 0.0005125, Tokens/sec: 36150.85\n",
      "Step: 2556, Training Loss: 1.98285, LR: 0.0005121, Tokens/sec: 33109.20\n",
      "Step: 2557, Training Loss: 2.39808, LR: 0.0005118, Tokens/sec: 41014.79\n",
      "Step: 2558, Training Loss: 2.70406, LR: 0.0005114, Tokens/sec: 41358.70\n",
      "Step: 2559, Training Loss: 2.55030, LR: 0.0005111, Tokens/sec: 41458.78\n",
      "Step: 2560, Training Loss: 2.53546, LR: 0.0005107, Tokens/sec: 37826.09\n",
      "Step: 2561, Training Loss: 1.92431, LR: 0.0005104, Tokens/sec: 41319.98\n",
      "Step: 2562, Training Loss: 2.92556, LR: 0.0005100, Tokens/sec: 39275.48\n",
      "Step: 2563, Training Loss: 2.32458, LR: 0.0005097, Tokens/sec: 40499.62\n",
      "Step: 2564, Training Loss: 2.46838, LR: 0.0005093, Tokens/sec: 40768.85\n",
      "Step: 2565, Training Loss: 2.53173, LR: 0.0005090, Tokens/sec: 36817.07\n",
      "Step: 2566, Training Loss: 2.04031, LR: 0.0005086, Tokens/sec: 36369.84\n",
      "Step: 2567, Training Loss: 2.73276, LR: 0.0005083, Tokens/sec: 40608.48\n",
      "Step: 2568, Training Loss: 2.18535, LR: 0.0005079, Tokens/sec: 40200.60\n",
      "Step: 2569, Training Loss: 2.18197, LR: 0.0005075, Tokens/sec: 35878.80\n",
      "Step: 2570, Training Loss: 2.60040, LR: 0.0005072, Tokens/sec: 37171.12\n",
      "Step: 2571, Training Loss: 2.13266, LR: 0.0005068, Tokens/sec: 38200.19\n",
      "Step: 2572, Training Loss: 2.48646, LR: 0.0005065, Tokens/sec: 41397.02\n",
      "Step: 2573, Training Loss: 2.39042, LR: 0.0005061, Tokens/sec: 36793.40\n",
      "Step: 2574, Training Loss: 2.48892, LR: 0.0005058, Tokens/sec: 32159.33\n",
      "Step: 2575, Training Loss: 2.81054, LR: 0.0005054, Tokens/sec: 39149.42\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 2575, Eval Loss: 2.46779\n",
      "Step: 2576, Training Loss: 2.68169, LR: 0.0005051, Tokens/sec: 40696.90\n",
      "Step: 2577, Training Loss: 2.73790, LR: 0.0005047, Tokens/sec: 41324.79\n",
      "Step: 2578, Training Loss: 2.36667, LR: 0.0005044, Tokens/sec: 37814.34\n",
      "Step: 2579, Training Loss: 2.30754, LR: 0.0005040, Tokens/sec: 37519.34\n",
      "Step: 2580, Training Loss: 2.20560, LR: 0.0005037, Tokens/sec: 27747.15\n",
      "Step: 2581, Training Loss: 2.26777, LR: 0.0005033, Tokens/sec: 38795.38\n",
      "Step: 2582, Training Loss: 2.79680, LR: 0.0005030, Tokens/sec: 40537.24\n",
      "Step: 2583, Training Loss: 2.62620, LR: 0.0005026, Tokens/sec: 41519.75\n",
      "Step: 2584, Training Loss: 2.24373, LR: 0.0005023, Tokens/sec: 41519.04\n",
      "Step: 2585, Training Loss: 2.33545, LR: 0.0005019, Tokens/sec: 37875.23\n",
      "Step: 2586, Training Loss: 2.27389, LR: 0.0005016, Tokens/sec: 37945.47\n",
      "Step: 2587, Training Loss: 2.51719, LR: 0.0005012, Tokens/sec: 41315.11\n",
      "Step: 2588, Training Loss: 1.93177, LR: 0.0005009, Tokens/sec: 41366.44\n",
      "Step: 2589, Training Loss: 1.84991, LR: 0.0005005, Tokens/sec: 37880.88\n",
      "Step: 2590, Training Loss: 2.67321, LR: 0.0005002, Tokens/sec: 34851.20\n",
      "Step: 2591, Training Loss: 2.61145, LR: 0.0004998, Tokens/sec: 41207.49\n",
      "Step: 2592, Training Loss: 2.48018, LR: 0.0004995, Tokens/sec: 41476.54\n",
      "Step: 2593, Training Loss: 2.28175, LR: 0.0004991, Tokens/sec: 29741.96\n",
      "Step: 2594, Training Loss: 2.08424, LR: 0.0004988, Tokens/sec: 30831.56\n",
      "Step: 2595, Training Loss: 2.35125, LR: 0.0004984, Tokens/sec: 41182.87\n",
      "Step: 2596, Training Loss: 2.26377, LR: 0.0004981, Tokens/sec: 40990.30\n",
      "Step: 2597, Training Loss: 2.48606, LR: 0.0004977, Tokens/sec: 38652.37\n",
      "Step: 2598, Training Loss: 2.50423, LR: 0.0004974, Tokens/sec: 37239.06\n",
      "Step: 2599, Training Loss: 2.15912, LR: 0.0004970, Tokens/sec: 41411.31\n",
      "Step: 2600, Training Loss: 2.00339, LR: 0.0004967, Tokens/sec: 38972.65\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 2600, Eval Loss: 2.34003\n",
      "Step: 2601, Training Loss: 2.18708, LR: 0.0004963, Tokens/sec: 41314.73\n",
      "Step: 2602, Training Loss: 2.56551, LR: 0.0004960, Tokens/sec: 41288.14\n",
      "Step: 2603, Training Loss: 2.36032, LR: 0.0004956, Tokens/sec: 39962.02\n",
      "Step: 2604, Training Loss: 2.70643, LR: 0.0004953, Tokens/sec: 41048.33\n",
      "Step: 2605, Training Loss: 2.26783, LR: 0.0004949, Tokens/sec: 41467.84\n",
      "Step: 2606, Training Loss: 2.30705, LR: 0.0004946, Tokens/sec: 40707.46\n",
      "Step: 2607, Training Loss: 2.17110, LR: 0.0004942, Tokens/sec: 41431.63\n",
      "Step: 2608, Training Loss: 2.80644, LR: 0.0004939, Tokens/sec: 41012.64\n",
      "Step: 2609, Training Loss: 2.42099, LR: 0.0004935, Tokens/sec: 40078.69\n",
      "Step: 2610, Training Loss: 2.61162, LR: 0.0004932, Tokens/sec: 41466.84\n",
      "Step: 2611, Training Loss: 2.84504, LR: 0.0004928, Tokens/sec: 41455.05\n",
      "Step: 2612, Training Loss: 2.79064, LR: 0.0004925, Tokens/sec: 41408.27\n",
      "Step: 2613, Training Loss: 2.39398, LR: 0.0004921, Tokens/sec: 41354.28\n",
      "Step: 2614, Training Loss: 2.26940, LR: 0.0004918, Tokens/sec: 38236.81\n",
      "Step: 2615, Training Loss: 1.70179, LR: 0.0004914, Tokens/sec: 37736.95\n",
      "Step: 2616, Training Loss: 1.98296, LR: 0.0004911, Tokens/sec: 37877.56\n",
      "Step: 2617, Training Loss: 2.04088, LR: 0.0004907, Tokens/sec: 38295.08\n",
      "Step: 2618, Training Loss: 2.44522, LR: 0.0004904, Tokens/sec: 41356.06\n",
      "Step: 2619, Training Loss: 2.30251, LR: 0.0004900, Tokens/sec: 40307.23\n",
      "Step: 2620, Training Loss: 2.59331, LR: 0.0004897, Tokens/sec: 41414.50\n",
      "Step: 2621, Training Loss: 2.22155, LR: 0.0004893, Tokens/sec: 41401.32\n",
      "Step: 2622, Training Loss: 2.12037, LR: 0.0004890, Tokens/sec: 40273.77\n",
      "Step: 2623, Training Loss: 2.37156, LR: 0.0004886, Tokens/sec: 41438.35\n",
      "Step: 2624, Training Loss: 2.31847, LR: 0.0004883, Tokens/sec: 41478.99\n",
      "Step: 2625, Training Loss: 1.47221, LR: 0.0004879, Tokens/sec: 39411.16\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 2625, Eval Loss: 2.41449\n",
      "Step: 2626, Training Loss: 2.13072, LR: 0.0004876, Tokens/sec: 41298.79\n",
      "Step: 2627, Training Loss: 2.60304, LR: 0.0004872, Tokens/sec: 38705.87\n",
      "Step: 2628, Training Loss: 2.83240, LR: 0.0004869, Tokens/sec: 41539.97\n",
      "Step: 2629, Training Loss: 1.96509, LR: 0.0004865, Tokens/sec: 41446.45\n",
      "Step: 2630, Training Loss: 2.30326, LR: 0.0004862, Tokens/sec: 41432.32\n",
      "Step: 2631, Training Loss: 2.02736, LR: 0.0004858, Tokens/sec: 41552.15\n",
      "Step: 2632, Training Loss: 2.49585, LR: 0.0004855, Tokens/sec: 41488.19\n",
      "Step: 2633, Training Loss: 2.62015, LR: 0.0004851, Tokens/sec: 41436.51\n",
      "Step: 2634, Training Loss: 2.28703, LR: 0.0004848, Tokens/sec: 39660.44\n",
      "Step: 2635, Training Loss: 2.72459, LR: 0.0004844, Tokens/sec: 40628.89\n",
      "Step: 2636, Training Loss: 2.70690, LR: 0.0004841, Tokens/sec: 36149.99\n",
      "Step: 2637, Training Loss: 3.16882, LR: 0.0004837, Tokens/sec: 36523.48\n",
      "Step: 2638, Training Loss: 1.92247, LR: 0.0004834, Tokens/sec: 40685.32\n",
      "Step: 2639, Training Loss: 2.57469, LR: 0.0004830, Tokens/sec: 41230.72\n",
      "Step: 2640, Training Loss: 2.53243, LR: 0.0004827, Tokens/sec: 38435.39\n",
      "Step: 2641, Training Loss: 2.41978, LR: 0.0004823, Tokens/sec: 38707.78\n",
      "Step: 2642, Training Loss: 2.40854, LR: 0.0004820, Tokens/sec: 38719.44\n",
      "Step: 2643, Training Loss: 2.66731, LR: 0.0004816, Tokens/sec: 41383.71\n",
      "Step: 2644, Training Loss: 2.36908, LR: 0.0004813, Tokens/sec: 30474.01\n",
      "Step: 2645, Training Loss: 1.93026, LR: 0.0004809, Tokens/sec: 40606.78\n",
      "Step: 2646, Training Loss: 2.31646, LR: 0.0004806, Tokens/sec: 39817.44\n",
      "Step: 2647, Training Loss: 2.60331, LR: 0.0004802, Tokens/sec: 33641.36\n",
      "Step: 2648, Training Loss: 2.63880, LR: 0.0004799, Tokens/sec: 37915.21\n",
      "Step: 2649, Training Loss: 2.47674, LR: 0.0004796, Tokens/sec: 40524.91\n",
      "Step: 2650, Training Loss: 2.70068, LR: 0.0004792, Tokens/sec: 40803.65\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 2650, Eval Loss: 2.41443\n",
      "Step: 2651, Training Loss: 1.95166, LR: 0.0004789, Tokens/sec: 37626.22\n",
      "Step: 2652, Training Loss: 2.17309, LR: 0.0004785, Tokens/sec: 40929.57\n",
      "Step: 2653, Training Loss: 2.52652, LR: 0.0004782, Tokens/sec: 41140.11\n",
      "Step: 2654, Training Loss: 2.90764, LR: 0.0004778, Tokens/sec: 41426.45\n",
      "Step: 2655, Training Loss: 2.59777, LR: 0.0004775, Tokens/sec: 40850.23\n",
      "Step: 2656, Training Loss: 2.06600, LR: 0.0004771, Tokens/sec: 41085.79\n",
      "Step: 2657, Training Loss: 2.19553, LR: 0.0004768, Tokens/sec: 41384.16\n",
      "Step: 2658, Training Loss: 2.29632, LR: 0.0004764, Tokens/sec: 41387.27\n",
      "Step: 2659, Training Loss: 2.32151, LR: 0.0004761, Tokens/sec: 41382.02\n",
      "Step: 2660, Training Loss: 2.13611, LR: 0.0004757, Tokens/sec: 41308.52\n",
      "Step: 2661, Training Loss: 1.74146, LR: 0.0004754, Tokens/sec: 41273.12\n",
      "Step: 2662, Training Loss: 1.93420, LR: 0.0004750, Tokens/sec: 41390.66\n",
      "Step: 2663, Training Loss: 2.20419, LR: 0.0004747, Tokens/sec: 41356.55\n",
      "Step: 2664, Training Loss: 1.92490, LR: 0.0004743, Tokens/sec: 40838.67\n",
      "Step: 2665, Training Loss: 2.30067, LR: 0.0004740, Tokens/sec: 36962.34\n",
      "Step: 2666, Training Loss: 2.66354, LR: 0.0004736, Tokens/sec: 41305.44\n",
      "Step: 2667, Training Loss: 2.11855, LR: 0.0004733, Tokens/sec: 41340.43\n",
      "Step: 2668, Training Loss: 2.41767, LR: 0.0004729, Tokens/sec: 41401.67\n",
      "Step: 2669, Training Loss: 1.76657, LR: 0.0004726, Tokens/sec: 41289.36\n",
      "Step: 2670, Training Loss: 2.43525, LR: 0.0004722, Tokens/sec: 40998.26\n",
      "Step: 2671, Training Loss: 2.38881, LR: 0.0004719, Tokens/sec: 41386.09\n",
      "Step: 2672, Training Loss: 1.89603, LR: 0.0004715, Tokens/sec: 41279.13\n",
      "Step: 2673, Training Loss: 1.84552, LR: 0.0004712, Tokens/sec: 41217.95\n",
      "Step: 2674, Training Loss: 1.89940, LR: 0.0004709, Tokens/sec: 41319.12\n",
      "Step: 2675, Training Loss: 2.71022, LR: 0.0004705, Tokens/sec: 41377.08\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 2675, Eval Loss: 2.32452\n",
      "Step: 2676, Training Loss: 2.53825, LR: 0.0004702, Tokens/sec: 40968.13\n",
      "Step: 2677, Training Loss: 2.37075, LR: 0.0004698, Tokens/sec: 40698.49\n",
      "Step: 2678, Training Loss: 2.28678, LR: 0.0004695, Tokens/sec: 41151.16\n",
      "Step: 2679, Training Loss: 2.36564, LR: 0.0004691, Tokens/sec: 40231.73\n",
      "Step: 2680, Training Loss: 2.43414, LR: 0.0004688, Tokens/sec: 41483.25\n",
      "Step: 2681, Training Loss: 1.65948, LR: 0.0004684, Tokens/sec: 39829.71\n",
      "Step: 2682, Training Loss: 1.86880, LR: 0.0004681, Tokens/sec: 41315.41\n",
      "Step: 2683, Training Loss: 2.64398, LR: 0.0004677, Tokens/sec: 40325.30\n",
      "Step: 2684, Training Loss: 2.46304, LR: 0.0004674, Tokens/sec: 41197.42\n",
      "Step: 2685, Training Loss: 2.32237, LR: 0.0004670, Tokens/sec: 41371.98\n",
      "Step: 2686, Training Loss: 2.05693, LR: 0.0004667, Tokens/sec: 40843.03\n",
      "Step: 2687, Training Loss: 1.78267, LR: 0.0004663, Tokens/sec: 41399.12\n",
      "Step: 2688, Training Loss: 2.40098, LR: 0.0004660, Tokens/sec: 40965.24\n",
      "Step: 2689, Training Loss: 2.37171, LR: 0.0004657, Tokens/sec: 40355.75\n",
      "Step: 2690, Training Loss: 2.41174, LR: 0.0004653, Tokens/sec: 41345.52\n",
      "Step: 2691, Training Loss: 2.03694, LR: 0.0004650, Tokens/sec: 41316.88\n",
      "Step: 2692, Training Loss: 2.02658, LR: 0.0004646, Tokens/sec: 41339.18\n",
      "Step: 2693, Training Loss: 2.22897, LR: 0.0004643, Tokens/sec: 41405.84\n",
      "Step: 2694, Training Loss: 1.96876, LR: 0.0004639, Tokens/sec: 37431.39\n",
      "Step: 2695, Training Loss: 2.76957, LR: 0.0004636, Tokens/sec: 38208.23\n",
      "Step: 2696, Training Loss: 2.14589, LR: 0.0004632, Tokens/sec: 41268.55\n",
      "Step: 2697, Training Loss: 2.67806, LR: 0.0004629, Tokens/sec: 39995.57\n",
      "Step: 2698, Training Loss: 2.25200, LR: 0.0004625, Tokens/sec: 41316.13\n",
      "Step: 2699, Training Loss: 2.66641, LR: 0.0004622, Tokens/sec: 40984.42\n",
      "Step: 2700, Training Loss: 2.72506, LR: 0.0004618, Tokens/sec: 40240.87\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 2700, Eval Loss: 2.12353\n",
      "Step: 2701, Training Loss: 1.91599, LR: 0.0004615, Tokens/sec: 40733.51\n",
      "Step: 2702, Training Loss: 2.28777, LR: 0.0004611, Tokens/sec: 41362.79\n",
      "Step: 2703, Training Loss: 2.99560, LR: 0.0004608, Tokens/sec: 41146.45\n",
      "Step: 2704, Training Loss: 2.23569, LR: 0.0004605, Tokens/sec: 40966.71\n",
      "Step: 2705, Training Loss: 2.50025, LR: 0.0004601, Tokens/sec: 41263.30\n",
      "Step: 2706, Training Loss: 2.14519, LR: 0.0004598, Tokens/sec: 40085.77\n",
      "Step: 2707, Training Loss: 2.14188, LR: 0.0004594, Tokens/sec: 40998.36\n",
      "Step: 2708, Training Loss: 1.90425, LR: 0.0004591, Tokens/sec: 40165.19\n",
      "Step: 2709, Training Loss: 1.84486, LR: 0.0004587, Tokens/sec: 41011.84\n",
      "Step: 2710, Training Loss: 2.23157, LR: 0.0004584, Tokens/sec: 40811.52\n",
      "Step: 2711, Training Loss: 1.92091, LR: 0.0004580, Tokens/sec: 40231.41\n",
      "Step: 2712, Training Loss: 2.51701, LR: 0.0004577, Tokens/sec: 40893.19\n",
      "Step: 2713, Training Loss: 2.05575, LR: 0.0004573, Tokens/sec: 41267.68\n",
      "Step: 2714, Training Loss: 1.82876, LR: 0.0004570, Tokens/sec: 40919.52\n",
      "Step: 2715, Training Loss: 2.42764, LR: 0.0004567, Tokens/sec: 41215.45\n",
      "Step: 2716, Training Loss: 2.38943, LR: 0.0004563, Tokens/sec: 39832.93\n",
      "Step: 2717, Training Loss: 2.18494, LR: 0.0004560, Tokens/sec: 41339.67\n",
      "Step: 2718, Training Loss: 2.14188, LR: 0.0004556, Tokens/sec: 41235.76\n",
      "Step: 2719, Training Loss: 1.87913, LR: 0.0004553, Tokens/sec: 41387.41\n",
      "Step: 2720, Training Loss: 1.77601, LR: 0.0004549, Tokens/sec: 40685.40\n",
      "Step: 2721, Training Loss: 1.87746, LR: 0.0004546, Tokens/sec: 40131.56\n",
      "Step: 2722, Training Loss: 2.28739, LR: 0.0004542, Tokens/sec: 39027.34\n",
      "Step: 2723, Training Loss: 2.40948, LR: 0.0004539, Tokens/sec: 41284.91\n",
      "Step: 2724, Training Loss: 2.11599, LR: 0.0004536, Tokens/sec: 37458.10\n",
      "Step: 2725, Training Loss: 2.51773, LR: 0.0004532, Tokens/sec: 41317.59\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 2725, Eval Loss: 2.16865\n",
      "Step: 2726, Training Loss: 2.26259, LR: 0.0004529, Tokens/sec: 41266.93\n",
      "Step: 2727, Training Loss: 1.79114, LR: 0.0004525, Tokens/sec: 41360.03\n",
      "Step: 2728, Training Loss: 2.19884, LR: 0.0004522, Tokens/sec: 41322.86\n",
      "Step: 2729, Training Loss: 2.22852, LR: 0.0004518, Tokens/sec: 40130.74\n",
      "Step: 2730, Training Loss: 2.71779, LR: 0.0004515, Tokens/sec: 40443.49\n",
      "Step: 2731, Training Loss: 2.41898, LR: 0.0004511, Tokens/sec: 41334.14\n",
      "Step: 2732, Training Loss: 1.60692, LR: 0.0004508, Tokens/sec: 40176.34\n",
      "Step: 2733, Training Loss: 2.11943, LR: 0.0004505, Tokens/sec: 40899.97\n",
      "Step: 2734, Training Loss: 2.10922, LR: 0.0004501, Tokens/sec: 41358.87\n",
      "Step: 2735, Training Loss: 2.15057, LR: 0.0004498, Tokens/sec: 40715.54\n",
      "Step: 2736, Training Loss: 2.43168, LR: 0.0004494, Tokens/sec: 41311.43\n",
      "Step: 2737, Training Loss: 2.14395, LR: 0.0004491, Tokens/sec: 39498.62\n",
      "Step: 2738, Training Loss: 2.24461, LR: 0.0004487, Tokens/sec: 41327.59\n",
      "Step: 2739, Training Loss: 2.26457, LR: 0.0004484, Tokens/sec: 40873.47\n",
      "Step: 2740, Training Loss: 2.42317, LR: 0.0004480, Tokens/sec: 39524.91\n",
      "Step: 2741, Training Loss: 2.76528, LR: 0.0004477, Tokens/sec: 40851.02\n",
      "Step: 2742, Training Loss: 1.79028, LR: 0.0004474, Tokens/sec: 41233.27\n",
      "Step: 2743, Training Loss: 2.03672, LR: 0.0004470, Tokens/sec: 41342.83\n",
      "Step: 2744, Training Loss: 1.97195, LR: 0.0004467, Tokens/sec: 41278.09\n",
      "Step: 2745, Training Loss: 2.12109, LR: 0.0004463, Tokens/sec: 39560.90\n",
      "Step: 2746, Training Loss: 2.01915, LR: 0.0004460, Tokens/sec: 24678.74\n",
      "Step: 2747, Training Loss: 2.58898, LR: 0.0004456, Tokens/sec: 39364.45\n",
      "Step: 2748, Training Loss: 2.09062, LR: 0.0004453, Tokens/sec: 41332.21\n",
      "Step: 2749, Training Loss: 2.67244, LR: 0.0004450, Tokens/sec: 26037.16\n",
      "Step: 2750, Training Loss: 1.98208, LR: 0.0004446, Tokens/sec: 41359.69\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 2750, Eval Loss: 2.11416\n",
      "Step: 2751, Training Loss: 1.80615, LR: 0.0004443, Tokens/sec: 39191.18\n",
      "Step: 2752, Training Loss: 2.34070, LR: 0.0004439, Tokens/sec: 41244.08\n",
      "Step: 2753, Training Loss: 1.98427, LR: 0.0004436, Tokens/sec: 39852.62\n",
      "Step: 2754, Training Loss: 3.11221, LR: 0.0004432, Tokens/sec: 41338.22\n",
      "Step: 2755, Training Loss: 1.91241, LR: 0.0004429, Tokens/sec: 41381.82\n",
      "Step: 2756, Training Loss: 2.07968, LR: 0.0004426, Tokens/sec: 40671.24\n",
      "Step: 2757, Training Loss: 1.86215, LR: 0.0004422, Tokens/sec: 41036.02\n",
      "Step: 2758, Training Loss: 2.20207, LR: 0.0004419, Tokens/sec: 41331.37\n",
      "Step: 2759, Training Loss: 2.12491, LR: 0.0004415, Tokens/sec: 39301.99\n",
      "Step: 2760, Training Loss: 1.69039, LR: 0.0004412, Tokens/sec: 41364.57\n",
      "Step: 2761, Training Loss: 2.41104, LR: 0.0004408, Tokens/sec: 40058.06\n",
      "Step: 2762, Training Loss: 2.36306, LR: 0.0004405, Tokens/sec: 40121.33\n",
      "Step: 2763, Training Loss: 2.41107, LR: 0.0004402, Tokens/sec: 41280.96\n",
      "Step: 2764, Training Loss: 1.89782, LR: 0.0004398, Tokens/sec: 41290.97\n",
      "Step: 2765, Training Loss: 2.75039, LR: 0.0004395, Tokens/sec: 41403.44\n",
      "Step: 2766, Training Loss: 1.13653, LR: 0.0004391, Tokens/sec: 40355.86\n",
      "Step: 2767, Training Loss: 1.71170, LR: 0.0004388, Tokens/sec: 40860.59\n",
      "Step: 2768, Training Loss: 2.62882, LR: 0.0004385, Tokens/sec: 41403.15\n",
      "Step: 2769, Training Loss: 1.94568, LR: 0.0004381, Tokens/sec: 39995.37\n",
      "Step: 2770, Training Loss: 2.41834, LR: 0.0004378, Tokens/sec: 41336.60\n",
      "Step: 2771, Training Loss: 1.83692, LR: 0.0004374, Tokens/sec: 41362.97\n",
      "Step: 2772, Training Loss: 2.32316, LR: 0.0004371, Tokens/sec: 41300.32\n",
      "Step: 2773, Training Loss: 1.81699, LR: 0.0004367, Tokens/sec: 40989.28\n",
      "Step: 2774, Training Loss: 1.93677, LR: 0.0004364, Tokens/sec: 41338.82\n",
      "Step: 2775, Training Loss: 2.56840, LR: 0.0004361, Tokens/sec: 41345.95\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 2775, Eval Loss: 2.13890\n",
      "Step: 2776, Training Loss: 1.90616, LR: 0.0004357, Tokens/sec: 41308.14\n",
      "Step: 2777, Training Loss: 2.26310, LR: 0.0004354, Tokens/sec: 40590.87\n",
      "Step: 2778, Training Loss: 1.84100, LR: 0.0004350, Tokens/sec: 41352.81\n",
      "Step: 2779, Training Loss: 1.87321, LR: 0.0004347, Tokens/sec: 41287.82\n",
      "Step: 2780, Training Loss: 2.16846, LR: 0.0004344, Tokens/sec: 41283.12\n",
      "Step: 2781, Training Loss: 2.05532, LR: 0.0004340, Tokens/sec: 41251.38\n",
      "Step: 2782, Training Loss: 1.27764, LR: 0.0004337, Tokens/sec: 40618.94\n",
      "Step: 2783, Training Loss: 1.95898, LR: 0.0004333, Tokens/sec: 40004.61\n",
      "Step: 2784, Training Loss: 1.91707, LR: 0.0004330, Tokens/sec: 41371.61\n",
      "Step: 2785, Training Loss: 1.70362, LR: 0.0004327, Tokens/sec: 40272.43\n",
      "Step: 2786, Training Loss: 2.41551, LR: 0.0004323, Tokens/sec: 41291.44\n",
      "Step: 2787, Training Loss: 1.82201, LR: 0.0004320, Tokens/sec: 40986.88\n",
      "Step: 2788, Training Loss: 2.81469, LR: 0.0004316, Tokens/sec: 41344.90\n",
      "Step: 2789, Training Loss: 2.33634, LR: 0.0004313, Tokens/sec: 41357.10\n",
      "Step: 2790, Training Loss: 2.27808, LR: 0.0004309, Tokens/sec: 41298.20\n",
      "Step: 2791, Training Loss: 1.79091, LR: 0.0004306, Tokens/sec: 40231.75\n",
      "Step: 2792, Training Loss: 2.48282, LR: 0.0004303, Tokens/sec: 25094.27\n",
      "Step: 2793, Training Loss: 1.92479, LR: 0.0004299, Tokens/sec: 41039.48\n",
      "Step: 2794, Training Loss: 2.13842, LR: 0.0004296, Tokens/sec: 41380.05\n",
      "Step: 2795, Training Loss: 1.90884, LR: 0.0004292, Tokens/sec: 40487.25\n",
      "Step: 2796, Training Loss: 2.14845, LR: 0.0004289, Tokens/sec: 40810.06\n",
      "Step: 2797, Training Loss: 1.50849, LR: 0.0004286, Tokens/sec: 41255.59\n",
      "Step: 2798, Training Loss: 2.15194, LR: 0.0004282, Tokens/sec: 40591.74\n",
      "Step: 2799, Training Loss: 2.30488, LR: 0.0004279, Tokens/sec: 41438.79\n",
      "Step: 2800, Training Loss: 2.50072, LR: 0.0004276, Tokens/sec: 39898.06\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 2800, Eval Loss: 2.07278\n",
      "Step: 2801, Training Loss: 2.09926, LR: 0.0004272, Tokens/sec: 41288.78\n",
      "Step: 2802, Training Loss: 1.62791, LR: 0.0004269, Tokens/sec: 41321.67\n",
      "Step: 2803, Training Loss: 2.38415, LR: 0.0004265, Tokens/sec: 36631.62\n",
      "Step: 2804, Training Loss: 1.93340, LR: 0.0004262, Tokens/sec: 41281.38\n",
      "Step: 2805, Training Loss: 1.83534, LR: 0.0004259, Tokens/sec: 36483.54\n",
      "Step: 2806, Training Loss: 1.91749, LR: 0.0004255, Tokens/sec: 36549.41\n",
      "Step: 2807, Training Loss: 2.11599, LR: 0.0004252, Tokens/sec: 40695.86\n",
      "Step: 2808, Training Loss: 2.44778, LR: 0.0004248, Tokens/sec: 29803.98\n",
      "Step: 2809, Training Loss: 2.24066, LR: 0.0004245, Tokens/sec: 30408.30\n",
      "Step: 2810, Training Loss: 2.16171, LR: 0.0004242, Tokens/sec: 36362.07\n",
      "Step: 2811, Training Loss: 1.46521, LR: 0.0004238, Tokens/sec: 31913.62\n",
      "Step: 2812, Training Loss: 1.93171, LR: 0.0004235, Tokens/sec: 41340.72\n",
      "Step: 2813, Training Loss: 2.52412, LR: 0.0004231, Tokens/sec: 40616.55\n",
      "Step: 2814, Training Loss: 1.92924, LR: 0.0004228, Tokens/sec: 41460.94\n",
      "Step: 2815, Training Loss: 1.47416, LR: 0.0004225, Tokens/sec: 40532.31\n",
      "Step: 2816, Training Loss: 2.32367, LR: 0.0004221, Tokens/sec: 28896.92\n",
      "Step: 2817, Training Loss: 2.20943, LR: 0.0004218, Tokens/sec: 38582.72\n",
      "Step: 2818, Training Loss: 2.17400, LR: 0.0004215, Tokens/sec: 41267.56\n",
      "Step: 2819, Training Loss: 1.80925, LR: 0.0004211, Tokens/sec: 41480.74\n",
      "Step: 2820, Training Loss: 1.98935, LR: 0.0004208, Tokens/sec: 40712.72\n",
      "Step: 2821, Training Loss: 1.95530, LR: 0.0004204, Tokens/sec: 41559.28\n",
      "Step: 2822, Training Loss: 2.26208, LR: 0.0004201, Tokens/sec: 41437.00\n",
      "Step: 2823, Training Loss: 2.65448, LR: 0.0004198, Tokens/sec: 41537.09\n",
      "Step: 2824, Training Loss: 1.86281, LR: 0.0004194, Tokens/sec: 41534.45\n",
      "Step: 2825, Training Loss: 2.41974, LR: 0.0004191, Tokens/sec: 41303.08\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 2825, Eval Loss: 2.06605\n",
      "Step: 2826, Training Loss: 2.27039, LR: 0.0004187, Tokens/sec: 40614.84\n",
      "Step: 2827, Training Loss: 1.84946, LR: 0.0004184, Tokens/sec: 30497.77\n",
      "Step: 2828, Training Loss: 1.65708, LR: 0.0004181, Tokens/sec: 41358.71\n",
      "Step: 2829, Training Loss: 1.63324, LR: 0.0004177, Tokens/sec: 27210.30\n",
      "Step: 2830, Training Loss: 2.05440, LR: 0.0004174, Tokens/sec: 41372.52\n",
      "Step: 2831, Training Loss: 1.76332, LR: 0.0004171, Tokens/sec: 41454.96\n",
      "Step: 2832, Training Loss: 1.46232, LR: 0.0004167, Tokens/sec: 26933.71\n",
      "Step: 2833, Training Loss: 2.49128, LR: 0.0004164, Tokens/sec: 40656.24\n",
      "Step: 2834, Training Loss: 1.53907, LR: 0.0004161, Tokens/sec: 40637.27\n",
      "Step: 2835, Training Loss: 1.67556, LR: 0.0004157, Tokens/sec: 41411.28\n",
      "Step: 2836, Training Loss: 1.36320, LR: 0.0004154, Tokens/sec: 41450.23\n",
      "Step: 2837, Training Loss: 2.18723, LR: 0.0004150, Tokens/sec: 31070.64\n",
      "Step: 2838, Training Loss: 1.63186, LR: 0.0004147, Tokens/sec: 30714.07\n",
      "Step: 2839, Training Loss: 2.15919, LR: 0.0004144, Tokens/sec: 39849.50\n",
      "Step: 2840, Training Loss: 1.77260, LR: 0.0004140, Tokens/sec: 41503.08\n",
      "Step: 2841, Training Loss: 1.82739, LR: 0.0004137, Tokens/sec: 41571.14\n",
      "Step: 2842, Training Loss: 1.84204, LR: 0.0004134, Tokens/sec: 41518.19\n",
      "Step: 2843, Training Loss: 1.99432, LR: 0.0004130, Tokens/sec: 40444.58\n",
      "Step: 2844, Training Loss: 2.16107, LR: 0.0004127, Tokens/sec: 41390.94\n",
      "Step: 2845, Training Loss: 1.88472, LR: 0.0004124, Tokens/sec: 41401.34\n",
      "Step: 2846, Training Loss: 2.02464, LR: 0.0004120, Tokens/sec: 40552.87\n",
      "Step: 2847, Training Loss: 1.87345, LR: 0.0004117, Tokens/sec: 40434.76\n",
      "Step: 2848, Training Loss: 1.83204, LR: 0.0004113, Tokens/sec: 39501.60\n",
      "Step: 2849, Training Loss: 1.68878, LR: 0.0004110, Tokens/sec: 41461.34\n",
      "Step: 2850, Training Loss: 1.62475, LR: 0.0004107, Tokens/sec: 41533.95\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 2850, Eval Loss: 2.07983\n",
      "Step: 2851, Training Loss: 2.26518, LR: 0.0004103, Tokens/sec: 41137.68\n",
      "Step: 2852, Training Loss: 1.77585, LR: 0.0004100, Tokens/sec: 41454.17\n",
      "Step: 2853, Training Loss: 1.91210, LR: 0.0004097, Tokens/sec: 40751.89\n",
      "Step: 2854, Training Loss: 2.03897, LR: 0.0004093, Tokens/sec: 40666.18\n",
      "Step: 2855, Training Loss: 2.02770, LR: 0.0004090, Tokens/sec: 39459.62\n",
      "Step: 2856, Training Loss: 2.01739, LR: 0.0004087, Tokens/sec: 28609.03\n",
      "Step: 2857, Training Loss: 2.02789, LR: 0.0004083, Tokens/sec: 33417.41\n",
      "Step: 2858, Training Loss: 2.16915, LR: 0.0004080, Tokens/sec: 41402.65\n",
      "Step: 2859, Training Loss: 1.74440, LR: 0.0004077, Tokens/sec: 39646.51\n",
      "Step: 2860, Training Loss: 2.03788, LR: 0.0004073, Tokens/sec: 37339.51\n",
      "Step: 2861, Training Loss: 1.91678, LR: 0.0004070, Tokens/sec: 40912.03\n",
      "Step: 2862, Training Loss: 1.85853, LR: 0.0004067, Tokens/sec: 41493.77\n",
      "Step: 2863, Training Loss: 1.74736, LR: 0.0004063, Tokens/sec: 40765.73\n",
      "Step: 2864, Training Loss: 2.11304, LR: 0.0004060, Tokens/sec: 26568.10\n",
      "Step: 2865, Training Loss: 2.24167, LR: 0.0004057, Tokens/sec: 31987.45\n",
      "Step: 2866, Training Loss: 1.73604, LR: 0.0004053, Tokens/sec: 41298.37\n",
      "Step: 2867, Training Loss: 2.17520, LR: 0.0004050, Tokens/sec: 27379.21\n",
      "Step: 2868, Training Loss: 2.25054, LR: 0.0004046, Tokens/sec: 41433.72\n",
      "Step: 2869, Training Loss: 1.85698, LR: 0.0004043, Tokens/sec: 40832.86\n",
      "Step: 2870, Training Loss: 2.17299, LR: 0.0004040, Tokens/sec: 41503.02\n",
      "Step: 2871, Training Loss: 2.13797, LR: 0.0004036, Tokens/sec: 40628.34\n",
      "Step: 2872, Training Loss: 1.76019, LR: 0.0004033, Tokens/sec: 26521.81\n",
      "Step: 2873, Training Loss: 2.21403, LR: 0.0004030, Tokens/sec: 41128.19\n",
      "Step: 2874, Training Loss: 2.44371, LR: 0.0004026, Tokens/sec: 35941.14\n",
      "Step: 2875, Training Loss: 1.99755, LR: 0.0004023, Tokens/sec: 40580.67\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 2875, Eval Loss: 2.03399\n",
      "Step: 2876, Training Loss: 1.89844, LR: 0.0004020, Tokens/sec: 27715.36\n",
      "Step: 2877, Training Loss: 2.13856, LR: 0.0004016, Tokens/sec: 33499.33\n",
      "Step: 2878, Training Loss: 1.66498, LR: 0.0004013, Tokens/sec: 35348.48\n",
      "Step: 2879, Training Loss: 1.71949, LR: 0.0004010, Tokens/sec: 31797.89\n",
      "Step: 2880, Training Loss: 1.33438, LR: 0.0004006, Tokens/sec: 39077.45\n",
      "Step: 2881, Training Loss: 1.60153, LR: 0.0004003, Tokens/sec: 37206.27\n",
      "Step: 2882, Training Loss: 2.05208, LR: 0.0004000, Tokens/sec: 41394.59\n",
      "Step: 2883, Training Loss: 2.02414, LR: 0.0003997, Tokens/sec: 37801.14\n",
      "Step: 2884, Training Loss: 1.95048, LR: 0.0003993, Tokens/sec: 40574.49\n",
      "Step: 2885, Training Loss: 1.71092, LR: 0.0003990, Tokens/sec: 28523.54\n",
      "Step: 2886, Training Loss: 1.87506, LR: 0.0003987, Tokens/sec: 41330.09\n",
      "Step: 2887, Training Loss: 2.03154, LR: 0.0003983, Tokens/sec: 41451.31\n",
      "Step: 2888, Training Loss: 1.99524, LR: 0.0003980, Tokens/sec: 41515.06\n",
      "Step: 2889, Training Loss: 2.17934, LR: 0.0003977, Tokens/sec: 41024.16\n",
      "Step: 2890, Training Loss: 2.79229, LR: 0.0003973, Tokens/sec: 29020.35\n",
      "Step: 2891, Training Loss: 2.21020, LR: 0.0003970, Tokens/sec: 33994.61\n",
      "Step: 2892, Training Loss: 2.36625, LR: 0.0003967, Tokens/sec: 41452.66\n",
      "Step: 2893, Training Loss: 2.07469, LR: 0.0003963, Tokens/sec: 40904.89\n",
      "Step: 2894, Training Loss: 2.01133, LR: 0.0003960, Tokens/sec: 40588.51\n",
      "Step: 2895, Training Loss: 1.45486, LR: 0.0003957, Tokens/sec: 40222.62\n",
      "Step: 2896, Training Loss: 1.67211, LR: 0.0003953, Tokens/sec: 27301.43\n",
      "Step: 2897, Training Loss: 1.50797, LR: 0.0003950, Tokens/sec: 37447.55\n",
      "Step: 2898, Training Loss: 1.65678, LR: 0.0003947, Tokens/sec: 32243.17\n",
      "Step: 2899, Training Loss: 1.70251, LR: 0.0003943, Tokens/sec: 41477.87\n",
      "Step: 2900, Training Loss: 2.04539, LR: 0.0003940, Tokens/sec: 26884.16\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 2900, Eval Loss: 1.97717\n",
      "Step: 2901, Training Loss: 1.65411, LR: 0.0003937, Tokens/sec: 41377.05\n",
      "Step: 2902, Training Loss: 1.46020, LR: 0.0003933, Tokens/sec: 27121.38\n",
      "Step: 2903, Training Loss: 1.92319, LR: 0.0003930, Tokens/sec: 38210.02\n",
      "Step: 2904, Training Loss: 1.83629, LR: 0.0003927, Tokens/sec: 40632.94\n",
      "Step: 2905, Training Loss: 1.87313, LR: 0.0003924, Tokens/sec: 40597.15\n",
      "Step: 2906, Training Loss: 2.01233, LR: 0.0003920, Tokens/sec: 41522.13\n",
      "Step: 2907, Training Loss: 2.10157, LR: 0.0003917, Tokens/sec: 41321.26\n",
      "Step: 2908, Training Loss: 1.93048, LR: 0.0003914, Tokens/sec: 31833.81\n",
      "Step: 2909, Training Loss: 1.64238, LR: 0.0003910, Tokens/sec: 39172.15\n",
      "Step: 2910, Training Loss: 1.79913, LR: 0.0003907, Tokens/sec: 40539.78\n",
      "Step: 2911, Training Loss: 1.67092, LR: 0.0003904, Tokens/sec: 31021.15\n",
      "Step: 2912, Training Loss: 1.37083, LR: 0.0003900, Tokens/sec: 35650.33\n",
      "Step: 2913, Training Loss: 1.98968, LR: 0.0003897, Tokens/sec: 41390.65\n",
      "Step: 2914, Training Loss: 2.04632, LR: 0.0003894, Tokens/sec: 27901.57\n",
      "Step: 2915, Training Loss: 1.73168, LR: 0.0003891, Tokens/sec: 41396.01\n",
      "Step: 2916, Training Loss: 1.93283, LR: 0.0003887, Tokens/sec: 30264.29\n",
      "Step: 2917, Training Loss: 1.51450, LR: 0.0003884, Tokens/sec: 40497.45\n",
      "Step: 2918, Training Loss: 1.34074, LR: 0.0003881, Tokens/sec: 29365.97\n",
      "Step: 2919, Training Loss: 2.12519, LR: 0.0003877, Tokens/sec: 41414.52\n",
      "Step: 2920, Training Loss: 1.93789, LR: 0.0003874, Tokens/sec: 41432.49\n",
      "Step: 2921, Training Loss: 2.34102, LR: 0.0003871, Tokens/sec: 41373.69\n",
      "Step: 2922, Training Loss: 2.57295, LR: 0.0003868, Tokens/sec: 35188.70\n",
      "Step: 2923, Training Loss: 2.01267, LR: 0.0003864, Tokens/sec: 41378.90\n",
      "Step: 2924, Training Loss: 2.07486, LR: 0.0003861, Tokens/sec: 40627.72\n",
      "Step: 2925, Training Loss: 1.78237, LR: 0.0003858, Tokens/sec: 29116.91\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 2925, Eval Loss: 1.96031\n",
      "Step: 2926, Training Loss: 2.46618, LR: 0.0003854, Tokens/sec: 41295.30\n",
      "Step: 2927, Training Loss: 1.53389, LR: 0.0003851, Tokens/sec: 39868.53\n",
      "Step: 2928, Training Loss: 1.53818, LR: 0.0003848, Tokens/sec: 37543.60\n",
      "Step: 2929, Training Loss: 1.92426, LR: 0.0003845, Tokens/sec: 40444.97\n",
      "Step: 2930, Training Loss: 2.04465, LR: 0.0003841, Tokens/sec: 41315.80\n",
      "Step: 2931, Training Loss: 1.64020, LR: 0.0003838, Tokens/sec: 35631.07\n",
      "Step: 2932, Training Loss: 2.98293, LR: 0.0003835, Tokens/sec: 30631.72\n",
      "Step: 2933, Training Loss: 2.10586, LR: 0.0003831, Tokens/sec: 37203.39\n",
      "Step: 2934, Training Loss: 2.40237, LR: 0.0003828, Tokens/sec: 39455.83\n",
      "Step: 2935, Training Loss: 1.86931, LR: 0.0003825, Tokens/sec: 39346.00\n",
      "Step: 2936, Training Loss: 1.64398, LR: 0.0003822, Tokens/sec: 39555.40\n",
      "Step: 2937, Training Loss: 1.89388, LR: 0.0003818, Tokens/sec: 41313.45\n",
      "Step: 2938, Training Loss: 1.94655, LR: 0.0003815, Tokens/sec: 41454.35\n",
      "Step: 2939, Training Loss: 2.35560, LR: 0.0003812, Tokens/sec: 38563.34\n",
      "Step: 2940, Training Loss: 1.38049, LR: 0.0003809, Tokens/sec: 38573.51\n",
      "Step: 2941, Training Loss: 2.13052, LR: 0.0003805, Tokens/sec: 38539.66\n",
      "Step: 2942, Training Loss: 1.76643, LR: 0.0003802, Tokens/sec: 41100.16\n",
      "Step: 2943, Training Loss: 1.97497, LR: 0.0003799, Tokens/sec: 40435.92\n",
      "Step: 2944, Training Loss: 1.56662, LR: 0.0003795, Tokens/sec: 41253.89\n",
      "Step: 2945, Training Loss: 1.73924, LR: 0.0003792, Tokens/sec: 41358.14\n",
      "Step: 2946, Training Loss: 1.91368, LR: 0.0003789, Tokens/sec: 41316.37\n",
      "Step: 2947, Training Loss: 1.77260, LR: 0.0003786, Tokens/sec: 40279.44\n",
      "Step: 2948, Training Loss: 2.08124, LR: 0.0003782, Tokens/sec: 40921.98\n",
      "Step: 2949, Training Loss: 1.65290, LR: 0.0003779, Tokens/sec: 40242.16\n",
      "Step: 2950, Training Loss: 2.03345, LR: 0.0003776, Tokens/sec: 40702.21\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 2950, Eval Loss: 1.93410\n",
      "Step: 2951, Training Loss: 1.85278, LR: 0.0003773, Tokens/sec: 40449.16\n",
      "Step: 2952, Training Loss: 1.80526, LR: 0.0003769, Tokens/sec: 39043.30\n",
      "Step: 2953, Training Loss: 1.83547, LR: 0.0003766, Tokens/sec: 41216.73\n",
      "Step: 2954, Training Loss: 1.64458, LR: 0.0003763, Tokens/sec: 41458.65\n",
      "Step: 2955, Training Loss: 1.75968, LR: 0.0003760, Tokens/sec: 39867.46\n",
      "Step: 2956, Training Loss: 1.79089, LR: 0.0003756, Tokens/sec: 41130.68\n",
      "Step: 2957, Training Loss: 1.62886, LR: 0.0003753, Tokens/sec: 40374.54\n",
      "Step: 2958, Training Loss: 1.55659, LR: 0.0003750, Tokens/sec: 40758.81\n",
      "Step: 2959, Training Loss: 1.49636, LR: 0.0003747, Tokens/sec: 41039.25\n",
      "Step: 2960, Training Loss: 2.14566, LR: 0.0003743, Tokens/sec: 39097.62\n",
      "Step: 2961, Training Loss: 1.81014, LR: 0.0003740, Tokens/sec: 36330.79\n",
      "Step: 2962, Training Loss: 1.62101, LR: 0.0003737, Tokens/sec: 40915.25\n",
      "Step: 2963, Training Loss: 1.84447, LR: 0.0003734, Tokens/sec: 41478.75\n",
      "Step: 2964, Training Loss: 1.66616, LR: 0.0003730, Tokens/sec: 41249.70\n",
      "Step: 2965, Training Loss: 1.80267, LR: 0.0003727, Tokens/sec: 37680.29\n",
      "Step: 2966, Training Loss: 1.90930, LR: 0.0003724, Tokens/sec: 37381.16\n",
      "Step: 2967, Training Loss: 2.12851, LR: 0.0003721, Tokens/sec: 36548.72\n",
      "Step: 2968, Training Loss: 2.14242, LR: 0.0003717, Tokens/sec: 40706.50\n",
      "Step: 2969, Training Loss: 1.97774, LR: 0.0003714, Tokens/sec: 40488.09\n",
      "Step: 2970, Training Loss: 1.87680, LR: 0.0003711, Tokens/sec: 40667.33\n",
      "Step: 2971, Training Loss: 2.04230, LR: 0.0003708, Tokens/sec: 40191.05\n",
      "Step: 2972, Training Loss: 2.14792, LR: 0.0003704, Tokens/sec: 39931.60\n",
      "Step: 2973, Training Loss: 2.44908, LR: 0.0003701, Tokens/sec: 39981.79\n",
      "Step: 2974, Training Loss: 2.69278, LR: 0.0003698, Tokens/sec: 40491.80\n",
      "Step: 2975, Training Loss: 1.55860, LR: 0.0003695, Tokens/sec: 40516.17\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 2975, Eval Loss: 1.81199\n",
      "Step: 2976, Training Loss: 1.47178, LR: 0.0003692, Tokens/sec: 40832.74\n",
      "Step: 2977, Training Loss: 2.21298, LR: 0.0003688, Tokens/sec: 41286.41\n",
      "Step: 2978, Training Loss: 2.18385, LR: 0.0003685, Tokens/sec: 39700.74\n",
      "Step: 2979, Training Loss: 1.79642, LR: 0.0003682, Tokens/sec: 40398.52\n",
      "Step: 2980, Training Loss: 1.48314, LR: 0.0003679, Tokens/sec: 39980.91\n",
      "Step: 2981, Training Loss: 2.26602, LR: 0.0003675, Tokens/sec: 41428.84\n",
      "Step: 2982, Training Loss: 1.81442, LR: 0.0003672, Tokens/sec: 39689.45\n",
      "Step: 2983, Training Loss: 2.15205, LR: 0.0003669, Tokens/sec: 39932.02\n",
      "Step: 2984, Training Loss: 1.43798, LR: 0.0003666, Tokens/sec: 41382.06\n",
      "Step: 2985, Training Loss: 2.03198, LR: 0.0003662, Tokens/sec: 40781.07\n",
      "Step: 2986, Training Loss: 2.07360, LR: 0.0003659, Tokens/sec: 40532.78\n",
      "Step: 2987, Training Loss: 1.57954, LR: 0.0003656, Tokens/sec: 39190.69\n",
      "Step: 2988, Training Loss: 1.69048, LR: 0.0003653, Tokens/sec: 40451.39\n",
      "Step: 2989, Training Loss: 2.05847, LR: 0.0003650, Tokens/sec: 41299.85\n",
      "Step: 2990, Training Loss: 1.68079, LR: 0.0003646, Tokens/sec: 39471.86\n",
      "Step: 2991, Training Loss: 2.09391, LR: 0.0003643, Tokens/sec: 41447.01\n",
      "Step: 2992, Training Loss: 1.37902, LR: 0.0003640, Tokens/sec: 41246.57\n",
      "Step: 2993, Training Loss: 1.28827, LR: 0.0003637, Tokens/sec: 30133.10\n",
      "Step: 2994, Training Loss: 2.03419, LR: 0.0003634, Tokens/sec: 29259.01\n",
      "Step: 2995, Training Loss: 2.03619, LR: 0.0003630, Tokens/sec: 30822.97\n",
      "Step: 2996, Training Loss: 1.98456, LR: 0.0003627, Tokens/sec: 40308.23\n",
      "Step: 2997, Training Loss: 1.94902, LR: 0.0003624, Tokens/sec: 38040.87\n",
      "Step: 2998, Training Loss: 1.61904, LR: 0.0003621, Tokens/sec: 36468.96\n",
      "Step: 2999, Training Loss: 1.74085, LR: 0.0003618, Tokens/sec: 37642.68\n",
      "Step: 3000, Training Loss: 1.66369, LR: 0.0003614, Tokens/sec: 38044.58\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 3000, Eval Loss: 1.89683\n",
      "Step: 3001, Training Loss: 1.93924, LR: 0.0003611, Tokens/sec: 35347.18\n",
      "Step: 3002, Training Loss: 2.04486, LR: 0.0003608, Tokens/sec: 36261.13\n",
      "Step: 3003, Training Loss: 1.42287, LR: 0.0003605, Tokens/sec: 36465.15\n",
      "Step: 3004, Training Loss: 1.99515, LR: 0.0003602, Tokens/sec: 36718.53\n",
      "Step: 3005, Training Loss: 1.37694, LR: 0.0003598, Tokens/sec: 38455.28\n",
      "Step: 3006, Training Loss: 1.60243, LR: 0.0003595, Tokens/sec: 38904.63\n",
      "Step: 3007, Training Loss: 1.62197, LR: 0.0003592, Tokens/sec: 37965.73\n",
      "Step: 3008, Training Loss: 1.86251, LR: 0.0003589, Tokens/sec: 38147.44\n",
      "Step: 3009, Training Loss: 1.86106, LR: 0.0003586, Tokens/sec: 36777.83\n",
      "Step: 3010, Training Loss: 1.21215, LR: 0.0003582, Tokens/sec: 36861.25\n",
      "Step: 3011, Training Loss: 1.51833, LR: 0.0003579, Tokens/sec: 38219.74\n",
      "Step: 3012, Training Loss: 1.71991, LR: 0.0003576, Tokens/sec: 38030.35\n",
      "Step: 3013, Training Loss: 2.79913, LR: 0.0003573, Tokens/sec: 37773.73\n",
      "Step: 3014, Training Loss: 1.91973, LR: 0.0003570, Tokens/sec: 35146.41\n",
      "Step: 3015, Training Loss: 1.66260, LR: 0.0003566, Tokens/sec: 34973.61\n",
      "Step: 3016, Training Loss: 2.18339, LR: 0.0003563, Tokens/sec: 36937.40\n",
      "Step: 3017, Training Loss: 1.55314, LR: 0.0003560, Tokens/sec: 35165.52\n",
      "Step: 3018, Training Loss: 1.80544, LR: 0.0003557, Tokens/sec: 37788.81\n",
      "Step: 3019, Training Loss: 1.95429, LR: 0.0003554, Tokens/sec: 37168.42\n",
      "Step: 3020, Training Loss: 1.86259, LR: 0.0003550, Tokens/sec: 36820.92\n",
      "Step: 3021, Training Loss: 1.88963, LR: 0.0003547, Tokens/sec: 36302.51\n",
      "Step: 3022, Training Loss: 1.68751, LR: 0.0003544, Tokens/sec: 41407.22\n",
      "Step: 3023, Training Loss: 1.30267, LR: 0.0003541, Tokens/sec: 41395.97\n",
      "Step: 3024, Training Loss: 1.73747, LR: 0.0003538, Tokens/sec: 40562.78\n",
      "Step: 3025, Training Loss: 1.76085, LR: 0.0003535, Tokens/sec: 41389.08\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 3025, Eval Loss: 1.78526\n",
      "Step: 3026, Training Loss: 2.14455, LR: 0.0003531, Tokens/sec: 39949.76\n",
      "Step: 3027, Training Loss: 1.92179, LR: 0.0003528, Tokens/sec: 40804.08\n",
      "Step: 3028, Training Loss: 2.06267, LR: 0.0003525, Tokens/sec: 41292.44\n",
      "Step: 3029, Training Loss: 1.89320, LR: 0.0003522, Tokens/sec: 39717.72\n",
      "Step: 3030, Training Loss: 1.19233, LR: 0.0003519, Tokens/sec: 41384.86\n",
      "Step: 3031, Training Loss: 1.41995, LR: 0.0003516, Tokens/sec: 27312.35\n",
      "Step: 3032, Training Loss: 1.82105, LR: 0.0003512, Tokens/sec: 39678.47\n",
      "Step: 3033, Training Loss: 2.01687, LR: 0.0003509, Tokens/sec: 40751.65\n",
      "Step: 3034, Training Loss: 1.89801, LR: 0.0003506, Tokens/sec: 40900.53\n",
      "Step: 3035, Training Loss: 2.20491, LR: 0.0003503, Tokens/sec: 41344.38\n",
      "Step: 3036, Training Loss: 1.96284, LR: 0.0003500, Tokens/sec: 37723.20\n",
      "Step: 3037, Training Loss: 2.00879, LR: 0.0003497, Tokens/sec: 37348.92\n",
      "Step: 3038, Training Loss: 1.70120, LR: 0.0003493, Tokens/sec: 40603.66\n",
      "Step: 3039, Training Loss: 1.63917, LR: 0.0003490, Tokens/sec: 40657.22\n",
      "Step: 3040, Training Loss: 1.73583, LR: 0.0003487, Tokens/sec: 40786.06\n",
      "Step: 3041, Training Loss: 2.15856, LR: 0.0003484, Tokens/sec: 41404.64\n",
      "Step: 3042, Training Loss: 2.22042, LR: 0.0003481, Tokens/sec: 41560.93\n",
      "Step: 3043, Training Loss: 1.59095, LR: 0.0003478, Tokens/sec: 41594.25\n",
      "Step: 3044, Training Loss: 1.94164, LR: 0.0003475, Tokens/sec: 41519.45\n",
      "Step: 3045, Training Loss: 1.52025, LR: 0.0003471, Tokens/sec: 41400.93\n",
      "Step: 3046, Training Loss: 1.34979, LR: 0.0003468, Tokens/sec: 41517.38\n",
      "Step: 3047, Training Loss: 1.58947, LR: 0.0003465, Tokens/sec: 40536.20\n",
      "Step: 3048, Training Loss: 1.36064, LR: 0.0003462, Tokens/sec: 40176.73\n",
      "Step: 3049, Training Loss: 1.37334, LR: 0.0003459, Tokens/sec: 40704.60\n",
      "Step: 3050, Training Loss: 2.27551, LR: 0.0003456, Tokens/sec: 41474.80\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 3050, Eval Loss: 1.66952\n",
      "Step: 3051, Training Loss: 2.48190, LR: 0.0003452, Tokens/sec: 41288.69\n",
      "Step: 3052, Training Loss: 2.17841, LR: 0.0003449, Tokens/sec: 41317.51\n",
      "Step: 3053, Training Loss: 1.68267, LR: 0.0003446, Tokens/sec: 41526.06\n",
      "Step: 3054, Training Loss: 1.18398, LR: 0.0003443, Tokens/sec: 41429.28\n",
      "Step: 3055, Training Loss: 1.23341, LR: 0.0003440, Tokens/sec: 40634.19\n",
      "Step: 3056, Training Loss: 1.91600, LR: 0.0003437, Tokens/sec: 41458.65\n",
      "Step: 3057, Training Loss: 1.62030, LR: 0.0003434, Tokens/sec: 40771.91\n",
      "Step: 3058, Training Loss: 1.35675, LR: 0.0003431, Tokens/sec: 41437.57\n",
      "Step: 3059, Training Loss: 1.44562, LR: 0.0003427, Tokens/sec: 40262.33\n",
      "Step: 3060, Training Loss: 2.18442, LR: 0.0003424, Tokens/sec: 41462.74\n",
      "Step: 3061, Training Loss: 1.80929, LR: 0.0003421, Tokens/sec: 41542.62\n",
      "Step: 3062, Training Loss: 1.37327, LR: 0.0003418, Tokens/sec: 41466.65\n",
      "Step: 3063, Training Loss: 1.72785, LR: 0.0003415, Tokens/sec: 40582.29\n",
      "Step: 3064, Training Loss: 1.95412, LR: 0.0003412, Tokens/sec: 41516.03\n",
      "Step: 3065, Training Loss: 1.74064, LR: 0.0003409, Tokens/sec: 41080.03\n",
      "Step: 3066, Training Loss: 0.94793, LR: 0.0003406, Tokens/sec: 41347.74\n",
      "Step: 3067, Training Loss: 1.79123, LR: 0.0003402, Tokens/sec: 41341.77\n",
      "Step: 3068, Training Loss: 1.90899, LR: 0.0003399, Tokens/sec: 41489.44\n",
      "Step: 3069, Training Loss: 1.35681, LR: 0.0003396, Tokens/sec: 41446.86\n",
      "Step: 3070, Training Loss: 1.26535, LR: 0.0003393, Tokens/sec: 41512.78\n",
      "Step: 3071, Training Loss: 1.81513, LR: 0.0003390, Tokens/sec: 40501.42\n",
      "Step: 3072, Training Loss: 1.33823, LR: 0.0003387, Tokens/sec: 41482.92\n",
      "Step: 3073, Training Loss: 2.20358, LR: 0.0003384, Tokens/sec: 41428.87\n",
      "Step: 3074, Training Loss: 2.14811, LR: 0.0003381, Tokens/sec: 40610.75\n",
      "Step: 3075, Training Loss: 1.91413, LR: 0.0003377, Tokens/sec: 40749.56\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 3075, Eval Loss: 1.79847\n",
      "Step: 3076, Training Loss: 0.82713, LR: 0.0003374, Tokens/sec: 41379.71\n",
      "Step: 3077, Training Loss: 1.71165, LR: 0.0003371, Tokens/sec: 41510.58\n",
      "Step: 3078, Training Loss: 1.39248, LR: 0.0003368, Tokens/sec: 41450.67\n",
      "Step: 3079, Training Loss: 2.03684, LR: 0.0003365, Tokens/sec: 40559.83\n",
      "Step: 3080, Training Loss: 1.49195, LR: 0.0003362, Tokens/sec: 41461.77\n",
      "Step: 3081, Training Loss: 1.17705, LR: 0.0003359, Tokens/sec: 41444.69\n",
      "Step: 3082, Training Loss: 1.82313, LR: 0.0003356, Tokens/sec: 40869.88\n",
      "Step: 3083, Training Loss: 1.65138, LR: 0.0003353, Tokens/sec: 41537.71\n",
      "Step: 3084, Training Loss: 1.61249, LR: 0.0003350, Tokens/sec: 41129.92\n",
      "Step: 3085, Training Loss: 0.95705, LR: 0.0003346, Tokens/sec: 41392.36\n",
      "Step: 3086, Training Loss: 2.12318, LR: 0.0003343, Tokens/sec: 41573.72\n",
      "Step: 3087, Training Loss: 1.84214, LR: 0.0003340, Tokens/sec: 40713.25\n",
      "Step: 3088, Training Loss: 1.95681, LR: 0.0003337, Tokens/sec: 41419.02\n",
      "Step: 3089, Training Loss: 1.30119, LR: 0.0003334, Tokens/sec: 41547.35\n",
      "Step: 3090, Training Loss: 1.97827, LR: 0.0003331, Tokens/sec: 40732.46\n",
      "Step: 3091, Training Loss: 1.36960, LR: 0.0003328, Tokens/sec: 41589.68\n",
      "Step: 3092, Training Loss: 1.90814, LR: 0.0003325, Tokens/sec: 41451.49\n",
      "Step: 3093, Training Loss: 1.06913, LR: 0.0003322, Tokens/sec: 41498.21\n",
      "Step: 3094, Training Loss: 1.35663, LR: 0.0003319, Tokens/sec: 41480.89\n",
      "Step: 3095, Training Loss: 1.90931, LR: 0.0003315, Tokens/sec: 40718.93\n",
      "Step: 3096, Training Loss: 1.47085, LR: 0.0003312, Tokens/sec: 40834.23\n",
      "Step: 3097, Training Loss: 2.59099, LR: 0.0003309, Tokens/sec: 41547.24\n",
      "Step: 3098, Training Loss: 1.85691, LR: 0.0003306, Tokens/sec: 39975.10\n",
      "Step: 3099, Training Loss: 1.35173, LR: 0.0003303, Tokens/sec: 41373.60\n",
      "Step: 3100, Training Loss: 1.70767, LR: 0.0003300, Tokens/sec: 41565.42\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 3100, Eval Loss: 1.82858\n",
      "Step: 3101, Training Loss: 2.12408, LR: 0.0003297, Tokens/sec: 40679.67\n",
      "Step: 3102, Training Loss: 1.56013, LR: 0.0003294, Tokens/sec: 41465.64\n",
      "Step: 3103, Training Loss: 1.86352, LR: 0.0003291, Tokens/sec: 40623.73\n",
      "Step: 3104, Training Loss: 1.79146, LR: 0.0003288, Tokens/sec: 41545.17\n",
      "Step: 3105, Training Loss: 1.62837, LR: 0.0003285, Tokens/sec: 41439.57\n",
      "Step: 3106, Training Loss: 1.53439, LR: 0.0003282, Tokens/sec: 40760.65\n",
      "Step: 3107, Training Loss: 1.84737, LR: 0.0003279, Tokens/sec: 41534.84\n",
      "Step: 3108, Training Loss: 2.26739, LR: 0.0003276, Tokens/sec: 39856.07\n",
      "Step: 3109, Training Loss: 2.27498, LR: 0.0003272, Tokens/sec: 41518.53\n",
      "Step: 3110, Training Loss: 1.26358, LR: 0.0003269, Tokens/sec: 40847.98\n",
      "Step: 3111, Training Loss: 1.46961, LR: 0.0003266, Tokens/sec: 40411.30\n",
      "Step: 3112, Training Loss: 0.92497, LR: 0.0003263, Tokens/sec: 41520.31\n",
      "Step: 3113, Training Loss: 1.69005, LR: 0.0003260, Tokens/sec: 41535.97\n",
      "Step: 3114, Training Loss: 1.07578, LR: 0.0003257, Tokens/sec: 40364.84\n",
      "Step: 3115, Training Loss: 2.34167, LR: 0.0003254, Tokens/sec: 41540.62\n",
      "Step: 3116, Training Loss: 1.32446, LR: 0.0003251, Tokens/sec: 41586.34\n",
      "Step: 3117, Training Loss: 1.66975, LR: 0.0003248, Tokens/sec: 41439.52\n",
      "Step: 3118, Training Loss: 1.58009, LR: 0.0003245, Tokens/sec: 41513.64\n",
      "Step: 3119, Training Loss: 1.43607, LR: 0.0003242, Tokens/sec: 36203.09\n",
      "Step: 3120, Training Loss: 1.21254, LR: 0.0003239, Tokens/sec: 28682.10\n",
      "Step: 3121, Training Loss: 1.79132, LR: 0.0003236, Tokens/sec: 36908.87\n",
      "Step: 3122, Training Loss: 1.95184, LR: 0.0003233, Tokens/sec: 41370.00\n",
      "Step: 3123, Training Loss: 2.01977, LR: 0.0003230, Tokens/sec: 41428.25\n",
      "Step: 3124, Training Loss: 2.20461, LR: 0.0003227, Tokens/sec: 31859.34\n",
      "Step: 3125, Training Loss: 1.40434, LR: 0.0003224, Tokens/sec: 36854.23\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 3125, Eval Loss: 1.68934\n",
      "Step: 3126, Training Loss: 1.69501, LR: 0.0003221, Tokens/sec: 41354.12\n",
      "Step: 3127, Training Loss: 1.41957, LR: 0.0003217, Tokens/sec: 41549.77\n",
      "Step: 3128, Training Loss: 1.67277, LR: 0.0003214, Tokens/sec: 41269.16\n",
      "Step: 3129, Training Loss: 2.03801, LR: 0.0003211, Tokens/sec: 41373.86\n",
      "Step: 3130, Training Loss: 1.96838, LR: 0.0003208, Tokens/sec: 41141.01\n",
      "Step: 3131, Training Loss: 1.74677, LR: 0.0003205, Tokens/sec: 39731.86\n",
      "Step: 3132, Training Loss: 1.54492, LR: 0.0003202, Tokens/sec: 40068.34\n",
      "Step: 3133, Training Loss: 1.30413, LR: 0.0003199, Tokens/sec: 41515.88\n",
      "Step: 3134, Training Loss: 1.22105, LR: 0.0003196, Tokens/sec: 40782.28\n",
      "Step: 3135, Training Loss: 1.45159, LR: 0.0003193, Tokens/sec: 41170.97\n",
      "Step: 3136, Training Loss: 1.49277, LR: 0.0003190, Tokens/sec: 40865.62\n",
      "Step: 3137, Training Loss: 2.03408, LR: 0.0003187, Tokens/sec: 40940.97\n",
      "Step: 3138, Training Loss: 1.31337, LR: 0.0003184, Tokens/sec: 41352.94\n",
      "Step: 3139, Training Loss: 1.29051, LR: 0.0003181, Tokens/sec: 38107.53\n",
      "Step: 3140, Training Loss: 2.26604, LR: 0.0003178, Tokens/sec: 39237.96\n",
      "Step: 3141, Training Loss: 1.35921, LR: 0.0003175, Tokens/sec: 41252.78\n",
      "Step: 3142, Training Loss: 1.80529, LR: 0.0003172, Tokens/sec: 40709.12\n",
      "Step: 3143, Training Loss: 1.78349, LR: 0.0003169, Tokens/sec: 40672.51\n",
      "Step: 3144, Training Loss: 1.07730, LR: 0.0003166, Tokens/sec: 40551.99\n",
      "Step: 3145, Training Loss: 2.16077, LR: 0.0003163, Tokens/sec: 37652.82\n",
      "Step: 3146, Training Loss: 1.40744, LR: 0.0003160, Tokens/sec: 41336.97\n",
      "Step: 3147, Training Loss: 1.26408, LR: 0.0003157, Tokens/sec: 40670.52\n",
      "Step: 3148, Training Loss: 2.14897, LR: 0.0003154, Tokens/sec: 40953.62\n",
      "Step: 3149, Training Loss: 1.96135, LR: 0.0003151, Tokens/sec: 41414.10\n",
      "Step: 3150, Training Loss: 1.30095, LR: 0.0003148, Tokens/sec: 41039.55\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 3150, Eval Loss: 1.58278\n",
      "Step: 3151, Training Loss: 1.52298, LR: 0.0003145, Tokens/sec: 40559.43\n",
      "Step: 3152, Training Loss: 1.11335, LR: 0.0003142, Tokens/sec: 40612.22\n",
      "Step: 3153, Training Loss: 1.91137, LR: 0.0003139, Tokens/sec: 41564.27\n",
      "Step: 3154, Training Loss: 1.53950, LR: 0.0003136, Tokens/sec: 41502.19\n",
      "Step: 3155, Training Loss: 1.73362, LR: 0.0003133, Tokens/sec: 40505.35\n",
      "Step: 3156, Training Loss: 1.03739, LR: 0.0003130, Tokens/sec: 41368.54\n",
      "Step: 3157, Training Loss: 1.28338, LR: 0.0003127, Tokens/sec: 41505.25\n",
      "Step: 3158, Training Loss: 1.26499, LR: 0.0003124, Tokens/sec: 41486.97\n",
      "Step: 3159, Training Loss: 2.07989, LR: 0.0003121, Tokens/sec: 41544.35\n",
      "Step: 3160, Training Loss: 1.09911, LR: 0.0003118, Tokens/sec: 41335.96\n",
      "Step: 3161, Training Loss: 1.72215, LR: 0.0003115, Tokens/sec: 41561.17\n",
      "Step: 3162, Training Loss: 2.07453, LR: 0.0003112, Tokens/sec: 40866.08\n",
      "Step: 3163, Training Loss: 1.38399, LR: 0.0003109, Tokens/sec: 40209.71\n",
      "Step: 3164, Training Loss: 1.45185, LR: 0.0003106, Tokens/sec: 41439.37\n",
      "Step: 3165, Training Loss: 0.96406, LR: 0.0003103, Tokens/sec: 41378.54\n",
      "Step: 3166, Training Loss: 1.73953, LR: 0.0003100, Tokens/sec: 41427.46\n",
      "Step: 3167, Training Loss: 1.37020, LR: 0.0003097, Tokens/sec: 38317.70\n",
      "Step: 3168, Training Loss: 1.91684, LR: 0.0003094, Tokens/sec: 41233.16\n",
      "Step: 3169, Training Loss: 1.77474, LR: 0.0003091, Tokens/sec: 41475.95\n",
      "Step: 3170, Training Loss: 1.41358, LR: 0.0003088, Tokens/sec: 41415.98\n",
      "Step: 3171, Training Loss: 0.83459, LR: 0.0003085, Tokens/sec: 39903.36\n",
      "Step: 3172, Training Loss: 1.54039, LR: 0.0003082, Tokens/sec: 41572.10\n",
      "Step: 3173, Training Loss: 1.42890, LR: 0.0003079, Tokens/sec: 41526.67\n",
      "Step: 3174, Training Loss: 1.79199, LR: 0.0003076, Tokens/sec: 40619.79\n",
      "Step: 3175, Training Loss: 1.21815, LR: 0.0003073, Tokens/sec: 41380.08\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 3175, Eval Loss: 1.65977\n",
      "Step: 3176, Training Loss: 1.37667, LR: 0.0003070, Tokens/sec: 40643.36\n",
      "Step: 3177, Training Loss: 1.23160, LR: 0.0003067, Tokens/sec: 40899.02\n",
      "Step: 3178, Training Loss: 1.45200, LR: 0.0003064, Tokens/sec: 39054.05\n",
      "Step: 3179, Training Loss: 1.41249, LR: 0.0003061, Tokens/sec: 40927.97\n",
      "Step: 3180, Training Loss: 1.62403, LR: 0.0003058, Tokens/sec: 41398.85\n",
      "Step: 3181, Training Loss: 1.37992, LR: 0.0003055, Tokens/sec: 40333.82\n",
      "Step: 3182, Training Loss: 1.46298, LR: 0.0003052, Tokens/sec: 41355.43\n",
      "Step: 3183, Training Loss: 1.88845, LR: 0.0003049, Tokens/sec: 39079.37\n",
      "Step: 3184, Training Loss: 1.60438, LR: 0.0003047, Tokens/sec: 41054.91\n",
      "Step: 3185, Training Loss: 0.97130, LR: 0.0003044, Tokens/sec: 41373.11\n",
      "Step: 3186, Training Loss: 1.51877, LR: 0.0003041, Tokens/sec: 38917.80\n",
      "Step: 3187, Training Loss: 1.71307, LR: 0.0003038, Tokens/sec: 38929.35\n",
      "Step: 3188, Training Loss: 2.05999, LR: 0.0003035, Tokens/sec: 39019.06\n",
      "Step: 3189, Training Loss: 2.15735, LR: 0.0003032, Tokens/sec: 38582.89\n",
      "Step: 3190, Training Loss: 1.37727, LR: 0.0003029, Tokens/sec: 37418.49\n",
      "Step: 3191, Training Loss: 1.46721, LR: 0.0003026, Tokens/sec: 41364.44\n",
      "Step: 3192, Training Loss: 1.61700, LR: 0.0003023, Tokens/sec: 41320.96\n",
      "Step: 3193, Training Loss: 1.57353, LR: 0.0003020, Tokens/sec: 41341.00\n",
      "Step: 3194, Training Loss: 1.79599, LR: 0.0003017, Tokens/sec: 39362.39\n",
      "Step: 3195, Training Loss: 2.39812, LR: 0.0003014, Tokens/sec: 40725.51\n",
      "Step: 3196, Training Loss: 1.68691, LR: 0.0003011, Tokens/sec: 41325.89\n",
      "Step: 3197, Training Loss: 1.33266, LR: 0.0003008, Tokens/sec: 39913.40\n",
      "Step: 3198, Training Loss: 1.93683, LR: 0.0003005, Tokens/sec: 41289.97\n",
      "Step: 3199, Training Loss: 1.53246, LR: 0.0003002, Tokens/sec: 41372.56\n",
      "Step: 3200, Training Loss: 1.55567, LR: 0.0002999, Tokens/sec: 41467.60\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 3200, Eval Loss: 1.43455\n",
      "Step: 3201, Training Loss: 1.35569, LR: 0.0002996, Tokens/sec: 41180.09\n",
      "Step: 3202, Training Loss: 0.97996, LR: 0.0002994, Tokens/sec: 31250.07\n",
      "Step: 3203, Training Loss: 0.90619, LR: 0.0002991, Tokens/sec: 41379.45\n",
      "Step: 3204, Training Loss: 1.73727, LR: 0.0002988, Tokens/sec: 40922.94\n",
      "Step: 3205, Training Loss: 1.34734, LR: 0.0002985, Tokens/sec: 39551.01\n",
      "Step: 3206, Training Loss: 1.38255, LR: 0.0002982, Tokens/sec: 41386.39\n",
      "Step: 3207, Training Loss: 1.04380, LR: 0.0002979, Tokens/sec: 38102.77\n",
      "Step: 3208, Training Loss: 1.53511, LR: 0.0002976, Tokens/sec: 41335.66\n",
      "Step: 3209, Training Loss: 1.01075, LR: 0.0002973, Tokens/sec: 41313.07\n",
      "Step: 3210, Training Loss: 1.90238, LR: 0.0002970, Tokens/sec: 37486.22\n",
      "Step: 3211, Training Loss: 1.64055, LR: 0.0002967, Tokens/sec: 38352.17\n",
      "Step: 3212, Training Loss: 1.73527, LR: 0.0002964, Tokens/sec: 40926.74\n",
      "Step: 3213, Training Loss: 1.43573, LR: 0.0002961, Tokens/sec: 41276.41\n",
      "Step: 3214, Training Loss: 1.79375, LR: 0.0002958, Tokens/sec: 39915.44\n",
      "Step: 3215, Training Loss: 1.68716, LR: 0.0002956, Tokens/sec: 39539.17\n",
      "Step: 3216, Training Loss: 1.63024, LR: 0.0002953, Tokens/sec: 41048.22\n",
      "Step: 3217, Training Loss: 1.23423, LR: 0.0002950, Tokens/sec: 40948.67\n",
      "Step: 3218, Training Loss: 0.95790, LR: 0.0002947, Tokens/sec: 37057.90\n",
      "Step: 3219, Training Loss: 1.48423, LR: 0.0002944, Tokens/sec: 41291.90\n",
      "Step: 3220, Training Loss: 1.82055, LR: 0.0002941, Tokens/sec: 40365.18\n",
      "Step: 3221, Training Loss: 1.00429, LR: 0.0002938, Tokens/sec: 41023.59\n",
      "Step: 3222, Training Loss: 1.29014, LR: 0.0002935, Tokens/sec: 39511.91\n",
      "Step: 3223, Training Loss: 2.04508, LR: 0.0002932, Tokens/sec: 24122.01\n",
      "Step: 3224, Training Loss: 1.44608, LR: 0.0002929, Tokens/sec: 41237.62\n",
      "Step: 3225, Training Loss: 1.61195, LR: 0.0002927, Tokens/sec: 39922.56\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 3225, Eval Loss: 1.55324\n",
      "Step: 3226, Training Loss: 1.89919, LR: 0.0002924, Tokens/sec: 40995.75\n",
      "Step: 3227, Training Loss: 1.82963, LR: 0.0002921, Tokens/sec: 40882.65\n",
      "Step: 3228, Training Loss: 1.63105, LR: 0.0002918, Tokens/sec: 40622.82\n",
      "Step: 3229, Training Loss: 0.92731, LR: 0.0002915, Tokens/sec: 41305.08\n",
      "Step: 3230, Training Loss: 1.25489, LR: 0.0002912, Tokens/sec: 40353.76\n",
      "Step: 3231, Training Loss: 1.47477, LR: 0.0002909, Tokens/sec: 41330.66\n",
      "Step: 3232, Training Loss: 2.05483, LR: 0.0002906, Tokens/sec: 41314.25\n",
      "Step: 3233, Training Loss: 1.51359, LR: 0.0002903, Tokens/sec: 40703.71\n",
      "Step: 3234, Training Loss: 1.37475, LR: 0.0002901, Tokens/sec: 41344.28\n",
      "Step: 3235, Training Loss: 2.13236, LR: 0.0002898, Tokens/sec: 41325.55\n",
      "Step: 3236, Training Loss: 1.19563, LR: 0.0002895, Tokens/sec: 41284.44\n",
      "Step: 3237, Training Loss: 1.55643, LR: 0.0002892, Tokens/sec: 40696.13\n",
      "Step: 3238, Training Loss: 1.27973, LR: 0.0002889, Tokens/sec: 41127.64\n",
      "Step: 3239, Training Loss: 1.52324, LR: 0.0002886, Tokens/sec: 40682.44\n",
      "Step: 3240, Training Loss: 1.69397, LR: 0.0002883, Tokens/sec: 41372.00\n",
      "Step: 3241, Training Loss: 0.94706, LR: 0.0002880, Tokens/sec: 40451.06\n",
      "Step: 3242, Training Loss: 1.06619, LR: 0.0002878, Tokens/sec: 41257.53\n",
      "Step: 3243, Training Loss: 1.70064, LR: 0.0002875, Tokens/sec: 41354.17\n",
      "Step: 3244, Training Loss: 0.81665, LR: 0.0002872, Tokens/sec: 41322.14\n",
      "Step: 3245, Training Loss: 0.94005, LR: 0.0002869, Tokens/sec: 41349.51\n",
      "Step: 3246, Training Loss: 1.23298, LR: 0.0002866, Tokens/sec: 40955.32\n",
      "Step: 3247, Training Loss: 1.35691, LR: 0.0002863, Tokens/sec: 41280.78\n",
      "Step: 3248, Training Loss: 0.83013, LR: 0.0002860, Tokens/sec: 40667.88\n",
      "Step: 3249, Training Loss: 1.09096, LR: 0.0002858, Tokens/sec: 40663.71\n",
      "Step: 3250, Training Loss: 1.72889, LR: 0.0002855, Tokens/sec: 41345.22\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 3250, Eval Loss: 1.39854\n",
      "Step: 3251, Training Loss: 1.39348, LR: 0.0002852, Tokens/sec: 41245.93\n",
      "Step: 3252, Training Loss: 1.45945, LR: 0.0002849, Tokens/sec: 41322.80\n",
      "Step: 3253, Training Loss: 1.24835, LR: 0.0002846, Tokens/sec: 41340.98\n",
      "Step: 3254, Training Loss: 1.08247, LR: 0.0002843, Tokens/sec: 41003.70\n",
      "Step: 3255, Training Loss: 1.53904, LR: 0.0002840, Tokens/sec: 41372.93\n",
      "Step: 3256, Training Loss: 0.97300, LR: 0.0002838, Tokens/sec: 41389.06\n",
      "Step: 3257, Training Loss: 1.51578, LR: 0.0002835, Tokens/sec: 40488.87\n",
      "Step: 3258, Training Loss: 1.06156, LR: 0.0002832, Tokens/sec: 41234.05\n",
      "Step: 3259, Training Loss: 1.02593, LR: 0.0002829, Tokens/sec: 41358.46\n",
      "Step: 3260, Training Loss: 1.15080, LR: 0.0002826, Tokens/sec: 41279.80\n",
      "Step: 3261, Training Loss: 1.39753, LR: 0.0002823, Tokens/sec: 40612.26\n",
      "Step: 3262, Training Loss: 1.79642, LR: 0.0002821, Tokens/sec: 39949.89\n",
      "Step: 3263, Training Loss: 1.44954, LR: 0.0002818, Tokens/sec: 41160.52\n",
      "Step: 3264, Training Loss: 1.42772, LR: 0.0002815, Tokens/sec: 41286.46\n",
      "Step: 3265, Training Loss: 1.74424, LR: 0.0002812, Tokens/sec: 40695.02\n",
      "Step: 3266, Training Loss: 0.70834, LR: 0.0002809, Tokens/sec: 41240.98\n",
      "Step: 3267, Training Loss: 1.73584, LR: 0.0002806, Tokens/sec: 41344.47\n",
      "Step: 3268, Training Loss: 1.51826, LR: 0.0002804, Tokens/sec: 40627.71\n",
      "Step: 3269, Training Loss: 1.69981, LR: 0.0002801, Tokens/sec: 41269.94\n",
      "Step: 3270, Training Loss: 1.90588, LR: 0.0002798, Tokens/sec: 41217.52\n",
      "Step: 3271, Training Loss: 1.44880, LR: 0.0002795, Tokens/sec: 41274.10\n",
      "Step: 3272, Training Loss: 1.71099, LR: 0.0002792, Tokens/sec: 41313.37\n",
      "Step: 3273, Training Loss: 1.92275, LR: 0.0002789, Tokens/sec: 40355.65\n",
      "Step: 3274, Training Loss: 1.55878, LR: 0.0002787, Tokens/sec: 41297.57\n",
      "Step: 3275, Training Loss: 1.38587, LR: 0.0002784, Tokens/sec: 41355.35\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 3275, Eval Loss: 1.37635\n",
      "Step: 3276, Training Loss: 1.00843, LR: 0.0002781, Tokens/sec: 41193.07\n",
      "Step: 3277, Training Loss: 0.88331, LR: 0.0002778, Tokens/sec: 41297.69\n",
      "Step: 3278, Training Loss: 1.73887, LR: 0.0002775, Tokens/sec: 40733.95\n",
      "Step: 3279, Training Loss: 1.70433, LR: 0.0002773, Tokens/sec: 40681.76\n",
      "Step: 3280, Training Loss: 1.26583, LR: 0.0002770, Tokens/sec: 38770.17\n",
      "Step: 3281, Training Loss: 1.38875, LR: 0.0002767, Tokens/sec: 36182.30\n",
      "Step: 3282, Training Loss: 1.60264, LR: 0.0002764, Tokens/sec: 33496.81\n",
      "Step: 3283, Training Loss: 1.11006, LR: 0.0002761, Tokens/sec: 34044.70\n",
      "Step: 3284, Training Loss: 1.21702, LR: 0.0002759, Tokens/sec: 40740.97\n",
      "Step: 3285, Training Loss: 2.02364, LR: 0.0002756, Tokens/sec: 39605.82\n",
      "Step: 3286, Training Loss: 2.27427, LR: 0.0002753, Tokens/sec: 39729.47\n",
      "Step: 3287, Training Loss: 1.08521, LR: 0.0002750, Tokens/sec: 38013.87\n",
      "Step: 3288, Training Loss: 1.57338, LR: 0.0002747, Tokens/sec: 36186.36\n",
      "Step: 3289, Training Loss: 1.12750, LR: 0.0002745, Tokens/sec: 40514.63\n",
      "Step: 3290, Training Loss: 0.81852, LR: 0.0002742, Tokens/sec: 38969.67\n",
      "Step: 3291, Training Loss: 1.11210, LR: 0.0002739, Tokens/sec: 41192.79\n",
      "Step: 3292, Training Loss: 1.12922, LR: 0.0002736, Tokens/sec: 41389.56\n",
      "Step: 3293, Training Loss: 1.62330, LR: 0.0002733, Tokens/sec: 38971.40\n",
      "Step: 3294, Training Loss: 1.33408, LR: 0.0002731, Tokens/sec: 37591.79\n",
      "Step: 3295, Training Loss: 1.84930, LR: 0.0002728, Tokens/sec: 41339.81\n",
      "Step: 3296, Training Loss: 1.14937, LR: 0.0002725, Tokens/sec: 39548.24\n",
      "Step: 3297, Training Loss: 1.84724, LR: 0.0002722, Tokens/sec: 41380.03\n",
      "Step: 3298, Training Loss: 1.65514, LR: 0.0002720, Tokens/sec: 40971.32\n",
      "Step: 3299, Training Loss: 1.50036, LR: 0.0002717, Tokens/sec: 40573.22\n",
      "Step: 3300, Training Loss: 1.48061, LR: 0.0002714, Tokens/sec: 40776.17\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 3300, Eval Loss: 1.36015\n",
      "Step: 3301, Training Loss: 1.70751, LR: 0.0002711, Tokens/sec: 36183.34\n",
      "Step: 3302, Training Loss: 1.83637, LR: 0.0002709, Tokens/sec: 40439.41\n",
      "Step: 3303, Training Loss: 0.91083, LR: 0.0002706, Tokens/sec: 41047.54\n",
      "Step: 3304, Training Loss: 1.11369, LR: 0.0002703, Tokens/sec: 40619.25\n",
      "Step: 3305, Training Loss: 1.20250, LR: 0.0002700, Tokens/sec: 40742.85\n",
      "Step: 3306, Training Loss: 1.08580, LR: 0.0002697, Tokens/sec: 40416.69\n",
      "Step: 3307, Training Loss: 1.08328, LR: 0.0002695, Tokens/sec: 41416.95\n",
      "Step: 3308, Training Loss: 1.64460, LR: 0.0002692, Tokens/sec: 40611.60\n",
      "Step: 3309, Training Loss: 1.35699, LR: 0.0002689, Tokens/sec: 40782.80\n",
      "Step: 3310, Training Loss: 0.82683, LR: 0.0002686, Tokens/sec: 40027.80\n",
      "Step: 3311, Training Loss: 1.29781, LR: 0.0002684, Tokens/sec: 39010.42\n",
      "Step: 3312, Training Loss: 1.12545, LR: 0.0002681, Tokens/sec: 39466.90\n",
      "Step: 3313, Training Loss: 1.13729, LR: 0.0002678, Tokens/sec: 40636.25\n",
      "Step: 3314, Training Loss: 1.14191, LR: 0.0002675, Tokens/sec: 39904.16\n",
      "Step: 3315, Training Loss: 1.21564, LR: 0.0002673, Tokens/sec: 39693.74\n",
      "Step: 3316, Training Loss: 1.76020, LR: 0.0002670, Tokens/sec: 40813.55\n",
      "Step: 3317, Training Loss: 1.92098, LR: 0.0002667, Tokens/sec: 40112.42\n",
      "Step: 3318, Training Loss: 1.46993, LR: 0.0002664, Tokens/sec: 40381.34\n",
      "Step: 3319, Training Loss: 1.20571, LR: 0.0002662, Tokens/sec: 40710.10\n",
      "Step: 3320, Training Loss: 1.31648, LR: 0.0002659, Tokens/sec: 41049.75\n",
      "Step: 3321, Training Loss: 1.52058, LR: 0.0002656, Tokens/sec: 39387.34\n",
      "Step: 3322, Training Loss: 1.14282, LR: 0.0002654, Tokens/sec: 37913.74\n",
      "Step: 3323, Training Loss: 1.50297, LR: 0.0002651, Tokens/sec: 40711.25\n",
      "Step: 3324, Training Loss: 1.32765, LR: 0.0002648, Tokens/sec: 41257.21\n",
      "Step: 3325, Training Loss: 1.56036, LR: 0.0002645, Tokens/sec: 40685.30\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 3325, Eval Loss: 1.40859\n",
      "Step: 3326, Training Loss: 1.13738, LR: 0.0002643, Tokens/sec: 40453.02\n",
      "Step: 3327, Training Loss: 1.53034, LR: 0.0002640, Tokens/sec: 40023.65\n",
      "Step: 3328, Training Loss: 1.20162, LR: 0.0002637, Tokens/sec: 40238.67\n",
      "Step: 3329, Training Loss: 1.57879, LR: 0.0002634, Tokens/sec: 41112.52\n",
      "Step: 3330, Training Loss: 0.79123, LR: 0.0002632, Tokens/sec: 40874.73\n",
      "Step: 3331, Training Loss: 1.65157, LR: 0.0002629, Tokens/sec: 41058.17\n",
      "Step: 3332, Training Loss: 1.16440, LR: 0.0002626, Tokens/sec: 40963.79\n",
      "Step: 3333, Training Loss: 0.98548, LR: 0.0002624, Tokens/sec: 38705.60\n",
      "Step: 3334, Training Loss: 1.24803, LR: 0.0002621, Tokens/sec: 40675.96\n",
      "Step: 3335, Training Loss: 0.98302, LR: 0.0002618, Tokens/sec: 39952.36\n",
      "Step: 3336, Training Loss: 1.84820, LR: 0.0002615, Tokens/sec: 38408.81\n",
      "Step: 3337, Training Loss: 2.02370, LR: 0.0002613, Tokens/sec: 41016.11\n",
      "Step: 3338, Training Loss: 1.83022, LR: 0.0002610, Tokens/sec: 39743.66\n",
      "Step: 3339, Training Loss: 1.18069, LR: 0.0002607, Tokens/sec: 36483.74\n",
      "Step: 3340, Training Loss: 1.55569, LR: 0.0002605, Tokens/sec: 32023.61\n",
      "Step: 3341, Training Loss: 1.21929, LR: 0.0002602, Tokens/sec: 41347.68\n",
      "Step: 3342, Training Loss: 1.28566, LR: 0.0002599, Tokens/sec: 41138.71\n",
      "Step: 3343, Training Loss: 1.50181, LR: 0.0002597, Tokens/sec: 37807.70\n",
      "Step: 3344, Training Loss: 1.29375, LR: 0.0002594, Tokens/sec: 37309.65\n",
      "Step: 3345, Training Loss: 2.17547, LR: 0.0002591, Tokens/sec: 37320.94\n",
      "Step: 3346, Training Loss: 1.35407, LR: 0.0002588, Tokens/sec: 37108.39\n",
      "Step: 3347, Training Loss: 1.48463, LR: 0.0002586, Tokens/sec: 37770.31\n",
      "Step: 3348, Training Loss: 1.83274, LR: 0.0002583, Tokens/sec: 37546.14\n",
      "Step: 3349, Training Loss: 1.46224, LR: 0.0002580, Tokens/sec: 37990.72\n",
      "Step: 3350, Training Loss: 1.83232, LR: 0.0002578, Tokens/sec: 37678.87\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 3350, Eval Loss: 1.33177\n",
      "Step: 3351, Training Loss: 1.02858, LR: 0.0002575, Tokens/sec: 37883.53\n",
      "Step: 3352, Training Loss: 0.86232, LR: 0.0002572, Tokens/sec: 37699.06\n",
      "Step: 3353, Training Loss: 1.18053, LR: 0.0002570, Tokens/sec: 37649.43\n",
      "Step: 3354, Training Loss: 1.88877, LR: 0.0002567, Tokens/sec: 41407.46\n",
      "Step: 3355, Training Loss: 0.80948, LR: 0.0002564, Tokens/sec: 41311.69\n",
      "Step: 3356, Training Loss: 1.25475, LR: 0.0002562, Tokens/sec: 41324.46\n",
      "Step: 3357, Training Loss: 1.10253, LR: 0.0002559, Tokens/sec: 37534.93\n",
      "Step: 3358, Training Loss: 1.49183, LR: 0.0002556, Tokens/sec: 33287.89\n",
      "Step: 3359, Training Loss: 1.54288, LR: 0.0002554, Tokens/sec: 29643.10\n",
      "Step: 3360, Training Loss: 1.48916, LR: 0.0002551, Tokens/sec: 30053.54\n",
      "Step: 3361, Training Loss: 1.19857, LR: 0.0002548, Tokens/sec: 33141.14\n",
      "Step: 3362, Training Loss: 1.36455, LR: 0.0002546, Tokens/sec: 29437.41\n",
      "Step: 3363, Training Loss: 1.12952, LR: 0.0002543, Tokens/sec: 40631.73\n",
      "Step: 3364, Training Loss: 1.10908, LR: 0.0002540, Tokens/sec: 41443.56\n",
      "Step: 3365, Training Loss: 1.00210, LR: 0.0002538, Tokens/sec: 39982.50\n",
      "Step: 3366, Training Loss: 1.02489, LR: 0.0002535, Tokens/sec: 41411.92\n",
      "Step: 3367, Training Loss: 1.09405, LR: 0.0002532, Tokens/sec: 41451.05\n",
      "Step: 3368, Training Loss: 1.60084, LR: 0.0002530, Tokens/sec: 41379.03\n",
      "Step: 3369, Training Loss: 1.52709, LR: 0.0002527, Tokens/sec: 41425.47\n",
      "Step: 3370, Training Loss: 1.50140, LR: 0.0002524, Tokens/sec: 40463.18\n",
      "Step: 3371, Training Loss: 1.51586, LR: 0.0002522, Tokens/sec: 41432.04\n",
      "Step: 3372, Training Loss: 1.08569, LR: 0.0002519, Tokens/sec: 41248.50\n",
      "Step: 3373, Training Loss: 1.16328, LR: 0.0002516, Tokens/sec: 39730.76\n",
      "Step: 3374, Training Loss: 0.94265, LR: 0.0002514, Tokens/sec: 39936.83\n",
      "Step: 3375, Training Loss: 2.00487, LR: 0.0002511, Tokens/sec: 41489.86\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 3375, Eval Loss: 1.31340\n",
      "Step: 3376, Training Loss: 0.84677, LR: 0.0002509, Tokens/sec: 36976.55\n",
      "Step: 3377, Training Loss: 1.57368, LR: 0.0002506, Tokens/sec: 35210.99\n",
      "Step: 3378, Training Loss: 1.39442, LR: 0.0002503, Tokens/sec: 37332.82\n",
      "Step: 3379, Training Loss: 1.19701, LR: 0.0002501, Tokens/sec: 37685.07\n",
      "Step: 3380, Training Loss: 1.59424, LR: 0.0002498, Tokens/sec: 39622.07\n",
      "Step: 3381, Training Loss: 1.22391, LR: 0.0002495, Tokens/sec: 40489.17\n",
      "Step: 3382, Training Loss: 1.41841, LR: 0.0002493, Tokens/sec: 41266.90\n",
      "Step: 3383, Training Loss: 1.17489, LR: 0.0002490, Tokens/sec: 41353.87\n",
      "Step: 3384, Training Loss: 1.07442, LR: 0.0002488, Tokens/sec: 40968.55\n",
      "Step: 3385, Training Loss: 1.27536, LR: 0.0002485, Tokens/sec: 40688.28\n",
      "Step: 3386, Training Loss: 1.24854, LR: 0.0002482, Tokens/sec: 37755.52\n",
      "Step: 3387, Training Loss: 1.87914, LR: 0.0002480, Tokens/sec: 38101.28\n",
      "Step: 3388, Training Loss: 1.11234, LR: 0.0002477, Tokens/sec: 37894.84\n",
      "Step: 3389, Training Loss: 1.33259, LR: 0.0002474, Tokens/sec: 37949.97\n",
      "Step: 3390, Training Loss: 0.99315, LR: 0.0002472, Tokens/sec: 35743.48\n",
      "Step: 3391, Training Loss: 1.25600, LR: 0.0002469, Tokens/sec: 37153.03\n",
      "Step: 3392, Training Loss: 1.35957, LR: 0.0002467, Tokens/sec: 37152.86\n",
      "Step: 3393, Training Loss: 1.34281, LR: 0.0002464, Tokens/sec: 37840.61\n",
      "Step: 3394, Training Loss: 1.36631, LR: 0.0002461, Tokens/sec: 37903.52\n",
      "Step: 3395, Training Loss: 1.50942, LR: 0.0002459, Tokens/sec: 37093.02\n",
      "Step: 3396, Training Loss: 1.25223, LR: 0.0002456, Tokens/sec: 37297.16\n",
      "Step: 3397, Training Loss: 0.83441, LR: 0.0002454, Tokens/sec: 37473.76\n",
      "Step: 3398, Training Loss: 1.12294, LR: 0.0002451, Tokens/sec: 38143.30\n",
      "Step: 3399, Training Loss: 1.03718, LR: 0.0002448, Tokens/sec: 38192.08\n",
      "Step: 3400, Training Loss: 1.66414, LR: 0.0002446, Tokens/sec: 37170.56\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 3400, Eval Loss: 1.27277\n",
      "Step: 3401, Training Loss: 0.98035, LR: 0.0002443, Tokens/sec: 40759.87\n",
      "Step: 3402, Training Loss: 0.72162, LR: 0.0002441, Tokens/sec: 38810.12\n",
      "Step: 3403, Training Loss: 1.58304, LR: 0.0002438, Tokens/sec: 40425.99\n",
      "Step: 3404, Training Loss: 1.45531, LR: 0.0002435, Tokens/sec: 41206.83\n",
      "Step: 3405, Training Loss: 1.72616, LR: 0.0002433, Tokens/sec: 39422.41\n",
      "Step: 3406, Training Loss: 1.15753, LR: 0.0002430, Tokens/sec: 40936.32\n",
      "Step: 3407, Training Loss: 0.91557, LR: 0.0002428, Tokens/sec: 40728.65\n",
      "Step: 3408, Training Loss: 1.05303, LR: 0.0002425, Tokens/sec: 41228.76\n",
      "Step: 3409, Training Loss: 2.07707, LR: 0.0002423, Tokens/sec: 40938.81\n",
      "Step: 3410, Training Loss: 1.52074, LR: 0.0002420, Tokens/sec: 39305.30\n",
      "Step: 3411, Training Loss: 0.72573, LR: 0.0002417, Tokens/sec: 40955.80\n",
      "Step: 3412, Training Loss: 1.34894, LR: 0.0002415, Tokens/sec: 40456.13\n",
      "Step: 3413, Training Loss: 0.93360, LR: 0.0002412, Tokens/sec: 39913.57\n",
      "Step: 3414, Training Loss: 0.96624, LR: 0.0002410, Tokens/sec: 41352.54\n",
      "Step: 3415, Training Loss: 0.95169, LR: 0.0002407, Tokens/sec: 40918.88\n",
      "Step: 3416, Training Loss: 1.05807, LR: 0.0002405, Tokens/sec: 40931.56\n",
      "Step: 3417, Training Loss: 1.13351, LR: 0.0002402, Tokens/sec: 41270.05\n",
      "Step: 3418, Training Loss: 0.95043, LR: 0.0002399, Tokens/sec: 40331.70\n",
      "Step: 3419, Training Loss: 1.58033, LR: 0.0002397, Tokens/sec: 40837.94\n",
      "Step: 3420, Training Loss: 0.97634, LR: 0.0002394, Tokens/sec: 41189.83\n",
      "Step: 3421, Training Loss: 1.30250, LR: 0.0002392, Tokens/sec: 39432.74\n",
      "Step: 3422, Training Loss: 1.10137, LR: 0.0002389, Tokens/sec: 40379.69\n",
      "Step: 3423, Training Loss: 1.82945, LR: 0.0002387, Tokens/sec: 40921.63\n",
      "Step: 3424, Training Loss: 1.32450, LR: 0.0002384, Tokens/sec: 41340.62\n",
      "Step: 3425, Training Loss: 0.82218, LR: 0.0002382, Tokens/sec: 40862.73\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 3425, Eval Loss: 1.18464\n",
      "Step: 3426, Training Loss: 1.31708, LR: 0.0002379, Tokens/sec: 39862.97\n",
      "Step: 3427, Training Loss: 1.50579, LR: 0.0002377, Tokens/sec: 38746.66\n",
      "Step: 3428, Training Loss: 1.39169, LR: 0.0002374, Tokens/sec: 41325.74\n",
      "Step: 3429, Training Loss: 1.44028, LR: 0.0002371, Tokens/sec: 41418.14\n",
      "Step: 3430, Training Loss: 1.07787, LR: 0.0002369, Tokens/sec: 41522.89\n",
      "Step: 3431, Training Loss: 1.22797, LR: 0.0002366, Tokens/sec: 41107.95\n",
      "Step: 3432, Training Loss: 1.53455, LR: 0.0002364, Tokens/sec: 40742.55\n",
      "Step: 3433, Training Loss: 1.66686, LR: 0.0002361, Tokens/sec: 40980.11\n",
      "Step: 3434, Training Loss: 0.91392, LR: 0.0002359, Tokens/sec: 39937.36\n",
      "Step: 3435, Training Loss: 1.24661, LR: 0.0002356, Tokens/sec: 40864.85\n",
      "Step: 3436, Training Loss: 0.72758, LR: 0.0002354, Tokens/sec: 41051.51\n",
      "Step: 3437, Training Loss: 1.20766, LR: 0.0002351, Tokens/sec: 41375.43\n",
      "Step: 3438, Training Loss: 0.69402, LR: 0.0002349, Tokens/sec: 40688.13\n",
      "Step: 3439, Training Loss: 1.28719, LR: 0.0002346, Tokens/sec: 41400.15\n",
      "Step: 3440, Training Loss: 1.62790, LR: 0.0002344, Tokens/sec: 40612.19\n",
      "Step: 3441, Training Loss: 1.21199, LR: 0.0002341, Tokens/sec: 39887.20\n",
      "Step: 3442, Training Loss: 1.28859, LR: 0.0002339, Tokens/sec: 39109.65\n",
      "Step: 3443, Training Loss: 0.40909, LR: 0.0002336, Tokens/sec: 34407.55\n",
      "Step: 3444, Training Loss: 1.11651, LR: 0.0002334, Tokens/sec: 41312.46\n",
      "Step: 3445, Training Loss: 0.77784, LR: 0.0002331, Tokens/sec: 41191.61\n",
      "Step: 3446, Training Loss: 1.18533, LR: 0.0002329, Tokens/sec: 40910.69\n",
      "Step: 3447, Training Loss: 0.85575, LR: 0.0002326, Tokens/sec: 39520.99\n",
      "Step: 3448, Training Loss: 1.11680, LR: 0.0002324, Tokens/sec: 41053.12\n",
      "Step: 3449, Training Loss: 0.89237, LR: 0.0002321, Tokens/sec: 40807.49\n",
      "Step: 3450, Training Loss: 1.14153, LR: 0.0002319, Tokens/sec: 40329.22\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 3450, Eval Loss: 1.30241\n",
      "Step: 3451, Training Loss: 1.11400, LR: 0.0002316, Tokens/sec: 37932.07\n",
      "Step: 3452, Training Loss: 1.35944, LR: 0.0002314, Tokens/sec: 37176.85\n",
      "Step: 3453, Training Loss: 1.29516, LR: 0.0002311, Tokens/sec: 37840.48\n",
      "Step: 3454, Training Loss: 0.90199, LR: 0.0002309, Tokens/sec: 36583.65\n",
      "Step: 3455, Training Loss: 0.78395, LR: 0.0002306, Tokens/sec: 38143.50\n",
      "Step: 3456, Training Loss: 0.87887, LR: 0.0002304, Tokens/sec: 38069.07\n",
      "Step: 3457, Training Loss: 1.61865, LR: 0.0002301, Tokens/sec: 39355.65\n",
      "Step: 3458, Training Loss: 0.87721, LR: 0.0002299, Tokens/sec: 40959.71\n",
      "Step: 3459, Training Loss: 1.77686, LR: 0.0002296, Tokens/sec: 40812.37\n",
      "Step: 3460, Training Loss: 1.01237, LR: 0.0002294, Tokens/sec: 41078.26\n",
      "Step: 3461, Training Loss: 1.77522, LR: 0.0002291, Tokens/sec: 40956.68\n",
      "Step: 3462, Training Loss: 0.96492, LR: 0.0002289, Tokens/sec: 40517.23\n",
      "Step: 3463, Training Loss: 1.49510, LR: 0.0002286, Tokens/sec: 40612.55\n",
      "Step: 3464, Training Loss: 0.88800, LR: 0.0002284, Tokens/sec: 40694.43\n",
      "Step: 3465, Training Loss: 0.91827, LR: 0.0002281, Tokens/sec: 38993.32\n",
      "Step: 3466, Training Loss: 1.10286, LR: 0.0002279, Tokens/sec: 41320.21\n",
      "Step: 3467, Training Loss: 1.52871, LR: 0.0002277, Tokens/sec: 29005.45\n",
      "Step: 3468, Training Loss: 0.98211, LR: 0.0002274, Tokens/sec: 29323.12\n",
      "Step: 3469, Training Loss: 1.10806, LR: 0.0002272, Tokens/sec: 35920.50\n",
      "Step: 3470, Training Loss: 1.04173, LR: 0.0002269, Tokens/sec: 34797.88\n",
      "Step: 3471, Training Loss: 1.22846, LR: 0.0002267, Tokens/sec: 29416.37\n",
      "Step: 3472, Training Loss: 0.66571, LR: 0.0002264, Tokens/sec: 31015.33\n",
      "Step: 3473, Training Loss: 1.21256, LR: 0.0002262, Tokens/sec: 34441.99\n",
      "Step: 3474, Training Loss: 1.19292, LR: 0.0002259, Tokens/sec: 37433.21\n",
      "Step: 3475, Training Loss: 0.76917, LR: 0.0002257, Tokens/sec: 30394.95\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 3475, Eval Loss: 1.29704\n",
      "Step: 3476, Training Loss: 2.30787, LR: 0.0002254, Tokens/sec: 33506.73\n",
      "Step: 3477, Training Loss: 1.21846, LR: 0.0002252, Tokens/sec: 33319.94\n",
      "Step: 3478, Training Loss: 1.14079, LR: 0.0002250, Tokens/sec: 37276.03\n",
      "Step: 3479, Training Loss: 1.66433, LR: 0.0002247, Tokens/sec: 30947.46\n",
      "Step: 3480, Training Loss: 0.73898, LR: 0.0002245, Tokens/sec: 41062.64\n",
      "Step: 3481, Training Loss: 0.85582, LR: 0.0002242, Tokens/sec: 36769.78\n",
      "Step: 3482, Training Loss: 1.39633, LR: 0.0002240, Tokens/sec: 37591.24\n",
      "Step: 3483, Training Loss: 1.51302, LR: 0.0002237, Tokens/sec: 28210.65\n",
      "Step: 3484, Training Loss: 0.91954, LR: 0.0002235, Tokens/sec: 39807.45\n",
      "Step: 3485, Training Loss: 1.15305, LR: 0.0002233, Tokens/sec: 33650.92\n",
      "Step: 3486, Training Loss: 0.99196, LR: 0.0002230, Tokens/sec: 33697.50\n",
      "Step: 3487, Training Loss: 0.81201, LR: 0.0002228, Tokens/sec: 39359.11\n",
      "Step: 3488, Training Loss: 1.13644, LR: 0.0002225, Tokens/sec: 37521.55\n",
      "Step: 3489, Training Loss: 0.97091, LR: 0.0002223, Tokens/sec: 36757.24\n",
      "Step: 3490, Training Loss: 0.71740, LR: 0.0002220, Tokens/sec: 33961.84\n",
      "Step: 3491, Training Loss: 1.12228, LR: 0.0002218, Tokens/sec: 36989.82\n",
      "Step: 3492, Training Loss: 1.04179, LR: 0.0002216, Tokens/sec: 38513.06\n",
      "Step: 3493, Training Loss: 1.13721, LR: 0.0002213, Tokens/sec: 36446.95\n",
      "Step: 3494, Training Loss: 1.28949, LR: 0.0002211, Tokens/sec: 33552.02\n",
      "Step: 3495, Training Loss: 0.75528, LR: 0.0002208, Tokens/sec: 34858.85\n",
      "Step: 3496, Training Loss: 0.81850, LR: 0.0002206, Tokens/sec: 34716.09\n",
      "Step: 3497, Training Loss: 0.93650, LR: 0.0002204, Tokens/sec: 37484.13\n",
      "Step: 3498, Training Loss: 1.10469, LR: 0.0002201, Tokens/sec: 35209.08\n",
      "Step: 3499, Training Loss: 1.16760, LR: 0.0002199, Tokens/sec: 36068.18\n",
      "Step: 3500, Training Loss: 1.36332, LR: 0.0002196, Tokens/sec: 39238.89\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 3500, Eval Loss: 1.14895\n",
      "Step: 3501, Training Loss: 1.12526, LR: 0.0002194, Tokens/sec: 36920.65\n",
      "Step: 3502, Training Loss: 1.66977, LR: 0.0002192, Tokens/sec: 38377.09\n",
      "Step: 3503, Training Loss: 1.13377, LR: 0.0002189, Tokens/sec: 41463.89\n",
      "Step: 3504, Training Loss: 1.22915, LR: 0.0002187, Tokens/sec: 38497.84\n",
      "Step: 3505, Training Loss: 1.38876, LR: 0.0002184, Tokens/sec: 37259.30\n",
      "Step: 3506, Training Loss: 1.17429, LR: 0.0002182, Tokens/sec: 38385.08\n",
      "Step: 3507, Training Loss: 1.27325, LR: 0.0002180, Tokens/sec: 39396.47\n",
      "Step: 3508, Training Loss: 0.75775, LR: 0.0002177, Tokens/sec: 41046.26\n",
      "Step: 3509, Training Loss: 0.89677, LR: 0.0002175, Tokens/sec: 37402.37\n",
      "Step: 3510, Training Loss: 1.55729, LR: 0.0002173, Tokens/sec: 30972.96\n",
      "Step: 3511, Training Loss: 1.24959, LR: 0.0002170, Tokens/sec: 41485.78\n",
      "Step: 3512, Training Loss: 0.94271, LR: 0.0002168, Tokens/sec: 41336.28\n",
      "Step: 3513, Training Loss: 0.95811, LR: 0.0002165, Tokens/sec: 41521.74\n",
      "Step: 3514, Training Loss: 0.79333, LR: 0.0002163, Tokens/sec: 40703.29\n",
      "Step: 3515, Training Loss: 1.15423, LR: 0.0002161, Tokens/sec: 41375.77\n",
      "Step: 3516, Training Loss: 0.74939, LR: 0.0002158, Tokens/sec: 41355.24\n",
      "Step: 3517, Training Loss: 1.13162, LR: 0.0002156, Tokens/sec: 40771.86\n",
      "Step: 3518, Training Loss: 0.95719, LR: 0.0002154, Tokens/sec: 40467.77\n",
      "Step: 3519, Training Loss: 0.61955, LR: 0.0002151, Tokens/sec: 41054.00\n",
      "Step: 3520, Training Loss: 1.07686, LR: 0.0002149, Tokens/sec: 40532.60\n",
      "Step: 3521, Training Loss: 0.73399, LR: 0.0002147, Tokens/sec: 41439.93\n",
      "Step: 3522, Training Loss: 1.37646, LR: 0.0002144, Tokens/sec: 41410.90\n",
      "Step: 3523, Training Loss: 0.91798, LR: 0.0002142, Tokens/sec: 40784.41\n",
      "Step: 3524, Training Loss: 1.50036, LR: 0.0002139, Tokens/sec: 41460.68\n",
      "Step: 3525, Training Loss: 0.85998, LR: 0.0002137, Tokens/sec: 40526.34\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 3525, Eval Loss: 1.14220\n",
      "Step: 3526, Training Loss: 1.38434, LR: 0.0002135, Tokens/sec: 40623.79\n",
      "Step: 3527, Training Loss: 1.07592, LR: 0.0002132, Tokens/sec: 39688.62\n",
      "Step: 3528, Training Loss: 0.85039, LR: 0.0002130, Tokens/sec: 40099.28\n",
      "Step: 3529, Training Loss: 0.78211, LR: 0.0002128, Tokens/sec: 39405.81\n",
      "Step: 3530, Training Loss: 0.50047, LR: 0.0002125, Tokens/sec: 40552.94\n",
      "Step: 3531, Training Loss: 1.29602, LR: 0.0002123, Tokens/sec: 41128.10\n",
      "Step: 3532, Training Loss: 1.72468, LR: 0.0002121, Tokens/sec: 41468.37\n",
      "Step: 3533, Training Loss: 1.49127, LR: 0.0002118, Tokens/sec: 40570.54\n",
      "Step: 3534, Training Loss: 0.83985, LR: 0.0002116, Tokens/sec: 41526.78\n",
      "Step: 3535, Training Loss: 1.26025, LR: 0.0002114, Tokens/sec: 41339.91\n",
      "Step: 3536, Training Loss: 1.45172, LR: 0.0002111, Tokens/sec: 37529.84\n",
      "Step: 3537, Training Loss: 0.76531, LR: 0.0002109, Tokens/sec: 38044.86\n",
      "Step: 3538, Training Loss: 1.54846, LR: 0.0002107, Tokens/sec: 40021.98\n",
      "Step: 3539, Training Loss: 0.75664, LR: 0.0002104, Tokens/sec: 41469.04\n",
      "Step: 3540, Training Loss: 0.68027, LR: 0.0002102, Tokens/sec: 41480.46\n",
      "Step: 3541, Training Loss: 1.04571, LR: 0.0002100, Tokens/sec: 41253.90\n",
      "Step: 3542, Training Loss: 0.66261, LR: 0.0002098, Tokens/sec: 41452.38\n",
      "Step: 3543, Training Loss: 1.16855, LR: 0.0002095, Tokens/sec: 41312.12\n",
      "Step: 3544, Training Loss: 0.74293, LR: 0.0002093, Tokens/sec: 41384.85\n",
      "Step: 3545, Training Loss: 1.57483, LR: 0.0002091, Tokens/sec: 41466.76\n",
      "Step: 3546, Training Loss: 1.23718, LR: 0.0002088, Tokens/sec: 40312.59\n",
      "Step: 3547, Training Loss: 1.16175, LR: 0.0002086, Tokens/sec: 41487.71\n",
      "Step: 3548, Training Loss: 1.19531, LR: 0.0002084, Tokens/sec: 41192.53\n",
      "Step: 3549, Training Loss: 0.63806, LR: 0.0002081, Tokens/sec: 40390.34\n",
      "Step: 3550, Training Loss: 0.79254, LR: 0.0002079, Tokens/sec: 41385.54\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 3550, Eval Loss: 1.03599\n",
      "Step: 3551, Training Loss: 0.71265, LR: 0.0002077, Tokens/sec: 41220.00\n",
      "Step: 3552, Training Loss: 0.56761, LR: 0.0002075, Tokens/sec: 41436.16\n",
      "Step: 3553, Training Loss: 0.78187, LR: 0.0002072, Tokens/sec: 39909.02\n",
      "Step: 3554, Training Loss: 0.81201, LR: 0.0002070, Tokens/sec: 40068.70\n",
      "Step: 3555, Training Loss: 1.06962, LR: 0.0002068, Tokens/sec: 40764.87\n",
      "Step: 3556, Training Loss: 1.28611, LR: 0.0002065, Tokens/sec: 40927.42\n",
      "Step: 3557, Training Loss: 0.92237, LR: 0.0002063, Tokens/sec: 40964.88\n",
      "Step: 3558, Training Loss: 0.72571, LR: 0.0002061, Tokens/sec: 41101.87\n",
      "Step: 3559, Training Loss: 0.99577, LR: 0.0002059, Tokens/sec: 41440.89\n",
      "Step: 3560, Training Loss: 1.28715, LR: 0.0002056, Tokens/sec: 40979.36\n",
      "Step: 3561, Training Loss: 0.66178, LR: 0.0002054, Tokens/sec: 41405.36\n",
      "Step: 3562, Training Loss: 1.07438, LR: 0.0002052, Tokens/sec: 39885.14\n",
      "Step: 3563, Training Loss: 1.34306, LR: 0.0002050, Tokens/sec: 41058.97\n",
      "Step: 3564, Training Loss: 1.14719, LR: 0.0002047, Tokens/sec: 36603.11\n",
      "Step: 3565, Training Loss: 1.15162, LR: 0.0002045, Tokens/sec: 37661.08\n",
      "Step: 3566, Training Loss: 0.84650, LR: 0.0002043, Tokens/sec: 37764.20\n",
      "Step: 3567, Training Loss: 1.15729, LR: 0.0002040, Tokens/sec: 39923.76\n",
      "Step: 3568, Training Loss: 1.06329, LR: 0.0002038, Tokens/sec: 26478.91\n",
      "Step: 3569, Training Loss: 1.60684, LR: 0.0002036, Tokens/sec: 40908.37\n",
      "Step: 3570, Training Loss: 1.01116, LR: 0.0002034, Tokens/sec: 40997.36\n",
      "Step: 3571, Training Loss: 1.49605, LR: 0.0002031, Tokens/sec: 41435.40\n",
      "Step: 3572, Training Loss: 1.00946, LR: 0.0002029, Tokens/sec: 28244.78\n",
      "Step: 3573, Training Loss: 1.95183, LR: 0.0002027, Tokens/sec: 38583.58\n",
      "Step: 3574, Training Loss: 0.76783, LR: 0.0002025, Tokens/sec: 41442.18\n",
      "Step: 3575, Training Loss: 1.06987, LR: 0.0002023, Tokens/sec: 36762.64\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 3575, Eval Loss: 0.92956\n",
      "Step: 3576, Training Loss: 0.80243, LR: 0.0002020, Tokens/sec: 36242.84\n",
      "Step: 3577, Training Loss: 0.94025, LR: 0.0002018, Tokens/sec: 40538.32\n",
      "Step: 3578, Training Loss: 0.75078, LR: 0.0002016, Tokens/sec: 41138.86\n",
      "Step: 3579, Training Loss: 1.07156, LR: 0.0002014, Tokens/sec: 34921.26\n",
      "Step: 3580, Training Loss: 1.24190, LR: 0.0002011, Tokens/sec: 31928.06\n",
      "Step: 3581, Training Loss: 0.77513, LR: 0.0002009, Tokens/sec: 38985.17\n",
      "Step: 3582, Training Loss: 1.23492, LR: 0.0002007, Tokens/sec: 36950.89\n",
      "Step: 3583, Training Loss: 1.05184, LR: 0.0002005, Tokens/sec: 40220.11\n",
      "Step: 3584, Training Loss: 1.23563, LR: 0.0002002, Tokens/sec: 36765.65\n",
      "Step: 3585, Training Loss: 1.21564, LR: 0.0002000, Tokens/sec: 35422.23\n",
      "Step: 3586, Training Loss: 1.04595, LR: 0.0001998, Tokens/sec: 36865.43\n",
      "Step: 3587, Training Loss: 1.74614, LR: 0.0001996, Tokens/sec: 40809.78\n",
      "Step: 3588, Training Loss: 1.37751, LR: 0.0001994, Tokens/sec: 40958.87\n",
      "Step: 3589, Training Loss: 1.34372, LR: 0.0001991, Tokens/sec: 39614.93\n",
      "Step: 3590, Training Loss: 0.93365, LR: 0.0001989, Tokens/sec: 41088.80\n",
      "Step: 3591, Training Loss: 1.06298, LR: 0.0001987, Tokens/sec: 40448.95\n",
      "Step: 3592, Training Loss: 0.93650, LR: 0.0001985, Tokens/sec: 36893.10\n",
      "Step: 3593, Training Loss: 1.41577, LR: 0.0001983, Tokens/sec: 33336.58\n",
      "Step: 3594, Training Loss: 0.69481, LR: 0.0001980, Tokens/sec: 38925.19\n",
      "Step: 3595, Training Loss: 0.53112, LR: 0.0001978, Tokens/sec: 41267.12\n",
      "Step: 3596, Training Loss: 1.19320, LR: 0.0001976, Tokens/sec: 40078.73\n",
      "Step: 3597, Training Loss: 0.76245, LR: 0.0001974, Tokens/sec: 35522.41\n",
      "Step: 3598, Training Loss: 1.27105, LR: 0.0001972, Tokens/sec: 38327.21\n",
      "Step: 3599, Training Loss: 1.25974, LR: 0.0001969, Tokens/sec: 40273.33\n",
      "Step: 3600, Training Loss: 1.35158, LR: 0.0001967, Tokens/sec: 40582.85\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 3600, Eval Loss: 0.99886\n",
      "Step: 3601, Training Loss: 0.51189, LR: 0.0001965, Tokens/sec: 36792.91\n",
      "Step: 3602, Training Loss: 1.14611, LR: 0.0001963, Tokens/sec: 37715.36\n",
      "Step: 3603, Training Loss: 1.39358, LR: 0.0001961, Tokens/sec: 41096.85\n",
      "Step: 3604, Training Loss: 0.45221, LR: 0.0001958, Tokens/sec: 40325.69\n",
      "Step: 3605, Training Loss: 0.71101, LR: 0.0001956, Tokens/sec: 41412.81\n",
      "Step: 3606, Training Loss: 0.57456, LR: 0.0001954, Tokens/sec: 37472.27\n",
      "Step: 3607, Training Loss: 0.94788, LR: 0.0001952, Tokens/sec: 38329.06\n",
      "Step: 3608, Training Loss: 0.67305, LR: 0.0001950, Tokens/sec: 41391.37\n",
      "Step: 3609, Training Loss: 0.80174, LR: 0.0001948, Tokens/sec: 40988.78\n",
      "Step: 3610, Training Loss: 0.55723, LR: 0.0001945, Tokens/sec: 35062.40\n",
      "Step: 3611, Training Loss: 1.15290, LR: 0.0001943, Tokens/sec: 30956.67\n",
      "Step: 3612, Training Loss: 0.88186, LR: 0.0001941, Tokens/sec: 28738.99\n",
      "Step: 3613, Training Loss: 0.59398, LR: 0.0001939, Tokens/sec: 41326.60\n",
      "Step: 3614, Training Loss: 1.57234, LR: 0.0001937, Tokens/sec: 39282.35\n",
      "Step: 3615, Training Loss: 1.39896, LR: 0.0001935, Tokens/sec: 38126.47\n",
      "Step: 3616, Training Loss: 1.57516, LR: 0.0001933, Tokens/sec: 28469.33\n",
      "Step: 3617, Training Loss: 0.55320, LR: 0.0001930, Tokens/sec: 38396.98\n",
      "Step: 3618, Training Loss: 0.71633, LR: 0.0001928, Tokens/sec: 33522.99\n",
      "Step: 3619, Training Loss: 0.50411, LR: 0.0001926, Tokens/sec: 29337.26\n",
      "Step: 3620, Training Loss: 0.91615, LR: 0.0001924, Tokens/sec: 40185.68\n",
      "Step: 3621, Training Loss: 1.24934, LR: 0.0001922, Tokens/sec: 41459.13\n",
      "Step: 3622, Training Loss: 1.10228, LR: 0.0001920, Tokens/sec: 34658.37\n",
      "Step: 3623, Training Loss: 0.60533, LR: 0.0001918, Tokens/sec: 36336.14\n",
      "Step: 3624, Training Loss: 0.74021, LR: 0.0001915, Tokens/sec: 36548.59\n",
      "Step: 3625, Training Loss: 0.90397, LR: 0.0001913, Tokens/sec: 38540.30\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 3625, Eval Loss: 1.06405\n",
      "Step: 3626, Training Loss: 0.92593, LR: 0.0001911, Tokens/sec: 39589.65\n",
      "Step: 3627, Training Loss: 1.21409, LR: 0.0001909, Tokens/sec: 41267.02\n",
      "Step: 3628, Training Loss: 1.15680, LR: 0.0001907, Tokens/sec: 41459.23\n",
      "Step: 3629, Training Loss: 1.28765, LR: 0.0001905, Tokens/sec: 40439.31\n",
      "Step: 3630, Training Loss: 0.44809, LR: 0.0001903, Tokens/sec: 41172.88\n",
      "Step: 3631, Training Loss: 1.29017, LR: 0.0001900, Tokens/sec: 32514.76\n",
      "Step: 3632, Training Loss: 0.85664, LR: 0.0001898, Tokens/sec: 37748.39\n",
      "Step: 3633, Training Loss: 0.67857, LR: 0.0001896, Tokens/sec: 38078.63\n",
      "Step: 3634, Training Loss: 0.82870, LR: 0.0001894, Tokens/sec: 40803.33\n",
      "Step: 3635, Training Loss: 0.94946, LR: 0.0001892, Tokens/sec: 35326.07\n",
      "Step: 3636, Training Loss: 1.27744, LR: 0.0001890, Tokens/sec: 37433.70\n",
      "Step: 3637, Training Loss: 0.74916, LR: 0.0001888, Tokens/sec: 34309.56\n",
      "Step: 3638, Training Loss: 0.79828, LR: 0.0001886, Tokens/sec: 39159.10\n",
      "Step: 3639, Training Loss: 0.64849, LR: 0.0001884, Tokens/sec: 40290.80\n",
      "Step: 3640, Training Loss: 0.92568, LR: 0.0001882, Tokens/sec: 39370.65\n",
      "Step: 3641, Training Loss: 1.01727, LR: 0.0001879, Tokens/sec: 36448.15\n",
      "Step: 3642, Training Loss: 1.16053, LR: 0.0001877, Tokens/sec: 41357.21\n",
      "Step: 3643, Training Loss: 1.36811, LR: 0.0001875, Tokens/sec: 41341.73\n",
      "Step: 3644, Training Loss: 1.24107, LR: 0.0001873, Tokens/sec: 30800.71\n",
      "Step: 3645, Training Loss: 0.79082, LR: 0.0001871, Tokens/sec: 41305.21\n",
      "Step: 3646, Training Loss: 0.46933, LR: 0.0001869, Tokens/sec: 41263.06\n",
      "Step: 3647, Training Loss: 1.15106, LR: 0.0001867, Tokens/sec: 32896.59\n",
      "Step: 3648, Training Loss: 1.22573, LR: 0.0001865, Tokens/sec: 39588.20\n",
      "Step: 3649, Training Loss: 1.21919, LR: 0.0001863, Tokens/sec: 33307.70\n",
      "Step: 3650, Training Loss: 0.81753, LR: 0.0001861, Tokens/sec: 37419.44\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 3650, Eval Loss: 0.87956\n",
      "Step: 3651, Training Loss: 0.46650, LR: 0.0001859, Tokens/sec: 41338.29\n",
      "Step: 3652, Training Loss: 0.96542, LR: 0.0001857, Tokens/sec: 40315.82\n",
      "Step: 3653, Training Loss: 0.71613, LR: 0.0001854, Tokens/sec: 40580.40\n",
      "Step: 3654, Training Loss: 0.73886, LR: 0.0001852, Tokens/sec: 30124.17\n",
      "Step: 3655, Training Loss: 1.38932, LR: 0.0001850, Tokens/sec: 38846.82\n",
      "Step: 3656, Training Loss: 0.58209, LR: 0.0001848, Tokens/sec: 35856.59\n",
      "Step: 3657, Training Loss: 0.79803, LR: 0.0001846, Tokens/sec: 41335.27\n",
      "Step: 3658, Training Loss: 0.60799, LR: 0.0001844, Tokens/sec: 41172.64\n",
      "Step: 3659, Training Loss: 1.23581, LR: 0.0001842, Tokens/sec: 41426.02\n",
      "Step: 3660, Training Loss: 1.18053, LR: 0.0001840, Tokens/sec: 37839.07\n",
      "Step: 3661, Training Loss: 0.97312, LR: 0.0001838, Tokens/sec: 30944.76\n",
      "Step: 3662, Training Loss: 1.02014, LR: 0.0001836, Tokens/sec: 41280.81\n",
      "Step: 3663, Training Loss: 1.47072, LR: 0.0001834, Tokens/sec: 35309.69\n",
      "Step: 3664, Training Loss: 0.72817, LR: 0.0001832, Tokens/sec: 40183.82\n",
      "Step: 3665, Training Loss: 0.82172, LR: 0.0001830, Tokens/sec: 40391.06\n",
      "Step: 3666, Training Loss: 0.54391, LR: 0.0001828, Tokens/sec: 41471.28\n",
      "Step: 3667, Training Loss: 1.44546, LR: 0.0001826, Tokens/sec: 41510.07\n",
      "Step: 3668, Training Loss: 0.70184, LR: 0.0001824, Tokens/sec: 34590.85\n",
      "Step: 3669, Training Loss: 0.82771, LR: 0.0001822, Tokens/sec: 35127.92\n",
      "Step: 3670, Training Loss: 0.61346, LR: 0.0001820, Tokens/sec: 40111.99\n",
      "Step: 3671, Training Loss: 0.97484, LR: 0.0001818, Tokens/sec: 41429.90\n",
      "Step: 3672, Training Loss: 0.84372, LR: 0.0001816, Tokens/sec: 39175.23\n",
      "Step: 3673, Training Loss: 0.81537, LR: 0.0001814, Tokens/sec: 40406.31\n",
      "Step: 3674, Training Loss: 0.88649, LR: 0.0001812, Tokens/sec: 41239.10\n",
      "Step: 3675, Training Loss: 0.98538, LR: 0.0001809, Tokens/sec: 39799.25\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 3675, Eval Loss: 1.19270\n",
      "Step: 3676, Training Loss: 0.35841, LR: 0.0001807, Tokens/sec: 40917.37\n",
      "Step: 3677, Training Loss: 0.84163, LR: 0.0001805, Tokens/sec: 40251.80\n",
      "Step: 3678, Training Loss: 1.15642, LR: 0.0001803, Tokens/sec: 40071.15\n",
      "Step: 3679, Training Loss: 1.12366, LR: 0.0001801, Tokens/sec: 40666.48\n",
      "Step: 3680, Training Loss: 1.32883, LR: 0.0001799, Tokens/sec: 39421.07\n",
      "Step: 3681, Training Loss: 0.96600, LR: 0.0001797, Tokens/sec: 39593.60\n",
      "Step: 3682, Training Loss: 0.72588, LR: 0.0001795, Tokens/sec: 40582.57\n",
      "Step: 3683, Training Loss: 0.65037, LR: 0.0001793, Tokens/sec: 38241.16\n",
      "Step: 3684, Training Loss: 0.94872, LR: 0.0001791, Tokens/sec: 41443.27\n",
      "Step: 3685, Training Loss: 0.84221, LR: 0.0001789, Tokens/sec: 41430.42\n",
      "Step: 3686, Training Loss: 1.09867, LR: 0.0001787, Tokens/sec: 41434.82\n",
      "Step: 3687, Training Loss: 0.36417, LR: 0.0001785, Tokens/sec: 41441.16\n",
      "Step: 3688, Training Loss: 1.48968, LR: 0.0001783, Tokens/sec: 41425.31\n",
      "Step: 3689, Training Loss: 0.51692, LR: 0.0001781, Tokens/sec: 40891.53\n",
      "Step: 3690, Training Loss: 1.04375, LR: 0.0001779, Tokens/sec: 39639.08\n",
      "Step: 3691, Training Loss: 0.85006, LR: 0.0001777, Tokens/sec: 39379.15\n",
      "Step: 3692, Training Loss: 0.63573, LR: 0.0001776, Tokens/sec: 40388.48\n",
      "Step: 3693, Training Loss: 1.36725, LR: 0.0001774, Tokens/sec: 28299.79\n",
      "Step: 3694, Training Loss: 1.34495, LR: 0.0001772, Tokens/sec: 28722.44\n",
      "Step: 3695, Training Loss: 1.12273, LR: 0.0001770, Tokens/sec: 34390.45\n",
      "Step: 3696, Training Loss: 0.45870, LR: 0.0001768, Tokens/sec: 34660.93\n",
      "Step: 3697, Training Loss: 0.90298, LR: 0.0001766, Tokens/sec: 37651.56\n",
      "Step: 3698, Training Loss: 0.96814, LR: 0.0001764, Tokens/sec: 37408.62\n",
      "Step: 3699, Training Loss: 0.59091, LR: 0.0001762, Tokens/sec: 31362.21\n",
      "Step: 3700, Training Loss: 0.83094, LR: 0.0001760, Tokens/sec: 29295.42\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 3700, Eval Loss: 0.86363\n",
      "Step: 3701, Training Loss: 0.78015, LR: 0.0001758, Tokens/sec: 33650.02\n",
      "Step: 3702, Training Loss: 1.46752, LR: 0.0001756, Tokens/sec: 33385.51\n",
      "Step: 3703, Training Loss: 0.68682, LR: 0.0001754, Tokens/sec: 36669.78\n",
      "Step: 3704, Training Loss: 0.44575, LR: 0.0001752, Tokens/sec: 37376.43\n",
      "Step: 3705, Training Loss: 0.88224, LR: 0.0001750, Tokens/sec: 34706.83\n",
      "Step: 3706, Training Loss: 0.83542, LR: 0.0001748, Tokens/sec: 38526.91\n",
      "Step: 3707, Training Loss: 0.53273, LR: 0.0001746, Tokens/sec: 34931.72\n",
      "Step: 3708, Training Loss: 0.86745, LR: 0.0001744, Tokens/sec: 35237.78\n",
      "Step: 3709, Training Loss: 1.08904, LR: 0.0001742, Tokens/sec: 37778.53\n",
      "Step: 3710, Training Loss: 0.74022, LR: 0.0001740, Tokens/sec: 36966.68\n",
      "Step: 3711, Training Loss: 1.05006, LR: 0.0001738, Tokens/sec: 35734.20\n",
      "Step: 3712, Training Loss: 0.92339, LR: 0.0001736, Tokens/sec: 36278.76\n",
      "Step: 3713, Training Loss: 1.10555, LR: 0.0001734, Tokens/sec: 36115.40\n",
      "Step: 3714, Training Loss: 0.36073, LR: 0.0001732, Tokens/sec: 36806.07\n",
      "Step: 3715, Training Loss: 1.24274, LR: 0.0001731, Tokens/sec: 32884.45\n",
      "Step: 3716, Training Loss: 0.64681, LR: 0.0001729, Tokens/sec: 35388.92\n",
      "Step: 3717, Training Loss: 0.72802, LR: 0.0001727, Tokens/sec: 29081.01\n",
      "Step: 3718, Training Loss: 1.58193, LR: 0.0001725, Tokens/sec: 33680.45\n",
      "Step: 3719, Training Loss: 0.96865, LR: 0.0001723, Tokens/sec: 35243.86\n",
      "Step: 3720, Training Loss: 0.84315, LR: 0.0001721, Tokens/sec: 36738.88\n",
      "Step: 3721, Training Loss: 0.87999, LR: 0.0001719, Tokens/sec: 37267.38\n",
      "Step: 3722, Training Loss: 1.19061, LR: 0.0001717, Tokens/sec: 36002.35\n",
      "Step: 3723, Training Loss: 0.59041, LR: 0.0001715, Tokens/sec: 36494.41\n",
      "Step: 3724, Training Loss: 0.86517, LR: 0.0001713, Tokens/sec: 34714.27\n",
      "Step: 3725, Training Loss: 1.00865, LR: 0.0001711, Tokens/sec: 37344.15\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 3725, Eval Loss: 0.82622\n",
      "Step: 3726, Training Loss: 0.86317, LR: 0.0001710, Tokens/sec: 40742.32\n",
      "Step: 3727, Training Loss: 2.16458, LR: 0.0001708, Tokens/sec: 38227.30\n",
      "Step: 3728, Training Loss: 0.50996, LR: 0.0001706, Tokens/sec: 34240.18\n",
      "Step: 3729, Training Loss: 1.22770, LR: 0.0001704, Tokens/sec: 37781.39\n",
      "Step: 3730, Training Loss: 0.90131, LR: 0.0001702, Tokens/sec: 29423.79\n",
      "Step: 3731, Training Loss: 0.58590, LR: 0.0001700, Tokens/sec: 38218.02\n",
      "Step: 3732, Training Loss: 0.94846, LR: 0.0001698, Tokens/sec: 30971.27\n",
      "Step: 3733, Training Loss: 1.23301, LR: 0.0001696, Tokens/sec: 30528.82\n",
      "Step: 3734, Training Loss: 0.85929, LR: 0.0001694, Tokens/sec: 39931.61\n",
      "Step: 3735, Training Loss: 1.33008, LR: 0.0001692, Tokens/sec: 41387.43\n",
      "Step: 3736, Training Loss: 0.92913, LR: 0.0001691, Tokens/sec: 40142.64\n",
      "Step: 3737, Training Loss: 1.11487, LR: 0.0001689, Tokens/sec: 40660.85\n",
      "Step: 3738, Training Loss: 0.93086, LR: 0.0001687, Tokens/sec: 40692.29\n",
      "Step: 3739, Training Loss: 0.93418, LR: 0.0001685, Tokens/sec: 39614.69\n",
      "Step: 3740, Training Loss: 0.78870, LR: 0.0001683, Tokens/sec: 41468.48\n",
      "Step: 3741, Training Loss: 0.63652, LR: 0.0001681, Tokens/sec: 37384.33\n",
      "Step: 3742, Training Loss: 0.79183, LR: 0.0001679, Tokens/sec: 28056.28\n",
      "Step: 3743, Training Loss: 1.48381, LR: 0.0001678, Tokens/sec: 37729.18\n",
      "Step: 3744, Training Loss: 0.88783, LR: 0.0001676, Tokens/sec: 41140.39\n",
      "Step: 3745, Training Loss: 1.67067, LR: 0.0001674, Tokens/sec: 38504.19\n",
      "Step: 3746, Training Loss: 1.13042, LR: 0.0001672, Tokens/sec: 39633.89\n",
      "Step: 3747, Training Loss: 0.58946, LR: 0.0001670, Tokens/sec: 39381.00\n",
      "Step: 3748, Training Loss: 0.96926, LR: 0.0001668, Tokens/sec: 40745.41\n",
      "Step: 3749, Training Loss: 1.08445, LR: 0.0001666, Tokens/sec: 41222.71\n",
      "Step: 3750, Training Loss: 0.93840, LR: 0.0001665, Tokens/sec: 41033.45\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 3750, Eval Loss: 0.92405\n",
      "Step: 3751, Training Loss: 1.45960, LR: 0.0001663, Tokens/sec: 41150.61\n",
      "Step: 3752, Training Loss: 0.77903, LR: 0.0001661, Tokens/sec: 41295.72\n",
      "Step: 3753, Training Loss: 0.67266, LR: 0.0001659, Tokens/sec: 34279.95\n",
      "Step: 3754, Training Loss: 0.65692, LR: 0.0001657, Tokens/sec: 36996.57\n",
      "Step: 3755, Training Loss: 0.77064, LR: 0.0001655, Tokens/sec: 39822.87\n",
      "Step: 3756, Training Loss: 1.07821, LR: 0.0001654, Tokens/sec: 40814.05\n",
      "Step: 3757, Training Loss: 1.20930, LR: 0.0001652, Tokens/sec: 41269.39\n",
      "Step: 3758, Training Loss: 0.90472, LR: 0.0001650, Tokens/sec: 39827.45\n",
      "Step: 3759, Training Loss: 1.42445, LR: 0.0001648, Tokens/sec: 41328.84\n",
      "Step: 3760, Training Loss: 1.31446, LR: 0.0001646, Tokens/sec: 41313.45\n",
      "Step: 3761, Training Loss: 0.83730, LR: 0.0001644, Tokens/sec: 41305.80\n",
      "Step: 3762, Training Loss: 0.59894, LR: 0.0001643, Tokens/sec: 39166.68\n",
      "Step: 3763, Training Loss: 0.79139, LR: 0.0001641, Tokens/sec: 38416.91\n",
      "Step: 3764, Training Loss: 0.63361, LR: 0.0001639, Tokens/sec: 35644.57\n",
      "Step: 3765, Training Loss: 1.18946, LR: 0.0001637, Tokens/sec: 39920.15\n",
      "Step: 3766, Training Loss: 1.17388, LR: 0.0001635, Tokens/sec: 40664.56\n",
      "Step: 3767, Training Loss: 0.81781, LR: 0.0001634, Tokens/sec: 29827.19\n",
      "Step: 3768, Training Loss: 0.73299, LR: 0.0001632, Tokens/sec: 33207.13\n",
      "Step: 3769, Training Loss: 0.74143, LR: 0.0001630, Tokens/sec: 24916.56\n",
      "Step: 3770, Training Loss: 0.60301, LR: 0.0001628, Tokens/sec: 29807.77\n",
      "Step: 3771, Training Loss: 1.03405, LR: 0.0001626, Tokens/sec: 38979.09\n",
      "Step: 3772, Training Loss: 0.59952, LR: 0.0001625, Tokens/sec: 31323.38\n",
      "Step: 3773, Training Loss: 0.65419, LR: 0.0001623, Tokens/sec: 41494.54\n",
      "Step: 3774, Training Loss: 1.29463, LR: 0.0001621, Tokens/sec: 40142.17\n",
      "Step: 3775, Training Loss: 0.80855, LR: 0.0001619, Tokens/sec: 41613.59\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 3775, Eval Loss: 0.88445\n",
      "Step: 3776, Training Loss: 0.59529, LR: 0.0001617, Tokens/sec: 39620.23\n",
      "Step: 3777, Training Loss: 0.75194, LR: 0.0001616, Tokens/sec: 40774.50\n",
      "Step: 3778, Training Loss: 1.05819, LR: 0.0001614, Tokens/sec: 41345.82\n",
      "Step: 3779, Training Loss: 0.90393, LR: 0.0001612, Tokens/sec: 38657.00\n",
      "Step: 3780, Training Loss: 0.89933, LR: 0.0001610, Tokens/sec: 41311.25\n",
      "Step: 3781, Training Loss: 0.90795, LR: 0.0001608, Tokens/sec: 27737.38\n",
      "Step: 3782, Training Loss: 0.54415, LR: 0.0001607, Tokens/sec: 30799.07\n",
      "Step: 3783, Training Loss: 0.92646, LR: 0.0001605, Tokens/sec: 38339.85\n",
      "Step: 3784, Training Loss: 0.60163, LR: 0.0001603, Tokens/sec: 40087.82\n",
      "Step: 3785, Training Loss: 0.65776, LR: 0.0001601, Tokens/sec: 40686.67\n",
      "Step: 3786, Training Loss: 0.77117, LR: 0.0001600, Tokens/sec: 41339.23\n",
      "Step: 3787, Training Loss: 1.02657, LR: 0.0001598, Tokens/sec: 40849.25\n",
      "Step: 3788, Training Loss: 0.33029, LR: 0.0001596, Tokens/sec: 41292.86\n",
      "Step: 3789, Training Loss: 1.19924, LR: 0.0001594, Tokens/sec: 41353.64\n",
      "Step: 3790, Training Loss: 0.78620, LR: 0.0001593, Tokens/sec: 36857.35\n",
      "Step: 3791, Training Loss: 0.49538, LR: 0.0001591, Tokens/sec: 41366.86\n",
      "Step: 3792, Training Loss: 1.33345, LR: 0.0001589, Tokens/sec: 41393.08\n",
      "Step: 3793, Training Loss: 0.87469, LR: 0.0001587, Tokens/sec: 41003.69\n",
      "Step: 3794, Training Loss: 0.74849, LR: 0.0001586, Tokens/sec: 30979.65\n",
      "Step: 3795, Training Loss: 1.21839, LR: 0.0001584, Tokens/sec: 40318.93\n",
      "Step: 3796, Training Loss: 1.26448, LR: 0.0001582, Tokens/sec: 41412.96\n",
      "Step: 3797, Training Loss: 0.90105, LR: 0.0001580, Tokens/sec: 37394.44\n",
      "Step: 3798, Training Loss: 0.51124, LR: 0.0001579, Tokens/sec: 40651.97\n",
      "Step: 3799, Training Loss: 1.03995, LR: 0.0001577, Tokens/sec: 41138.93\n",
      "Step: 3800, Training Loss: 0.54534, LR: 0.0001575, Tokens/sec: 40517.63\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 3800, Eval Loss: 0.87149\n",
      "Step: 3801, Training Loss: 0.66543, LR: 0.0001574, Tokens/sec: 41300.17\n",
      "Step: 3802, Training Loss: 0.84016, LR: 0.0001572, Tokens/sec: 41320.74\n",
      "Step: 3803, Training Loss: 0.37458, LR: 0.0001570, Tokens/sec: 41071.34\n",
      "Step: 3804, Training Loss: 0.68371, LR: 0.0001568, Tokens/sec: 41264.85\n",
      "Step: 3805, Training Loss: 1.11666, LR: 0.0001567, Tokens/sec: 38873.56\n",
      "Step: 3806, Training Loss: 1.11734, LR: 0.0001565, Tokens/sec: 41381.78\n",
      "Step: 3807, Training Loss: 0.94957, LR: 0.0001563, Tokens/sec: 34228.81\n",
      "Step: 3808, Training Loss: 0.92428, LR: 0.0001562, Tokens/sec: 40175.14\n",
      "Step: 3809, Training Loss: 0.87418, LR: 0.0001560, Tokens/sec: 24297.68\n",
      "Step: 3810, Training Loss: 0.74744, LR: 0.0001558, Tokens/sec: 40070.53\n",
      "Step: 3811, Training Loss: 0.86217, LR: 0.0001556, Tokens/sec: 35894.05\n",
      "Step: 3812, Training Loss: 0.67060, LR: 0.0001555, Tokens/sec: 41239.70\n",
      "Step: 3813, Training Loss: 0.43175, LR: 0.0001553, Tokens/sec: 40954.43\n",
      "Step: 3814, Training Loss: 1.19521, LR: 0.0001551, Tokens/sec: 41377.18\n",
      "Step: 3815, Training Loss: 0.59223, LR: 0.0001550, Tokens/sec: 40813.02\n",
      "Step: 3816, Training Loss: 0.66663, LR: 0.0001548, Tokens/sec: 41397.83\n",
      "Step: 3817, Training Loss: 0.50194, LR: 0.0001546, Tokens/sec: 41342.48\n",
      "Step: 3818, Training Loss: 0.75493, LR: 0.0001545, Tokens/sec: 39669.12\n",
      "Step: 3819, Training Loss: 0.72659, LR: 0.0001543, Tokens/sec: 41347.14\n",
      "Step: 3820, Training Loss: 0.86883, LR: 0.0001541, Tokens/sec: 41403.62\n",
      "Step: 3821, Training Loss: 0.85680, LR: 0.0001540, Tokens/sec: 41392.75\n",
      "Step: 3822, Training Loss: 0.51508, LR: 0.0001538, Tokens/sec: 41319.82\n",
      "Step: 3823, Training Loss: 1.09845, LR: 0.0001536, Tokens/sec: 40699.47\n",
      "Step: 3824, Training Loss: 0.82856, LR: 0.0001535, Tokens/sec: 41415.47\n",
      "Step: 3825, Training Loss: 0.94739, LR: 0.0001533, Tokens/sec: 41432.28\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 3825, Eval Loss: 0.78296\n",
      "Step: 3826, Training Loss: 0.94361, LR: 0.0001531, Tokens/sec: 40909.08\n",
      "Step: 3827, Training Loss: 0.90971, LR: 0.0001530, Tokens/sec: 41390.01\n",
      "Step: 3828, Training Loss: 1.29475, LR: 0.0001528, Tokens/sec: 41035.05\n",
      "Step: 3829, Training Loss: 0.56525, LR: 0.0001526, Tokens/sec: 41416.19\n",
      "Step: 3830, Training Loss: 0.75939, LR: 0.0001525, Tokens/sec: 41380.03\n",
      "Step: 3831, Training Loss: 0.58444, LR: 0.0001523, Tokens/sec: 40607.17\n",
      "Step: 3832, Training Loss: 0.60565, LR: 0.0001521, Tokens/sec: 41420.86\n",
      "Step: 3833, Training Loss: 0.72524, LR: 0.0001520, Tokens/sec: 41044.34\n",
      "Step: 3834, Training Loss: 1.09259, LR: 0.0001518, Tokens/sec: 40847.93\n",
      "Step: 3835, Training Loss: 0.89266, LR: 0.0001516, Tokens/sec: 41418.79\n",
      "Step: 3836, Training Loss: 0.62714, LR: 0.0001515, Tokens/sec: 41236.97\n",
      "Step: 3837, Training Loss: 0.98167, LR: 0.0001513, Tokens/sec: 41318.92\n",
      "Step: 3838, Training Loss: 0.62383, LR: 0.0001511, Tokens/sec: 41248.89\n",
      "Step: 3839, Training Loss: 0.59973, LR: 0.0001510, Tokens/sec: 40417.00\n",
      "Step: 3840, Training Loss: 1.03159, LR: 0.0001508, Tokens/sec: 41300.89\n",
      "Step: 3841, Training Loss: 0.96603, LR: 0.0001507, Tokens/sec: 40823.39\n",
      "Step: 3842, Training Loss: 0.74728, LR: 0.0001505, Tokens/sec: 39894.70\n",
      "Step: 3843, Training Loss: 0.99132, LR: 0.0001503, Tokens/sec: 40353.69\n",
      "Step: 3844, Training Loss: 0.64643, LR: 0.0001502, Tokens/sec: 41256.83\n",
      "Step: 3845, Training Loss: 0.40995, LR: 0.0001500, Tokens/sec: 40421.92\n",
      "Step: 3846, Training Loss: 0.76739, LR: 0.0001498, Tokens/sec: 41104.66\n",
      "Step: 3847, Training Loss: 0.77955, LR: 0.0001497, Tokens/sec: 41338.17\n",
      "Step: 3848, Training Loss: 0.80695, LR: 0.0001495, Tokens/sec: 41436.12\n",
      "Step: 3849, Training Loss: 0.61667, LR: 0.0001494, Tokens/sec: 39305.23\n",
      "Step: 3850, Training Loss: 0.79095, LR: 0.0001492, Tokens/sec: 40658.23\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 3850, Eval Loss: 0.80192\n",
      "Step: 3851, Training Loss: 1.08583, LR: 0.0001490, Tokens/sec: 41339.25\n",
      "Step: 3852, Training Loss: 0.52068, LR: 0.0001489, Tokens/sec: 41440.04\n",
      "Step: 3853, Training Loss: 0.90159, LR: 0.0001487, Tokens/sec: 41256.71\n",
      "Step: 3854, Training Loss: 0.72000, LR: 0.0001486, Tokens/sec: 41364.83\n",
      "Step: 3855, Training Loss: 1.15310, LR: 0.0001484, Tokens/sec: 40973.02\n",
      "Step: 3856, Training Loss: 0.38077, LR: 0.0001482, Tokens/sec: 41314.04\n",
      "Step: 3857, Training Loss: 0.95012, LR: 0.0001481, Tokens/sec: 41072.08\n",
      "Step: 3858, Training Loss: 0.70521, LR: 0.0001479, Tokens/sec: 41441.18\n",
      "Step: 3859, Training Loss: 0.59201, LR: 0.0001478, Tokens/sec: 40389.81\n",
      "Step: 3860, Training Loss: 0.91214, LR: 0.0001476, Tokens/sec: 41473.52\n",
      "Step: 3861, Training Loss: 0.61940, LR: 0.0001474, Tokens/sec: 41337.84\n",
      "Step: 3862, Training Loss: 0.47067, LR: 0.0001473, Tokens/sec: 41274.72\n",
      "Step: 3863, Training Loss: 0.93184, LR: 0.0001471, Tokens/sec: 40206.40\n",
      "Step: 3864, Training Loss: 0.81835, LR: 0.0001470, Tokens/sec: 41430.17\n",
      "Step: 3865, Training Loss: 0.92475, LR: 0.0001468, Tokens/sec: 41394.33\n",
      "Step: 3866, Training Loss: 0.96263, LR: 0.0001467, Tokens/sec: 40103.48\n",
      "Step: 3867, Training Loss: 0.57343, LR: 0.0001465, Tokens/sec: 41431.94\n",
      "Step: 3868, Training Loss: 0.56205, LR: 0.0001464, Tokens/sec: 40360.65\n",
      "Step: 3869, Training Loss: 0.32492, LR: 0.0001462, Tokens/sec: 41445.11\n",
      "Step: 3870, Training Loss: 0.39749, LR: 0.0001460, Tokens/sec: 39329.44\n",
      "Step: 3871, Training Loss: 1.39583, LR: 0.0001459, Tokens/sec: 40800.68\n",
      "Step: 3872, Training Loss: 0.42805, LR: 0.0001457, Tokens/sec: 40314.75\n",
      "Step: 3873, Training Loss: 0.62978, LR: 0.0001456, Tokens/sec: 41386.50\n",
      "Step: 3874, Training Loss: 0.97789, LR: 0.0001454, Tokens/sec: 40486.28\n",
      "Step: 3875, Training Loss: 1.21675, LR: 0.0001453, Tokens/sec: 41423.94\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 3875, Eval Loss: 0.74870\n",
      "Step: 3876, Training Loss: 0.44416, LR: 0.0001451, Tokens/sec: 41295.90\n",
      "Step: 3877, Training Loss: 1.00154, LR: 0.0001450, Tokens/sec: 41416.81\n",
      "Step: 3878, Training Loss: 1.37445, LR: 0.0001448, Tokens/sec: 39587.41\n",
      "Step: 3879, Training Loss: 1.34022, LR: 0.0001447, Tokens/sec: 40910.17\n",
      "Step: 3880, Training Loss: 0.50743, LR: 0.0001445, Tokens/sec: 41338.16\n",
      "Step: 3881, Training Loss: 0.89233, LR: 0.0001443, Tokens/sec: 41512.37\n",
      "Step: 3882, Training Loss: 0.66963, LR: 0.0001442, Tokens/sec: 41269.77\n",
      "Step: 3883, Training Loss: 1.06044, LR: 0.0001440, Tokens/sec: 41405.67\n",
      "Step: 3884, Training Loss: 0.77148, LR: 0.0001439, Tokens/sec: 41134.64\n",
      "Step: 3885, Training Loss: 1.24745, LR: 0.0001437, Tokens/sec: 41344.07\n",
      "Step: 3886, Training Loss: 0.38662, LR: 0.0001436, Tokens/sec: 41293.52\n",
      "Step: 3887, Training Loss: 0.74251, LR: 0.0001434, Tokens/sec: 40534.70\n",
      "Step: 3888, Training Loss: 0.51829, LR: 0.0001433, Tokens/sec: 41431.65\n",
      "Step: 3889, Training Loss: 0.75877, LR: 0.0001431, Tokens/sec: 41465.40\n",
      "Step: 3890, Training Loss: 0.87159, LR: 0.0001430, Tokens/sec: 40934.00\n",
      "Step: 3891, Training Loss: 0.43551, LR: 0.0001428, Tokens/sec: 40436.46\n",
      "Step: 3892, Training Loss: 0.49179, LR: 0.0001427, Tokens/sec: 41353.02\n",
      "Step: 3893, Training Loss: 0.71520, LR: 0.0001425, Tokens/sec: 40340.03\n",
      "Step: 3894, Training Loss: 0.72054, LR: 0.0001424, Tokens/sec: 41378.58\n",
      "Step: 3895, Training Loss: 0.69798, LR: 0.0001422, Tokens/sec: 40735.33\n",
      "Step: 3896, Training Loss: 0.96315, LR: 0.0001421, Tokens/sec: 41354.49\n",
      "Step: 3897, Training Loss: 0.74327, LR: 0.0001419, Tokens/sec: 40389.62\n",
      "Step: 3898, Training Loss: 0.69834, LR: 0.0001418, Tokens/sec: 40598.00\n",
      "Step: 3899, Training Loss: 0.97750, LR: 0.0001416, Tokens/sec: 41229.65\n",
      "Step: 3900, Training Loss: 0.48549, LR: 0.0001415, Tokens/sec: 41001.71\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 3900, Eval Loss: 0.71445\n",
      "Step: 3901, Training Loss: 0.93079, LR: 0.0001413, Tokens/sec: 41240.67\n",
      "Step: 3902, Training Loss: 0.56250, LR: 0.0001412, Tokens/sec: 40390.63\n",
      "Step: 3903, Training Loss: 0.36513, LR: 0.0001410, Tokens/sec: 40568.65\n",
      "Step: 3904, Training Loss: 0.50244, LR: 0.0001409, Tokens/sec: 41354.73\n",
      "Step: 3905, Training Loss: 0.70409, LR: 0.0001408, Tokens/sec: 41325.85\n",
      "Step: 3906, Training Loss: 0.88727, LR: 0.0001406, Tokens/sec: 40559.29\n",
      "Step: 3907, Training Loss: 0.74493, LR: 0.0001405, Tokens/sec: 40494.49\n",
      "Step: 3908, Training Loss: 1.08729, LR: 0.0001403, Tokens/sec: 41129.87\n",
      "Step: 3909, Training Loss: 1.05410, LR: 0.0001402, Tokens/sec: 39123.50\n",
      "Step: 3910, Training Loss: 0.70533, LR: 0.0001400, Tokens/sec: 41361.84\n",
      "Step: 3911, Training Loss: 0.98968, LR: 0.0001399, Tokens/sec: 40533.67\n",
      "Step: 3912, Training Loss: 0.86059, LR: 0.0001397, Tokens/sec: 41361.13\n",
      "Step: 3913, Training Loss: 0.86696, LR: 0.0001396, Tokens/sec: 40326.59\n",
      "Step: 3914, Training Loss: 0.70895, LR: 0.0001394, Tokens/sec: 40911.89\n",
      "Step: 3915, Training Loss: 1.01981, LR: 0.0001393, Tokens/sec: 40407.56\n",
      "Step: 3916, Training Loss: 0.60560, LR: 0.0001392, Tokens/sec: 41416.71\n",
      "Step: 3917, Training Loss: 0.95670, LR: 0.0001390, Tokens/sec: 41403.22\n",
      "Step: 3918, Training Loss: 0.56079, LR: 0.0001389, Tokens/sec: 41425.16\n",
      "Step: 3919, Training Loss: 0.52750, LR: 0.0001387, Tokens/sec: 40758.09\n",
      "Step: 3920, Training Loss: 0.59693, LR: 0.0001386, Tokens/sec: 41236.02\n",
      "Step: 3921, Training Loss: 1.16042, LR: 0.0001384, Tokens/sec: 40322.78\n",
      "Step: 3922, Training Loss: 0.50874, LR: 0.0001383, Tokens/sec: 40580.70\n",
      "Step: 3923, Training Loss: 1.31017, LR: 0.0001382, Tokens/sec: 41392.55\n",
      "Step: 3924, Training Loss: 0.74692, LR: 0.0001380, Tokens/sec: 41393.69\n",
      "Step: 3925, Training Loss: 0.36981, LR: 0.0001379, Tokens/sec: 41389.56\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 3925, Eval Loss: 0.70861\n",
      "Step: 3926, Training Loss: 0.60138, LR: 0.0001377, Tokens/sec: 41355.39\n",
      "Step: 3927, Training Loss: 0.51223, LR: 0.0001376, Tokens/sec: 40579.50\n",
      "Step: 3928, Training Loss: 0.47850, LR: 0.0001374, Tokens/sec: 41334.55\n",
      "Step: 3929, Training Loss: 0.63569, LR: 0.0001373, Tokens/sec: 41248.47\n",
      "Step: 3930, Training Loss: 0.71961, LR: 0.0001372, Tokens/sec: 40509.63\n",
      "Step: 3931, Training Loss: 0.82798, LR: 0.0001370, Tokens/sec: 40381.36\n",
      "Step: 3932, Training Loss: 0.74212, LR: 0.0001369, Tokens/sec: 41515.20\n",
      "Step: 3933, Training Loss: 0.80947, LR: 0.0001367, Tokens/sec: 41406.46\n",
      "Step: 3934, Training Loss: 0.55240, LR: 0.0001366, Tokens/sec: 40474.85\n",
      "Step: 3935, Training Loss: 0.64035, LR: 0.0001365, Tokens/sec: 40872.35\n",
      "Step: 3936, Training Loss: 0.60482, LR: 0.0001363, Tokens/sec: 41407.34\n",
      "Step: 3937, Training Loss: 1.21244, LR: 0.0001362, Tokens/sec: 41382.30\n",
      "Step: 3938, Training Loss: 0.48627, LR: 0.0001360, Tokens/sec: 41025.81\n",
      "Step: 3939, Training Loss: 0.39258, LR: 0.0001359, Tokens/sec: 41428.00\n",
      "Step: 3940, Training Loss: 0.75161, LR: 0.0001358, Tokens/sec: 41467.83\n",
      "Step: 3941, Training Loss: 0.66200, LR: 0.0001356, Tokens/sec: 40316.03\n",
      "Step: 3942, Training Loss: 0.72480, LR: 0.0001355, Tokens/sec: 33050.98\n",
      "Step: 3943, Training Loss: 0.60520, LR: 0.0001354, Tokens/sec: 38286.41\n",
      "Step: 3944, Training Loss: 0.90351, LR: 0.0001352, Tokens/sec: 41235.23\n",
      "Step: 3945, Training Loss: 0.78285, LR: 0.0001351, Tokens/sec: 25270.46\n",
      "Step: 3946, Training Loss: 0.51184, LR: 0.0001349, Tokens/sec: 41268.03\n",
      "Step: 3947, Training Loss: 0.44553, LR: 0.0001348, Tokens/sec: 41315.97\n",
      "Step: 3948, Training Loss: 0.47175, LR: 0.0001347, Tokens/sec: 41402.40\n",
      "Step: 3949, Training Loss: 0.42260, LR: 0.0001345, Tokens/sec: 37095.61\n",
      "Step: 3950, Training Loss: 0.44392, LR: 0.0001344, Tokens/sec: 38379.10\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 3950, Eval Loss: 0.72271\n",
      "Step: 3951, Training Loss: 1.02433, LR: 0.0001343, Tokens/sec: 41244.07\n",
      "Step: 3952, Training Loss: 0.57242, LR: 0.0001341, Tokens/sec: 31651.23\n",
      "Step: 3953, Training Loss: 1.06623, LR: 0.0001340, Tokens/sec: 38587.27\n",
      "Step: 3954, Training Loss: 0.58330, LR: 0.0001339, Tokens/sec: 41089.95\n",
      "Step: 3955, Training Loss: 0.69964, LR: 0.0001337, Tokens/sec: 40847.60\n",
      "Step: 3956, Training Loss: 0.33643, LR: 0.0001336, Tokens/sec: 35555.54\n",
      "Step: 3957, Training Loss: 0.73842, LR: 0.0001335, Tokens/sec: 27857.14\n",
      "Step: 3958, Training Loss: 0.63417, LR: 0.0001333, Tokens/sec: 37171.78\n",
      "Step: 3959, Training Loss: 0.46302, LR: 0.0001332, Tokens/sec: 41317.05\n",
      "Step: 3960, Training Loss: 0.90073, LR: 0.0001331, Tokens/sec: 40139.44\n",
      "Step: 3961, Training Loss: 0.63015, LR: 0.0001329, Tokens/sec: 41322.57\n",
      "Step: 3962, Training Loss: 0.42500, LR: 0.0001328, Tokens/sec: 40991.19\n",
      "Step: 3963, Training Loss: 0.91515, LR: 0.0001327, Tokens/sec: 41325.10\n",
      "Step: 3964, Training Loss: 0.39755, LR: 0.0001325, Tokens/sec: 39583.49\n",
      "Step: 3965, Training Loss: 1.09637, LR: 0.0001324, Tokens/sec: 41289.93\n",
      "Step: 3966, Training Loss: 0.59231, LR: 0.0001323, Tokens/sec: 41416.30\n",
      "Step: 3967, Training Loss: 1.30727, LR: 0.0001321, Tokens/sec: 41021.77\n",
      "Step: 3968, Training Loss: 0.95891, LR: 0.0001320, Tokens/sec: 41319.38\n",
      "Step: 3969, Training Loss: 0.38856, LR: 0.0001319, Tokens/sec: 41219.61\n",
      "Step: 3970, Training Loss: 0.65199, LR: 0.0001318, Tokens/sec: 41290.78\n",
      "Step: 3971, Training Loss: 0.90927, LR: 0.0001316, Tokens/sec: 40900.08\n",
      "Step: 3972, Training Loss: 0.58695, LR: 0.0001315, Tokens/sec: 40141.67\n",
      "Step: 3973, Training Loss: 1.23668, LR: 0.0001314, Tokens/sec: 41361.20\n",
      "Step: 3974, Training Loss: 1.22163, LR: 0.0001312, Tokens/sec: 41452.79\n",
      "Step: 3975, Training Loss: 0.45068, LR: 0.0001311, Tokens/sec: 41351.73\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 3975, Eval Loss: 0.55159\n",
      "Step: 3976, Training Loss: 0.84969, LR: 0.0001310, Tokens/sec: 24998.12\n",
      "Step: 3977, Training Loss: 0.25686, LR: 0.0001308, Tokens/sec: 41352.80\n",
      "Step: 3978, Training Loss: 0.44161, LR: 0.0001307, Tokens/sec: 29158.28\n",
      "Step: 3979, Training Loss: 0.38228, LR: 0.0001306, Tokens/sec: 41329.50\n",
      "Step: 3980, Training Loss: 0.51776, LR: 0.0001305, Tokens/sec: 41186.07\n",
      "Step: 3981, Training Loss: 0.63462, LR: 0.0001303, Tokens/sec: 41323.74\n",
      "Step: 3982, Training Loss: 0.33223, LR: 0.0001302, Tokens/sec: 40995.74\n",
      "Step: 3983, Training Loss: 0.66390, LR: 0.0001301, Tokens/sec: 41024.49\n",
      "Step: 3984, Training Loss: 0.28516, LR: 0.0001300, Tokens/sec: 40546.04\n",
      "Step: 3985, Training Loss: 0.61478, LR: 0.0001298, Tokens/sec: 41298.38\n",
      "Step: 3986, Training Loss: 0.96516, LR: 0.0001297, Tokens/sec: 41307.50\n",
      "Step: 3987, Training Loss: 0.58986, LR: 0.0001296, Tokens/sec: 40148.39\n",
      "Step: 3988, Training Loss: 1.27209, LR: 0.0001294, Tokens/sec: 41335.74\n",
      "Step: 3989, Training Loss: 0.39253, LR: 0.0001293, Tokens/sec: 41295.41\n",
      "Step: 3990, Training Loss: 0.70275, LR: 0.0001292, Tokens/sec: 40815.55\n",
      "Step: 3991, Training Loss: 0.52217, LR: 0.0001291, Tokens/sec: 40819.63\n",
      "Step: 3992, Training Loss: 0.53521, LR: 0.0001289, Tokens/sec: 40430.65\n",
      "Step: 3993, Training Loss: 0.34921, LR: 0.0001288, Tokens/sec: 41108.84\n",
      "Step: 3994, Training Loss: 0.62758, LR: 0.0001287, Tokens/sec: 40834.80\n",
      "Step: 3995, Training Loss: 1.09582, LR: 0.0001286, Tokens/sec: 38618.97\n",
      "Step: 3996, Training Loss: 0.94947, LR: 0.0001285, Tokens/sec: 39711.65\n",
      "Step: 3997, Training Loss: 0.38247, LR: 0.0001283, Tokens/sec: 41338.89\n",
      "Step: 3998, Training Loss: 0.73760, LR: 0.0001282, Tokens/sec: 41337.85\n",
      "Step: 3999, Training Loss: 0.60683, LR: 0.0001281, Tokens/sec: 41357.53\n",
      "Step: 4000, Training Loss: 1.21973, LR: 0.0001280, Tokens/sec: 40203.18\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 4000, Eval Loss: 0.71317\n",
      "Step: 4001, Training Loss: 0.59830, LR: 0.0001278, Tokens/sec: 41281.79\n",
      "Step: 4002, Training Loss: 0.64576, LR: 0.0001277, Tokens/sec: 41327.84\n",
      "Step: 4003, Training Loss: 0.54267, LR: 0.0001276, Tokens/sec: 40666.94\n",
      "Step: 4004, Training Loss: 0.32322, LR: 0.0001275, Tokens/sec: 39875.46\n",
      "Step: 4005, Training Loss: 1.08053, LR: 0.0001274, Tokens/sec: 41450.79\n",
      "Step: 4006, Training Loss: 0.61750, LR: 0.0001272, Tokens/sec: 28657.91\n",
      "Step: 4007, Training Loss: 0.40216, LR: 0.0001271, Tokens/sec: 35962.71\n",
      "Step: 4008, Training Loss: 0.68177, LR: 0.0001270, Tokens/sec: 30909.31\n",
      "Step: 4009, Training Loss: 0.38808, LR: 0.0001269, Tokens/sec: 36666.37\n",
      "Step: 4010, Training Loss: 0.65972, LR: 0.0001268, Tokens/sec: 39401.95\n",
      "Step: 4011, Training Loss: 1.11808, LR: 0.0001266, Tokens/sec: 34390.86\n",
      "Step: 4012, Training Loss: 0.51862, LR: 0.0001265, Tokens/sec: 41349.93\n",
      "Step: 4013, Training Loss: 0.64505, LR: 0.0001264, Tokens/sec: 41361.69\n",
      "Step: 4014, Training Loss: 0.65412, LR: 0.0001263, Tokens/sec: 41246.77\n",
      "Step: 4015, Training Loss: 1.35365, LR: 0.0001262, Tokens/sec: 40663.83\n",
      "Step: 4016, Training Loss: 0.64298, LR: 0.0001260, Tokens/sec: 38271.06\n",
      "Step: 4017, Training Loss: 1.09794, LR: 0.0001259, Tokens/sec: 37532.71\n",
      "Step: 4018, Training Loss: 0.46615, LR: 0.0001258, Tokens/sec: 39284.76\n",
      "Step: 4019, Training Loss: 0.47722, LR: 0.0001257, Tokens/sec: 39480.10\n",
      "Step: 4020, Training Loss: 0.57750, LR: 0.0001256, Tokens/sec: 29418.48\n",
      "Step: 4021, Training Loss: 1.34199, LR: 0.0001254, Tokens/sec: 31298.66\n",
      "Step: 4022, Training Loss: 1.03044, LR: 0.0001253, Tokens/sec: 40509.23\n",
      "Step: 4023, Training Loss: 0.39031, LR: 0.0001252, Tokens/sec: 41388.36\n",
      "Step: 4024, Training Loss: 0.41653, LR: 0.0001251, Tokens/sec: 41405.09\n",
      "Step: 4025, Training Loss: 0.39872, LR: 0.0001250, Tokens/sec: 40335.61\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 4025, Eval Loss: 0.57004\n",
      "Step: 4026, Training Loss: 0.65351, LR: 0.0001249, Tokens/sec: 40653.67\n",
      "Step: 4027, Training Loss: 0.63341, LR: 0.0001248, Tokens/sec: 32481.28\n",
      "Step: 4028, Training Loss: 0.71617, LR: 0.0001246, Tokens/sec: 35349.68\n",
      "Step: 4029, Training Loss: 0.66928, LR: 0.0001245, Tokens/sec: 37987.21\n",
      "Step: 4030, Training Loss: 0.68272, LR: 0.0001244, Tokens/sec: 39997.02\n",
      "Step: 4031, Training Loss: 0.69861, LR: 0.0001243, Tokens/sec: 41277.38\n",
      "Step: 4032, Training Loss: 0.46971, LR: 0.0001242, Tokens/sec: 39616.12\n",
      "Step: 4033, Training Loss: 0.35150, LR: 0.0001241, Tokens/sec: 36117.52\n",
      "Step: 4034, Training Loss: 0.87593, LR: 0.0001240, Tokens/sec: 36888.67\n",
      "Step: 4035, Training Loss: 0.33315, LR: 0.0001238, Tokens/sec: 36502.72\n",
      "Step: 4036, Training Loss: 0.43680, LR: 0.0001237, Tokens/sec: 34041.83\n",
      "Step: 4037, Training Loss: 1.18190, LR: 0.0001236, Tokens/sec: 30363.57\n",
      "Step: 4038, Training Loss: 0.59830, LR: 0.0001235, Tokens/sec: 28472.80\n",
      "Step: 4039, Training Loss: 0.48534, LR: 0.0001234, Tokens/sec: 39929.30\n",
      "Step: 4040, Training Loss: 0.54188, LR: 0.0001233, Tokens/sec: 41225.48\n",
      "Step: 4041, Training Loss: 1.06426, LR: 0.0001232, Tokens/sec: 39892.15\n",
      "Step: 4042, Training Loss: 0.44891, LR: 0.0001230, Tokens/sec: 32018.53\n",
      "Step: 4043, Training Loss: 1.12130, LR: 0.0001229, Tokens/sec: 31273.27\n",
      "Step: 4044, Training Loss: 0.47994, LR: 0.0001228, Tokens/sec: 41262.34\n",
      "Step: 4045, Training Loss: 0.55421, LR: 0.0001227, Tokens/sec: 41299.90\n",
      "Step: 4046, Training Loss: 0.95569, LR: 0.0001226, Tokens/sec: 41385.25\n",
      "Step: 4047, Training Loss: 0.49360, LR: 0.0001225, Tokens/sec: 40850.41\n",
      "Step: 4048, Training Loss: 0.54712, LR: 0.0001224, Tokens/sec: 40552.75\n",
      "Step: 4049, Training Loss: 0.30133, LR: 0.0001223, Tokens/sec: 37309.64\n",
      "Step: 4050, Training Loss: 0.42444, LR: 0.0001222, Tokens/sec: 26699.45\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 4050, Eval Loss: 0.62872\n",
      "Step: 4051, Training Loss: 0.88705, LR: 0.0001221, Tokens/sec: 41224.96\n",
      "Step: 4052, Training Loss: 0.36396, LR: 0.0001219, Tokens/sec: 38279.02\n",
      "Step: 4053, Training Loss: 0.43914, LR: 0.0001218, Tokens/sec: 35355.26\n",
      "Step: 4054, Training Loss: 0.47870, LR: 0.0001217, Tokens/sec: 40834.90\n",
      "Step: 4055, Training Loss: 0.51520, LR: 0.0001216, Tokens/sec: 41373.13\n",
      "Step: 4056, Training Loss: 0.32048, LR: 0.0001215, Tokens/sec: 41286.54\n",
      "Step: 4057, Training Loss: 0.82893, LR: 0.0001214, Tokens/sec: 41420.28\n",
      "Step: 4058, Training Loss: 0.64067, LR: 0.0001213, Tokens/sec: 40593.32\n",
      "Step: 4059, Training Loss: 0.31962, LR: 0.0001212, Tokens/sec: 41509.00\n",
      "Step: 4060, Training Loss: 0.90572, LR: 0.0001211, Tokens/sec: 40676.80\n",
      "Step: 4061, Training Loss: 0.81366, LR: 0.0001210, Tokens/sec: 41222.07\n",
      "Step: 4062, Training Loss: 0.68701, LR: 0.0001209, Tokens/sec: 39807.86\n",
      "Step: 4063, Training Loss: 0.29366, LR: 0.0001208, Tokens/sec: 41391.21\n",
      "Step: 4064, Training Loss: 0.45526, LR: 0.0001207, Tokens/sec: 29364.70\n",
      "Step: 4065, Training Loss: 0.48252, LR: 0.0001206, Tokens/sec: 36261.52\n",
      "Step: 4066, Training Loss: 0.59208, LR: 0.0001205, Tokens/sec: 40527.85\n",
      "Step: 4067, Training Loss: 0.91896, LR: 0.0001203, Tokens/sec: 40942.81\n",
      "Step: 4068, Training Loss: 0.58958, LR: 0.0001202, Tokens/sec: 40732.87\n",
      "Step: 4069, Training Loss: 0.73499, LR: 0.0001201, Tokens/sec: 41291.64\n",
      "Step: 4070, Training Loss: 0.55006, LR: 0.0001200, Tokens/sec: 38379.48\n",
      "Step: 4071, Training Loss: 0.30434, LR: 0.0001199, Tokens/sec: 37981.01\n",
      "Step: 4072, Training Loss: 0.53299, LR: 0.0001198, Tokens/sec: 40721.79\n",
      "Step: 4073, Training Loss: 0.70825, LR: 0.0001197, Tokens/sec: 41321.57\n",
      "Step: 4074, Training Loss: 0.39095, LR: 0.0001196, Tokens/sec: 41290.13\n",
      "Step: 4075, Training Loss: 1.04149, LR: 0.0001195, Tokens/sec: 40765.55\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 4075, Eval Loss: 0.63388\n",
      "Step: 4076, Training Loss: 0.70272, LR: 0.0001194, Tokens/sec: 41201.21\n",
      "Step: 4077, Training Loss: 0.69703, LR: 0.0001193, Tokens/sec: 41384.02\n",
      "Step: 4078, Training Loss: 0.99300, LR: 0.0001192, Tokens/sec: 41359.81\n",
      "Step: 4079, Training Loss: 0.53368, LR: 0.0001191, Tokens/sec: 40854.53\n",
      "Step: 4080, Training Loss: 0.50322, LR: 0.0001190, Tokens/sec: 36579.01\n",
      "Step: 4081, Training Loss: 0.47892, LR: 0.0001189, Tokens/sec: 41285.41\n",
      "Step: 4082, Training Loss: 0.30693, LR: 0.0001188, Tokens/sec: 41341.04\n",
      "Step: 4083, Training Loss: 0.82769, LR: 0.0001187, Tokens/sec: 40781.65\n",
      "Step: 4084, Training Loss: 0.57299, LR: 0.0001186, Tokens/sec: 41274.71\n",
      "Step: 4085, Training Loss: 0.37943, LR: 0.0001185, Tokens/sec: 41382.09\n",
      "Step: 4086, Training Loss: 0.33591, LR: 0.0001184, Tokens/sec: 41416.65\n",
      "Step: 4087, Training Loss: 0.45866, LR: 0.0001183, Tokens/sec: 40649.95\n",
      "Step: 4088, Training Loss: 0.34388, LR: 0.0001182, Tokens/sec: 40840.29\n",
      "Step: 4089, Training Loss: 0.31283, LR: 0.0001181, Tokens/sec: 41368.47\n",
      "Step: 4090, Training Loss: 0.32014, LR: 0.0001180, Tokens/sec: 41176.09\n",
      "Step: 4091, Training Loss: 0.40661, LR: 0.0001179, Tokens/sec: 40668.14\n",
      "Step: 4092, Training Loss: 0.34254, LR: 0.0001178, Tokens/sec: 41289.18\n",
      "Step: 4093, Training Loss: 0.67154, LR: 0.0001177, Tokens/sec: 41399.74\n",
      "Step: 4094, Training Loss: 0.90318, LR: 0.0001176, Tokens/sec: 40647.57\n",
      "Step: 4095, Training Loss: 0.42579, LR: 0.0001175, Tokens/sec: 30715.72\n",
      "Step: 4096, Training Loss: 0.69607, LR: 0.0001174, Tokens/sec: 40306.95\n",
      "Step: 4097, Training Loss: 0.60502, LR: 0.0001173, Tokens/sec: 41354.38\n",
      "Step: 4098, Training Loss: 0.60541, LR: 0.0001172, Tokens/sec: 40573.96\n",
      "Step: 4099, Training Loss: 0.40325, LR: 0.0001171, Tokens/sec: 41350.77\n",
      "Step: 4100, Training Loss: 0.30894, LR: 0.0001170, Tokens/sec: 34514.44\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 4100, Eval Loss: 0.62550\n",
      "Step: 4101, Training Loss: 0.29727, LR: 0.0001169, Tokens/sec: 27012.89\n",
      "Step: 4102, Training Loss: 0.73085, LR: 0.0001168, Tokens/sec: 41237.02\n",
      "Step: 4103, Training Loss: 0.48956, LR: 0.0001167, Tokens/sec: 26911.48\n",
      "Step: 4104, Training Loss: 0.41147, LR: 0.0001166, Tokens/sec: 41214.49\n",
      "Step: 4105, Training Loss: 0.61021, LR: 0.0001166, Tokens/sec: 40097.00\n",
      "Step: 4106, Training Loss: 0.41112, LR: 0.0001165, Tokens/sec: 41298.19\n",
      "Step: 4107, Training Loss: 0.59019, LR: 0.0001164, Tokens/sec: 41396.28\n",
      "Step: 4108, Training Loss: 0.60019, LR: 0.0001163, Tokens/sec: 31521.12\n",
      "Step: 4109, Training Loss: 0.62806, LR: 0.0001162, Tokens/sec: 41448.68\n",
      "Step: 4110, Training Loss: 0.57840, LR: 0.0001161, Tokens/sec: 40962.63\n",
      "Step: 4111, Training Loss: 0.79951, LR: 0.0001160, Tokens/sec: 41379.57\n",
      "Step: 4112, Training Loss: 0.84716, LR: 0.0001159, Tokens/sec: 41472.28\n",
      "Step: 4113, Training Loss: 0.42336, LR: 0.0001158, Tokens/sec: 37883.77\n",
      "Step: 4114, Training Loss: 0.54080, LR: 0.0001157, Tokens/sec: 35457.63\n",
      "Step: 4115, Training Loss: 0.45524, LR: 0.0001156, Tokens/sec: 41397.11\n",
      "Step: 4116, Training Loss: 0.67318, LR: 0.0001155, Tokens/sec: 41403.58\n",
      "Step: 4117, Training Loss: 0.35866, LR: 0.0001154, Tokens/sec: 41143.69\n",
      "Step: 4118, Training Loss: 0.35704, LR: 0.0001153, Tokens/sec: 40931.02\n",
      "Step: 4119, Training Loss: 0.50662, LR: 0.0001152, Tokens/sec: 41522.76\n",
      "Step: 4120, Training Loss: 0.48391, LR: 0.0001152, Tokens/sec: 41487.96\n",
      "Step: 4121, Training Loss: 0.68282, LR: 0.0001151, Tokens/sec: 26765.82\n",
      "Step: 4122, Training Loss: 0.45646, LR: 0.0001150, Tokens/sec: 39348.18\n",
      "Step: 4123, Training Loss: 0.58729, LR: 0.0001149, Tokens/sec: 40756.83\n",
      "Step: 4124, Training Loss: 0.59004, LR: 0.0001148, Tokens/sec: 28027.45\n",
      "Step: 4125, Training Loss: 0.61141, LR: 0.0001147, Tokens/sec: 41444.70\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 4125, Eval Loss: 0.54059\n",
      "Step: 4126, Training Loss: 0.74316, LR: 0.0001146, Tokens/sec: 41358.86\n",
      "Step: 4127, Training Loss: 0.73007, LR: 0.0001145, Tokens/sec: 41472.95\n",
      "Step: 4128, Training Loss: 0.48641, LR: 0.0001144, Tokens/sec: 41457.47\n",
      "Step: 4129, Training Loss: 0.64003, LR: 0.0001144, Tokens/sec: 41431.80\n",
      "Step: 4130, Training Loss: 0.55318, LR: 0.0001143, Tokens/sec: 34857.60\n",
      "Step: 4131, Training Loss: 0.75264, LR: 0.0001142, Tokens/sec: 35519.84\n",
      "Step: 4132, Training Loss: 0.47665, LR: 0.0001141, Tokens/sec: 37968.80\n",
      "Step: 4133, Training Loss: 0.54845, LR: 0.0001140, Tokens/sec: 38642.64\n",
      "Step: 4134, Training Loss: 0.44872, LR: 0.0001139, Tokens/sec: 33934.05\n",
      "Step: 4135, Training Loss: 0.37801, LR: 0.0001138, Tokens/sec: 40181.55\n",
      "Step: 4136, Training Loss: 0.53257, LR: 0.0001137, Tokens/sec: 39145.62\n",
      "Step: 4137, Training Loss: 0.43353, LR: 0.0001137, Tokens/sec: 38108.68\n",
      "Step: 4138, Training Loss: 0.39329, LR: 0.0001136, Tokens/sec: 41225.97\n",
      "Step: 4139, Training Loss: 0.90830, LR: 0.0001135, Tokens/sec: 41426.07\n",
      "Step: 4140, Training Loss: 0.40063, LR: 0.0001134, Tokens/sec: 40749.74\n",
      "Step: 4141, Training Loss: 0.44765, LR: 0.0001133, Tokens/sec: 41437.11\n",
      "Step: 4142, Training Loss: 0.37672, LR: 0.0001132, Tokens/sec: 40745.66\n",
      "Step: 4143, Training Loss: 0.45755, LR: 0.0001131, Tokens/sec: 41543.79\n",
      "Step: 4144, Training Loss: 0.46604, LR: 0.0001131, Tokens/sec: 41528.69\n",
      "Step: 4145, Training Loss: 0.56767, LR: 0.0001130, Tokens/sec: 37580.65\n",
      "Step: 4146, Training Loss: 0.80518, LR: 0.0001129, Tokens/sec: 38269.01\n",
      "Step: 4147, Training Loss: 0.94605, LR: 0.0001128, Tokens/sec: 41463.69\n",
      "Step: 4148, Training Loss: 0.31599, LR: 0.0001127, Tokens/sec: 40821.01\n",
      "Step: 4149, Training Loss: 0.44242, LR: 0.0001126, Tokens/sec: 37909.80\n",
      "Step: 4150, Training Loss: 0.67846, LR: 0.0001126, Tokens/sec: 38127.93\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 4150, Eval Loss: 0.57079\n",
      "Step: 4151, Training Loss: 0.46344, LR: 0.0001125, Tokens/sec: 41310.94\n",
      "Step: 4152, Training Loss: 0.61826, LR: 0.0001124, Tokens/sec: 41475.04\n",
      "Step: 4153, Training Loss: 0.57028, LR: 0.0001123, Tokens/sec: 41531.19\n",
      "Step: 4154, Training Loss: 0.69334, LR: 0.0001122, Tokens/sec: 41519.68\n",
      "Step: 4155, Training Loss: 0.40389, LR: 0.0001121, Tokens/sec: 40940.34\n",
      "Step: 4156, Training Loss: 0.57528, LR: 0.0001121, Tokens/sec: 41533.09\n",
      "Step: 4157, Training Loss: 0.80689, LR: 0.0001120, Tokens/sec: 40661.37\n",
      "Step: 4158, Training Loss: 0.36919, LR: 0.0001119, Tokens/sec: 40924.44\n",
      "Step: 4159, Training Loss: 0.23598, LR: 0.0001118, Tokens/sec: 40697.25\n",
      "Step: 4160, Training Loss: 0.55095, LR: 0.0001117, Tokens/sec: 41511.15\n",
      "Step: 4161, Training Loss: 0.23560, LR: 0.0001117, Tokens/sec: 41390.82\n",
      "Step: 4162, Training Loss: 0.56137, LR: 0.0001116, Tokens/sec: 41536.35\n",
      "Step: 4163, Training Loss: 0.51545, LR: 0.0001115, Tokens/sec: 41500.07\n",
      "Step: 4164, Training Loss: 0.37559, LR: 0.0001114, Tokens/sec: 41387.39\n",
      "Step: 4165, Training Loss: 0.57761, LR: 0.0001113, Tokens/sec: 41515.54\n",
      "Step: 4166, Training Loss: 0.50480, LR: 0.0001113, Tokens/sec: 40450.80\n",
      "Step: 4167, Training Loss: 0.33423, LR: 0.0001112, Tokens/sec: 41417.81\n",
      "Step: 4168, Training Loss: 0.69391, LR: 0.0001111, Tokens/sec: 40728.89\n",
      "Step: 4169, Training Loss: 0.54462, LR: 0.0001110, Tokens/sec: 36891.19\n",
      "Step: 4170, Training Loss: 0.33500, LR: 0.0001110, Tokens/sec: 40304.81\n",
      "Step: 4171, Training Loss: 0.29395, LR: 0.0001109, Tokens/sec: 40923.18\n",
      "Step: 4172, Training Loss: 0.38648, LR: 0.0001108, Tokens/sec: 41402.63\n",
      "Step: 4173, Training Loss: 0.74219, LR: 0.0001107, Tokens/sec: 37436.33\n",
      "Step: 4174, Training Loss: 0.69195, LR: 0.0001106, Tokens/sec: 35298.75\n",
      "Step: 4175, Training Loss: 0.50205, LR: 0.0001106, Tokens/sec: 41471.13\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 4175, Eval Loss: 0.47100\n",
      "Step: 4176, Training Loss: 0.45334, LR: 0.0001105, Tokens/sec: 41353.53\n",
      "Step: 4177, Training Loss: 0.54637, LR: 0.0001104, Tokens/sec: 41503.30\n",
      "Step: 4178, Training Loss: 0.58990, LR: 0.0001103, Tokens/sec: 40733.51\n",
      "Step: 4179, Training Loss: 0.96337, LR: 0.0001103, Tokens/sec: 40923.50\n",
      "Step: 4180, Training Loss: 0.36027, LR: 0.0001102, Tokens/sec: 40748.74\n",
      "Step: 4181, Training Loss: 0.31243, LR: 0.0001101, Tokens/sec: 41433.86\n",
      "Step: 4182, Training Loss: 0.86350, LR: 0.0001100, Tokens/sec: 41534.87\n",
      "Step: 4183, Training Loss: 0.34550, LR: 0.0001100, Tokens/sec: 41597.50\n",
      "Step: 4184, Training Loss: 0.65606, LR: 0.0001099, Tokens/sec: 41553.14\n",
      "Step: 4185, Training Loss: 0.30815, LR: 0.0001098, Tokens/sec: 41357.27\n",
      "Step: 4186, Training Loss: 0.61297, LR: 0.0001098, Tokens/sec: 41302.14\n",
      "Step: 4187, Training Loss: 0.86622, LR: 0.0001097, Tokens/sec: 34278.14\n",
      "Step: 4188, Training Loss: 0.54962, LR: 0.0001096, Tokens/sec: 41456.79\n",
      "Step: 4189, Training Loss: 0.38447, LR: 0.0001095, Tokens/sec: 40673.72\n",
      "Step: 4190, Training Loss: 0.34611, LR: 0.0001095, Tokens/sec: 41542.46\n",
      "Step: 4191, Training Loss: 0.89125, LR: 0.0001094, Tokens/sec: 41500.12\n",
      "Step: 4192, Training Loss: 0.53149, LR: 0.0001093, Tokens/sec: 41524.25\n",
      "Step: 4193, Training Loss: 0.98578, LR: 0.0001092, Tokens/sec: 41403.30\n",
      "Step: 4194, Training Loss: 0.66317, LR: 0.0001092, Tokens/sec: 41426.43\n",
      "Step: 4195, Training Loss: 1.22671, LR: 0.0001091, Tokens/sec: 40807.63\n",
      "Step: 4196, Training Loss: 0.61564, LR: 0.0001090, Tokens/sec: 41408.97\n",
      "Step: 4197, Training Loss: 0.61532, LR: 0.0001090, Tokens/sec: 41530.86\n",
      "Step: 4198, Training Loss: 0.58454, LR: 0.0001089, Tokens/sec: 40886.14\n",
      "Step: 4199, Training Loss: 0.50910, LR: 0.0001088, Tokens/sec: 41553.50\n",
      "Step: 4200, Training Loss: 0.44957, LR: 0.0001088, Tokens/sec: 41499.01\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 4200, Eval Loss: 0.55663\n",
      "Step: 4201, Training Loss: 0.19242, LR: 0.0001087, Tokens/sec: 33398.05\n",
      "Step: 4202, Training Loss: 0.51721, LR: 0.0001086, Tokens/sec: 35407.26\n",
      "Step: 4203, Training Loss: 0.46283, LR: 0.0001085, Tokens/sec: 36769.65\n",
      "Step: 4204, Training Loss: 0.48203, LR: 0.0001085, Tokens/sec: 33994.21\n",
      "Step: 4205, Training Loss: 0.41698, LR: 0.0001084, Tokens/sec: 32055.02\n",
      "Step: 4206, Training Loss: 0.46005, LR: 0.0001083, Tokens/sec: 29750.34\n",
      "Step: 4207, Training Loss: 0.21536, LR: 0.0001083, Tokens/sec: 40903.02\n",
      "Step: 4208, Training Loss: 0.48252, LR: 0.0001082, Tokens/sec: 41144.05\n",
      "Step: 4209, Training Loss: 0.62091, LR: 0.0001081, Tokens/sec: 41366.52\n",
      "Step: 4210, Training Loss: 0.26700, LR: 0.0001081, Tokens/sec: 41368.68\n",
      "Step: 4211, Training Loss: 0.32007, LR: 0.0001080, Tokens/sec: 41376.65\n",
      "Step: 4212, Training Loss: 0.37192, LR: 0.0001079, Tokens/sec: 41365.63\n",
      "Step: 4213, Training Loss: 0.33686, LR: 0.0001079, Tokens/sec: 41371.40\n",
      "Step: 4214, Training Loss: 0.55097, LR: 0.0001078, Tokens/sec: 39298.01\n",
      "Step: 4215, Training Loss: 0.55701, LR: 0.0001077, Tokens/sec: 40725.36\n",
      "Step: 4216, Training Loss: 0.42181, LR: 0.0001077, Tokens/sec: 40667.13\n",
      "Step: 4217, Training Loss: 0.36201, LR: 0.0001076, Tokens/sec: 38790.37\n",
      "Step: 4218, Training Loss: 0.46326, LR: 0.0001076, Tokens/sec: 37778.15\n",
      "Step: 4219, Training Loss: 0.47451, LR: 0.0001075, Tokens/sec: 38115.16\n",
      "Step: 4220, Training Loss: 0.22065, LR: 0.0001074, Tokens/sec: 37089.04\n",
      "Step: 4221, Training Loss: 0.32633, LR: 0.0001074, Tokens/sec: 35777.06\n",
      "Step: 4222, Training Loss: 0.46860, LR: 0.0001073, Tokens/sec: 39608.33\n",
      "Step: 4223, Training Loss: 0.79486, LR: 0.0001072, Tokens/sec: 40646.61\n",
      "Step: 4224, Training Loss: 0.40449, LR: 0.0001072, Tokens/sec: 40118.77\n",
      "Step: 4225, Training Loss: 0.55961, LR: 0.0001071, Tokens/sec: 29196.25\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 4225, Eval Loss: 0.49636\n",
      "Step: 4226, Training Loss: 0.35688, LR: 0.0001070, Tokens/sec: 34713.96\n",
      "Step: 4227, Training Loss: 0.91203, LR: 0.0001070, Tokens/sec: 37265.27\n",
      "Step: 4228, Training Loss: 0.47259, LR: 0.0001069, Tokens/sec: 37168.14\n",
      "Step: 4229, Training Loss: 0.50185, LR: 0.0001069, Tokens/sec: 41298.55\n",
      "Step: 4230, Training Loss: 0.57317, LR: 0.0001068, Tokens/sec: 12726.60\n",
      "Step: 4231, Training Loss: 0.74652, LR: 0.0001067, Tokens/sec: 36336.38\n",
      "Step: 4232, Training Loss: 0.35031, LR: 0.0001067, Tokens/sec: 39458.04\n",
      "Step: 4233, Training Loss: 0.60995, LR: 0.0001066, Tokens/sec: 32062.80\n",
      "Step: 4234, Training Loss: 0.61101, LR: 0.0001066, Tokens/sec: 29419.15\n",
      "Step: 4235, Training Loss: 0.29083, LR: 0.0001065, Tokens/sec: 41326.39\n",
      "Step: 4236, Training Loss: 0.19799, LR: 0.0001064, Tokens/sec: 37865.56\n",
      "Step: 4237, Training Loss: 0.24761, LR: 0.0001064, Tokens/sec: 36338.32\n",
      "Step: 4238, Training Loss: 0.51029, LR: 0.0001063, Tokens/sec: 37521.40\n",
      "Step: 4239, Training Loss: 0.46606, LR: 0.0001063, Tokens/sec: 41388.48\n",
      "Step: 4240, Training Loss: 0.55309, LR: 0.0001062, Tokens/sec: 39952.46\n",
      "Step: 4241, Training Loss: 0.57117, LR: 0.0001061, Tokens/sec: 31392.75\n",
      "Step: 4242, Training Loss: 1.02450, LR: 0.0001061, Tokens/sec: 36153.97\n",
      "Step: 4243, Training Loss: 0.20568, LR: 0.0001060, Tokens/sec: 35545.63\n",
      "Step: 4244, Training Loss: 0.39453, LR: 0.0001060, Tokens/sec: 37973.76\n",
      "Step: 4245, Training Loss: 0.27352, LR: 0.0001059, Tokens/sec: 40392.08\n",
      "Step: 4246, Training Loss: 0.48843, LR: 0.0001059, Tokens/sec: 30877.27\n",
      "Step: 4247, Training Loss: 0.44972, LR: 0.0001058, Tokens/sec: 32572.11\n",
      "Step: 4248, Training Loss: 0.47483, LR: 0.0001057, Tokens/sec: 28749.95\n",
      "Step: 4249, Training Loss: 0.26040, LR: 0.0001057, Tokens/sec: 27657.15\n",
      "Step: 4250, Training Loss: 0.53225, LR: 0.0001056, Tokens/sec: 40748.87\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 4250, Eval Loss: 0.44245\n",
      "Step: 4251, Training Loss: 0.29395, LR: 0.0001056, Tokens/sec: 40675.69\n",
      "Step: 4252, Training Loss: 0.42119, LR: 0.0001055, Tokens/sec: 41515.40\n",
      "Step: 4253, Training Loss: 0.22366, LR: 0.0001055, Tokens/sec: 35008.23\n",
      "Step: 4254, Training Loss: 0.28443, LR: 0.0001054, Tokens/sec: 29670.99\n",
      "Step: 4255, Training Loss: 0.24724, LR: 0.0001054, Tokens/sec: 40589.98\n",
      "Step: 4256, Training Loss: 0.53784, LR: 0.0001053, Tokens/sec: 40620.97\n",
      "Step: 4257, Training Loss: 0.25463, LR: 0.0001052, Tokens/sec: 33486.65\n",
      "Step: 4258, Training Loss: 0.19382, LR: 0.0001052, Tokens/sec: 28541.66\n",
      "Step: 4259, Training Loss: 0.42281, LR: 0.0001051, Tokens/sec: 40890.91\n",
      "Step: 4260, Training Loss: 0.76885, LR: 0.0001051, Tokens/sec: 41507.81\n",
      "Step: 4261, Training Loss: 0.46677, LR: 0.0001050, Tokens/sec: 40070.48\n",
      "Step: 4262, Training Loss: 0.49981, LR: 0.0001050, Tokens/sec: 41401.16\n",
      "Step: 4263, Training Loss: 0.39154, LR: 0.0001049, Tokens/sec: 41488.35\n",
      "Step: 4264, Training Loss: 0.46948, LR: 0.0001049, Tokens/sec: 41430.23\n",
      "Step: 4265, Training Loss: 0.53099, LR: 0.0001048, Tokens/sec: 41501.14\n",
      "Step: 4266, Training Loss: 0.39490, LR: 0.0001048, Tokens/sec: 41438.80\n",
      "Step: 4267, Training Loss: 0.52444, LR: 0.0001047, Tokens/sec: 41513.77\n",
      "Step: 4268, Training Loss: 0.35983, LR: 0.0001047, Tokens/sec: 30728.08\n",
      "Step: 4269, Training Loss: 0.38903, LR: 0.0001046, Tokens/sec: 39775.43\n",
      "Step: 4270, Training Loss: 0.18170, LR: 0.0001046, Tokens/sec: 41316.74\n",
      "Step: 4271, Training Loss: 0.47220, LR: 0.0001045, Tokens/sec: 38977.82\n",
      "Step: 4272, Training Loss: 0.48134, LR: 0.0001045, Tokens/sec: 41320.06\n",
      "Step: 4273, Training Loss: 0.64435, LR: 0.0001044, Tokens/sec: 40824.47\n",
      "Step: 4274, Training Loss: 0.67715, LR: 0.0001044, Tokens/sec: 40464.89\n",
      "Step: 4275, Training Loss: 0.27412, LR: 0.0001043, Tokens/sec: 41403.59\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 4275, Eval Loss: 0.48300\n",
      "Step: 4276, Training Loss: 0.41636, LR: 0.0001043, Tokens/sec: 41409.49\n",
      "Step: 4277, Training Loss: 0.19201, LR: 0.0001042, Tokens/sec: 39183.74\n",
      "Step: 4278, Training Loss: 0.48858, LR: 0.0001042, Tokens/sec: 36564.83\n",
      "Step: 4279, Training Loss: 0.24979, LR: 0.0001041, Tokens/sec: 39042.45\n",
      "Step: 4280, Training Loss: 0.39100, LR: 0.0001041, Tokens/sec: 41374.47\n",
      "Step: 4281, Training Loss: 0.29977, LR: 0.0001040, Tokens/sec: 41438.26\n",
      "Step: 4282, Training Loss: 0.21016, LR: 0.0001040, Tokens/sec: 41437.46\n",
      "Step: 4283, Training Loss: 0.43479, LR: 0.0001039, Tokens/sec: 41053.88\n",
      "Step: 4284, Training Loss: 0.36142, LR: 0.0001039, Tokens/sec: 41397.33\n",
      "Step: 4285, Training Loss: 0.29312, LR: 0.0001039, Tokens/sec: 39272.12\n",
      "Step: 4286, Training Loss: 0.14947, LR: 0.0001038, Tokens/sec: 41368.78\n",
      "Step: 4287, Training Loss: 0.35430, LR: 0.0001038, Tokens/sec: 40476.34\n",
      "Step: 4288, Training Loss: 0.35516, LR: 0.0001037, Tokens/sec: 32802.94\n",
      "Step: 4289, Training Loss: 0.46478, LR: 0.0001037, Tokens/sec: 25741.03\n",
      "Step: 4290, Training Loss: 0.57910, LR: 0.0001036, Tokens/sec: 34182.87\n",
      "Step: 4291, Training Loss: 0.58211, LR: 0.0001036, Tokens/sec: 36292.16\n",
      "Step: 4292, Training Loss: 0.30590, LR: 0.0001035, Tokens/sec: 30644.34\n",
      "Step: 4293, Training Loss: 0.47566, LR: 0.0001035, Tokens/sec: 32002.20\n",
      "Step: 4294, Training Loss: 0.56074, LR: 0.0001034, Tokens/sec: 35632.68\n",
      "Step: 4295, Training Loss: 0.64553, LR: 0.0001034, Tokens/sec: 39816.81\n",
      "Step: 4296, Training Loss: 0.38967, LR: 0.0001034, Tokens/sec: 40165.64\n",
      "Step: 4297, Training Loss: 0.23626, LR: 0.0001033, Tokens/sec: 35816.75\n",
      "Step: 4298, Training Loss: 0.33384, LR: 0.0001033, Tokens/sec: 36605.05\n",
      "Step: 4299, Training Loss: 0.39904, LR: 0.0001032, Tokens/sec: 37439.67\n",
      "Step: 4300, Training Loss: 0.81097, LR: 0.0001032, Tokens/sec: 36690.60\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 4300, Eval Loss: 0.52470\n",
      "Step: 4301, Training Loss: 0.23245, LR: 0.0001031, Tokens/sec: 35662.01\n",
      "Step: 4302, Training Loss: 0.65184, LR: 0.0001031, Tokens/sec: 41296.04\n",
      "Step: 4303, Training Loss: 0.49513, LR: 0.0001031, Tokens/sec: 41320.15\n",
      "Step: 4304, Training Loss: 0.30331, LR: 0.0001030, Tokens/sec: 39263.06\n",
      "Step: 4305, Training Loss: 0.65835, LR: 0.0001030, Tokens/sec: 40480.35\n",
      "Step: 4306, Training Loss: 0.69890, LR: 0.0001029, Tokens/sec: 39668.78\n",
      "Step: 4307, Training Loss: 0.34749, LR: 0.0001029, Tokens/sec: 41411.39\n",
      "Step: 4308, Training Loss: 0.10752, LR: 0.0001029, Tokens/sec: 41427.04\n",
      "Step: 4309, Training Loss: 0.60896, LR: 0.0001028, Tokens/sec: 39585.99\n",
      "Step: 4310, Training Loss: 0.79161, LR: 0.0001028, Tokens/sec: 40629.94\n",
      "Step: 4311, Training Loss: 0.84468, LR: 0.0001027, Tokens/sec: 41411.32\n",
      "Step: 4312, Training Loss: 0.49531, LR: 0.0001027, Tokens/sec: 40744.63\n",
      "Step: 4313, Training Loss: 0.41553, LR: 0.0001027, Tokens/sec: 40474.81\n",
      "Step: 4314, Training Loss: 0.39376, LR: 0.0001026, Tokens/sec: 41237.19\n",
      "Step: 4315, Training Loss: 0.24663, LR: 0.0001026, Tokens/sec: 41360.70\n",
      "Step: 4316, Training Loss: 0.33752, LR: 0.0001026, Tokens/sec: 37635.14\n",
      "Step: 4317, Training Loss: 0.41687, LR: 0.0001025, Tokens/sec: 35415.49\n",
      "Step: 4318, Training Loss: 0.64748, LR: 0.0001025, Tokens/sec: 39883.03\n",
      "Step: 4319, Training Loss: 0.64586, LR: 0.0001024, Tokens/sec: 40757.60\n",
      "Step: 4320, Training Loss: 0.83926, LR: 0.0001024, Tokens/sec: 41315.64\n",
      "Step: 4321, Training Loss: 0.66059, LR: 0.0001024, Tokens/sec: 40433.31\n",
      "Step: 4322, Training Loss: 0.40768, LR: 0.0001023, Tokens/sec: 33237.37\n",
      "Step: 4323, Training Loss: 0.29351, LR: 0.0001023, Tokens/sec: 36360.78\n",
      "Step: 4324, Training Loss: 0.71260, LR: 0.0001023, Tokens/sec: 35638.18\n",
      "Step: 4325, Training Loss: 0.26634, LR: 0.0001022, Tokens/sec: 35944.31\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 4325, Eval Loss: 0.48827\n",
      "Step: 4326, Training Loss: 0.54834, LR: 0.0001022, Tokens/sec: 36373.33\n",
      "Step: 4327, Training Loss: 0.24523, LR: 0.0001022, Tokens/sec: 37911.69\n",
      "Step: 4328, Training Loss: 0.50276, LR: 0.0001021, Tokens/sec: 40997.40\n",
      "Step: 4329, Training Loss: 0.18165, LR: 0.0001021, Tokens/sec: 29319.56\n",
      "Step: 4330, Training Loss: 0.61841, LR: 0.0001021, Tokens/sec: 27723.75\n",
      "Step: 4331, Training Loss: 0.43096, LR: 0.0001020, Tokens/sec: 30987.27\n",
      "Step: 4332, Training Loss: 0.18874, LR: 0.0001020, Tokens/sec: 41313.74\n",
      "Step: 4333, Training Loss: 0.44837, LR: 0.0001020, Tokens/sec: 41292.80\n",
      "Step: 4334, Training Loss: 0.32646, LR: 0.0001019, Tokens/sec: 39729.25\n",
      "Step: 4335, Training Loss: 0.34124, LR: 0.0001019, Tokens/sec: 41474.22\n",
      "Step: 4336, Training Loss: 0.37022, LR: 0.0001019, Tokens/sec: 41335.49\n",
      "Step: 4337, Training Loss: 0.42245, LR: 0.0001018, Tokens/sec: 41461.00\n",
      "Step: 4338, Training Loss: 0.26250, LR: 0.0001018, Tokens/sec: 36174.85\n",
      "Step: 4339, Training Loss: 0.29671, LR: 0.0001018, Tokens/sec: 26569.77\n",
      "Step: 4340, Training Loss: 0.56220, LR: 0.0001017, Tokens/sec: 41227.74\n",
      "Step: 4341, Training Loss: 0.45777, LR: 0.0001017, Tokens/sec: 40626.54\n",
      "Step: 4342, Training Loss: 0.51957, LR: 0.0001017, Tokens/sec: 37598.27\n",
      "Step: 4343, Training Loss: 0.36676, LR: 0.0001016, Tokens/sec: 41418.19\n",
      "Step: 4344, Training Loss: 0.23863, LR: 0.0001016, Tokens/sec: 40173.20\n",
      "Step: 4345, Training Loss: 0.27690, LR: 0.0001016, Tokens/sec: 41471.01\n",
      "Step: 4346, Training Loss: 0.32614, LR: 0.0001016, Tokens/sec: 41297.98\n",
      "Step: 4347, Training Loss: 0.59436, LR: 0.0001015, Tokens/sec: 27379.43\n",
      "Step: 4348, Training Loss: 0.33582, LR: 0.0001015, Tokens/sec: 41257.72\n",
      "Step: 4349, Training Loss: 0.30136, LR: 0.0001015, Tokens/sec: 41489.26\n",
      "Step: 4350, Training Loss: 0.55601, LR: 0.0001014, Tokens/sec: 41017.20\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 4350, Eval Loss: 0.44799\n",
      "Step: 4351, Training Loss: 0.40289, LR: 0.0001014, Tokens/sec: 29039.79\n",
      "Step: 4352, Training Loss: 0.35689, LR: 0.0001014, Tokens/sec: 34794.09\n",
      "Step: 4353, Training Loss: 0.62193, LR: 0.0001014, Tokens/sec: 38315.87\n",
      "Step: 4354, Training Loss: 0.62539, LR: 0.0001013, Tokens/sec: 41383.77\n",
      "Step: 4355, Training Loss: 0.26042, LR: 0.0001013, Tokens/sec: 36155.65\n",
      "Step: 4356, Training Loss: 0.43396, LR: 0.0001013, Tokens/sec: 33904.37\n",
      "Step: 4357, Training Loss: 0.19320, LR: 0.0001012, Tokens/sec: 26856.47\n",
      "Step: 4358, Training Loss: 0.54424, LR: 0.0001012, Tokens/sec: 41004.59\n",
      "Step: 4359, Training Loss: 0.16777, LR: 0.0001012, Tokens/sec: 40717.98\n",
      "Step: 4360, Training Loss: 0.31677, LR: 0.0001012, Tokens/sec: 40089.08\n",
      "Step: 4361, Training Loss: 0.32572, LR: 0.0001011, Tokens/sec: 38461.14\n",
      "Step: 4362, Training Loss: 0.32339, LR: 0.0001011, Tokens/sec: 38396.44\n",
      "Step: 4363, Training Loss: 0.14468, LR: 0.0001011, Tokens/sec: 40927.43\n",
      "Step: 4364, Training Loss: 0.47634, LR: 0.0001011, Tokens/sec: 41457.57\n",
      "Step: 4365, Training Loss: 0.28032, LR: 0.0001010, Tokens/sec: 27024.31\n",
      "Step: 4366, Training Loss: 0.16414, LR: 0.0001010, Tokens/sec: 40082.81\n",
      "Step: 4367, Training Loss: 0.36873, LR: 0.0001010, Tokens/sec: 27540.49\n",
      "Step: 4368, Training Loss: 0.35557, LR: 0.0001010, Tokens/sec: 38171.97\n",
      "Step: 4369, Training Loss: 0.72808, LR: 0.0001010, Tokens/sec: 41406.15\n",
      "Step: 4370, Training Loss: 0.28215, LR: 0.0001009, Tokens/sec: 41513.54\n",
      "Step: 4371, Training Loss: 0.40815, LR: 0.0001009, Tokens/sec: 39203.76\n",
      "Step: 4372, Training Loss: 0.45413, LR: 0.0001009, Tokens/sec: 33734.46\n",
      "Step: 4373, Training Loss: 0.47314, LR: 0.0001009, Tokens/sec: 41411.75\n",
      "Step: 4374, Training Loss: 0.43204, LR: 0.0001008, Tokens/sec: 41505.52\n",
      "Step: 4375, Training Loss: 0.62010, LR: 0.0001008, Tokens/sec: 41451.21\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 4375, Eval Loss: 0.37114\n",
      "Step: 4376, Training Loss: 0.37264, LR: 0.0001008, Tokens/sec: 37815.15\n",
      "Step: 4377, Training Loss: 0.50224, LR: 0.0001008, Tokens/sec: 41190.67\n",
      "Step: 4378, Training Loss: 0.54517, LR: 0.0001008, Tokens/sec: 33441.07\n",
      "Step: 4379, Training Loss: 0.59587, LR: 0.0001007, Tokens/sec: 33929.21\n",
      "Step: 4380, Training Loss: 0.60934, LR: 0.0001007, Tokens/sec: 33385.15\n",
      "Step: 4381, Training Loss: 0.62573, LR: 0.0001007, Tokens/sec: 32645.11\n",
      "Step: 4382, Training Loss: 0.74410, LR: 0.0001007, Tokens/sec: 41349.51\n",
      "Step: 4383, Training Loss: 0.23430, LR: 0.0001007, Tokens/sec: 40576.04\n",
      "Step: 4384, Training Loss: 0.48652, LR: 0.0001006, Tokens/sec: 38692.59\n",
      "Step: 4385, Training Loss: 0.60209, LR: 0.0001006, Tokens/sec: 36401.63\n",
      "Step: 4386, Training Loss: 0.33532, LR: 0.0001006, Tokens/sec: 36841.25\n",
      "Step: 4387, Training Loss: 0.33088, LR: 0.0001006, Tokens/sec: 38199.70\n",
      "Step: 4388, Training Loss: 0.40720, LR: 0.0001006, Tokens/sec: 36850.50\n",
      "Step: 4389, Training Loss: 0.31312, LR: 0.0001005, Tokens/sec: 37912.59\n",
      "Step: 4390, Training Loss: 0.30981, LR: 0.0001005, Tokens/sec: 39506.97\n",
      "Step: 4391, Training Loss: 0.25988, LR: 0.0001005, Tokens/sec: 29100.74\n",
      "Step: 4392, Training Loss: 0.35171, LR: 0.0001005, Tokens/sec: 34937.62\n",
      "Step: 4393, Training Loss: 0.49727, LR: 0.0001005, Tokens/sec: 41412.51\n",
      "Step: 4394, Training Loss: 0.57510, LR: 0.0001005, Tokens/sec: 41420.27\n",
      "Step: 4395, Training Loss: 0.28687, LR: 0.0001004, Tokens/sec: 39626.78\n",
      "Step: 4396, Training Loss: 0.23567, LR: 0.0001004, Tokens/sec: 41423.56\n",
      "Step: 4397, Training Loss: 0.20462, LR: 0.0001004, Tokens/sec: 41441.94\n",
      "Step: 4398, Training Loss: 0.27353, LR: 0.0001004, Tokens/sec: 41514.98\n",
      "Step: 4399, Training Loss: 0.32995, LR: 0.0001004, Tokens/sec: 41440.75\n",
      "Step: 4400, Training Loss: 0.28435, LR: 0.0001004, Tokens/sec: 39076.86\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 4400, Eval Loss: 0.49377\n",
      "Step: 4401, Training Loss: 0.40266, LR: 0.0001004, Tokens/sec: 41343.77\n",
      "Step: 4402, Training Loss: 0.17964, LR: 0.0001003, Tokens/sec: 41344.94\n",
      "Step: 4403, Training Loss: 0.50033, LR: 0.0001003, Tokens/sec: 41435.25\n",
      "Step: 4404, Training Loss: 0.88159, LR: 0.0001003, Tokens/sec: 41515.41\n",
      "Step: 4405, Training Loss: 0.34708, LR: 0.0001003, Tokens/sec: 40514.07\n",
      "Step: 4406, Training Loss: 0.17219, LR: 0.0001003, Tokens/sec: 41469.18\n",
      "Step: 4407, Training Loss: 0.31860, LR: 0.0001003, Tokens/sec: 38766.28\n",
      "Step: 4408, Training Loss: 0.66149, LR: 0.0001003, Tokens/sec: 40125.60\n",
      "Step: 4409, Training Loss: 0.48299, LR: 0.0001003, Tokens/sec: 41409.92\n",
      "Step: 4410, Training Loss: 0.49840, LR: 0.0001002, Tokens/sec: 41365.83\n",
      "Step: 4411, Training Loss: 0.17344, LR: 0.0001002, Tokens/sec: 41340.47\n",
      "Step: 4412, Training Loss: 0.68871, LR: 0.0001002, Tokens/sec: 41500.47\n",
      "Step: 4413, Training Loss: 0.31690, LR: 0.0001002, Tokens/sec: 40426.20\n",
      "Step: 4414, Training Loss: 0.18915, LR: 0.0001002, Tokens/sec: 41482.56\n",
      "Step: 4415, Training Loss: 0.30478, LR: 0.0001002, Tokens/sec: 41483.39\n",
      "Step: 4416, Training Loss: 0.41369, LR: 0.0001002, Tokens/sec: 39929.28\n",
      "Step: 4417, Training Loss: 0.21505, LR: 0.0001002, Tokens/sec: 35744.42\n",
      "Step: 4418, Training Loss: 0.28987, LR: 0.0001002, Tokens/sec: 40493.17\n",
      "Step: 4419, Training Loss: 0.28280, LR: 0.0001002, Tokens/sec: 36086.09\n",
      "Step: 4420, Training Loss: 0.38757, LR: 0.0001001, Tokens/sec: 37471.78\n",
      "Step: 4421, Training Loss: 0.45156, LR: 0.0001001, Tokens/sec: 41419.17\n",
      "Step: 4422, Training Loss: 0.30873, LR: 0.0001001, Tokens/sec: 29555.18\n",
      "Step: 4423, Training Loss: 0.47382, LR: 0.0001001, Tokens/sec: 41341.81\n",
      "Step: 4424, Training Loss: 0.27852, LR: 0.0001001, Tokens/sec: 41037.77\n",
      "Step: 4425, Training Loss: 0.64191, LR: 0.0001001, Tokens/sec: 41442.93\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 4425, Eval Loss: 0.47849\n",
      "Step: 4426, Training Loss: 1.04743, LR: 0.0001001, Tokens/sec: 39788.98\n",
      "Step: 4427, Training Loss: 0.36025, LR: 0.0001001, Tokens/sec: 41512.89\n",
      "Step: 4428, Training Loss: 0.50094, LR: 0.0001001, Tokens/sec: 40888.40\n",
      "Step: 4429, Training Loss: 0.21639, LR: 0.0001001, Tokens/sec: 41509.71\n",
      "Step: 4430, Training Loss: 0.21908, LR: 0.0001001, Tokens/sec: 40555.22\n",
      "Step: 4431, Training Loss: 0.22251, LR: 0.0001001, Tokens/sec: 41459.54\n",
      "Step: 4432, Training Loss: 0.30026, LR: 0.0001001, Tokens/sec: 41453.74\n",
      "Step: 4433, Training Loss: 0.20439, LR: 0.0001000, Tokens/sec: 41512.71\n",
      "Step: 4434, Training Loss: 0.42292, LR: 0.0001000, Tokens/sec: 30243.66\n",
      "Step: 4435, Training Loss: 0.30379, LR: 0.0001000, Tokens/sec: 30453.47\n",
      "Step: 4436, Training Loss: 0.20865, LR: 0.0001000, Tokens/sec: 40506.62\n",
      "Step: 4437, Training Loss: 0.23933, LR: 0.0001000, Tokens/sec: 30958.89\n",
      "Step: 4438, Training Loss: 0.28622, LR: 0.0001000, Tokens/sec: 38836.54\n",
      "Step: 4439, Training Loss: 0.40998, LR: 0.0001000, Tokens/sec: 37591.48\n",
      "Step: 4440, Training Loss: 0.46766, LR: 0.0001000, Tokens/sec: 41264.47\n",
      "Step: 4441, Training Loss: 0.56576, LR: 0.0001000, Tokens/sec: 40845.43\n",
      "Step: 4442, Training Loss: 0.50154, LR: 0.0001000, Tokens/sec: 40400.31\n",
      "Step: 4443, Training Loss: 0.48879, LR: 0.0001000, Tokens/sec: 41422.52\n",
      "Step: 4444, Training Loss: 0.38880, LR: 0.0001000, Tokens/sec: 39719.94\n",
      "Step: 4445, Training Loss: 0.26223, LR: 0.0001000, Tokens/sec: 28479.00\n",
      "Step: 4446, Training Loss: 0.17898, LR: 0.0001000, Tokens/sec: 41428.97\n",
      "Step: 4447, Training Loss: 0.40047, LR: 0.0001000, Tokens/sec: 40608.84\n",
      "Step: 4448, Training Loss: 0.41405, LR: 0.0001000, Tokens/sec: 41452.85\n",
      "Step: 4449, Training Loss: 0.20460, LR: 0.0001000, Tokens/sec: 41409.07\n",
      "Step: 4450, Training Loss: 0.50983, LR: 0.0001000, Tokens/sec: 41501.51\n",
      "Computing Eval loss, steps: 20\n",
      "Step: 4450, Eval Loss: 0.46286\n",
      "Step: 4451, Training Loss: 0.32620, LR: 0.0001000, Tokens/sec: 40662.30\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "8b5596eda083de0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T06:45:57.202501Z",
     "start_time": "2025-02-24T06:45:55.223056Z"
    }
   },
   "source": [
    "input_text = \"\"\"\n",
    "ALL. Content, content.\n",
    "\n",
    "MENENIUS. O sir, you are not right: have you not known\n",
    "The worthiest men have done't?\n",
    "\n",
    "CORIOLANUS.\n",
    "\"\"\".strip()\n",
    "\n",
    "input_ids = tokenizer([input_text], return_tensors=\"pt\")['input_ids'].to(trainer.device)\n",
    "idx = model.generate(input_ids, temperature=0.25, top_k=50, max_new_tokens=128)\n",
    "print(tokenizer.batch_decode(idx)[0])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALL. Content, content.\n",
      "\n",
      "MENENIUS. O sir, you are not right: have you not known\n",
      "The worthiest men have done't?\n",
      "\n",
      "CORIOLANUS.\n",
      "O, content him!\n",
      "The gods have lost a deed with men in Rome,\n",
      "And none but we are merry, not without cause.\n",
      "The enemy is foolish, and we must change;\n",
      "If you receive the king, you must not use\n",
      "How things goers, and be at further\n",
      "As you can wish your honour and your nature\n",
      "As it is like to be so.\n",
      "\n",
      "HAMLET.\n",
      "Thats certain,\n",
      "And must changet.\n",
      "\n",
      "QUEEN.\n",
      "To cut and kill him.\n",
      "\n",
      "HAMLET.\n",
      "Have you forgot me?\n",
      "\n",
      "POL\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "82a94862-9944-4f3c-9c4c-d5d9b6e62e55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T06:45:57.206115Z",
     "start_time": "2025-02-24T06:45:57.204852Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
