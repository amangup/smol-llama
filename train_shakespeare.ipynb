{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-16T09:15:20.436693Z",
     "start_time": "2024-12-16T09:15:19.511403Z"
    }
   },
   "source": [
    "from model import ModelConfig, LlamaModel\n",
    "from train import TrainerConfig, DataLoader, Trainer\n",
    "\n",
    "from transformers import AutoTokenizer"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:15:20.440801Z",
     "start_time": "2024-12-16T09:15:20.439078Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer_id = \"HuggingFaceTB/SmolLM2-135M\"",
   "id": "2f28fa23c987e72b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:15:20.670956Z",
     "start_time": "2024-12-16T09:15:20.484288Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ],
   "id": "9bb4e51aa142abee",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:15:20.678652Z",
     "start_time": "2024-12-16T09:15:20.677029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_config = ModelConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=576,\n",
    "    d_head=64,\n",
    "    d_mlp_proj=1536,\n",
    "    n_layers=30,\n",
    "    n_kv_heads=3,\n",
    "    n_attn_heads=9,\n",
    "    rms_norm_eps=1e-5,\n",
    "    initializer_range=0.041666666666666664,\n",
    "    rope_theta=100000.0,\n",
    "    padding_idx=tokenizer.pad_token_id\n",
    ")"
   ],
   "id": "cde027092af8291e",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:15:20.722233Z",
     "start_time": "2024-12-16T09:15:20.720451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_config = TrainerConfig(\n",
    "    per_device_train_batch_size=32,\n",
    "    max_seq_len=128,\n",
    "    num_epochs=12,\n",
    "    eval_interval_steps=25,\n",
    "    learning_rate=1e-4,\n",
    "    grad_clip_norm=1.0,\n",
    "    val_size=0.2,\n",
    "    log_dir=\"runs/shakespeare\",\n",
    "    warmup_ratio=0.1\n",
    ")"
   ],
   "id": "809773e662327a12",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:15:20.766365Z",
     "start_time": "2024-12-16T09:15:20.764228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"data/tiny_shakespeare.txt\") as f:\n",
    "    text = f.read()"
   ],
   "id": "374f398bb34f7ac1",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:15:23.391460Z",
     "start_time": "2024-12-16T09:15:20.808857Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = LlamaModel(model_config)\n",
    "dataloader = DataLoader(train_config, tokenizer, text=text)\n",
    "trainer = Trainer(train_config, model)"
   ],
   "id": "9a912a0ec92039d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens                   | 341,120\n",
      "Num Trainable Params           | 162,826,560\n",
      "Train device                   | cuda, NVIDIA GeForce RTX 3090, N=1\n",
      "Training precision             | torch.bfloat16\n",
      "Flash Attention                | True\n",
      "torch.compile()                | True\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:18:17.472064Z",
     "start_time": "2024-12-16T09:15:23.398622Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train(dataloader)",
   "id": "ee8c2059258a0195",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training steps                 | 804 \n",
      "Step: 0, Training Loss: 11.31816, LR: 0.0000013, Tokens/sec: 170.54935834817286\n",
      "Computing Eval loss, steps: 17\n",
      "Step: 0, Eval Loss: 11.27680\n",
      "Step: 1, Training Loss: 11.29193, LR: 0.0000025, Tokens/sec: 181.5487660845126\n",
      "Step: 2, Training Loss: 11.29968, LR: 0.0000038, Tokens/sec: 74733.96459704115\n",
      "Step: 3, Training Loss: 11.26571, LR: 0.0000050, Tokens/sec: 75182.05492310377\n",
      "Step: 4, Training Loss: 11.23035, LR: 0.0000063, Tokens/sec: 81731.63147243724\n",
      "Step: 5, Training Loss: 11.22250, LR: 0.0000075, Tokens/sec: 89066.35475309359\n",
      "Step: 6, Training Loss: 11.23940, LR: 0.0000087, Tokens/sec: 70858.7236512347\n",
      "Step: 7, Training Loss: 11.09252, LR: 0.0000100, Tokens/sec: 83605.2084499759\n",
      "Step: 8, Training Loss: 11.12186, LR: 0.0000113, Tokens/sec: 87362.69968022296\n",
      "Step: 9, Training Loss: 11.01938, LR: 0.0000125, Tokens/sec: 74693.19033100348\n",
      "Step: 10, Training Loss: 10.94824, LR: 0.0000138, Tokens/sec: 88827.23607928514\n",
      "Step: 11, Training Loss: 10.95432, LR: 0.0000150, Tokens/sec: 91465.25348486373\n",
      "Step: 12, Training Loss: 10.87606, LR: 0.0000163, Tokens/sec: 68460.1590384207\n",
      "Step: 13, Training Loss: 10.74125, LR: 0.0000175, Tokens/sec: 84779.62173087624\n",
      "Step: 14, Training Loss: 10.64515, LR: 0.0000188, Tokens/sec: 82241.32012932893\n",
      "Step: 15, Training Loss: 10.46258, LR: 0.0000200, Tokens/sec: 75887.80298747058\n",
      "Step: 16, Training Loss: 10.53003, LR: 0.0000213, Tokens/sec: 83289.59239923088\n",
      "Step: 17, Training Loss: 10.34101, LR: 0.0000225, Tokens/sec: 74090.876373935\n",
      "Step: 18, Training Loss: 10.24749, LR: 0.0000238, Tokens/sec: 87385.86713789476\n",
      "Step: 19, Training Loss: 10.14624, LR: 0.0000250, Tokens/sec: 80757.58979431048\n",
      "Step: 20, Training Loss: 9.98405, LR: 0.0000263, Tokens/sec: 87206.66512739859\n",
      "Step: 21, Training Loss: 9.90096, LR: 0.0000275, Tokens/sec: 66395.55579691887\n",
      "Step: 22, Training Loss: 9.83485, LR: 0.0000287, Tokens/sec: 91205.29963374368\n",
      "Step: 23, Training Loss: 9.66836, LR: 0.0000300, Tokens/sec: 74200.66581707049\n",
      "Step: 24, Training Loss: 9.49859, LR: 0.0000313, Tokens/sec: 91017.97270129013\n",
      "Step: 25, Training Loss: 9.33049, LR: 0.0000325, Tokens/sec: 72313.14133005918\n",
      "Computing Eval loss, steps: 17\n",
      "Step: 25, Eval Loss: 9.33409\n",
      "Step: 26, Training Loss: 9.34332, LR: 0.0000338, Tokens/sec: 81805.16844880678\n",
      "Step: 27, Training Loss: 9.25925, LR: 0.0000350, Tokens/sec: 91678.18370593713\n",
      "Step: 28, Training Loss: 9.14553, LR: 0.0000363, Tokens/sec: 65123.09043606975\n",
      "Step: 29, Training Loss: 9.00084, LR: 0.0000375, Tokens/sec: 92494.01787209799\n",
      "Step: 30, Training Loss: 9.01025, LR: 0.0000387, Tokens/sec: 83472.94400061983\n",
      "Step: 31, Training Loss: 9.03533, LR: 0.0000400, Tokens/sec: 74980.51578243077\n",
      "Step: 32, Training Loss: 8.88043, LR: 0.0000412, Tokens/sec: 91229.6764876393\n",
      "Step: 33, Training Loss: 8.84377, LR: 0.0000425, Tokens/sec: 72435.08743080436\n",
      "Step: 34, Training Loss: 8.71799, LR: 0.0000437, Tokens/sec: 92492.80646011468\n",
      "Step: 35, Training Loss: 8.71106, LR: 0.0000450, Tokens/sec: 75941.52049895487\n",
      "Step: 36, Training Loss: 8.67245, LR: 0.0000463, Tokens/sec: 82894.70747669642\n",
      "Step: 37, Training Loss: 8.54786, LR: 0.0000475, Tokens/sec: 76264.48224875492\n",
      "Step: 38, Training Loss: 8.69257, LR: 0.0000487, Tokens/sec: 94283.8331536121\n",
      "Step: 39, Training Loss: 8.51006, LR: 0.0000500, Tokens/sec: 72810.5239145299\n",
      "Step: 40, Training Loss: 8.41970, LR: 0.0000512, Tokens/sec: 92332.47335313416\n",
      "Step: 41, Training Loss: 8.45948, LR: 0.0000525, Tokens/sec: 71249.43521007366\n",
      "Step: 42, Training Loss: 8.47169, LR: 0.0000537, Tokens/sec: 85344.14358917833\n",
      "Step: 43, Training Loss: 8.31504, LR: 0.0000550, Tokens/sec: 81196.46962955261\n",
      "Step: 44, Training Loss: 8.39372, LR: 0.0000563, Tokens/sec: 94486.5571420073\n",
      "Step: 45, Training Loss: 8.25344, LR: 0.0000575, Tokens/sec: 61619.68639215127\n",
      "Step: 46, Training Loss: 8.20755, LR: 0.0000588, Tokens/sec: 91673.40491022264\n",
      "Step: 47, Training Loss: 8.13747, LR: 0.0000600, Tokens/sec: 83294.11635631001\n",
      "Step: 48, Training Loss: 8.00097, LR: 0.0000613, Tokens/sec: 67336.5458417213\n",
      "Step: 49, Training Loss: 7.98972, LR: 0.0000625, Tokens/sec: 92927.06811701636\n",
      "Step: 50, Training Loss: 8.00846, LR: 0.0000638, Tokens/sec: 92564.01787591133\n",
      "Computing Eval loss, steps: 17\n",
      "Step: 50, Eval Loss: 7.98872\n",
      "Step: 51, Training Loss: 8.06137, LR: 0.0000650, Tokens/sec: 90498.14144658254\n",
      "Step: 52, Training Loss: 8.08714, LR: 0.0000662, Tokens/sec: 91197.03274766778\n",
      "Step: 53, Training Loss: 7.89532, LR: 0.0000675, Tokens/sec: 73358.77297484982\n",
      "Step: 54, Training Loss: 7.81051, LR: 0.0000688, Tokens/sec: 80672.87639186959\n",
      "Step: 55, Training Loss: 7.75655, LR: 0.0000700, Tokens/sec: 90178.0018096483\n",
      "Step: 56, Training Loss: 7.76948, LR: 0.0000713, Tokens/sec: 73437.93864635538\n",
      "Step: 57, Training Loss: 7.78726, LR: 0.0000725, Tokens/sec: 91285.60960531278\n",
      "Step: 58, Training Loss: 7.64728, LR: 0.0000738, Tokens/sec: 73145.04496515605\n",
      "Step: 59, Training Loss: 7.54261, LR: 0.0000750, Tokens/sec: 87629.82253959683\n",
      "Step: 60, Training Loss: 7.50852, LR: 0.0000762, Tokens/sec: 82809.57356196096\n",
      "Step: 61, Training Loss: 7.60244, LR: 0.0000775, Tokens/sec: 61799.6588738809\n",
      "Step: 62, Training Loss: 7.49109, LR: 0.0000788, Tokens/sec: 64454.35080568351\n",
      "Step: 63, Training Loss: 7.43148, LR: 0.0000800, Tokens/sec: 61819.87012201871\n",
      "Step: 64, Training Loss: 7.36107, LR: 0.0000813, Tokens/sec: 64316.91051970064\n",
      "Step: 65, Training Loss: 7.23324, LR: 0.0000825, Tokens/sec: 64059.9460985074\n",
      "Step: 66, Training Loss: 7.30760, LR: 0.0000838, Tokens/sec: 60858.34303873397\n",
      "Step: 67, Training Loss: 7.17857, LR: 0.0000850, Tokens/sec: 63752.583198664666\n",
      "Step: 68, Training Loss: 7.16104, LR: 0.0000863, Tokens/sec: 63114.664957489484\n",
      "Step: 69, Training Loss: 7.10505, LR: 0.0000875, Tokens/sec: 66727.53273342083\n",
      "Step: 70, Training Loss: 7.12036, LR: 0.0000888, Tokens/sec: 93647.94837125082\n",
      "Step: 71, Training Loss: 7.04593, LR: 0.0000900, Tokens/sec: 81342.20517514818\n",
      "Step: 72, Training Loss: 6.94053, LR: 0.0000912, Tokens/sec: 84196.60029829606\n",
      "Step: 73, Training Loss: 6.92407, LR: 0.0000925, Tokens/sec: 68161.05565556476\n",
      "Step: 74, Training Loss: 6.76412, LR: 0.0000938, Tokens/sec: 85830.3590616838\n",
      "Step: 75, Training Loss: 6.79877, LR: 0.0000950, Tokens/sec: 76543.26289526856\n",
      "Computing Eval loss, steps: 17\n",
      "Step: 75, Eval Loss: 6.81102\n",
      "Step: 76, Training Loss: 6.76489, LR: 0.0000963, Tokens/sec: 88605.30192458186\n",
      "Step: 77, Training Loss: 6.67464, LR: 0.0000975, Tokens/sec: 91551.28283356392\n",
      "Step: 78, Training Loss: 6.74080, LR: 0.0000988, Tokens/sec: 61459.89979161939\n",
      "Step: 79, Training Loss: 6.73644, LR: 0.0001000, Tokens/sec: 68779.40567962588\n",
      "Step: 80, Training Loss: 6.58552, LR: 0.0001000, Tokens/sec: 67664.461976694\n",
      "Step: 81, Training Loss: 6.54341, LR: 0.0001000, Tokens/sec: 71434.13353430555\n",
      "Step: 82, Training Loss: 6.50266, LR: 0.0001000, Tokens/sec: 78240.80591394492\n",
      "Step: 83, Training Loss: 6.63603, LR: 0.0001000, Tokens/sec: 85537.75072817456\n",
      "Step: 84, Training Loss: 6.52897, LR: 0.0001000, Tokens/sec: 83485.77402727568\n",
      "Step: 85, Training Loss: 6.51698, LR: 0.0001000, Tokens/sec: 65727.46040813968\n",
      "Step: 86, Training Loss: 6.40615, LR: 0.0001000, Tokens/sec: 88797.89032519495\n",
      "Step: 87, Training Loss: 6.23905, LR: 0.0001000, Tokens/sec: 88800.52581811104\n",
      "Step: 88, Training Loss: 6.37435, LR: 0.0001000, Tokens/sec: 67197.71895613146\n",
      "Step: 89, Training Loss: 6.36940, LR: 0.0001000, Tokens/sec: 79723.31028671835\n",
      "Step: 90, Training Loss: 6.37998, LR: 0.0001000, Tokens/sec: 79676.1987295935\n",
      "Step: 91, Training Loss: 6.27823, LR: 0.0000999, Tokens/sec: 79226.1615590868\n",
      "Step: 92, Training Loss: 6.20003, LR: 0.0000999, Tokens/sec: 71401.90653279825\n",
      "Step: 93, Training Loss: 6.20673, LR: 0.0000999, Tokens/sec: 82050.68047753474\n",
      "Step: 94, Training Loss: 6.23257, LR: 0.0000999, Tokens/sec: 80945.3835823478\n",
      "Step: 95, Training Loss: 6.10593, LR: 0.0000999, Tokens/sec: 78225.81857565619\n",
      "Step: 96, Training Loss: 6.07998, LR: 0.0000999, Tokens/sec: 80205.60833297002\n",
      "Step: 97, Training Loss: 6.12423, LR: 0.0000999, Tokens/sec: 86102.71128525166\n",
      "Step: 98, Training Loss: 6.18590, LR: 0.0000999, Tokens/sec: 78239.50568712785\n",
      "Step: 99, Training Loss: 6.05757, LR: 0.0000998, Tokens/sec: 86429.45144390714\n",
      "Step: 100, Training Loss: 5.97267, LR: 0.0000998, Tokens/sec: 63819.02619043697\n",
      "Computing Eval loss, steps: 17\n",
      "Step: 100, Eval Loss: 6.04971\n",
      "Step: 101, Training Loss: 6.00904, LR: 0.0000998, Tokens/sec: 87607.93088802717\n",
      "Step: 102, Training Loss: 6.00848, LR: 0.0000998, Tokens/sec: 89623.82172940273\n",
      "Step: 103, Training Loss: 5.93400, LR: 0.0000998, Tokens/sec: 75535.29462546682\n",
      "Step: 104, Training Loss: 5.87924, LR: 0.0000998, Tokens/sec: 81487.26513612052\n",
      "Step: 105, Training Loss: 6.09962, LR: 0.0000997, Tokens/sec: 87169.62319231046\n",
      "Step: 106, Training Loss: 5.87319, LR: 0.0000997, Tokens/sec: 65278.794242235606\n",
      "Step: 107, Training Loss: 5.76205, LR: 0.0000997, Tokens/sec: 90406.77907714744\n",
      "Step: 108, Training Loss: 5.89678, LR: 0.0000997, Tokens/sec: 91266.53258782861\n",
      "Step: 109, Training Loss: 5.93782, LR: 0.0000996, Tokens/sec: 69437.57749693593\n",
      "Step: 110, Training Loss: 5.78092, LR: 0.0000996, Tokens/sec: 80867.12618950514\n",
      "Step: 111, Training Loss: 5.93261, LR: 0.0000996, Tokens/sec: 90886.60124358641\n",
      "Step: 112, Training Loss: 5.82091, LR: 0.0000996, Tokens/sec: 65106.11629034814\n",
      "Step: 113, Training Loss: 5.77509, LR: 0.0000995, Tokens/sec: 91604.45606843855\n",
      "Step: 114, Training Loss: 5.64987, LR: 0.0000995, Tokens/sec: 85722.42384160325\n",
      "Step: 115, Training Loss: 5.54147, LR: 0.0000995, Tokens/sec: 67778.61207759677\n",
      "Step: 116, Training Loss: 5.62999, LR: 0.0000995, Tokens/sec: 82026.0169825004\n",
      "Step: 117, Training Loss: 5.68649, LR: 0.0000994, Tokens/sec: 90789.88484315076\n",
      "Step: 118, Training Loss: 5.67480, LR: 0.0000994, Tokens/sec: 72124.39754286036\n",
      "Step: 119, Training Loss: 5.89090, LR: 0.0000994, Tokens/sec: 88696.72996636281\n",
      "Step: 120, Training Loss: 5.69282, LR: 0.0000993, Tokens/sec: 85804.28983393808\n",
      "Step: 121, Training Loss: 5.57574, LR: 0.0000993, Tokens/sec: 64976.57250368558\n",
      "Step: 122, Training Loss: 5.55609, LR: 0.0000993, Tokens/sec: 86421.19067195029\n",
      "Step: 123, Training Loss: 5.66112, LR: 0.0000992, Tokens/sec: 74967.78316632108\n",
      "Step: 124, Training Loss: 5.70942, LR: 0.0000992, Tokens/sec: 69909.70076724981\n",
      "Step: 125, Training Loss: 5.56367, LR: 0.0000991, Tokens/sec: 72324.43001646594\n",
      "Computing Eval loss, steps: 17\n",
      "Step: 125, Eval Loss: 5.59996\n",
      "Step: 126, Training Loss: 5.42095, LR: 0.0000991, Tokens/sec: 91252.42166818565\n",
      "Step: 127, Training Loss: 5.47257, LR: 0.0000991, Tokens/sec: 86800.2484883878\n",
      "Step: 128, Training Loss: 5.55983, LR: 0.0000990, Tokens/sec: 65221.00134848819\n",
      "Step: 129, Training Loss: 5.48909, LR: 0.0000990, Tokens/sec: 71878.89627501818\n",
      "Step: 130, Training Loss: 5.55605, LR: 0.0000989, Tokens/sec: 73373.55801906674\n",
      "Step: 131, Training Loss: 5.53132, LR: 0.0000989, Tokens/sec: 62752.71238595587\n",
      "Step: 132, Training Loss: 5.32291, LR: 0.0000989, Tokens/sec: 88836.4468583435\n",
      "Step: 133, Training Loss: 5.55562, LR: 0.0000988, Tokens/sec: 82332.59001625728\n",
      "Step: 134, Training Loss: 5.48685, LR: 0.0000988, Tokens/sec: 88876.52179045219\n",
      "Step: 135, Training Loss: 5.36628, LR: 0.0000987, Tokens/sec: 79483.02809657858\n",
      "Step: 136, Training Loss: 5.38671, LR: 0.0000987, Tokens/sec: 86182.46219913382\n",
      "Step: 137, Training Loss: 5.46949, LR: 0.0000986, Tokens/sec: 69550.18874096926\n",
      "Step: 138, Training Loss: 5.45031, LR: 0.0000986, Tokens/sec: 71615.38412487549\n",
      "Step: 139, Training Loss: 5.31703, LR: 0.0000985, Tokens/sec: 69406.03262820066\n",
      "Step: 140, Training Loss: 5.25637, LR: 0.0000985, Tokens/sec: 88957.45640390876\n",
      "Step: 141, Training Loss: 5.22025, LR: 0.0000984, Tokens/sec: 90993.50872060645\n",
      "Step: 142, Training Loss: 5.22175, LR: 0.0000984, Tokens/sec: 65044.261224365655\n",
      "Step: 143, Training Loss: 5.23577, LR: 0.0000983, Tokens/sec: 91024.00426291504\n",
      "Step: 144, Training Loss: 5.13562, LR: 0.0000983, Tokens/sec: 89548.54372758933\n",
      "Step: 145, Training Loss: 5.25589, LR: 0.0000982, Tokens/sec: 63670.98515636838\n",
      "Step: 146, Training Loss: 5.39827, LR: 0.0000982, Tokens/sec: 87386.8589784123\n",
      "Step: 147, Training Loss: 5.24065, LR: 0.0000981, Tokens/sec: 90816.53688585006\n",
      "Step: 148, Training Loss: 5.23384, LR: 0.0000981, Tokens/sec: 77468.80513932447\n",
      "Step: 149, Training Loss: 5.12613, LR: 0.0000980, Tokens/sec: 86375.12166811262\n",
      "Step: 150, Training Loss: 5.41147, LR: 0.0000979, Tokens/sec: 91849.18946552502\n",
      "Computing Eval loss, steps: 17\n",
      "Step: 150, Eval Loss: 5.32401\n",
      "Step: 151, Training Loss: 5.30878, LR: 0.0000979, Tokens/sec: 92344.33869314197\n",
      "Step: 152, Training Loss: 5.31696, LR: 0.0000978, Tokens/sec: 92784.12050387161\n",
      "Step: 153, Training Loss: 5.26019, LR: 0.0000978, Tokens/sec: 63950.98956036037\n",
      "Step: 154, Training Loss: 4.98123, LR: 0.0000977, Tokens/sec: 88057.20312097535\n",
      "Step: 155, Training Loss: 5.21610, LR: 0.0000976, Tokens/sec: 94199.98547014459\n",
      "Step: 156, Training Loss: 5.32927, LR: 0.0000976, Tokens/sec: 70642.84471118695\n",
      "Step: 157, Training Loss: 5.31250, LR: 0.0000975, Tokens/sec: 89233.09470583918\n",
      "Step: 158, Training Loss: 5.26610, LR: 0.0000974, Tokens/sec: 80646.8618669687\n",
      "Step: 159, Training Loss: 5.12106, LR: 0.0000974, Tokens/sec: 72766.33036498798\n",
      "Step: 160, Training Loss: 5.10168, LR: 0.0000973, Tokens/sec: 80628.43089846997\n",
      "Step: 161, Training Loss: 5.22805, LR: 0.0000972, Tokens/sec: 84539.09731913758\n",
      "Step: 162, Training Loss: 5.05684, LR: 0.0000972, Tokens/sec: 89949.43529047388\n",
      "Step: 163, Training Loss: 5.09671, LR: 0.0000971, Tokens/sec: 68238.20564947471\n",
      "Step: 164, Training Loss: 5.15369, LR: 0.0000970, Tokens/sec: 93086.64402548266\n",
      "Step: 165, Training Loss: 5.17450, LR: 0.0000970, Tokens/sec: 93159.34733562861\n",
      "Step: 166, Training Loss: 5.06236, LR: 0.0000969, Tokens/sec: 68179.58542816948\n",
      "Step: 167, Training Loss: 4.96670, LR: 0.0000968, Tokens/sec: 86576.31123181258\n",
      "Step: 168, Training Loss: 5.09215, LR: 0.0000968, Tokens/sec: 93495.19720036809\n",
      "Step: 169, Training Loss: 5.14944, LR: 0.0000967, Tokens/sec: 70123.01452587037\n",
      "Step: 170, Training Loss: 5.01023, LR: 0.0000966, Tokens/sec: 93553.47361201473\n",
      "Step: 171, Training Loss: 4.95898, LR: 0.0000965, Tokens/sec: 93577.13588073506\n",
      "Step: 172, Training Loss: 5.22425, LR: 0.0000965, Tokens/sec: 69168.95142599609\n",
      "Step: 173, Training Loss: 5.03032, LR: 0.0000964, Tokens/sec: 83429.50505705486\n",
      "Step: 174, Training Loss: 4.88950, LR: 0.0000963, Tokens/sec: 83707.74291733204\n",
      "Step: 175, Training Loss: 5.03506, LR: 0.0000962, Tokens/sec: 69468.92835371009\n",
      "Computing Eval loss, steps: 17\n",
      "Step: 175, Eval Loss: 5.16278\n",
      "Step: 176, Training Loss: 5.11358, LR: 0.0000962, Tokens/sec: 86363.39313046169\n",
      "Step: 177, Training Loss: 4.94203, LR: 0.0000961, Tokens/sec: 87971.92852756697\n",
      "Step: 178, Training Loss: 5.14945, LR: 0.0000960, Tokens/sec: 68572.46901439564\n",
      "Step: 179, Training Loss: 4.98053, LR: 0.0000959, Tokens/sec: 92741.51596663088\n",
      "Step: 180, Training Loss: 4.97436, LR: 0.0000958, Tokens/sec: 93929.89948890275\n",
      "Step: 181, Training Loss: 4.82218, LR: 0.0000957, Tokens/sec: 65443.07397472852\n",
      "Step: 182, Training Loss: 4.72513, LR: 0.0000957, Tokens/sec: 78040.82918928552\n",
      "Step: 183, Training Loss: 4.82899, LR: 0.0000956, Tokens/sec: 85747.18857013372\n",
      "Step: 184, Training Loss: 4.95105, LR: 0.0000955, Tokens/sec: 63647.41914844607\n",
      "Step: 185, Training Loss: 4.95101, LR: 0.0000954, Tokens/sec: 91878.07465700715\n",
      "Step: 186, Training Loss: 5.18942, LR: 0.0000953, Tokens/sec: 91472.65185417332\n",
      "Step: 187, Training Loss: 4.96827, LR: 0.0000952, Tokens/sec: 66282.01414013193\n",
      "Step: 188, Training Loss: 4.86809, LR: 0.0000951, Tokens/sec: 77085.61860455544\n",
      "Step: 189, Training Loss: 4.75418, LR: 0.0000951, Tokens/sec: 90643.83857911319\n",
      "Step: 190, Training Loss: 4.98214, LR: 0.0000950, Tokens/sec: 65535.069921341834\n",
      "Step: 191, Training Loss: 5.04301, LR: 0.0000949, Tokens/sec: 93251.9042665439\n",
      "Step: 192, Training Loss: 4.86456, LR: 0.0000948, Tokens/sec: 88927.25227582008\n",
      "Step: 193, Training Loss: 4.71559, LR: 0.0000947, Tokens/sec: 68583.79011187784\n",
      "Step: 194, Training Loss: 4.81426, LR: 0.0000946, Tokens/sec: 85254.4782677032\n",
      "Step: 195, Training Loss: 4.93772, LR: 0.0000945, Tokens/sec: 90577.03209338716\n",
      "Step: 196, Training Loss: 4.83987, LR: 0.0000944, Tokens/sec: 64773.35130678847\n",
      "Step: 197, Training Loss: 4.94347, LR: 0.0000943, Tokens/sec: 90188.50962269992\n",
      "Step: 198, Training Loss: 4.91480, LR: 0.0000942, Tokens/sec: 72855.22082895669\n",
      "Step: 199, Training Loss: 4.68958, LR: 0.0000941, Tokens/sec: 61935.31654372737\n",
      "Step: 200, Training Loss: 4.95606, LR: 0.0000940, Tokens/sec: 92670.66028206024\n",
      "Computing Eval loss, steps: 17\n",
      "Step: 200, Eval Loss: 5.07087\n",
      "Step: 201, Training Loss: 4.85701, LR: 0.0000939, Tokens/sec: 85129.7366900056\n",
      "Step: 202, Training Loss: 4.79903, LR: 0.0000938, Tokens/sec: 90819.69833639998\n",
      "Step: 203, Training Loss: 4.75739, LR: 0.0000937, Tokens/sec: 67271.73121924652\n",
      "Step: 204, Training Loss: 4.90874, LR: 0.0000936, Tokens/sec: 93826.55620047513\n",
      "Step: 205, Training Loss: 4.87323, LR: 0.0000935, Tokens/sec: 91617.3481527171\n",
      "Step: 206, Training Loss: 4.73456, LR: 0.0000934, Tokens/sec: 72177.60158576949\n",
      "Step: 207, Training Loss: 4.66040, LR: 0.0000933, Tokens/sec: 93947.45796242327\n",
      "Step: 208, Training Loss: 4.67203, LR: 0.0000932, Tokens/sec: 83004.40608955613\n",
      "Step: 209, Training Loss: 4.68236, LR: 0.0000931, Tokens/sec: 64656.66827611138\n",
      "Step: 210, Training Loss: 4.67510, LR: 0.0000930, Tokens/sec: 82182.22045107585\n",
      "Step: 211, Training Loss: 4.56328, LR: 0.0000929, Tokens/sec: 82424.86051197562\n",
      "Step: 212, Training Loss: 4.72190, LR: 0.0000928, Tokens/sec: 70367.70924458028\n",
      "Step: 213, Training Loss: 4.88450, LR: 0.0000927, Tokens/sec: 90238.99586457947\n",
      "Step: 214, Training Loss: 4.70209, LR: 0.0000926, Tokens/sec: 73276.32079683787\n",
      "Step: 215, Training Loss: 4.69925, LR: 0.0000925, Tokens/sec: 90758.52433949675\n",
      "Step: 216, Training Loss: 4.59131, LR: 0.0000924, Tokens/sec: 66286.73492905126\n",
      "Step: 217, Training Loss: 4.85581, LR: 0.0000923, Tokens/sec: 90135.02319036034\n",
      "Step: 218, Training Loss: 4.76194, LR: 0.0000922, Tokens/sec: 89859.6894556225\n",
      "Step: 219, Training Loss: 4.79682, LR: 0.0000921, Tokens/sec: 62489.91556860786\n",
      "Step: 220, Training Loss: 4.70621, LR: 0.0000919, Tokens/sec: 83648.20366581618\n",
      "Step: 221, Training Loss: 4.44943, LR: 0.0000918, Tokens/sec: 81136.4169333608\n",
      "Step: 222, Training Loss: 4.69745, LR: 0.0000917, Tokens/sec: 75942.11609565481\n",
      "Step: 223, Training Loss: 4.81591, LR: 0.0000916, Tokens/sec: 84597.22292435323\n",
      "Step: 224, Training Loss: 4.83005, LR: 0.0000915, Tokens/sec: 69092.30939702434\n",
      "Step: 225, Training Loss: 4.77886, LR: 0.0000914, Tokens/sec: 92708.93773918781\n",
      "Computing Eval loss, steps: 17\n",
      "Step: 225, Eval Loss: 5.00129\n",
      "Step: 226, Training Loss: 4.64834, LR: 0.0000913, Tokens/sec: 88575.9493685249\n",
      "Step: 227, Training Loss: 4.57212, LR: 0.0000912, Tokens/sec: 86845.02846268902\n",
      "Step: 228, Training Loss: 4.73520, LR: 0.0000910, Tokens/sec: 71103.00092427341\n",
      "Step: 229, Training Loss: 4.57527, LR: 0.0000909, Tokens/sec: 89380.02905153198\n",
      "Step: 230, Training Loss: 4.61855, LR: 0.0000908, Tokens/sec: 93476.99671228432\n",
      "Step: 231, Training Loss: 4.69148, LR: 0.0000907, Tokens/sec: 65987.60863580786\n",
      "Step: 232, Training Loss: 4.69248, LR: 0.0000906, Tokens/sec: 86576.53266761612\n",
      "Step: 233, Training Loss: 4.56226, LR: 0.0000904, Tokens/sec: 82996.75172128076\n",
      "Step: 234, Training Loss: 4.51233, LR: 0.0000903, Tokens/sec: 67003.99206929235\n",
      "Step: 235, Training Loss: 4.63321, LR: 0.0000902, Tokens/sec: 93197.05630263561\n",
      "Step: 236, Training Loss: 4.67026, LR: 0.0000901, Tokens/sec: 91684.61094497133\n",
      "Step: 237, Training Loss: 4.53706, LR: 0.0000900, Tokens/sec: 62944.4907198495\n",
      "Step: 238, Training Loss: 4.46964, LR: 0.0000898, Tokens/sec: 89976.74246242153\n",
      "Step: 239, Training Loss: 4.72789, LR: 0.0000897, Tokens/sec: 91081.97807221461\n",
      "Step: 240, Training Loss: 4.56824, LR: 0.0000896, Tokens/sec: 69188.3710903364\n",
      "Step: 241, Training Loss: 4.43024, LR: 0.0000895, Tokens/sec: 93390.7656149772\n",
      "Step: 242, Training Loss: 4.59053, LR: 0.0000893, Tokens/sec: 92789.35634326449\n",
      "Step: 243, Training Loss: 4.65984, LR: 0.0000892, Tokens/sec: 67493.35146688094\n",
      "Step: 244, Training Loss: 4.46496, LR: 0.0000891, Tokens/sec: 93224.25155519262\n",
      "Step: 245, Training Loss: 4.67563, LR: 0.0000890, Tokens/sec: 93286.24623835187\n",
      "Step: 246, Training Loss: 4.51273, LR: 0.0000888, Tokens/sec: 71277.80713669202\n",
      "Step: 247, Training Loss: 4.52032, LR: 0.0000887, Tokens/sec: 91620.52461358695\n",
      "Step: 248, Training Loss: 4.40844, LR: 0.0000886, Tokens/sec: 93361.34721395938\n",
      "Step: 249, Training Loss: 4.27538, LR: 0.0000884, Tokens/sec: 67086.63841126647\n",
      "Step: 250, Training Loss: 4.35510, LR: 0.0000883, Tokens/sec: 93211.71146720128\n",
      "Computing Eval loss, steps: 17\n",
      "Step: 250, Eval Loss: 4.98383\n",
      "Step: 251, Training Loss: 4.51162, LR: 0.0000882, Tokens/sec: 90064.11020853576\n",
      "Step: 252, Training Loss: 4.45012, LR: 0.0000880, Tokens/sec: 93551.50995825687\n",
      "Step: 253, Training Loss: 4.71846, LR: 0.0000879, Tokens/sec: 72225.29029005294\n",
      "Step: 254, Training Loss: 4.51237, LR: 0.0000878, Tokens/sec: 92602.08792169896\n",
      "Step: 255, Training Loss: 4.41603, LR: 0.0000876, Tokens/sec: 93349.81693100503\n",
      "Step: 256, Training Loss: 4.31390, LR: 0.0000875, Tokens/sec: 65572.2829853301\n",
      "Step: 257, Training Loss: 4.49804, LR: 0.0000874, Tokens/sec: 93370.1814301915\n",
      "Step: 258, Training Loss: 4.65242, LR: 0.0000872, Tokens/sec: 93196.69793725174\n",
      "Step: 259, Training Loss: 4.38963, LR: 0.0000871, Tokens/sec: 70992.34806227786\n",
      "Step: 260, Training Loss: 4.28475, LR: 0.0000870, Tokens/sec: 88789.36695216916\n",
      "Step: 261, Training Loss: 4.37690, LR: 0.0000868, Tokens/sec: 88890.79865811803\n",
      "Step: 262, Training Loss: 4.45792, LR: 0.0000867, Tokens/sec: 65081.291081082054\n",
      "Step: 263, Training Loss: 4.58414, LR: 0.0000865, Tokens/sec: 84500.95665205023\n",
      "Step: 264, Training Loss: 4.53682, LR: 0.0000864, Tokens/sec: 84515.1317743998\n",
      "Step: 265, Training Loss: 4.46003, LR: 0.0000863, Tokens/sec: 64204.79725562577\n",
      "Step: 266, Training Loss: 4.26103, LR: 0.0000861, Tokens/sec: 91021.82172845521\n",
      "Step: 267, Training Loss: 4.52279, LR: 0.0000860, Tokens/sec: 92385.6642377534\n",
      "Step: 268, Training Loss: 4.39766, LR: 0.0000858, Tokens/sec: 63833.1610633252\n",
      "Step: 269, Training Loss: 4.36452, LR: 0.0000857, Tokens/sec: 85478.7237406848\n",
      "Step: 270, Training Loss: 4.33786, LR: 0.0000856, Tokens/sec: 90711.14950420226\n",
      "Step: 271, Training Loss: 4.47378, LR: 0.0000854, Tokens/sec: 65147.92267815155\n",
      "Step: 272, Training Loss: 4.44374, LR: 0.0000853, Tokens/sec: 92885.20214689674\n",
      "Step: 273, Training Loss: 4.29435, LR: 0.0000851, Tokens/sec: 90567.28064628867\n",
      "Step: 274, Training Loss: 4.20845, LR: 0.0000850, Tokens/sec: 70703.93988165485\n",
      "Step: 275, Training Loss: 4.28536, LR: 0.0000848, Tokens/sec: 83534.69389477937\n",
      "Computing Eval loss, steps: 17\n",
      "Step: 275, Eval Loss: 4.97724\n",
      "Step: 276, Training Loss: 4.32528, LR: 0.0000847, Tokens/sec: 92082.99375522773\n",
      "Step: 277, Training Loss: 4.30764, LR: 0.0000845, Tokens/sec: 93170.68649423866\n",
      "Step: 278, Training Loss: 4.16278, LR: 0.0000844, Tokens/sec: 70383.26022360439\n",
      "Step: 279, Training Loss: 4.31448, LR: 0.0000842, Tokens/sec: 93805.30686492115\n",
      "Step: 280, Training Loss: 4.45207, LR: 0.0000841, Tokens/sec: 93049.15538895526\n",
      "Step: 281, Training Loss: 4.27492, LR: 0.0000839, Tokens/sec: 67337.61299091954\n",
      "Step: 282, Training Loss: 4.32757, LR: 0.0000838, Tokens/sec: 91137.38544817301\n",
      "Step: 283, Training Loss: 4.19122, LR: 0.0000836, Tokens/sec: 92893.52302613834\n",
      "Step: 284, Training Loss: 4.39154, LR: 0.0000835, Tokens/sec: 71956.7086232431\n",
      "Step: 285, Training Loss: 4.33148, LR: 0.0000833, Tokens/sec: 93323.31595377892\n",
      "Step: 286, Training Loss: 4.36792, LR: 0.0000832, Tokens/sec: 93776.24937427932\n",
      "Step: 287, Training Loss: 4.25531, LR: 0.0000830, Tokens/sec: 67429.24267322736\n",
      "Step: 288, Training Loss: 4.08228, LR: 0.0000829, Tokens/sec: 93306.22379955479\n",
      "Step: 289, Training Loss: 4.28202, LR: 0.0000827, Tokens/sec: 93688.99407382938\n",
      "Step: 290, Training Loss: 4.36578, LR: 0.0000826, Tokens/sec: 74246.29497466107\n",
      "Step: 291, Training Loss: 4.38660, LR: 0.0000824, Tokens/sec: 88037.25643252216\n",
      "Step: 292, Training Loss: 4.35215, LR: 0.0000823, Tokens/sec: 93448.50679924403\n",
      "Step: 293, Training Loss: 4.20130, LR: 0.0000821, Tokens/sec: 67406.6599207658\n",
      "Step: 294, Training Loss: 4.14668, LR: 0.0000820, Tokens/sec: 93680.72292432083\n",
      "Step: 295, Training Loss: 4.30274, LR: 0.0000818, Tokens/sec: 93440.72781810905\n",
      "Step: 296, Training Loss: 4.14203, LR: 0.0000816, Tokens/sec: 72122.43290364789\n",
      "Step: 297, Training Loss: 4.17997, LR: 0.0000815, Tokens/sec: 93704.80975879199\n",
      "Step: 298, Training Loss: 4.26137, LR: 0.0000813, Tokens/sec: 93711.20056902131\n",
      "Step: 299, Training Loss: 4.23904, LR: 0.0000812, Tokens/sec: 69218.61996972765\n",
      "Step: 300, Training Loss: 4.09008, LR: 0.0000810, Tokens/sec: 93488.02923522088\n",
      "Computing Eval loss, steps: 17\n",
      "Step: 300, Eval Loss: 4.97395\n",
      "Step: 301, Training Loss: 4.12519, LR: 0.0000808, Tokens/sec: 87827.83223359963\n",
      "Step: 302, Training Loss: 4.22743, LR: 0.0000807, Tokens/sec: 92968.11933607634\n",
      "Step: 303, Training Loss: 4.21927, LR: 0.0000805, Tokens/sec: 67306.68598123161\n",
      "Step: 304, Training Loss: 4.11535, LR: 0.0000804, Tokens/sec: 88056.44968978093\n",
      "Step: 305, Training Loss: 4.06478, LR: 0.0000802, Tokens/sec: 89078.62365293346\n",
      "Step: 306, Training Loss: 4.29573, LR: 0.0000800, Tokens/sec: 62975.90550575335\n",
      "Step: 307, Training Loss: 4.12571, LR: 0.0000799, Tokens/sec: 91359.64588876153\n",
      "Step: 308, Training Loss: 4.05262, LR: 0.0000797, Tokens/sec: 92443.86297452914\n",
      "Step: 309, Training Loss: 4.15116, LR: 0.0000796, Tokens/sec: 66174.19451519362\n",
      "Step: 310, Training Loss: 4.20510, LR: 0.0000794, Tokens/sec: 90628.17895771819\n",
      "Step: 311, Training Loss: 4.07641, LR: 0.0000792, Tokens/sec: 92136.20835603587\n",
      "Step: 312, Training Loss: 4.23616, LR: 0.0000791, Tokens/sec: 63528.54670686799\n",
      "Step: 313, Training Loss: 4.09499, LR: 0.0000789, Tokens/sec: 93528.07633782591\n",
      "Step: 314, Training Loss: 4.08195, LR: 0.0000787, Tokens/sec: 88297.39026977523\n",
      "Step: 315, Training Loss: 4.02113, LR: 0.0000786, Tokens/sec: 71761.09343965776\n",
      "Step: 316, Training Loss: 3.87913, LR: 0.0000784, Tokens/sec: 86259.09287846414\n",
      "Step: 317, Training Loss: 3.95226, LR: 0.0000782, Tokens/sec: 85191.42749014849\n",
      "Step: 318, Training Loss: 4.11433, LR: 0.0000781, Tokens/sec: 73057.24969714938\n",
      "Step: 319, Training Loss: 4.01609, LR: 0.0000779, Tokens/sec: 88493.78182664009\n",
      "Step: 320, Training Loss: 4.24094, LR: 0.0000777, Tokens/sec: 76934.1442983331\n",
      "Step: 321, Training Loss: 4.10884, LR: 0.0000776, Tokens/sec: 93635.08642362543\n",
      "Step: 322, Training Loss: 3.98089, LR: 0.0000774, Tokens/sec: 74841.73786870544\n",
      "Step: 323, Training Loss: 3.87982, LR: 0.0000772, Tokens/sec: 79744.3927406616\n",
      "Step: 324, Training Loss: 4.09565, LR: 0.0000770, Tokens/sec: 72821.10619016545\n",
      "Step: 325, Training Loss: 4.25055, LR: 0.0000769, Tokens/sec: 93676.5686147558\n",
      "Computing Eval loss, steps: 17\n",
      "Step: 325, Eval Loss: 4.95668\n",
      "Step: 326, Training Loss: 3.96545, LR: 0.0000767, Tokens/sec: 87441.72829176817\n",
      "Step: 327, Training Loss: 3.90609, LR: 0.0000765, Tokens/sec: 92802.58201471304\n",
      "Step: 328, Training Loss: 3.95319, LR: 0.0000764, Tokens/sec: 68645.42236104909\n",
      "Step: 329, Training Loss: 4.01059, LR: 0.0000762, Tokens/sec: 93164.51964137875\n",
      "Step: 330, Training Loss: 4.24385, LR: 0.0000760, Tokens/sec: 93194.72801830084\n",
      "Step: 331, Training Loss: 4.12466, LR: 0.0000758, Tokens/sec: 69381.54835721068\n",
      "Step: 332, Training Loss: 4.02012, LR: 0.0000757, Tokens/sec: 85709.67200877641\n",
      "Step: 333, Training Loss: 3.84671, LR: 0.0000755, Tokens/sec: 93370.64755353492\n",
      "Step: 334, Training Loss: 4.12929, LR: 0.0000753, Tokens/sec: 68747.53851278024\n",
      "Step: 335, Training Loss: 3.98293, LR: 0.0000752, Tokens/sec: 93541.78901404096\n",
      "Step: 336, Training Loss: 3.93527, LR: 0.0000750, Tokens/sec: 90645.58577646963\n",
      "Step: 337, Training Loss: 3.96478, LR: 0.0000748, Tokens/sec: 68836.67332347926\n",
      "Step: 338, Training Loss: 3.99935, LR: 0.0000746, Tokens/sec: 92053.1541235576\n",
      "Step: 339, Training Loss: 4.05128, LR: 0.0000744, Tokens/sec: 93232.69907105717\n",
      "Step: 340, Training Loss: 3.87460, LR: 0.0000743, Tokens/sec: 70387.63861304746\n",
      "Step: 341, Training Loss: 3.75852, LR: 0.0000741, Tokens/sec: 93897.06446236548\n",
      "Step: 342, Training Loss: 3.94856, LR: 0.0000739, Tokens/sec: 93392.23701705292\n",
      "Step: 343, Training Loss: 3.91246, LR: 0.0000737, Tokens/sec: 68740.62641089316\n",
      "Step: 344, Training Loss: 3.89885, LR: 0.0000736, Tokens/sec: 93399.60539971861\n",
      "Step: 345, Training Loss: 3.84197, LR: 0.0000734, Tokens/sec: 78851.13131893246\n",
      "Step: 346, Training Loss: 3.95735, LR: 0.0000732, Tokens/sec: 65373.25198146436\n",
      "Step: 347, Training Loss: 4.02572, LR: 0.0000730, Tokens/sec: 90084.1162551505\n",
      "Step: 348, Training Loss: 3.91632, LR: 0.0000729, Tokens/sec: 84845.6247687889\n",
      "Step: 349, Training Loss: 4.00131, LR: 0.0000727, Tokens/sec: 75069.710272304\n",
      "Step: 350, Training Loss: 3.77027, LR: 0.0000725, Tokens/sec: 86085.11102444511\n",
      "Computing Eval loss, steps: 17\n",
      "Step: 350, Eval Loss: 4.96524\n",
      "Step: 351, Training Loss: 4.02241, LR: 0.0000723, Tokens/sec: 85323.4233709158\n",
      "Step: 352, Training Loss: 3.94892, LR: 0.0000721, Tokens/sec: 89994.79564526876\n",
      "Step: 353, Training Loss: 3.92084, LR: 0.0000719, Tokens/sec: 64729.778695252426\n",
      "Step: 354, Training Loss: 3.87169, LR: 0.0000718, Tokens/sec: 91588.38292166902\n",
      "Step: 355, Training Loss: 3.69235, LR: 0.0000716, Tokens/sec: 76124.18613041574\n",
      "Step: 356, Training Loss: 3.86360, LR: 0.0000714, Tokens/sec: 71890.36774855804\n",
      "Step: 357, Training Loss: 3.97147, LR: 0.0000712, Tokens/sec: 72754.11889579381\n",
      "Step: 358, Training Loss: 4.00729, LR: 0.0000710, Tokens/sec: 81014.63232735895\n",
      "Step: 359, Training Loss: 3.92807, LR: 0.0000709, Tokens/sec: 88771.93648578678\n",
      "Step: 360, Training Loss: 3.80592, LR: 0.0000707, Tokens/sec: 65412.28808719606\n",
      "Step: 361, Training Loss: 3.77716, LR: 0.0000705, Tokens/sec: 89098.06243945853\n",
      "Step: 362, Training Loss: 3.86087, LR: 0.0000703, Tokens/sec: 87741.34708172303\n",
      "Step: 363, Training Loss: 3.73681, LR: 0.0000701, Tokens/sec: 64219.37338271797\n",
      "Step: 364, Training Loss: 3.80937, LR: 0.0000699, Tokens/sec: 90169.35239349488\n",
      "Step: 365, Training Loss: 3.86269, LR: 0.0000698, Tokens/sec: 90975.40627940926\n",
      "Step: 366, Training Loss: 3.83091, LR: 0.0000696, Tokens/sec: 67601.7371044329\n",
      "Step: 367, Training Loss: 3.63975, LR: 0.0000694, Tokens/sec: 79791.04253171438\n",
      "Step: 368, Training Loss: 3.73557, LR: 0.0000692, Tokens/sec: 72737.44073546611\n",
      "Step: 369, Training Loss: 3.84187, LR: 0.0000690, Tokens/sec: 76328.79800521827\n",
      "Step: 370, Training Loss: 3.78317, LR: 0.0000688, Tokens/sec: 78881.1676940269\n",
      "Step: 371, Training Loss: 3.70819, LR: 0.0000686, Tokens/sec: 77089.1614417785\n",
      "Step: 372, Training Loss: 3.67702, LR: 0.0000685, Tokens/sec: 79275.89520899793\n",
      "Step: 373, Training Loss: 3.91857, LR: 0.0000683, Tokens/sec: 78663.14444763357\n",
      "Step: 374, Training Loss: 3.70893, LR: 0.0000681, Tokens/sec: 77314.19107471524\n",
      "Step: 375, Training Loss: 3.64785, LR: 0.0000679, Tokens/sec: 70042.36553438303\n",
      "Computing Eval loss, steps: 17\n",
      "Step: 375, Eval Loss: 4.97961\n",
      "Step: 376, Training Loss: 3.74724, LR: 0.0000677, Tokens/sec: 77135.77686756935\n",
      "Step: 377, Training Loss: 3.80541, LR: 0.0000675, Tokens/sec: 90603.58332666488\n",
      "Step: 378, Training Loss: 3.70440, LR: 0.0000673, Tokens/sec: 62499.185570021466\n",
      "Step: 379, Training Loss: 3.81530, LR: 0.0000671, Tokens/sec: 86686.58129332759\n",
      "Step: 380, Training Loss: 3.70965, LR: 0.0000670, Tokens/sec: 91673.08278361471\n",
      "Step: 381, Training Loss: 3.67133, LR: 0.0000668, Tokens/sec: 66262.43882780935\n",
      "Step: 382, Training Loss: 3.65540, LR: 0.0000666, Tokens/sec: 78760.44898573952\n",
      "Step: 383, Training Loss: 3.49475, LR: 0.0000664, Tokens/sec: 73423.24082950663\n",
      "Step: 384, Training Loss: 3.60985, LR: 0.0000662, Tokens/sec: 71981.83639142931\n",
      "Step: 385, Training Loss: 3.75271, LR: 0.0000660, Tokens/sec: 74150.24783933892\n",
      "Step: 386, Training Loss: 3.64791, LR: 0.0000658, Tokens/sec: 58027.314686346144\n",
      "Step: 387, Training Loss: 3.81561, LR: 0.0000656, Tokens/sec: 90773.5531051537\n",
      "Step: 388, Training Loss: 3.69898, LR: 0.0000654, Tokens/sec: 89059.93498245662\n",
      "Step: 389, Training Loss: 3.57671, LR: 0.0000653, Tokens/sec: 85505.2237757711\n",
      "Step: 390, Training Loss: 3.47537, LR: 0.0000651, Tokens/sec: 63218.18075581607\n",
      "Step: 391, Training Loss: 3.69645, LR: 0.0000649, Tokens/sec: 89856.24362340242\n",
      "Step: 392, Training Loss: 3.85936, LR: 0.0000647, Tokens/sec: 88917.14053885822\n",
      "Step: 393, Training Loss: 3.60636, LR: 0.0000645, Tokens/sec: 64968.69336842793\n",
      "Step: 394, Training Loss: 3.55437, LR: 0.0000643, Tokens/sec: 88647.4189369877\n",
      "Step: 395, Training Loss: 3.56699, LR: 0.0000641, Tokens/sec: 88205.68531475503\n",
      "Step: 396, Training Loss: 3.61354, LR: 0.0000639, Tokens/sec: 62291.848503578374\n",
      "Step: 397, Training Loss: 3.87307, LR: 0.0000637, Tokens/sec: 86678.97018018579\n",
      "Step: 398, Training Loss: 3.73115, LR: 0.0000635, Tokens/sec: 88579.74787012076\n",
      "Step: 399, Training Loss: 3.63097, LR: 0.0000633, Tokens/sec: 65888.83947774717\n",
      "Step: 400, Training Loss: 3.50240, LR: 0.0000632, Tokens/sec: 88789.84813224955\n",
      "Computing Eval loss, steps: 17\n",
      "Step: 400, Eval Loss: 5.02397\n",
      "Step: 401, Training Loss: 3.72040, LR: 0.0000630, Tokens/sec: 94537.72735245703\n",
      "Step: 402, Training Loss: 3.59525, LR: 0.0000628, Tokens/sec: 94098.47708658777\n",
      "Step: 403, Training Loss: 3.56233, LR: 0.0000626, Tokens/sec: 67553.67367382128\n",
      "Step: 404, Training Loss: 3.57244, LR: 0.0000624, Tokens/sec: 93124.00408981934\n",
      "Step: 405, Training Loss: 3.55912, LR: 0.0000622, Tokens/sec: 86180.83385818687\n",
      "Step: 406, Training Loss: 3.63767, LR: 0.0000620, Tokens/sec: 67187.4955127846\n",
      "Step: 407, Training Loss: 3.47173, LR: 0.0000618, Tokens/sec: 94712.32380592129\n",
      "Step: 408, Training Loss: 3.39069, LR: 0.0000616, Tokens/sec: 86841.6276689151\n",
      "Step: 409, Training Loss: 3.58896, LR: 0.0000614, Tokens/sec: 64753.90953257202\n",
      "Step: 410, Training Loss: 3.49892, LR: 0.0000612, Tokens/sec: 94381.38578011337\n",
      "Step: 411, Training Loss: 3.48383, LR: 0.0000610, Tokens/sec: 90724.7518970475\n",
      "Step: 412, Training Loss: 3.41346, LR: 0.0000608, Tokens/sec: 68427.66212613118\n",
      "Step: 413, Training Loss: 3.55265, LR: 0.0000606, Tokens/sec: 94441.77418121819\n",
      "Step: 414, Training Loss: 3.64025, LR: 0.0000605, Tokens/sec: 93157.27307380394\n",
      "Step: 415, Training Loss: 3.49674, LR: 0.0000603, Tokens/sec: 63653.85135543487\n",
      "Step: 416, Training Loss: 3.61327, LR: 0.0000601, Tokens/sec: 92994.54771745655\n",
      "Step: 417, Training Loss: 3.40355, LR: 0.0000599, Tokens/sec: 94846.47641035974\n",
      "Step: 418, Training Loss: 3.61654, LR: 0.0000597, Tokens/sec: 70843.77792973618\n",
      "Step: 419, Training Loss: 3.51483, LR: 0.0000595, Tokens/sec: 94974.55894816038\n",
      "Step: 420, Training Loss: 3.54140, LR: 0.0000593, Tokens/sec: 94723.91056021\n",
      "Step: 421, Training Loss: 3.47209, LR: 0.0000591, Tokens/sec: 65408.92668150496\n",
      "Step: 422, Training Loss: 3.28370, LR: 0.0000589, Tokens/sec: 95136.16585130012\n",
      "Step: 423, Training Loss: 3.48025, LR: 0.0000587, Tokens/sec: 94503.2232530374\n",
      "Step: 424, Training Loss: 3.61222, LR: 0.0000585, Tokens/sec: 64622.43725620963\n",
      "Step: 425, Training Loss: 3.60730, LR: 0.0000583, Tokens/sec: 90765.36631720823\n",
      "Computing Eval loss, steps: 17\n",
      "Step: 425, Eval Loss: 5.05083\n",
      "Step: 426, Training Loss: 3.54048, LR: 0.0000581, Tokens/sec: 85223.79412403314\n",
      "Step: 427, Training Loss: 3.45732, LR: 0.0000579, Tokens/sec: 89239.45587484792\n",
      "Step: 428, Training Loss: 3.40993, LR: 0.0000577, Tokens/sec: 62572.78038970523\n",
      "Step: 429, Training Loss: 3.46451, LR: 0.0000575, Tokens/sec: 95168.7013853599\n",
      "Step: 430, Training Loss: 3.37628, LR: 0.0000573, Tokens/sec: 91947.91501287762\n",
      "Step: 431, Training Loss: 3.46247, LR: 0.0000571, Tokens/sec: 71957.16622332991\n",
      "Step: 432, Training Loss: 3.46394, LR: 0.0000570, Tokens/sec: 87303.86895898198\n",
      "Step: 433, Training Loss: 3.43704, LR: 0.0000568, Tokens/sec: 93665.408002747\n",
      "Step: 434, Training Loss: 3.29193, LR: 0.0000566, Tokens/sec: 62924.29346812519\n",
      "Step: 435, Training Loss: 3.38630, LR: 0.0000564, Tokens/sec: 90913.67937693922\n",
      "Step: 436, Training Loss: 3.47105, LR: 0.0000562, Tokens/sec: 79448.34606556332\n",
      "Step: 437, Training Loss: 3.41454, LR: 0.0000560, Tokens/sec: 70610.48035754623\n",
      "Step: 438, Training Loss: 3.34250, LR: 0.0000558, Tokens/sec: 76512.22890663252\n",
      "Step: 439, Training Loss: 3.31488, LR: 0.0000556, Tokens/sec: 71728.3095536871\n",
      "Step: 440, Training Loss: 3.52292, LR: 0.0000554, Tokens/sec: 73474.7649688041\n",
      "Step: 441, Training Loss: 3.34532, LR: 0.0000552, Tokens/sec: 62124.08652775471\n",
      "Step: 442, Training Loss: 3.29067, LR: 0.0000550, Tokens/sec: 94275.53257159372\n",
      "Step: 443, Training Loss: 3.37780, LR: 0.0000548, Tokens/sec: 75962.54193132654\n",
      "Step: 444, Training Loss: 3.41459, LR: 0.0000546, Tokens/sec: 94437.37789856855\n",
      "Step: 445, Training Loss: 3.35281, LR: 0.0000544, Tokens/sec: 76905.19958217764\n",
      "Step: 446, Training Loss: 3.45978, LR: 0.0000542, Tokens/sec: 78658.23794698862\n",
      "Step: 447, Training Loss: 3.32437, LR: 0.0000540, Tokens/sec: 77780.17802144196\n",
      "Step: 448, Training Loss: 3.27915, LR: 0.0000538, Tokens/sec: 75712.70592034826\n",
      "Step: 449, Training Loss: 3.29286, LR: 0.0000536, Tokens/sec: 74746.26189247092\n",
      "Step: 450, Training Loss: 3.15055, LR: 0.0000534, Tokens/sec: 81381.95298380374\n",
      "Computing Eval loss, steps: 17\n",
      "Step: 450, Eval Loss: 5.10715\n",
      "Step: 451, Training Loss: 3.24850, LR: 0.0000532, Tokens/sec: 93286.57131485727\n",
      "Step: 452, Training Loss: 3.37061, LR: 0.0000530, Tokens/sec: 92092.29582838954\n",
      "Step: 453, Training Loss: 3.26233, LR: 0.0000529, Tokens/sec: 72356.06888377553\n",
      "Step: 454, Training Loss: 3.42169, LR: 0.0000527, Tokens/sec: 84809.36693192492\n",
      "Step: 455, Training Loss: 3.33580, LR: 0.0000525, Tokens/sec: 94531.16005888634\n",
      "Step: 456, Training Loss: 3.23845, LR: 0.0000523, Tokens/sec: 65755.8190748968\n",
      "Step: 457, Training Loss: 3.12433, LR: 0.0000521, Tokens/sec: 83039.10810045278\n",
      "Step: 458, Training Loss: 3.32425, LR: 0.0000519, Tokens/sec: 82075.10389372251\n",
      "Step: 459, Training Loss: 3.48090, LR: 0.0000517, Tokens/sec: 74229.51897878095\n",
      "Step: 460, Training Loss: 3.24003, LR: 0.0000515, Tokens/sec: 94464.4501083784\n",
      "Step: 461, Training Loss: 3.21321, LR: 0.0000513, Tokens/sec: 80387.53543603011\n",
      "Step: 462, Training Loss: 3.20933, LR: 0.0000511, Tokens/sec: 75619.71194196153\n",
      "Step: 463, Training Loss: 3.20383, LR: 0.0000509, Tokens/sec: 69025.36471250665\n",
      "Step: 464, Training Loss: 3.43423, LR: 0.0000507, Tokens/sec: 93419.67416497922\n",
      "Step: 465, Training Loss: 3.35303, LR: 0.0000505, Tokens/sec: 72372.42169630727\n",
      "Step: 466, Training Loss: 3.27056, LR: 0.0000503, Tokens/sec: 92205.17569219368\n",
      "Step: 467, Training Loss: 3.15501, LR: 0.0000501, Tokens/sec: 73245.03396892449\n",
      "Step: 468, Training Loss: 3.33306, LR: 0.0000499, Tokens/sec: 86725.2372648613\n",
      "Step: 469, Training Loss: 3.22773, LR: 0.0000497, Tokens/sec: 70271.46229772424\n",
      "Step: 470, Training Loss: 3.19665, LR: 0.0000495, Tokens/sec: 94703.56443317061\n",
      "Step: 471, Training Loss: 3.23985, LR: 0.0000494, Tokens/sec: 73433.29763742705\n",
      "Step: 472, Training Loss: 3.20055, LR: 0.0000492, Tokens/sec: 92070.12551682092\n",
      "Step: 473, Training Loss: 3.26870, LR: 0.0000490, Tokens/sec: 70241.69694920609\n",
      "Step: 474, Training Loss: 3.12463, LR: 0.0000488, Tokens/sec: 88754.75902738997\n",
      "Step: 475, Training Loss: 3.04062, LR: 0.0000486, Tokens/sec: 93813.34864002555\n",
      "Computing Eval loss, steps: 17\n",
      "Step: 475, Eval Loss: 5.12534\n",
      "Step: 476, Training Loss: 3.22076, LR: 0.0000484, Tokens/sec: 88360.55488640453\n",
      "Step: 477, Training Loss: 3.13959, LR: 0.0000482, Tokens/sec: 92128.89706159742\n",
      "Step: 478, Training Loss: 3.11623, LR: 0.0000480, Tokens/sec: 68073.88256461149\n",
      "Step: 479, Training Loss: 3.07195, LR: 0.0000478, Tokens/sec: 94754.19406104907\n",
      "Step: 480, Training Loss: 3.19255, LR: 0.0000476, Tokens/sec: 95401.51424863431\n",
      "Step: 481, Training Loss: 3.26818, LR: 0.0000474, Tokens/sec: 67541.83471896932\n",
      "Step: 482, Training Loss: 3.15514, LR: 0.0000472, Tokens/sec: 94570.95309881278\n",
      "Step: 483, Training Loss: 3.24739, LR: 0.0000470, Tokens/sec: 92187.11914769521\n",
      "Step: 484, Training Loss: 3.05025, LR: 0.0000468, Tokens/sec: 70705.4081464788\n",
      "Step: 485, Training Loss: 3.25569, LR: 0.0000467, Tokens/sec: 82823.48160512015\n",
      "Step: 486, Training Loss: 3.16993, LR: 0.0000465, Tokens/sec: 93586.80428971178\n",
      "Step: 487, Training Loss: 3.19860, LR: 0.0000463, Tokens/sec: 63852.76749381814\n",
      "Step: 488, Training Loss: 3.11710, LR: 0.0000461, Tokens/sec: 94415.10888644792\n",
      "Step: 489, Training Loss: 2.97284, LR: 0.0000459, Tokens/sec: 94331.74456978828\n",
      "Step: 490, Training Loss: 3.16793, LR: 0.0000457, Tokens/sec: 69371.83280311812\n",
      "Step: 491, Training Loss: 3.26297, LR: 0.0000455, Tokens/sec: 94226.69409251453\n",
      "Step: 492, Training Loss: 3.26393, LR: 0.0000453, Tokens/sec: 92102.70981663965\n",
      "Step: 493, Training Loss: 3.18939, LR: 0.0000451, Tokens/sec: 63878.578797311486\n",
      "Step: 494, Training Loss: 3.13371, LR: 0.0000449, Tokens/sec: 90157.48967382833\n",
      "Step: 495, Training Loss: 3.10620, LR: 0.0000447, Tokens/sec: 93856.61697369031\n",
      "Step: 496, Training Loss: 3.15559, LR: 0.0000446, Tokens/sec: 69062.78213539717\n",
      "Step: 497, Training Loss: 3.06232, LR: 0.0000444, Tokens/sec: 93774.98909958104\n",
      "Step: 498, Training Loss: 3.12416, LR: 0.0000442, Tokens/sec: 85499.01617632665\n",
      "Step: 499, Training Loss: 3.11540, LR: 0.0000440, Tokens/sec: 66000.6285855263\n",
      "Step: 500, Training Loss: 3.11222, LR: 0.0000438, Tokens/sec: 94902.67008872557\n",
      "Computing Eval loss, steps: 17\n",
      "Step: 500, Eval Loss: 5.16460\n",
      "Step: 501, Training Loss: 2.97826, LR: 0.0000436, Tokens/sec: 91448.03067137995\n",
      "Step: 502, Training Loss: 3.07314, LR: 0.0000434, Tokens/sec: 87417.79051354108\n",
      "Step: 503, Training Loss: 3.13858, LR: 0.0000432, Tokens/sec: 62001.613398232854\n",
      "Step: 504, Training Loss: 3.11772, LR: 0.0000430, Tokens/sec: 85185.79685954447\n",
      "Step: 505, Training Loss: 3.05995, LR: 0.0000429, Tokens/sec: 77189.94089732155\n",
      "Step: 506, Training Loss: 3.01017, LR: 0.0000427, Tokens/sec: 69747.12020510634\n",
      "Step: 507, Training Loss: 3.19891, LR: 0.0000425, Tokens/sec: 89951.2032364421\n",
      "Step: 508, Training Loss: 3.05628, LR: 0.0000423, Tokens/sec: 72753.63041809753\n",
      "Step: 509, Training Loss: 3.00446, LR: 0.0000421, Tokens/sec: 92336.76949773578\n",
      "Step: 510, Training Loss: 3.06949, LR: 0.0000419, Tokens/sec: 73977.45092616777\n",
      "Step: 511, Training Loss: 3.08183, LR: 0.0000417, Tokens/sec: 82263.13268063277\n",
      "Step: 512, Training Loss: 3.03631, LR: 0.0000415, Tokens/sec: 69223.48989004096\n",
      "Step: 513, Training Loss: 3.13832, LR: 0.0000414, Tokens/sec: 93717.76592969602\n",
      "Step: 514, Training Loss: 3.01933, LR: 0.0000412, Tokens/sec: 77702.93579992214\n",
      "Step: 515, Training Loss: 2.96362, LR: 0.0000410, Tokens/sec: 83829.53967213235\n",
      "Step: 516, Training Loss: 2.98296, LR: 0.0000408, Tokens/sec: 71916.00604456387\n",
      "Step: 517, Training Loss: 2.81560, LR: 0.0000406, Tokens/sec: 79279.68675491917\n",
      "Step: 518, Training Loss: 2.92026, LR: 0.0000404, Tokens/sec: 80360.6450696234\n",
      "Step: 519, Training Loss: 3.02805, LR: 0.0000402, Tokens/sec: 93106.75416583824\n",
      "Step: 520, Training Loss: 2.90872, LR: 0.0000401, Tokens/sec: 73070.62552070372\n",
      "Step: 521, Training Loss: 3.07233, LR: 0.0000399, Tokens/sec: 90591.14327012637\n",
      "Step: 522, Training Loss: 3.00310, LR: 0.0000397, Tokens/sec: 61151.148575881365\n",
      "Step: 523, Training Loss: 2.90803, LR: 0.0000395, Tokens/sec: 91776.15929176191\n",
      "Step: 524, Training Loss: 2.80137, LR: 0.0000393, Tokens/sec: 92504.47277259726\n",
      "Step: 525, Training Loss: 2.97739, LR: 0.0000391, Tokens/sec: 70145.63319846643\n",
      "Computing Eval loss, steps: 17\n",
      "Step: 525, Eval Loss: 5.16181\n",
      "Step: 526, Training Loss: 3.13146, LR: 0.0000390, Tokens/sec: 91706.3146538439\n",
      "Step: 527, Training Loss: 2.90134, LR: 0.0000388, Tokens/sec: 93467.25712129963\n",
      "Step: 528, Training Loss: 2.88198, LR: 0.0000386, Tokens/sec: 70517.88674029267\n",
      "Step: 529, Training Loss: 2.89769, LR: 0.0000384, Tokens/sec: 90798.88722308706\n",
      "Step: 530, Training Loss: 2.87657, LR: 0.0000382, Tokens/sec: 92149.21117516459\n",
      "Step: 531, Training Loss: 3.09467, LR: 0.0000381, Tokens/sec: 65757.96310914817\n",
      "Step: 532, Training Loss: 3.03946, LR: 0.0000379, Tokens/sec: 94264.01186847419\n",
      "Step: 533, Training Loss: 2.96272, LR: 0.0000377, Tokens/sec: 91842.87504974724\n",
      "Step: 534, Training Loss: 2.87476, LR: 0.0000375, Tokens/sec: 70788.69686531812\n",
      "Step: 535, Training Loss: 3.04861, LR: 0.0000373, Tokens/sec: 84222.0774104691\n",
      "Step: 536, Training Loss: 2.95105, LR: 0.0000371, Tokens/sec: 94121.16146028978\n",
      "Step: 537, Training Loss: 2.89734, LR: 0.0000370, Tokens/sec: 64535.80346705572\n",
      "Step: 538, Training Loss: 2.93168, LR: 0.0000368, Tokens/sec: 90891.51215841254\n",
      "Step: 539, Training Loss: 2.87963, LR: 0.0000366, Tokens/sec: 91303.24143190715\n",
      "Step: 540, Training Loss: 2.96719, LR: 0.0000364, Tokens/sec: 69568.42287540388\n",
      "Step: 541, Training Loss: 2.84065, LR: 0.0000363, Tokens/sec: 94378.76090634639\n",
      "Step: 542, Training Loss: 2.75395, LR: 0.0000361, Tokens/sec: 94345.97645826812\n",
      "Step: 543, Training Loss: 2.93713, LR: 0.0000359, Tokens/sec: 64750.59291362692\n",
      "Step: 544, Training Loss: 2.85235, LR: 0.0000357, Tokens/sec: 94587.72544250282\n",
      "Step: 545, Training Loss: 2.82810, LR: 0.0000356, Tokens/sec: 93901.93584307257\n",
      "Step: 546, Training Loss: 2.78967, LR: 0.0000354, Tokens/sec: 65590.20269018193\n",
      "Step: 547, Training Loss: 2.91364, LR: 0.0000352, Tokens/sec: 91235.82352837107\n",
      "Step: 548, Training Loss: 3.00054, LR: 0.0000350, Tokens/sec: 85168.75712942884\n",
      "Step: 549, Training Loss: 2.90730, LR: 0.0000348, Tokens/sec: 63699.80570439831\n",
      "Step: 550, Training Loss: 3.00091, LR: 0.0000347, Tokens/sec: 92208.54042361578\n",
      "Computing Eval loss, steps: 17\n",
      "Step: 550, Eval Loss: 5.19875\n",
      "Step: 551, Training Loss: 2.80464, LR: 0.0000345, Tokens/sec: 93352.12106620698\n",
      "Step: 552, Training Loss: 2.98673, LR: 0.0000343, Tokens/sec: 91078.31634003992\n",
      "Step: 553, Training Loss: 2.90145, LR: 0.0000342, Tokens/sec: 66690.66618909156\n",
      "Step: 554, Training Loss: 2.92391, LR: 0.0000340, Tokens/sec: 90467.72941544815\n",
      "Step: 555, Training Loss: 2.84174, LR: 0.0000338, Tokens/sec: 91176.09708086189\n",
      "Step: 556, Training Loss: 2.71522, LR: 0.0000336, Tokens/sec: 61551.73825607946\n",
      "Step: 557, Training Loss: 2.88509, LR: 0.0000335, Tokens/sec: 88862.9377482222\n",
      "Step: 558, Training Loss: 2.97314, LR: 0.0000333, Tokens/sec: 85891.02374610449\n",
      "Step: 559, Training Loss: 2.96543, LR: 0.0000331, Tokens/sec: 63423.70938724392\n",
      "Step: 560, Training Loss: 2.89669, LR: 0.0000330, Tokens/sec: 81120.35447429038\n",
      "Step: 561, Training Loss: 2.87115, LR: 0.0000328, Tokens/sec: 92361.31765242708\n",
      "Step: 562, Training Loss: 2.82283, LR: 0.0000326, Tokens/sec: 66125.69733173658\n",
      "Step: 563, Training Loss: 2.87758, LR: 0.0000324, Tokens/sec: 92287.25697119354\n",
      "Step: 564, Training Loss: 2.79592, LR: 0.0000323, Tokens/sec: 93387.00319183897\n",
      "Step: 565, Training Loss: 2.87276, LR: 0.0000321, Tokens/sec: 67541.40258312253\n",
      "Step: 566, Training Loss: 2.86696, LR: 0.0000319, Tokens/sec: 84154.78415324369\n",
      "Step: 567, Training Loss: 2.85095, LR: 0.0000318, Tokens/sec: 92427.28330519154\n",
      "Step: 568, Training Loss: 2.70678, LR: 0.0000316, Tokens/sec: 66015.95713917281\n",
      "Step: 569, Training Loss: 2.79460, LR: 0.0000314, Tokens/sec: 92889.46351367235\n",
      "Step: 570, Training Loss: 2.88642, LR: 0.0000313, Tokens/sec: 93471.30970002119\n",
      "Step: 571, Training Loss: 2.86722, LR: 0.0000311, Tokens/sec: 68760.38686633855\n",
      "Step: 572, Training Loss: 2.79180, LR: 0.0000309, Tokens/sec: 81789.02469360651\n",
      "Step: 573, Training Loss: 2.73947, LR: 0.0000308, Tokens/sec: 94227.0842856411\n",
      "Step: 574, Training Loss: 2.92099, LR: 0.0000306, Tokens/sec: 67723.8644107115\n",
      "Step: 575, Training Loss: 2.78490, LR: 0.0000304, Tokens/sec: 82176.17601016517\n",
      "Computing Eval loss, steps: 17\n",
      "Step: 575, Eval Loss: 5.21591\n",
      "Step: 576, Training Loss: 2.72076, LR: 0.0000303, Tokens/sec: 91031.54182890791\n",
      "Step: 577, Training Loss: 2.78080, LR: 0.0000301, Tokens/sec: 94500.45642529221\n",
      "Step: 578, Training Loss: 2.79922, LR: 0.0000300, Tokens/sec: 72245.98643588506\n",
      "Step: 579, Training Loss: 2.77027, LR: 0.0000298, Tokens/sec: 80249.36393351194\n",
      "Step: 580, Training Loss: 2.87636, LR: 0.0000296, Tokens/sec: 93930.01366311181\n",
      "Step: 581, Training Loss: 2.76267, LR: 0.0000295, Tokens/sec: 70121.92930016623\n",
      "Step: 582, Training Loss: 2.69270, LR: 0.0000293, Tokens/sec: 94797.13472050862\n",
      "Step: 583, Training Loss: 2.72280, LR: 0.0000292, Tokens/sec: 94290.55062051205\n",
      "Step: 584, Training Loss: 2.56364, LR: 0.0000290, Tokens/sec: 71119.41331901541\n",
      "Step: 585, Training Loss: 2.66451, LR: 0.0000288, Tokens/sec: 75376.88118292578\n",
      "Step: 586, Training Loss: 2.77792, LR: 0.0000287, Tokens/sec: 91166.55712192641\n",
      "Step: 587, Training Loss: 2.66156, LR: 0.0000285, Tokens/sec: 72368.57541874184\n",
      "Step: 588, Training Loss: 2.79994, LR: 0.0000284, Tokens/sec: 94958.93039268629\n",
      "Step: 589, Training Loss: 2.75360, LR: 0.0000282, Tokens/sec: 71633.38940388993\n",
      "Step: 590, Training Loss: 2.65321, LR: 0.0000280, Tokens/sec: 87123.20516973193\n",
      "Step: 591, Training Loss: 2.54689, LR: 0.0000279, Tokens/sec: 67293.01192241872\n",
      "Step: 592, Training Loss: 2.71243, LR: 0.0000277, Tokens/sec: 91821.83532789009\n",
      "Step: 593, Training Loss: 2.85827, LR: 0.0000276, Tokens/sec: 73989.9870005795\n",
      "Step: 594, Training Loss: 2.63462, LR: 0.0000274, Tokens/sec: 90509.65399222402\n",
      "Step: 595, Training Loss: 2.60990, LR: 0.0000273, Tokens/sec: 71274.57613346797\n",
      "Step: 596, Training Loss: 2.63315, LR: 0.0000271, Tokens/sec: 87616.8436672622\n",
      "Step: 597, Training Loss: 2.60260, LR: 0.0000270, Tokens/sec: 74759.53885113107\n",
      "Step: 598, Training Loss: 2.80664, LR: 0.0000268, Tokens/sec: 91795.39032773189\n",
      "Step: 599, Training Loss: 2.76854, LR: 0.0000267, Tokens/sec: 74169.68074026227\n",
      "Step: 600, Training Loss: 2.69982, LR: 0.0000265, Tokens/sec: 90977.08950540792\n",
      "Computing Eval loss, steps: 17\n",
      "Step: 600, Eval Loss: 5.25468\n",
      "Step: 601, Training Loss: 2.61791, LR: 0.0000264, Tokens/sec: 91406.93368824953\n",
      "Step: 602, Training Loss: 2.78240, LR: 0.0000262, Tokens/sec: 93605.99094282957\n",
      "Step: 603, Training Loss: 2.70728, LR: 0.0000261, Tokens/sec: 69257.92767723517\n",
      "Step: 604, Training Loss: 2.66099, LR: 0.0000259, Tokens/sec: 93944.5813716718\n",
      "Step: 605, Training Loss: 2.70557, LR: 0.0000258, Tokens/sec: 91797.69651894885\n",
      "Step: 606, Training Loss: 2.66155, LR: 0.0000256, Tokens/sec: 67406.91061992478\n",
      "Step: 607, Training Loss: 2.75078, LR: 0.0000255, Tokens/sec: 94181.0028318981\n",
      "Step: 608, Training Loss: 2.62264, LR: 0.0000253, Tokens/sec: 94152.25321357074\n",
      "Step: 609, Training Loss: 2.53952, LR: 0.0000252, Tokens/sec: 70376.5425055713\n",
      "Step: 610, Training Loss: 2.70706, LR: 0.0000250, Tokens/sec: 91309.08697736448\n",
      "Step: 611, Training Loss: 2.61623, LR: 0.0000249, Tokens/sec: 94339.41622135851\n",
      "Step: 612, Training Loss: 2.59436, LR: 0.0000247, Tokens/sec: 70418.64762864403\n",
      "Step: 613, Training Loss: 2.54215, LR: 0.0000246, Tokens/sec: 94437.31695330508\n",
      "Step: 614, Training Loss: 2.64135, LR: 0.0000244, Tokens/sec: 94100.90046843413\n",
      "Step: 615, Training Loss: 2.73243, LR: 0.0000243, Tokens/sec: 72208.99751983423\n",
      "Step: 616, Training Loss: 2.65200, LR: 0.0000242, Tokens/sec: 94354.63067923399\n",
      "Step: 617, Training Loss: 2.76773, LR: 0.0000240, Tokens/sec: 94034.75515773175\n",
      "Step: 618, Training Loss: 2.58565, LR: 0.0000239, Tokens/sec: 72521.4737343116\n",
      "Step: 619, Training Loss: 2.74518, LR: 0.0000237, Tokens/sec: 92741.21149917202\n",
      "Step: 620, Training Loss: 2.67096, LR: 0.0000236, Tokens/sec: 89853.4287830266\n",
      "Step: 621, Training Loss: 2.68694, LR: 0.0000235, Tokens/sec: 70563.10529413856\n",
      "Step: 622, Training Loss: 2.59035, LR: 0.0000233, Tokens/sec: 94433.52852075902\n",
      "Step: 623, Training Loss: 2.47992, LR: 0.0000232, Tokens/sec: 94246.81197066353\n",
      "Step: 624, Training Loss: 2.65982, LR: 0.0000230, Tokens/sec: 70713.80876339547\n",
      "Step: 625, Training Loss: 2.73973, LR: 0.0000229, Tokens/sec: 89428.04217762194\n",
      "Computing Eval loss, steps: 17\n",
      "Step: 625, Eval Loss: 5.28239\n",
      "Step: 626, Training Loss: 2.72527, LR: 0.0000228, Tokens/sec: 81873.3829352068\n",
      "Step: 627, Training Loss: 2.66666, LR: 0.0000226, Tokens/sec: 83856.03801274314\n",
      "Step: 628, Training Loss: 2.64394, LR: 0.0000225, Tokens/sec: 67990.75339316706\n",
      "Step: 629, Training Loss: 2.59739, LR: 0.0000224, Tokens/sec: 89233.39407141035\n",
      "Step: 630, Training Loss: 2.65096, LR: 0.0000222, Tokens/sec: 74391.15128998776\n",
      "Step: 631, Training Loss: 2.56692, LR: 0.0000221, Tokens/sec: 91186.34751894079\n",
      "Step: 632, Training Loss: 2.65538, LR: 0.0000220, Tokens/sec: 70120.40594954924\n",
      "Step: 633, Training Loss: 2.62905, LR: 0.0000218, Tokens/sec: 82344.74903215836\n",
      "Step: 634, Training Loss: 2.62241, LR: 0.0000217, Tokens/sec: 73523.62021361185\n",
      "Step: 635, Training Loss: 2.47091, LR: 0.0000216, Tokens/sec: 78712.62579765465\n",
      "Step: 636, Training Loss: 2.56131, LR: 0.0000214, Tokens/sec: 79363.06952470179\n",
      "Step: 637, Training Loss: 2.65282, LR: 0.0000213, Tokens/sec: 83801.68984525192\n",
      "Step: 638, Training Loss: 2.63324, LR: 0.0000212, Tokens/sec: 69381.72816863512\n",
      "Step: 639, Training Loss: 2.56442, LR: 0.0000210, Tokens/sec: 89823.73605295003\n",
      "Step: 640, Training Loss: 2.52389, LR: 0.0000209, Tokens/sec: 69443.81455507537\n",
      "Step: 641, Training Loss: 2.71436, LR: 0.0000208, Tokens/sec: 81253.3213726864\n",
      "Step: 642, Training Loss: 2.57659, LR: 0.0000207, Tokens/sec: 76634.6980020311\n",
      "Step: 643, Training Loss: 2.51197, LR: 0.0000205, Tokens/sec: 83181.37230466014\n",
      "Step: 644, Training Loss: 2.56956, LR: 0.0000204, Tokens/sec: 73709.98970641998\n",
      "Step: 645, Training Loss: 2.59264, LR: 0.0000203, Tokens/sec: 91530.94103395587\n",
      "Step: 646, Training Loss: 2.57040, LR: 0.0000202, Tokens/sec: 72899.06833361842\n",
      "Step: 647, Training Loss: 2.66685, LR: 0.0000200, Tokens/sec: 85499.1250453357\n",
      "Step: 648, Training Loss: 2.57028, LR: 0.0000199, Tokens/sec: 67584.35306676148\n",
      "Step: 649, Training Loss: 2.49916, LR: 0.0000198, Tokens/sec: 86668.6480287674\n",
      "Step: 650, Training Loss: 2.52899, LR: 0.0000197, Tokens/sec: 67004.21786668466\n",
      "Computing Eval loss, steps: 17\n",
      "Step: 650, Eval Loss: 5.30247\n",
      "Step: 651, Training Loss: 2.37232, LR: 0.0000196, Tokens/sec: 82384.48371458214\n",
      "Step: 652, Training Loss: 2.47323, LR: 0.0000194, Tokens/sec: 93293.09856809671\n",
      "Step: 653, Training Loss: 2.58395, LR: 0.0000193, Tokens/sec: 67384.33394856099\n",
      "Step: 654, Training Loss: 2.45935, LR: 0.0000192, Tokens/sec: 90310.33394539377\n",
      "Step: 655, Training Loss: 2.59137, LR: 0.0000191, Tokens/sec: 91490.48067786798\n",
      "Step: 656, Training Loss: 2.55495, LR: 0.0000190, Tokens/sec: 67171.4573192087\n",
      "Step: 657, Training Loss: 2.46201, LR: 0.0000188, Tokens/sec: 82873.65533205628\n",
      "Step: 658, Training Loss: 2.37139, LR: 0.0000187, Tokens/sec: 93220.58953753773\n",
      "Step: 659, Training Loss: 2.52847, LR: 0.0000186, Tokens/sec: 72339.98408509065\n",
      "Step: 660, Training Loss: 2.67707, LR: 0.0000185, Tokens/sec: 91400.05991316425\n",
      "Step: 661, Training Loss: 2.45288, LR: 0.0000184, Tokens/sec: 66951.69301133114\n",
      "Step: 662, Training Loss: 2.41896, LR: 0.0000183, Tokens/sec: 94944.69123195004\n",
      "Step: 663, Training Loss: 2.43969, LR: 0.0000182, Tokens/sec: 84155.14896828665\n",
      "Step: 664, Training Loss: 2.41819, LR: 0.0000181, Tokens/sec: 64763.2900186465\n",
      "Step: 665, Training Loss: 2.60476, LR: 0.0000179, Tokens/sec: 88200.16389183812\n",
      "Step: 666, Training Loss: 2.56718, LR: 0.0000178, Tokens/sec: 93586.93687690987\n",
      "Step: 667, Training Loss: 2.51293, LR: 0.0000177, Tokens/sec: 74535.76556357286\n",
      "Step: 668, Training Loss: 2.41826, LR: 0.0000176, Tokens/sec: 88486.3910179356\n",
      "Step: 669, Training Loss: 2.59062, LR: 0.0000175, Tokens/sec: 88051.40310756318\n",
      "Step: 670, Training Loss: 2.51543, LR: 0.0000174, Tokens/sec: 68581.75410900393\n",
      "Step: 671, Training Loss: 2.48053, LR: 0.0000173, Tokens/sec: 94101.20529340474\n",
      "Step: 672, Training Loss: 2.51912, LR: 0.0000172, Tokens/sec: 73703.62459071014\n",
      "Step: 673, Training Loss: 2.47396, LR: 0.0000171, Tokens/sec: 89855.77446631175\n",
      "Step: 674, Training Loss: 2.56282, LR: 0.0000170, Tokens/sec: 67449.25810449885\n",
      "Step: 675, Training Loss: 2.44933, LR: 0.0000169, Tokens/sec: 76447.98616831463\n",
      "Computing Eval loss, steps: 17\n",
      "Step: 675, Eval Loss: 5.30708\n",
      "Step: 676, Training Loss: 2.37553, LR: 0.0000168, Tokens/sec: 87258.9195346123\n",
      "Step: 677, Training Loss: 2.53190, LR: 0.0000167, Tokens/sec: 86492.81089923068\n",
      "Step: 678, Training Loss: 2.45539, LR: 0.0000166, Tokens/sec: 70627.22632739363\n",
      "Step: 679, Training Loss: 2.42881, LR: 0.0000165, Tokens/sec: 90401.14226748323\n",
      "Step: 680, Training Loss: 2.37938, LR: 0.0000164, Tokens/sec: 91814.448292958\n",
      "Step: 681, Training Loss: 2.45780, LR: 0.0000163, Tokens/sec: 62939.53569524177\n",
      "Step: 682, Training Loss: 2.56075, LR: 0.0000162, Tokens/sec: 91262.98411285169\n",
      "Step: 683, Training Loss: 2.48372, LR: 0.0000161, Tokens/sec: 73715.98842779278\n",
      "Step: 684, Training Loss: 2.59478, LR: 0.0000160, Tokens/sec: 66422.31277097562\n",
      "Step: 685, Training Loss: 2.41587, LR: 0.0000159, Tokens/sec: 80150.97187073782\n",
      "Step: 686, Training Loss: 2.56632, LR: 0.0000158, Tokens/sec: 83877.71250972584\n",
      "Step: 687, Training Loss: 2.50395, LR: 0.0000157, Tokens/sec: 82815.98280042455\n",
      "Step: 688, Training Loss: 2.52525, LR: 0.0000156, Tokens/sec: 72035.13682469634\n",
      "Step: 689, Training Loss: 2.42869, LR: 0.0000155, Tokens/sec: 83873.05795928802\n",
      "Step: 690, Training Loss: 2.31602, LR: 0.0000154, Tokens/sec: 72982.04121063669\n",
      "Step: 691, Training Loss: 2.50380, LR: 0.0000153, Tokens/sec: 93583.94333245746\n",
      "Step: 692, Training Loss: 2.57433, LR: 0.0000152, Tokens/sec: 73035.42203384943\n",
      "Step: 693, Training Loss: 2.55208, LR: 0.0000151, Tokens/sec: 94782.50538978913\n",
      "Step: 694, Training Loss: 2.50010, LR: 0.0000150, Tokens/sec: 68553.9534413627\n",
      "Step: 695, Training Loss: 2.47726, LR: 0.0000149, Tokens/sec: 90787.29897959901\n",
      "Step: 696, Training Loss: 2.44279, LR: 0.0000149, Tokens/sec: 71673.06401743452\n",
      "Step: 697, Training Loss: 2.48832, LR: 0.0000148, Tokens/sec: 94398.03437176271\n",
      "Step: 698, Training Loss: 2.40330, LR: 0.0000147, Tokens/sec: 64339.27810532332\n",
      "Step: 699, Training Loss: 2.49830, LR: 0.0000146, Tokens/sec: 94467.44140562229\n",
      "Step: 700, Training Loss: 2.46728, LR: 0.0000145, Tokens/sec: 88620.11685129412\n",
      "Computing Eval loss, steps: 17\n",
      "Step: 700, Eval Loss: 5.32823\n",
      "Step: 701, Training Loss: 2.46987, LR: 0.0000144, Tokens/sec: 93001.01726705286\n",
      "Step: 702, Training Loss: 2.32236, LR: 0.0000143, Tokens/sec: 94220.34766235517\n",
      "Step: 703, Training Loss: 2.41022, LR: 0.0000143, Tokens/sec: 72172.40888988604\n",
      "Step: 704, Training Loss: 2.50300, LR: 0.0000142, Tokens/sec: 94139.52930232993\n",
      "Step: 705, Training Loss: 2.47537, LR: 0.0000141, Tokens/sec: 93891.1325463803\n",
      "Step: 706, Training Loss: 2.41767, LR: 0.0000140, Tokens/sec: 65335.333757232234\n",
      "Step: 707, Training Loss: 2.37325, LR: 0.0000139, Tokens/sec: 91500.53822960152\n",
      "Step: 708, Training Loss: 2.56338, LR: 0.0000138, Tokens/sec: 86943.30969823786\n",
      "Step: 709, Training Loss: 2.41920, LR: 0.0000138, Tokens/sec: 70479.94727990235\n",
      "Step: 710, Training Loss: 2.38004, LR: 0.0000137, Tokens/sec: 93644.58055228055\n",
      "Step: 711, Training Loss: 2.42358, LR: 0.0000136, Tokens/sec: 93092.30758540364\n",
      "Step: 712, Training Loss: 2.45219, LR: 0.0000135, Tokens/sec: 62875.91422884546\n",
      "Step: 713, Training Loss: 2.43013, LR: 0.0000135, Tokens/sec: 87110.09247950187\n",
      "Step: 714, Training Loss: 2.51477, LR: 0.0000134, Tokens/sec: 89344.65326400971\n",
      "Step: 715, Training Loss: 2.42795, LR: 0.0000133, Tokens/sec: 66457.70455933038\n",
      "Step: 716, Training Loss: 2.35237, LR: 0.0000132, Tokens/sec: 91815.08630085045\n",
      "Step: 717, Training Loss: 2.38272, LR: 0.0000132, Tokens/sec: 81406.94897862064\n",
      "Step: 718, Training Loss: 2.23666, LR: 0.0000131, Tokens/sec: 63833.36697990767\n",
      "Step: 719, Training Loss: 2.34699, LR: 0.0000130, Tokens/sec: 87656.80477356161\n",
      "Step: 720, Training Loss: 2.44897, LR: 0.0000130, Tokens/sec: 88868.24360771626\n",
      "Step: 721, Training Loss: 2.32081, LR: 0.0000129, Tokens/sec: 65707.33319120013\n",
      "Step: 722, Training Loss: 2.45211, LR: 0.0000128, Tokens/sec: 94023.61910674433\n",
      "Step: 723, Training Loss: 2.40509, LR: 0.0000128, Tokens/sec: 91046.35558214092\n",
      "Step: 724, Training Loss: 2.31979, LR: 0.0000127, Tokens/sec: 72531.97985940886\n",
      "Step: 725, Training Loss: 2.23552, LR: 0.0000126, Tokens/sec: 91247.31312685885\n",
      "Computing Eval loss, steps: 17\n",
      "Step: 725, Eval Loss: 5.34467\n",
      "Step: 726, Training Loss: 2.38777, LR: 0.0000126, Tokens/sec: 87550.08132892531\n",
      "Step: 727, Training Loss: 2.53888, LR: 0.0000125, Tokens/sec: 93200.44078279413\n",
      "Step: 728, Training Loss: 2.31863, LR: 0.0000124, Tokens/sec: 70756.44198447725\n",
      "Step: 729, Training Loss: 2.28935, LR: 0.0000124, Tokens/sec: 91890.61714437016\n",
      "Step: 730, Training Loss: 2.30129, LR: 0.0000123, Tokens/sec: 83572.60651145453\n",
      "Step: 731, Training Loss: 2.28562, LR: 0.0000122, Tokens/sec: 65109.76853272761\n",
      "Step: 732, Training Loss: 2.46139, LR: 0.0000122, Tokens/sec: 94011.70459017069\n",
      "Step: 733, Training Loss: 2.41799, LR: 0.0000121, Tokens/sec: 91027.53620911033\n",
      "Step: 734, Training Loss: 2.36790, LR: 0.0000121, Tokens/sec: 70557.0544913112\n",
      "Step: 735, Training Loss: 2.27600, LR: 0.0000120, Tokens/sec: 83822.13377079918\n",
      "Step: 736, Training Loss: 2.44608, LR: 0.0000119, Tokens/sec: 83583.62506067047\n",
      "Step: 737, Training Loss: 2.37605, LR: 0.0000119, Tokens/sec: 64753.41202142219\n",
      "Step: 738, Training Loss: 2.33855, LR: 0.0000118, Tokens/sec: 94786.71670054547\n",
      "Step: 739, Training Loss: 2.38059, LR: 0.0000118, Tokens/sec: 94541.2033830787\n",
      "Step: 740, Training Loss: 2.34094, LR: 0.0000117, Tokens/sec: 70454.3081607869\n",
      "Step: 741, Training Loss: 2.42886, LR: 0.0000117, Tokens/sec: 87677.89504453869\n",
      "Step: 742, Training Loss: 2.32508, LR: 0.0000116, Tokens/sec: 94052.18224295467\n",
      "Step: 743, Training Loss: 2.24802, LR: 0.0000116, Tokens/sec: 65177.23633348675\n",
      "Step: 744, Training Loss: 2.40231, LR: 0.0000115, Tokens/sec: 94241.41686411732\n",
      "Step: 745, Training Loss: 2.33656, LR: 0.0000115, Tokens/sec: 94887.88066667209\n",
      "Step: 746, Training Loss: 2.29975, LR: 0.0000114, Tokens/sec: 67689.30733410668\n",
      "Step: 747, Training Loss: 2.26377, LR: 0.0000114, Tokens/sec: 93470.16428876513\n",
      "Step: 748, Training Loss: 2.33844, LR: 0.0000113, Tokens/sec: 94600.64072238898\n",
      "Step: 749, Training Loss: 2.44721, LR: 0.0000113, Tokens/sec: 66491.63263760717\n",
      "Step: 750, Training Loss: 2.36162, LR: 0.0000112, Tokens/sec: 82515.79705647411\n",
      "Computing Eval loss, steps: 17\n",
      "Step: 750, Eval Loss: 5.35927\n",
      "Step: 751, Training Loss: 2.47723, LR: 0.0000112, Tokens/sec: 58163.33397423371\n",
      "Step: 752, Training Loss: 2.30122, LR: 0.0000111, Tokens/sec: 63051.52672555652\n",
      "Step: 753, Training Loss: 2.44182, LR: 0.0000111, Tokens/sec: 61140.45335651892\n",
      "Step: 754, Training Loss: 2.37833, LR: 0.0000111, Tokens/sec: 77043.39356140535\n",
      "Step: 755, Training Loss: 2.40515, LR: 0.0000110, Tokens/sec: 79338.55200907146\n",
      "Step: 756, Training Loss: 2.30311, LR: 0.0000110, Tokens/sec: 79207.51336649404\n",
      "Step: 757, Training Loss: 2.19482, LR: 0.0000109, Tokens/sec: 69446.3836390306\n",
      "Step: 758, Training Loss: 2.38069, LR: 0.0000109, Tokens/sec: 84116.55752838831\n",
      "Step: 759, Training Loss: 2.44216, LR: 0.0000109, Tokens/sec: 67869.05243096124\n",
      "Step: 760, Training Loss: 2.42261, LR: 0.0000108, Tokens/sec: 92951.27304333984\n",
      "Step: 761, Training Loss: 2.37583, LR: 0.0000108, Tokens/sec: 75584.31416493311\n",
      "Step: 762, Training Loss: 2.35268, LR: 0.0000107, Tokens/sec: 92775.83182556683\n",
      "Step: 763, Training Loss: 2.32533, LR: 0.0000107, Tokens/sec: 66822.39198844989\n",
      "Step: 764, Training Loss: 2.35944, LR: 0.0000107, Tokens/sec: 82385.90879279881\n",
      "Step: 765, Training Loss: 2.28146, LR: 0.0000106, Tokens/sec: 57444.080341210676\n",
      "Step: 766, Training Loss: 2.37315, LR: 0.0000106, Tokens/sec: 57409.484475885234\n",
      "Step: 767, Training Loss: 2.33873, LR: 0.0000106, Tokens/sec: 89439.05944166408\n",
      "Step: 768, Training Loss: 2.34100, LR: 0.0000105, Tokens/sec: 86174.96834322008\n",
      "Step: 769, Training Loss: 2.20208, LR: 0.0000105, Tokens/sec: 63621.24977467862\n",
      "Step: 770, Training Loss: 2.28829, LR: 0.0000105, Tokens/sec: 93841.31543321793\n",
      "Step: 771, Training Loss: 2.38184, LR: 0.0000105, Tokens/sec: 92339.29451012026\n",
      "Step: 772, Training Loss: 2.35096, LR: 0.0000104, Tokens/sec: 68953.03436646807\n",
      "Step: 773, Training Loss: 2.30260, LR: 0.0000104, Tokens/sec: 88094.39140811647\n",
      "Step: 774, Training Loss: 2.25709, LR: 0.0000104, Tokens/sec: 57788.2922226802\n",
      "Step: 775, Training Loss: 2.44554, LR: 0.0000104, Tokens/sec: 64319.879848524964\n",
      "Computing Eval loss, steps: 17\n",
      "Step: 775, Eval Loss: 5.37430\n",
      "Step: 776, Training Loss: 2.29936, LR: 0.0000103, Tokens/sec: 93673.1066026718\n",
      "Step: 777, Training Loss: 2.27020, LR: 0.0000103, Tokens/sec: 93643.65993585082\n",
      "Step: 778, Training Loss: 2.30953, LR: 0.0000103, Tokens/sec: 69860.0152680701\n",
      "Step: 779, Training Loss: 2.33793, LR: 0.0000103, Tokens/sec: 86139.30289428783\n",
      "Step: 780, Training Loss: 2.31931, LR: 0.0000102, Tokens/sec: 84562.02722695287\n",
      "Step: 781, Training Loss: 2.40157, LR: 0.0000102, Tokens/sec: 65603.15768490011\n",
      "Step: 782, Training Loss: 2.32287, LR: 0.0000102, Tokens/sec: 94084.04962547815\n",
      "Step: 783, Training Loss: 2.24706, LR: 0.0000102, Tokens/sec: 93552.03344195103\n",
      "Step: 784, Training Loss: 2.27199, LR: 0.0000102, Tokens/sec: 77411.83362384584\n",
      "Step: 785, Training Loss: 2.13161, LR: 0.0000102, Tokens/sec: 91440.952709536\n",
      "Step: 786, Training Loss: 2.24613, LR: 0.0000101, Tokens/sec: 82683.84993770269\n",
      "Step: 787, Training Loss: 2.34393, LR: 0.0000101, Tokens/sec: 62327.627053687735\n",
      "Step: 788, Training Loss: 2.21849, LR: 0.0000101, Tokens/sec: 90857.98934202174\n",
      "Step: 789, Training Loss: 2.34617, LR: 0.0000101, Tokens/sec: 87144.00789739916\n",
      "Step: 790, Training Loss: 2.30015, LR: 0.0000101, Tokens/sec: 68241.05807058966\n",
      "Step: 791, Training Loss: 2.22404, LR: 0.0000101, Tokens/sec: 91378.90046822035\n",
      "Step: 792, Training Loss: 2.13413, LR: 0.0000101, Tokens/sec: 93580.97564733782\n",
      "Step: 793, Training Loss: 2.28620, LR: 0.0000101, Tokens/sec: 63149.620305387274\n",
      "Step: 794, Training Loss: 2.43305, LR: 0.0000100, Tokens/sec: 91364.26363210683\n",
      "Step: 795, Training Loss: 2.21829, LR: 0.0000100, Tokens/sec: 88101.4041942898\n",
      "Step: 796, Training Loss: 2.19054, LR: 0.0000100, Tokens/sec: 66804.9673021796\n",
      "Step: 797, Training Loss: 2.20413, LR: 0.0000100, Tokens/sec: 92128.55306048822\n",
      "Step: 798, Training Loss: 2.18769, LR: 0.0000100, Tokens/sec: 90013.20024564308\n",
      "Step: 799, Training Loss: 2.35822, LR: 0.0000100, Tokens/sec: 72249.7598013333\n",
      "Step: 800, Training Loss: 2.31234, LR: 0.0000100, Tokens/sec: 87255.59406491216\n",
      "Computing Eval loss, steps: 17\n",
      "Step: 800, Eval Loss: 5.38786\n",
      "Step: 801, Training Loss: 2.26492, LR: 0.0000100, Tokens/sec: 92911.63608968654\n",
      "Step: 802, Training Loss: 2.17611, LR: 0.0000100, Tokens/sec: 94023.98602694321\n",
      "Step: 803, Training Loss: 2.33322, LR: 0.0000100, Tokens/sec: 68621.69357857548\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:18:18.388416Z",
     "start_time": "2024-12-16T09:18:17.485673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_ids = tokenizer([\"HAMLET:\\nTo be or\"], return_tensors=\"pt\")['input_ids'].to(trainer.device)\n",
    "idx = model.generate(input_ids, temperature=0.25, top_k=50, max_new_tokens=64)\n",
    "print(tokenizer.batch_decode(idx)[0])"
   ],
   "id": "8b5596eda083de0a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAMLET:\n",
      "To be or for with with with now.\n",
      "\n",
      "BENVOLIO:\n",
      "I'll be so my heart's thee.\n",
      "\n",
      "HORTENSIO:\n",
      "That's the king, I'll not I am him.\n",
      "\n",
      "BAPTISTA:\n",
      "I'll be so I have thee, and make thee,\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:18:18.391899Z",
     "start_time": "2024-12-16T09:18:18.390545Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "bc3bcc343073d67a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
