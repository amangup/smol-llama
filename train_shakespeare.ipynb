{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-16T08:08:11.270244Z",
     "start_time": "2024-12-16T08:08:10.327343Z"
    }
   },
   "source": [
    "from model import ModelConfig, LlamaModel\n",
    "from train import TrainerConfig, DataLoader, Trainer\n",
    "\n",
    "from transformers import AutoTokenizer"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T08:08:11.274529Z",
     "start_time": "2024-12-16T08:08:11.272674Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer_id = \"HuggingFaceTB/SmolLM2-135M\"",
   "id": "2f28fa23c987e72b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T08:08:13.080095Z",
     "start_time": "2024-12-16T08:08:11.316173Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ],
   "id": "9bb4e51aa142abee",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T08:08:13.086759Z",
     "start_time": "2024-12-16T08:08:13.085224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_config = ModelConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=576,\n",
    "    d_head=64,\n",
    "    d_mlp_proj=1536,\n",
    "    n_layers=30,\n",
    "    n_kv_heads=3,\n",
    "    n_attn_heads=9,\n",
    "    rms_norm_eps=1e-5,\n",
    "    initializer_range=0.041666666666666664,\n",
    "    rope_theta=100000.0,\n",
    "    padding_idx=tokenizer.pad_token_id\n",
    ")"
   ],
   "id": "cde027092af8291e",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T08:08:13.130436Z",
     "start_time": "2024-12-16T08:08:13.128506Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_config = TrainerConfig(\n",
    "    per_device_train_batch_size=8,\n",
    "    max_seq_len=2048,\n",
    "    num_epochs=20,\n",
    "    eval_interval_steps=100,\n",
    "    learning_rate=2e-3,\n",
    "    log_dir=\"runs/shakespeare\"\n",
    ")"
   ],
   "id": "809773e662327a12",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T08:08:13.174552Z",
     "start_time": "2024-12-16T08:08:13.172463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"data/tiny_shakespeare.txt\") as f:\n",
    "    text = f.read()"
   ],
   "id": "374f398bb34f7ac1",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T08:08:15.834523Z",
     "start_time": "2024-12-16T08:08:13.217025Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = LlamaModel(model_config)\n",
    "dataloader = DataLoader(train_config, tokenizer, text=text)\n",
    "trainer = Trainer(train_config, model)"
   ],
   "id": "9a912a0ec92039d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens                   | 342,016\n",
      "Num Trainable Params           | 162,826,560\n",
      "Train device                   | cuda, NVIDIA GeForce RTX 3090, N=1\n",
      "Training precision             | torch.bfloat16\n",
      "Flash Attention                | True\n",
      "torch.compile()                | True\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T08:11:08.195436Z",
     "start_time": "2024-12-16T08:08:15.842113Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train(dataloader)",
   "id": "ee8c2059258a0195",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training steps                 | 380 \n",
      "Step: 0, Training Loss: 11.26471, Tokens/sec: 1446.8442643655137\n",
      "Computing Eval loss, steps: 2\n",
      "Step: 0, Eval Loss: 9.97953\n",
      "Step: 1, Training Loss: 10.04189, Tokens/sec: 1700.9048912118005\n",
      "Step: 2, Training Loss: 13.90497, Tokens/sec: 87907.76110828476\n",
      "Step: 3, Training Loss: 9.56660, Tokens/sec: 87940.89869378602\n",
      "Step: 4, Training Loss: 9.34001, Tokens/sec: 86717.57252231214\n",
      "Step: 5, Training Loss: 8.73359, Tokens/sec: 89101.00748959089\n",
      "Step: 6, Training Loss: 8.24238, Tokens/sec: 88563.37954980384\n",
      "Step: 7, Training Loss: 7.67133, Tokens/sec: 86404.15442009721\n",
      "Step: 8, Training Loss: 7.83910, Tokens/sec: 84444.58094768302\n",
      "Step: 9, Training Loss: 7.22471, Tokens/sec: 86252.38484029603\n",
      "Step: 10, Training Loss: 6.94995, Tokens/sec: 84090.67894838427\n",
      "Step: 11, Training Loss: 6.71106, Tokens/sec: 87275.43867014289\n",
      "Step: 12, Training Loss: 6.71211, Tokens/sec: 84603.42958254786\n",
      "Step: 13, Training Loss: 6.76546, Tokens/sec: 88622.89087750185\n",
      "Step: 14, Training Loss: 6.82064, Tokens/sec: 87774.57701536875\n",
      "Step: 15, Training Loss: 6.60868, Tokens/sec: 68535.72335301517\n",
      "Step: 16, Training Loss: 6.70687, Tokens/sec: 83857.99301981382\n",
      "Step: 17, Training Loss: 6.65175, Tokens/sec: 87680.99705208447\n",
      "Step: 18, Training Loss: 6.50063, Tokens/sec: 64199.79855137184\n",
      "Step: 19, Training Loss: 6.62965, Tokens/sec: 88093.66197147887\n",
      "Step: 20, Training Loss: 6.59649, Tokens/sec: 86818.66411906463\n",
      "Step: 21, Training Loss: 6.55096, Tokens/sec: 82992.45422365038\n",
      "Step: 22, Training Loss: 6.55140, Tokens/sec: 78528.45121232116\n",
      "Step: 23, Training Loss: 6.54166, Tokens/sec: 84800.98279623404\n",
      "Step: 24, Training Loss: 6.62878, Tokens/sec: 83637.48658739739\n",
      "Step: 25, Training Loss: 6.53226, Tokens/sec: 85360.94181853384\n",
      "Step: 26, Training Loss: 6.40718, Tokens/sec: 87495.8586450095\n",
      "Step: 27, Training Loss: 6.67669, Tokens/sec: 85852.93542014729\n",
      "Step: 28, Training Loss: 6.49039, Tokens/sec: 86684.56969743333\n",
      "Step: 29, Training Loss: 6.40255, Tokens/sec: 62692.77602855386\n",
      "Step: 30, Training Loss: 6.43058, Tokens/sec: 84600.90802099905\n",
      "Step: 31, Training Loss: 6.55950, Tokens/sec: 84698.43606329481\n",
      "Step: 32, Training Loss: 6.51970, Tokens/sec: 82868.5863502677\n",
      "Step: 33, Training Loss: 6.47296, Tokens/sec: 60777.975747246375\n",
      "Step: 34, Training Loss: 6.42631, Tokens/sec: 78450.71445688038\n",
      "Step: 35, Training Loss: 6.52749, Tokens/sec: 62598.08363923451\n",
      "Step: 36, Training Loss: 6.46416, Tokens/sec: 82922.56868295414\n",
      "Step: 37, Training Loss: 6.33984, Tokens/sec: 86094.17852982915\n",
      "Step: 38, Training Loss: 6.42798, Tokens/sec: 85905.39976276139\n",
      "Step: 39, Training Loss: 6.49572, Tokens/sec: 88382.17020279294\n",
      "Step: 40, Training Loss: 6.43008, Tokens/sec: 88504.96021074349\n",
      "Step: 41, Training Loss: 6.45095, Tokens/sec: 86159.84905880957\n",
      "Step: 42, Training Loss: 6.48612, Tokens/sec: 88095.66466419262\n",
      "Step: 43, Training Loss: 6.57230, Tokens/sec: 88572.94460380313\n",
      "Step: 44, Training Loss: 6.49495, Tokens/sec: 86046.71618143933\n",
      "Step: 45, Training Loss: 6.36267, Tokens/sec: 87666.69614383644\n",
      "Step: 46, Training Loss: 6.45589, Tokens/sec: 84564.37275471778\n",
      "Step: 47, Training Loss: 6.47401, Tokens/sec: 85219.99562140429\n",
      "Step: 48, Training Loss: 6.38628, Tokens/sec: 85938.47266961697\n",
      "Step: 49, Training Loss: 6.38575, Tokens/sec: 85413.54465824913\n",
      "Step: 50, Training Loss: 6.50586, Tokens/sec: 86751.10299611956\n",
      "Step: 51, Training Loss: 6.45833, Tokens/sec: 86671.9591698782\n",
      "Step: 52, Training Loss: 6.42846, Tokens/sec: 88349.47155402177\n",
      "Step: 53, Training Loss: 6.37529, Tokens/sec: 87470.99787147416\n",
      "Step: 54, Training Loss: 6.47965, Tokens/sec: 85650.5262272781\n",
      "Step: 55, Training Loss: 6.41444, Tokens/sec: 88500.2942385753\n",
      "Step: 56, Training Loss: 6.28665, Tokens/sec: 87790.7547643499\n",
      "Step: 57, Training Loss: 6.37807, Tokens/sec: 87413.39563394323\n",
      "Step: 58, Training Loss: 6.44857, Tokens/sec: 87537.83744606843\n",
      "Step: 59, Training Loss: 6.38115, Tokens/sec: 85790.76252881445\n",
      "Step: 60, Training Loss: 6.41097, Tokens/sec: 83624.01320827368\n",
      "Step: 61, Training Loss: 6.45062, Tokens/sec: 83245.38557933795\n",
      "Step: 62, Training Loss: 6.53152, Tokens/sec: 82209.6071827443\n",
      "Step: 63, Training Loss: 6.45203, Tokens/sec: 84821.5453087029\n",
      "Step: 64, Training Loss: 6.33307, Tokens/sec: 82353.90005016525\n",
      "Step: 65, Training Loss: 6.47791, Tokens/sec: 84891.8589661043\n",
      "Step: 66, Training Loss: 6.44510, Tokens/sec: 88302.4931683401\n",
      "Step: 67, Training Loss: 6.35373, Tokens/sec: 85770.25999042651\n",
      "Step: 68, Training Loss: 6.35904, Tokens/sec: 87750.110776321\n",
      "Step: 69, Training Loss: 6.47456, Tokens/sec: 87173.17634307589\n",
      "Step: 70, Training Loss: 6.43359, Tokens/sec: 84279.44130641306\n",
      "Step: 71, Training Loss: 6.40898, Tokens/sec: 86731.22658866733\n",
      "Step: 72, Training Loss: 6.35199, Tokens/sec: 85788.66875556967\n",
      "Step: 73, Training Loss: 6.45775, Tokens/sec: 88338.85252558954\n",
      "Step: 74, Training Loss: 6.39270, Tokens/sec: 80554.27833653518\n",
      "Step: 75, Training Loss: 6.26794, Tokens/sec: 84361.08413992378\n",
      "Step: 76, Training Loss: 6.35818, Tokens/sec: 83336.68658611385\n",
      "Step: 77, Training Loss: 6.44193, Tokens/sec: 86499.20795220709\n",
      "Step: 78, Training Loss: 6.37198, Tokens/sec: 86491.32194600897\n",
      "Step: 79, Training Loss: 6.39948, Tokens/sec: 87465.67870440776\n",
      "Step: 80, Training Loss: 6.43864, Tokens/sec: 84785.16544555788\n",
      "Step: 81, Training Loss: 6.51640, Tokens/sec: 87958.51901107529\n",
      "Step: 82, Training Loss: 6.44185, Tokens/sec: 87067.72606471149\n",
      "Step: 83, Training Loss: 6.32040, Tokens/sec: 84323.15467697252\n",
      "Step: 84, Training Loss: 6.42846, Tokens/sec: 82171.42318949515\n",
      "Step: 85, Training Loss: 6.43463, Tokens/sec: 85086.58312870539\n",
      "Step: 86, Training Loss: 6.33681, Tokens/sec: 85197.78011568352\n",
      "Step: 87, Training Loss: 6.34271, Tokens/sec: 84648.4042975538\n",
      "Step: 88, Training Loss: 6.45671, Tokens/sec: 84585.33988511212\n",
      "Step: 89, Training Loss: 6.40355, Tokens/sec: 71008.79588150397\n",
      "Step: 90, Training Loss: 6.38533, Tokens/sec: 86042.76037180853\n",
      "Step: 91, Training Loss: 6.32651, Tokens/sec: 83300.87061989255\n",
      "Step: 92, Training Loss: 6.43338, Tokens/sec: 77011.80544567281\n",
      "Step: 93, Training Loss: 6.36063, Tokens/sec: 83835.23565792026\n",
      "Step: 94, Training Loss: 6.23228, Tokens/sec: 84203.44155927403\n",
      "Step: 95, Training Loss: 6.34908, Tokens/sec: 74814.46171138834\n",
      "Step: 96, Training Loss: 6.41038, Tokens/sec: 68383.05662456973\n",
      "Step: 97, Training Loss: 6.33327, Tokens/sec: 80882.07990190842\n",
      "Step: 98, Training Loss: 6.36575, Tokens/sec: 84523.48040942848\n",
      "Step: 99, Training Loss: 6.39987, Tokens/sec: 86870.10761512443\n",
      "Step: 100, Training Loss: 6.47661, Tokens/sec: 84726.7818042464\n",
      "Computing Eval loss, steps: 2\n",
      "Step: 100, Eval Loss: 6.40280\n",
      "Step: 101, Training Loss: 6.38875, Tokens/sec: 82736.88776807966\n",
      "Step: 102, Training Loss: 6.25761, Tokens/sec: 83679.16234626574\n",
      "Step: 103, Training Loss: 6.38651, Tokens/sec: 81220.32305743787\n",
      "Step: 104, Training Loss: 6.37426, Tokens/sec: 85947.04041280392\n",
      "Step: 105, Training Loss: 6.26436, Tokens/sec: 87735.42085737237\n",
      "Step: 106, Training Loss: 6.27070, Tokens/sec: 85264.77778949314\n",
      "Step: 107, Training Loss: 6.38531, Tokens/sec: 86850.91787152608\n",
      "Step: 108, Training Loss: 6.31584, Tokens/sec: 86525.95299277703\n",
      "Step: 109, Training Loss: 6.29508, Tokens/sec: 85121.66368631918\n",
      "Step: 110, Training Loss: 6.22809, Tokens/sec: 87623.90246762932\n",
      "Step: 111, Training Loss: 6.33486, Tokens/sec: 83951.3046565294\n",
      "Step: 112, Training Loss: 6.24525, Tokens/sec: 77263.45809168981\n",
      "Step: 113, Training Loss: 6.09568, Tokens/sec: 69389.3463918512\n",
      "Step: 114, Training Loss: 6.26232, Tokens/sec: 69005.16715377795\n",
      "Step: 115, Training Loss: 6.26989, Tokens/sec: 81866.23148718612\n",
      "Step: 116, Training Loss: 6.15212, Tokens/sec: 85429.24189338105\n",
      "Step: 117, Training Loss: 6.17987, Tokens/sec: 76008.7158958442\n",
      "Step: 118, Training Loss: 6.18600, Tokens/sec: 78229.83343430987\n",
      "Step: 119, Training Loss: 6.25496, Tokens/sec: 88055.58362484361\n",
      "Step: 120, Training Loss: 6.13759, Tokens/sec: 59326.984892807464\n",
      "Step: 121, Training Loss: 5.99610, Tokens/sec: 86083.57726374178\n",
      "Step: 122, Training Loss: 6.10428, Tokens/sec: 83696.83355719963\n",
      "Step: 123, Training Loss: 6.06433, Tokens/sec: 85857.35475499621\n",
      "Step: 124, Training Loss: 5.91988, Tokens/sec: 88863.83277790695\n",
      "Step: 125, Training Loss: 5.92518, Tokens/sec: 61498.48313606679\n",
      "Step: 126, Training Loss: 6.04214, Tokens/sec: 83773.47001323875\n",
      "Step: 127, Training Loss: 5.90566, Tokens/sec: 73837.8270125271\n",
      "Step: 128, Training Loss: 5.87010, Tokens/sec: 79326.2782013798\n",
      "Step: 129, Training Loss: 5.80887, Tokens/sec: 85831.36581105157\n",
      "Step: 130, Training Loss: 5.94414, Tokens/sec: 87771.68138326568\n",
      "Step: 131, Training Loss: 5.81092, Tokens/sec: 88270.47405170632\n",
      "Step: 132, Training Loss: 5.57944, Tokens/sec: 85761.3826870953\n",
      "Step: 133, Training Loss: 5.76454, Tokens/sec: 65473.26037837273\n",
      "Step: 134, Training Loss: 5.78685, Tokens/sec: 87975.86106596015\n",
      "Step: 135, Training Loss: 5.64533, Tokens/sec: 80812.216438924\n",
      "Step: 136, Training Loss: 5.71942, Tokens/sec: 88323.53536162154\n",
      "Step: 137, Training Loss: 5.69974, Tokens/sec: 86209.82820702704\n",
      "Step: 138, Training Loss: 5.80584, Tokens/sec: 60603.71614030336\n",
      "Step: 139, Training Loss: 5.72478, Tokens/sec: 85951.8730052667\n",
      "Step: 140, Training Loss: 5.56486, Tokens/sec: 66073.78377660186\n",
      "Step: 141, Training Loss: 5.72170, Tokens/sec: 82394.58001152842\n",
      "Step: 142, Training Loss: 5.67715, Tokens/sec: 73914.03460316152\n",
      "Step: 143, Training Loss: 5.54006, Tokens/sec: 71486.45283995777\n",
      "Step: 144, Training Loss: 5.54444, Tokens/sec: 88092.96143376881\n",
      "Step: 145, Training Loss: 5.70169, Tokens/sec: 76270.65614315539\n",
      "Step: 146, Training Loss: 5.53887, Tokens/sec: 87035.74571882137\n",
      "Step: 147, Training Loss: 5.54764, Tokens/sec: 86653.20745775859\n",
      "Step: 148, Training Loss: 5.50031, Tokens/sec: 88128.67009498908\n",
      "Step: 149, Training Loss: 5.66665, Tokens/sec: 87341.50001875465\n",
      "Step: 150, Training Loss: 5.54598, Tokens/sec: 83218.50789355364\n",
      "Step: 151, Training Loss: 5.33411, Tokens/sec: 76712.54170970297\n",
      "Step: 152, Training Loss: 5.50453, Tokens/sec: 79820.73852398597\n",
      "Step: 153, Training Loss: 5.55875, Tokens/sec: 82376.3932267622\n",
      "Step: 154, Training Loss: 5.42385, Tokens/sec: 84987.52661467802\n",
      "Step: 155, Training Loss: 5.50034, Tokens/sec: 83142.80619144336\n",
      "Step: 156, Training Loss: 5.52199, Tokens/sec: 71506.61687849491\n",
      "Step: 157, Training Loss: 5.64196, Tokens/sec: 87911.18553351657\n",
      "Step: 158, Training Loss: 5.56769, Tokens/sec: 76956.12738901292\n",
      "Step: 159, Training Loss: 5.38642, Tokens/sec: 84546.96683985542\n",
      "Step: 160, Training Loss: 5.56734, Tokens/sec: 87324.12229397261\n",
      "Step: 161, Training Loss: 5.51642, Tokens/sec: 71662.91744103933\n",
      "Step: 162, Training Loss: 5.38800, Tokens/sec: 74411.07228740772\n",
      "Step: 163, Training Loss: 5.39919, Tokens/sec: 74363.66079606513\n",
      "Step: 164, Training Loss: 5.55090, Tokens/sec: 78436.82981356513\n",
      "Step: 165, Training Loss: 5.37661, Tokens/sec: 74685.11340378082\n",
      "Step: 166, Training Loss: 5.40047, Tokens/sec: 76398.10823153117\n",
      "Step: 167, Training Loss: 5.35397, Tokens/sec: 84094.3593117109\n",
      "Step: 168, Training Loss: 5.52432, Tokens/sec: 74788.62149147136\n",
      "Step: 169, Training Loss: 5.41238, Tokens/sec: 76873.01879781515\n",
      "Step: 170, Training Loss: 5.18013, Tokens/sec: 75161.02874183624\n",
      "Step: 171, Training Loss: 5.36269, Tokens/sec: 83035.30152312307\n",
      "Step: 172, Training Loss: 5.43565, Tokens/sec: 73930.19957687364\n",
      "Step: 173, Training Loss: 5.29220, Tokens/sec: 85534.86863133704\n",
      "Step: 174, Training Loss: 5.37377, Tokens/sec: 68309.99598190056\n",
      "Step: 175, Training Loss: 5.36674, Tokens/sec: 73987.65245388399\n",
      "Step: 176, Training Loss: 5.49803, Tokens/sec: 85158.02932127162\n",
      "Step: 177, Training Loss: 5.45170, Tokens/sec: 74148.66726376321\n",
      "Step: 178, Training Loss: 5.26782, Tokens/sec: 71833.71022359568\n",
      "Step: 179, Training Loss: 5.41148, Tokens/sec: 82988.2462854126\n",
      "Step: 180, Training Loss: 5.41971, Tokens/sec: 80130.31348798763\n",
      "Step: 181, Training Loss: 5.30503, Tokens/sec: 84367.8922062619\n",
      "Step: 182, Training Loss: 5.27937, Tokens/sec: 74065.29141047028\n",
      "Step: 183, Training Loss: 5.49948, Tokens/sec: 77318.96525822379\n",
      "Step: 184, Training Loss: 5.27888, Tokens/sec: 81935.51285061173\n",
      "Step: 185, Training Loss: 5.32809, Tokens/sec: 86750.0878733523\n",
      "Step: 186, Training Loss: 5.24159, Tokens/sec: 79607.71123634056\n",
      "Step: 187, Training Loss: 5.41836, Tokens/sec: 72607.13158033781\n",
      "Step: 188, Training Loss: 5.29210, Tokens/sec: 73977.57685743383\n",
      "Step: 189, Training Loss: 5.09241, Tokens/sec: 80568.24214723193\n",
      "Step: 190, Training Loss: 5.24438, Tokens/sec: 84917.8889252381\n",
      "Step: 191, Training Loss: 5.32466, Tokens/sec: 76866.69614945563\n",
      "Step: 192, Training Loss: 5.16183, Tokens/sec: 75258.42419462581\n",
      "Step: 193, Training Loss: 5.22997, Tokens/sec: 85871.1627655145\n",
      "Step: 194, Training Loss: 5.25774, Tokens/sec: 82443.3474847498\n",
      "Step: 195, Training Loss: 5.36262, Tokens/sec: 84026.85059930779\n",
      "Step: 196, Training Loss: 5.32901, Tokens/sec: 87216.27223531101\n",
      "Step: 197, Training Loss: 5.11648, Tokens/sec: 83006.85860431612\n",
      "Step: 198, Training Loss: 5.31375, Tokens/sec: 74900.86641196042\n",
      "Step: 199, Training Loss: 5.27157, Tokens/sec: 72728.61249771551\n",
      "Step: 200, Training Loss: 5.15790, Tokens/sec: 78416.29448462432\n",
      "Computing Eval loss, steps: 2\n",
      "Step: 200, Eval Loss: 5.40389\n",
      "Step: 201, Training Loss: 5.14953, Tokens/sec: 86159.11459547163\n",
      "Step: 202, Training Loss: 5.30812, Tokens/sec: 81314.8104575258\n",
      "Step: 203, Training Loss: 5.12526, Tokens/sec: 74394.74534807648\n",
      "Step: 204, Training Loss: 5.16314, Tokens/sec: 82546.47865563414\n",
      "Step: 205, Training Loss: 5.12195, Tokens/sec: 81589.26648660199\n",
      "Step: 206, Training Loss: 5.25305, Tokens/sec: 78142.42632439289\n",
      "Step: 207, Training Loss: 5.18196, Tokens/sec: 76128.8845112349\n",
      "Step: 208, Training Loss: 4.89616, Tokens/sec: 85450.76360195655\n",
      "Step: 209, Training Loss: 5.16074, Tokens/sec: 87873.58663674291\n",
      "Step: 210, Training Loss: 5.17322, Tokens/sec: 87715.51165673473\n",
      "Step: 211, Training Loss: 5.08077, Tokens/sec: 87074.1306937559\n",
      "Step: 212, Training Loss: 5.09210, Tokens/sec: 86195.36328610203\n",
      "Step: 213, Training Loss: 5.13690, Tokens/sec: 85595.51462253075\n",
      "Step: 214, Training Loss: 5.26676, Tokens/sec: 69354.66948759294\n",
      "Step: 215, Training Loss: 5.21003, Tokens/sec: 84268.72137051154\n",
      "Step: 216, Training Loss: 5.04900, Tokens/sec: 73522.53704200409\n",
      "Step: 217, Training Loss: 5.06382, Tokens/sec: 66843.73893046209\n",
      "Step: 218, Training Loss: 5.20785, Tokens/sec: 84539.47464314144\n",
      "Step: 219, Training Loss: 5.02626, Tokens/sec: 82614.69790741053\n",
      "Step: 220, Training Loss: 5.10617, Tokens/sec: 66816.84170433576\n",
      "Step: 221, Training Loss: 5.18064, Tokens/sec: 88214.48550715747\n",
      "Step: 222, Training Loss: 4.99604, Tokens/sec: 86268.05944122623\n",
      "Step: 223, Training Loss: 5.06525, Tokens/sec: 87977.88958238934\n",
      "Step: 224, Training Loss: 4.95563, Tokens/sec: 88000.96337283486\n",
      "Step: 225, Training Loss: 5.16432, Tokens/sec: 84771.43076310758\n",
      "Step: 226, Training Loss: 5.05909, Tokens/sec: 87174.2060253297\n",
      "Step: 227, Training Loss: 4.77614, Tokens/sec: 85155.41661141468\n",
      "Step: 228, Training Loss: 5.00770, Tokens/sec: 85212.85877220231\n",
      "Step: 229, Training Loss: 5.03964, Tokens/sec: 86339.69214683033\n",
      "Step: 230, Training Loss: 4.90849, Tokens/sec: 83696.40728300228\n",
      "Step: 231, Training Loss: 5.01459, Tokens/sec: 73400.26860643247\n",
      "Step: 232, Training Loss: 4.95933, Tokens/sec: 82246.89233000824\n",
      "Step: 233, Training Loss: 5.14410, Tokens/sec: 74064.75436558215\n",
      "Step: 234, Training Loss: 5.06100, Tokens/sec: 81709.0920835017\n",
      "Step: 235, Training Loss: 4.86326, Tokens/sec: 58381.38507860466\n",
      "Step: 236, Training Loss: 5.03433, Tokens/sec: 72716.69413964819\n",
      "Step: 237, Training Loss: 5.02327, Tokens/sec: 85393.7249330369\n",
      "Step: 238, Training Loss: 4.92058, Tokens/sec: 88008.76258575566\n",
      "Step: 239, Training Loss: 4.96635, Tokens/sec: 88153.6609567019\n",
      "Step: 240, Training Loss: 5.04174, Tokens/sec: 85279.77008800404\n",
      "Step: 241, Training Loss: 4.92834, Tokens/sec: 85223.76664310442\n",
      "Step: 242, Training Loss: 4.90054, Tokens/sec: 84621.1887346425\n",
      "Step: 243, Training Loss: 4.87555, Tokens/sec: 86872.36920270135\n",
      "Step: 244, Training Loss: 5.08372, Tokens/sec: 85630.47464748916\n",
      "Step: 245, Training Loss: 4.92369, Tokens/sec: 82864.94627035879\n",
      "Step: 246, Training Loss: 4.66763, Tokens/sec: 87245.02403507616\n",
      "Step: 247, Training Loss: 4.92718, Tokens/sec: 87860.94679500163\n",
      "Step: 248, Training Loss: 4.93884, Tokens/sec: 85925.60374990065\n",
      "Step: 249, Training Loss: 4.79992, Tokens/sec: 88082.18898387061\n",
      "Step: 250, Training Loss: 4.86010, Tokens/sec: 86602.17151789588\n",
      "Step: 251, Training Loss: 4.85762, Tokens/sec: 88701.67983529525\n",
      "Step: 252, Training Loss: 5.01494, Tokens/sec: 88695.10125493539\n",
      "Step: 253, Training Loss: 4.91330, Tokens/sec: 86549.89076976619\n",
      "Step: 254, Training Loss: 4.75876, Tokens/sec: 74194.71058545983\n",
      "Step: 255, Training Loss: 4.65481, Tokens/sec: 84250.99242832983\n",
      "Step: 256, Training Loss: 4.91256, Tokens/sec: 78644.39111636781\n",
      "Step: 257, Training Loss: 4.77636, Tokens/sec: 85070.49651186014\n",
      "Step: 258, Training Loss: 4.79103, Tokens/sec: 83142.10286013599\n",
      "Step: 259, Training Loss: 4.91379, Tokens/sec: 86685.12280900552\n",
      "Step: 260, Training Loss: 4.72319, Tokens/sec: 84063.91092240404\n",
      "Step: 261, Training Loss: 4.76352, Tokens/sec: 70539.69154307428\n",
      "Step: 262, Training Loss: 4.70730, Tokens/sec: 87538.93937490424\n",
      "Step: 263, Training Loss: 4.89308, Tokens/sec: 88286.55159236019\n",
      "Step: 264, Training Loss: 4.81702, Tokens/sec: 78124.10072745488\n",
      "Step: 265, Training Loss: 4.51736, Tokens/sec: 78141.01979682263\n",
      "Step: 266, Training Loss: 4.75494, Tokens/sec: 87267.85394057664\n",
      "Step: 267, Training Loss: 4.83031, Tokens/sec: 70844.8905261209\n",
      "Step: 268, Training Loss: 4.66914, Tokens/sec: 85159.66483467216\n",
      "Step: 269, Training Loss: 4.72568, Tokens/sec: 81236.80174569221\n",
      "Step: 270, Training Loss: 4.72315, Tokens/sec: 73610.94685464425\n",
      "Step: 271, Training Loss: 4.89204, Tokens/sec: 85002.61560560146\n",
      "Step: 272, Training Loss: 4.78816, Tokens/sec: 86148.8470352805\n",
      "Step: 273, Training Loss: 4.64905, Tokens/sec: 86108.35200220204\n",
      "Step: 274, Training Loss: 4.47110, Tokens/sec: 88075.51636910316\n",
      "Step: 275, Training Loss: 4.80888, Tokens/sec: 84947.50882758814\n",
      "Step: 276, Training Loss: 4.65166, Tokens/sec: 88690.27598503008\n",
      "Step: 277, Training Loss: 4.68474, Tokens/sec: 87967.82679661356\n",
      "Step: 278, Training Loss: 4.80121, Tokens/sec: 84705.12833648227\n",
      "Step: 279, Training Loss: 4.62905, Tokens/sec: 88574.679918541\n",
      "Step: 280, Training Loss: 4.65642, Tokens/sec: 85517.08282102791\n",
      "Step: 281, Training Loss: 4.59139, Tokens/sec: 85586.51250205236\n",
      "Step: 282, Training Loss: 4.76960, Tokens/sec: 85840.64843406192\n",
      "Step: 283, Training Loss: 4.69706, Tokens/sec: 84120.53385996465\n",
      "Step: 284, Training Loss: 4.38197, Tokens/sec: 86115.66997504799\n",
      "Step: 285, Training Loss: 4.64516, Tokens/sec: 86410.39342998248\n",
      "Step: 286, Training Loss: 4.74490, Tokens/sec: 85615.72073498693\n",
      "Step: 287, Training Loss: 4.56669, Tokens/sec: 87945.45440893712\n",
      "Step: 288, Training Loss: 4.61241, Tokens/sec: 85255.49062935683\n",
      "Step: 289, Training Loss: 4.62483, Tokens/sec: 86778.24995146328\n",
      "Step: 290, Training Loss: 4.78443, Tokens/sec: 87872.05729861624\n",
      "Step: 291, Training Loss: 4.67837, Tokens/sec: 85652.67281914162\n",
      "Step: 292, Training Loss: 4.56080, Tokens/sec: 86748.66628822515\n",
      "Step: 293, Training Loss: 4.39425, Tokens/sec: 88712.37474779114\n",
      "Step: 294, Training Loss: 4.71579, Tokens/sec: 85202.8243497552\n",
      "Step: 295, Training Loss: 4.57081, Tokens/sec: 84767.10452878522\n",
      "Step: 296, Training Loss: 4.59704, Tokens/sec: 80242.17267910401\n",
      "Step: 297, Training Loss: 4.71719, Tokens/sec: 84561.17921990396\n",
      "Step: 298, Training Loss: 4.50587, Tokens/sec: 84920.522291703\n",
      "Step: 299, Training Loss: 4.57523, Tokens/sec: 85173.48488114218\n",
      "Step: 300, Training Loss: 4.50748, Tokens/sec: 87943.01339958476\n",
      "Computing Eval loss, steps: 2\n",
      "Step: 300, Eval Loss: 4.97089\n",
      "Step: 301, Training Loss: 4.66252, Tokens/sec: 85123.50654944211\n",
      "Step: 302, Training Loss: 4.58474, Tokens/sec: 87908.1011824958\n",
      "Step: 303, Training Loss: 4.27713, Tokens/sec: 87783.07550456998\n",
      "Step: 304, Training Loss: 4.53590, Tokens/sec: 88065.9197489867\n",
      "Step: 305, Training Loss: 4.64812, Tokens/sec: 84908.32339328065\n",
      "Step: 306, Training Loss: 4.49188, Tokens/sec: 87252.87383597248\n",
      "Step: 307, Training Loss: 4.51824, Tokens/sec: 83738.2363776892\n",
      "Step: 308, Training Loss: 4.54146, Tokens/sec: 82398.04751463172\n",
      "Step: 309, Training Loss: 4.71445, Tokens/sec: 85408.92022660119\n",
      "Step: 310, Training Loss: 4.57701, Tokens/sec: 81898.41198984372\n",
      "Step: 311, Training Loss: 4.46999, Tokens/sec: 85185.96339025702\n",
      "Step: 312, Training Loss: 4.30962, Tokens/sec: 87398.08020142617\n",
      "Step: 313, Training Loss: 4.63850, Tokens/sec: 86438.1014094232\n",
      "Step: 314, Training Loss: 4.45887, Tokens/sec: 87742.16232886548\n",
      "Step: 315, Training Loss: 4.50839, Tokens/sec: 87235.71668382882\n",
      "Step: 316, Training Loss: 4.64351, Tokens/sec: 88129.25790679593\n",
      "Step: 317, Training Loss: 4.42337, Tokens/sec: 87907.51112483993\n",
      "Step: 318, Training Loss: 4.49706, Tokens/sec: 84966.55308077768\n",
      "Step: 319, Training Loss: 4.43642, Tokens/sec: 86525.69572533685\n",
      "Step: 320, Training Loss: 4.61716, Tokens/sec: 85406.07039306097\n",
      "Step: 321, Training Loss: 4.50607, Tokens/sec: 86434.68544728655\n",
      "Step: 322, Training Loss: 4.20693, Tokens/sec: 86200.41568220529\n",
      "Step: 323, Training Loss: 4.49664, Tokens/sec: 84692.21109098157\n",
      "Step: 324, Training Loss: 4.57060, Tokens/sec: 86246.19586165693\n",
      "Step: 325, Training Loss: 4.42902, Tokens/sec: 86753.12503748645\n",
      "Step: 326, Training Loss: 4.46354, Tokens/sec: 84830.29454565546\n",
      "Step: 327, Training Loss: 4.49263, Tokens/sec: 88176.64007086455\n",
      "Step: 328, Training Loss: 4.65164, Tokens/sec: 84747.31184889867\n",
      "Step: 329, Training Loss: 4.56798, Tokens/sec: 86679.38334720785\n",
      "Step: 330, Training Loss: 4.39246, Tokens/sec: 88161.58024477056\n",
      "Step: 331, Training Loss: 4.26554, Tokens/sec: 86462.34558021491\n",
      "Step: 332, Training Loss: 4.58952, Tokens/sec: 82550.09330615486\n",
      "Step: 333, Training Loss: 4.43026, Tokens/sec: 71558.34986662926\n",
      "Step: 334, Training Loss: 4.45871, Tokens/sec: 78925.25993835845\n",
      "Step: 335, Training Loss: 4.56990, Tokens/sec: 82552.54317676154\n",
      "Step: 336, Training Loss: 4.40167, Tokens/sec: 86555.4983216927\n",
      "Step: 337, Training Loss: 4.43573, Tokens/sec: 82693.75638376253\n",
      "Step: 338, Training Loss: 4.37277, Tokens/sec: 78234.31006649978\n",
      "Step: 339, Training Loss: 4.55141, Tokens/sec: 72014.41156736195\n",
      "Step: 340, Training Loss: 4.48357, Tokens/sec: 85615.86255763777\n",
      "Step: 341, Training Loss: 4.14424, Tokens/sec: 87986.08823207732\n",
      "Step: 342, Training Loss: 4.46765, Tokens/sec: 72289.12675149516\n",
      "Step: 343, Training Loss: 4.52574, Tokens/sec: 76892.91499450814\n",
      "Step: 344, Training Loss: 4.39315, Tokens/sec: 88415.90505167157\n",
      "Step: 345, Training Loss: 4.39197, Tokens/sec: 72568.64606957887\n",
      "Step: 346, Training Loss: 4.46228, Tokens/sec: 83710.04557150604\n",
      "Step: 347, Training Loss: 4.56155, Tokens/sec: 73211.80861230046\n",
      "Step: 348, Training Loss: 4.47923, Tokens/sec: 80233.81300623508\n",
      "Step: 349, Training Loss: 4.33028, Tokens/sec: 84742.45026716865\n",
      "Step: 350, Training Loss: 4.17263, Tokens/sec: 79221.67834343041\n",
      "Step: 351, Training Loss: 4.53745, Tokens/sec: 85057.8893090264\n",
      "Step: 352, Training Loss: 4.33946, Tokens/sec: 88153.52483034658\n",
      "Step: 353, Training Loss: 4.38793, Tokens/sec: 87761.52891490204\n",
      "Step: 354, Training Loss: 4.50353, Tokens/sec: 83336.26778764321\n",
      "Step: 355, Training Loss: 4.30634, Tokens/sec: 85839.18903869219\n",
      "Step: 356, Training Loss: 4.36519, Tokens/sec: 85528.12988526335\n",
      "Step: 357, Training Loss: 4.31375, Tokens/sec: 65918.57638701789\n",
      "Step: 358, Training Loss: 4.46939, Tokens/sec: 69686.41529992067\n",
      "Step: 359, Training Loss: 4.36924, Tokens/sec: 81724.04657326905\n",
      "Step: 360, Training Loss: 4.07178, Tokens/sec: 81379.78390605506\n",
      "Step: 361, Training Loss: 4.35846, Tokens/sec: 80863.79391835177\n",
      "Step: 362, Training Loss: 4.45751, Tokens/sec: 83216.46889821503\n",
      "Step: 363, Training Loss: 4.28873, Tokens/sec: 61602.05807361047\n",
      "Step: 364, Training Loss: 4.33186, Tokens/sec: 66950.25340340601\n",
      "Step: 365, Training Loss: 4.32680, Tokens/sec: 86895.69807212662\n",
      "Step: 366, Training Loss: 4.49107, Tokens/sec: 87864.18569580518\n",
      "Step: 367, Training Loss: 4.40864, Tokens/sec: 87858.97454962319\n",
      "Step: 368, Training Loss: 4.26033, Tokens/sec: 78931.57442283098\n",
      "Step: 369, Training Loss: 4.10330, Tokens/sec: 61087.810902367615\n",
      "Step: 370, Training Loss: 4.45283, Tokens/sec: 80133.71805548723\n",
      "Step: 371, Training Loss: 4.29269, Tokens/sec: 85711.04180451477\n",
      "Step: 372, Training Loss: 4.34907, Tokens/sec: 75501.87897054304\n",
      "Step: 373, Training Loss: 4.43185, Tokens/sec: 67126.61013551243\n",
      "Step: 374, Training Loss: 4.25036, Tokens/sec: 66341.43195466303\n",
      "Step: 375, Training Loss: 4.31761, Tokens/sec: 79954.98705871396\n",
      "Step: 376, Training Loss: 4.26856, Tokens/sec: 82059.42799276601\n",
      "Step: 377, Training Loss: 4.45542, Tokens/sec: 84015.19918121274\n",
      "Step: 378, Training Loss: 4.34710, Tokens/sec: 79480.79905181626\n",
      "Step: 379, Training Loss: 4.01027, Tokens/sec: 84177.7797909438\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T08:11:09.139218Z",
     "start_time": "2024-12-16T08:11:08.207787Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_ids = tokenizer([\"HAMLET:\\nTo be or\"], return_tensors=\"pt\")['input_ids'].to(trainer.device)\n",
    "idx = model.generate(input_ids, temperature=0.25, top_k=50, max_new_tokens=64)\n",
    "print(tokenizer.batch_decode(idx)[0])"
   ],
   "id": "8b5596eda083de0a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAMLET:\n",
      "To be or the\n",
      "re I is I's\n",
      "And I is aer, and the king,\n",
      "And I have not not be the king,\n",
      "And, I have not not a king,\n",
      "And, I have been, and I will be,\n",
      "And, and a man, and I will be\n",
      "And,\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T08:11:09.143540Z",
     "start_time": "2024-12-16T08:11:09.142116Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "bc3bcc343073d67a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
