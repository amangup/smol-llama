{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T20:40:20.623738Z",
     "start_time": "2025-02-17T20:40:18.913509Z"
    }
   },
   "source": [
    "from model import ModelConfig, LlamaModel\n",
    "from train import TrainerConfig, SimpleDataLoader, Trainer\n",
    "\n",
    "from transformers import AutoTokenizer"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "2f28fa23c987e72b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T20:40:20.627969Z",
     "start_time": "2025-02-17T20:40:20.626307Z"
    }
   },
   "source": [
    "tokenizer_id = \"HuggingFaceTB/SmolLM2-135M\""
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "9bb4e51aa142abee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T20:40:20.821899Z",
     "start_time": "2025-02-17T20:40:20.672554Z"
    }
   },
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "cde027092af8291e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T20:40:20.828577Z",
     "start_time": "2025-02-17T20:40:20.826980Z"
    }
   },
   "source": [
    "model_config = ModelConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=576,\n",
    "    d_head=64,\n",
    "    d_mlp_proj=1536,\n",
    "    n_layers=4,\n",
    "    n_kv_heads=3,\n",
    "    n_attn_heads=9,\n",
    "    rms_norm_eps=1e-5,\n",
    "    initializer_range=0.041666666666666664,\n",
    "    rope_theta=100000.0,\n",
    "    padding_idx=tokenizer.pad_token_id\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "809773e662327a12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T20:40:21.149866Z",
     "start_time": "2025-02-17T20:40:21.148115Z"
    }
   },
   "source": [
    "train_config = TrainerConfig(\n",
    "    per_device_train_batch_size=8,\n",
    "    max_seq_len=1024,\n",
    "    num_epochs=64,\n",
    "    eval_interval_steps=25,\n",
    "    learning_rate=1e-4,\n",
    "    grad_clip_norm=1.0,\n",
    "    val_size=0.1,\n",
    "    log_dir=\"runs/shakespeare\",\n",
    "    warmup_ratio=0.1\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "374f398bb34f7ac1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T20:40:23.191018Z",
     "start_time": "2025-02-17T20:40:23.188812Z"
    }
   },
   "source": [
    "with open(\"data/tiny_shakespeare.txt\") as f:\n",
    "    text = f.read()"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "9a912a0ec92039d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T20:40:27.520398Z",
     "start_time": "2025-02-17T20:40:25.062004Z"
    }
   },
   "source": [
    "model = LlamaModel(model_config)\n",
    "dataloader = SimpleDataLoader(train_config, tokenizer, text=text)\n",
    "trainer = Trainer(train_config, model)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens                   | 342,016\n",
      "Num Trainable Params           | 205,307,712\n",
      "Train device                   | cuda, NVIDIA GeForce RTX 3090, N=1\n",
      "Training precision             | torch.bfloat16\n",
      "Flash Attention                | True\n",
      "torch.compile()                | True\n",
      "DistributedDataParallel        | False\n",
      "Batch size                     | 8,192\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "ee8c2059258a0195",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T20:53:24.448286Z",
     "start_time": "2025-02-17T20:40:34.362067Z"
    }
   },
   "source": [
    "trainer.train(dataloader)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training steps                 | 2,432 \n",
      "Step: 0, Training Loss: 11.27814, LR: 0.0000050, Tokens/sec: 245.21\n",
      "Step: 1, Training Loss: 11.26458, LR: 0.0000054, Tokens/sec: 257.17\n",
      "Step: 2, Training Loss: 11.16380, LR: 0.0000058, Tokens/sec: 43166.34\n",
      "Step: 3, Training Loss: 11.08978, LR: 0.0000062, Tokens/sec: 49253.00\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 3, Eval Loss: 11.05825\n",
      "Step: 4, Training Loss: 11.04331, LR: 0.0000066, Tokens/sec: 56529.39\n",
      "Step: 5, Training Loss: 11.02092, LR: 0.0000070, Tokens/sec: 53418.12\n",
      "Step: 6, Training Loss: 10.91256, LR: 0.0000073, Tokens/sec: 55672.44\n",
      "Step: 7, Training Loss: 10.78383, LR: 0.0000077, Tokens/sec: 54809.09\n",
      "Step: 8, Training Loss: 10.64979, LR: 0.0000081, Tokens/sec: 55418.08\n",
      "Step: 9, Training Loss: 10.64204, LR: 0.0000085, Tokens/sec: 49197.20\n",
      "Step: 10, Training Loss: 10.51057, LR: 0.0000089, Tokens/sec: 56150.28\n",
      "Step: 11, Training Loss: 10.47069, LR: 0.0000093, Tokens/sec: 56400.19\n",
      "Step: 12, Training Loss: 10.26100, LR: 0.0000097, Tokens/sec: 53085.10\n",
      "Step: 13, Training Loss: 10.19521, LR: 0.0000101, Tokens/sec: 53418.19\n",
      "Step: 14, Training Loss: 10.01456, LR: 0.0000105, Tokens/sec: 43277.49\n",
      "Step: 15, Training Loss: 9.94947, LR: 0.0000109, Tokens/sec: 56648.70\n",
      "Step: 16, Training Loss: 9.81297, LR: 0.0000113, Tokens/sec: 56478.68\n",
      "Step: 17, Training Loss: 9.80123, LR: 0.0000116, Tokens/sec: 56662.49\n",
      "Step: 18, Training Loss: 9.81448, LR: 0.0000120, Tokens/sec: 56819.03\n",
      "Step: 19, Training Loss: 9.57197, LR: 0.0000124, Tokens/sec: 55201.01\n",
      "Step: 20, Training Loss: 9.50208, LR: 0.0000128, Tokens/sec: 56469.28\n",
      "Step: 21, Training Loss: 9.38145, LR: 0.0000132, Tokens/sec: 56410.32\n",
      "Step: 22, Training Loss: 9.40542, LR: 0.0000136, Tokens/sec: 56083.35\n",
      "Step: 23, Training Loss: 9.43295, LR: 0.0000140, Tokens/sec: 56702.18\n",
      "Step: 24, Training Loss: 9.28906, LR: 0.0000144, Tokens/sec: 54907.41\n",
      "Step: 25, Training Loss: 9.25986, LR: 0.0000148, Tokens/sec: 56991.27\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 25, Eval Loss: 9.21867\n",
      "Step: 26, Training Loss: 9.17618, LR: 0.0000152, Tokens/sec: 56656.99\n",
      "Step: 27, Training Loss: 9.02580, LR: 0.0000156, Tokens/sec: 57018.06\n",
      "Step: 28, Training Loss: 9.12037, LR: 0.0000159, Tokens/sec: 55594.77\n",
      "Step: 29, Training Loss: 9.06298, LR: 0.0000163, Tokens/sec: 57137.76\n",
      "Step: 30, Training Loss: 9.04789, LR: 0.0000167, Tokens/sec: 55949.93\n",
      "Step: 31, Training Loss: 8.91593, LR: 0.0000171, Tokens/sec: 56575.87\n",
      "Step: 32, Training Loss: 8.88307, LR: 0.0000175, Tokens/sec: 56945.12\n",
      "Step: 33, Training Loss: 8.98514, LR: 0.0000179, Tokens/sec: 54290.82\n",
      "Step: 34, Training Loss: 8.92724, LR: 0.0000183, Tokens/sec: 56263.28\n",
      "Step: 35, Training Loss: 8.94364, LR: 0.0000187, Tokens/sec: 55426.28\n",
      "Step: 36, Training Loss: 8.82437, LR: 0.0000191, Tokens/sec: 55973.85\n",
      "Step: 37, Training Loss: 8.77758, LR: 0.0000195, Tokens/sec: 55523.78\n",
      "Step: 38, Training Loss: 8.73079, LR: 0.0000199, Tokens/sec: 55159.33\n",
      "Step: 39, Training Loss: 8.75054, LR: 0.0000202, Tokens/sec: 55977.08\n",
      "Step: 40, Training Loss: 8.69315, LR: 0.0000206, Tokens/sec: 56622.75\n",
      "Step: 41, Training Loss: 8.60717, LR: 0.0000210, Tokens/sec: 53735.17\n",
      "Step: 42, Training Loss: 8.66174, LR: 0.0000214, Tokens/sec: 56566.42\n",
      "Step: 43, Training Loss: 8.51254, LR: 0.0000218, Tokens/sec: 53116.41\n",
      "Step: 44, Training Loss: 8.55683, LR: 0.0000222, Tokens/sec: 57190.21\n",
      "Step: 45, Training Loss: 8.57153, LR: 0.0000226, Tokens/sec: 55104.03\n",
      "Step: 46, Training Loss: 8.55987, LR: 0.0000230, Tokens/sec: 57022.23\n",
      "Step: 47, Training Loss: 8.50390, LR: 0.0000234, Tokens/sec: 56102.80\n",
      "Step: 48, Training Loss: 8.47831, LR: 0.0000238, Tokens/sec: 54839.52\n",
      "Step: 49, Training Loss: 8.39916, LR: 0.0000242, Tokens/sec: 56201.70\n",
      "Step: 50, Training Loss: 8.42738, LR: 0.0000245, Tokens/sec: 57163.79\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 50, Eval Loss: 8.31575\n",
      "Step: 51, Training Loss: 8.41166, LR: 0.0000249, Tokens/sec: 57105.75\n",
      "Step: 52, Training Loss: 8.31557, LR: 0.0000253, Tokens/sec: 55886.28\n",
      "Step: 53, Training Loss: 8.39352, LR: 0.0000257, Tokens/sec: 56842.67\n",
      "Step: 54, Training Loss: 8.35714, LR: 0.0000261, Tokens/sec: 54987.71\n",
      "Step: 55, Training Loss: 8.30383, LR: 0.0000265, Tokens/sec: 56875.01\n",
      "Step: 56, Training Loss: 8.12977, LR: 0.0000269, Tokens/sec: 56951.80\n",
      "Step: 57, Training Loss: 8.25494, LR: 0.0000273, Tokens/sec: 56079.99\n",
      "Step: 58, Training Loss: 8.23732, LR: 0.0000277, Tokens/sec: 57129.70\n",
      "Step: 59, Training Loss: 8.20968, LR: 0.0000281, Tokens/sec: 53375.46\n",
      "Step: 60, Training Loss: 8.10253, LR: 0.0000285, Tokens/sec: 56585.82\n",
      "Step: 61, Training Loss: 8.03697, LR: 0.0000288, Tokens/sec: 55179.83\n",
      "Step: 62, Training Loss: 8.09649, LR: 0.0000292, Tokens/sec: 55885.25\n",
      "Step: 63, Training Loss: 8.08967, LR: 0.0000296, Tokens/sec: 55789.46\n",
      "Step: 64, Training Loss: 8.06857, LR: 0.0000300, Tokens/sec: 54812.44\n",
      "Step: 65, Training Loss: 8.01731, LR: 0.0000304, Tokens/sec: 55394.83\n",
      "Step: 66, Training Loss: 7.93636, LR: 0.0000308, Tokens/sec: 56554.28\n",
      "Step: 67, Training Loss: 7.89317, LR: 0.0000312, Tokens/sec: 54912.13\n",
      "Step: 68, Training Loss: 7.89070, LR: 0.0000316, Tokens/sec: 57176.85\n",
      "Step: 69, Training Loss: 7.88176, LR: 0.0000320, Tokens/sec: 54973.72\n",
      "Step: 70, Training Loss: 7.97893, LR: 0.0000324, Tokens/sec: 56975.93\n",
      "Step: 71, Training Loss: 7.90647, LR: 0.0000328, Tokens/sec: 56146.95\n",
      "Step: 72, Training Loss: 7.70797, LR: 0.0000331, Tokens/sec: 56028.47\n",
      "Step: 73, Training Loss: 7.85692, LR: 0.0000335, Tokens/sec: 57445.03\n",
      "Step: 74, Training Loss: 7.82119, LR: 0.0000339, Tokens/sec: 55011.52\n",
      "Step: 75, Training Loss: 7.82197, LR: 0.0000343, Tokens/sec: 56640.39\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 75, Eval Loss: 7.72989\n",
      "Step: 76, Training Loss: 7.59853, LR: 0.0000347, Tokens/sec: 55476.39\n",
      "Step: 77, Training Loss: 7.62159, LR: 0.0000351, Tokens/sec: 56562.45\n",
      "Step: 78, Training Loss: 7.65890, LR: 0.0000355, Tokens/sec: 55605.10\n",
      "Step: 79, Training Loss: 7.60127, LR: 0.0000359, Tokens/sec: 52312.32\n",
      "Step: 80, Training Loss: 7.60907, LR: 0.0000363, Tokens/sec: 43519.71\n",
      "Step: 81, Training Loss: 7.62828, LR: 0.0000367, Tokens/sec: 53559.84\n",
      "Step: 82, Training Loss: 7.60454, LR: 0.0000371, Tokens/sec: 52801.67\n",
      "Step: 83, Training Loss: 7.55841, LR: 0.0000374, Tokens/sec: 56268.19\n",
      "Step: 84, Training Loss: 7.58584, LR: 0.0000378, Tokens/sec: 51635.75\n",
      "Step: 85, Training Loss: 7.54420, LR: 0.0000382, Tokens/sec: 55691.55\n",
      "Step: 86, Training Loss: 7.52672, LR: 0.0000386, Tokens/sec: 54584.78\n",
      "Step: 87, Training Loss: 7.47321, LR: 0.0000390, Tokens/sec: 50942.79\n",
      "Step: 88, Training Loss: 7.43947, LR: 0.0000394, Tokens/sec: 55664.57\n",
      "Step: 89, Training Loss: 7.41417, LR: 0.0000398, Tokens/sec: 56451.40\n",
      "Step: 90, Training Loss: 7.43426, LR: 0.0000402, Tokens/sec: 56098.64\n",
      "Step: 91, Training Loss: 7.37081, LR: 0.0000406, Tokens/sec: 56135.73\n",
      "Step: 92, Training Loss: 7.17584, LR: 0.0000410, Tokens/sec: 38861.27\n",
      "Step: 93, Training Loss: 7.28322, LR: 0.0000414, Tokens/sec: 42041.92\n",
      "Step: 94, Training Loss: 7.40277, LR: 0.0000417, Tokens/sec: 54520.17\n",
      "Step: 95, Training Loss: 7.22372, LR: 0.0000421, Tokens/sec: 55120.48\n",
      "Step: 96, Training Loss: 7.26149, LR: 0.0000425, Tokens/sec: 54511.42\n",
      "Step: 97, Training Loss: 7.18556, LR: 0.0000429, Tokens/sec: 56562.48\n",
      "Step: 98, Training Loss: 7.21549, LR: 0.0000433, Tokens/sec: 53309.02\n",
      "Step: 99, Training Loss: 7.14740, LR: 0.0000437, Tokens/sec: 52193.87\n",
      "Step: 100, Training Loss: 7.20549, LR: 0.0000441, Tokens/sec: 53189.27\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 100, Eval Loss: 7.15985\n",
      "Step: 101, Training Loss: 7.06138, LR: 0.0000445, Tokens/sec: 56383.39\n",
      "Step: 102, Training Loss: 7.12476, LR: 0.0000449, Tokens/sec: 41948.71\n",
      "Step: 103, Training Loss: 7.17205, LR: 0.0000453, Tokens/sec: 56336.39\n",
      "Step: 104, Training Loss: 6.95992, LR: 0.0000457, Tokens/sec: 54217.48\n",
      "Step: 105, Training Loss: 6.90699, LR: 0.0000460, Tokens/sec: 50362.83\n",
      "Step: 106, Training Loss: 6.74565, LR: 0.0000464, Tokens/sec: 54987.11\n",
      "Step: 107, Training Loss: 6.89258, LR: 0.0000468, Tokens/sec: 54773.96\n",
      "Step: 108, Training Loss: 6.88240, LR: 0.0000472, Tokens/sec: 55451.66\n",
      "Step: 109, Training Loss: 6.89798, LR: 0.0000476, Tokens/sec: 53087.59\n",
      "Step: 110, Training Loss: 6.82298, LR: 0.0000480, Tokens/sec: 41728.13\n",
      "Step: 111, Training Loss: 6.97747, LR: 0.0000484, Tokens/sec: 57079.42\n",
      "Step: 112, Training Loss: 6.85555, LR: 0.0000488, Tokens/sec: 53199.02\n",
      "Step: 113, Training Loss: 6.86814, LR: 0.0000492, Tokens/sec: 56692.35\n",
      "Step: 114, Training Loss: 6.72854, LR: 0.0000496, Tokens/sec: 53717.58\n",
      "Step: 115, Training Loss: 6.75779, LR: 0.0000500, Tokens/sec: 56821.32\n",
      "Step: 116, Training Loss: 6.68925, LR: 0.0000503, Tokens/sec: 54451.89\n",
      "Step: 117, Training Loss: 6.59098, LR: 0.0000507, Tokens/sec: 55175.82\n",
      "Step: 118, Training Loss: 6.61566, LR: 0.0000511, Tokens/sec: 56214.72\n",
      "Step: 119, Training Loss: 6.75952, LR: 0.0000515, Tokens/sec: 53457.18\n",
      "Step: 120, Training Loss: 6.51763, LR: 0.0000519, Tokens/sec: 55747.53\n",
      "Step: 121, Training Loss: 6.56241, LR: 0.0000523, Tokens/sec: 55350.52\n",
      "Step: 122, Training Loss: 6.60386, LR: 0.0000527, Tokens/sec: 56676.50\n",
      "Step: 123, Training Loss: 6.49702, LR: 0.0000531, Tokens/sec: 55835.48\n",
      "Step: 124, Training Loss: 6.42840, LR: 0.0000535, Tokens/sec: 54423.21\n",
      "Step: 125, Training Loss: 6.49984, LR: 0.0000539, Tokens/sec: 55893.86\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 125, Eval Loss: 6.52783\n",
      "Step: 126, Training Loss: 6.48850, LR: 0.0000543, Tokens/sec: 56112.41\n",
      "Step: 127, Training Loss: 6.41131, LR: 0.0000547, Tokens/sec: 56224.76\n",
      "Step: 128, Training Loss: 6.26975, LR: 0.0000550, Tokens/sec: 56646.80\n",
      "Step: 129, Training Loss: 6.30109, LR: 0.0000554, Tokens/sec: 55987.40\n",
      "Step: 130, Training Loss: 6.36141, LR: 0.0000558, Tokens/sec: 54227.45\n",
      "Step: 131, Training Loss: 6.41774, LR: 0.0000562, Tokens/sec: 56948.23\n",
      "Step: 132, Training Loss: 6.34840, LR: 0.0000566, Tokens/sec: 55185.87\n",
      "Step: 133, Training Loss: 6.19571, LR: 0.0000570, Tokens/sec: 55669.68\n",
      "Step: 134, Training Loss: 6.16856, LR: 0.0000574, Tokens/sec: 54969.89\n",
      "Step: 135, Training Loss: 6.25005, LR: 0.0000578, Tokens/sec: 53836.61\n",
      "Step: 136, Training Loss: 6.14527, LR: 0.0000582, Tokens/sec: 54713.44\n",
      "Step: 137, Training Loss: 6.09486, LR: 0.0000586, Tokens/sec: 55979.78\n",
      "Step: 138, Training Loss: 6.09202, LR: 0.0000590, Tokens/sec: 54701.26\n",
      "Step: 139, Training Loss: 6.19827, LR: 0.0000593, Tokens/sec: 55997.00\n",
      "Step: 140, Training Loss: 6.01462, LR: 0.0000597, Tokens/sec: 54040.72\n",
      "Step: 141, Training Loss: 6.20963, LR: 0.0000601, Tokens/sec: 56407.09\n",
      "Step: 142, Training Loss: 5.86275, LR: 0.0000605, Tokens/sec: 55090.23\n",
      "Step: 143, Training Loss: 5.92303, LR: 0.0000609, Tokens/sec: 54670.65\n",
      "Step: 144, Training Loss: 6.02722, LR: 0.0000613, Tokens/sec: 55545.06\n",
      "Step: 145, Training Loss: 6.05664, LR: 0.0000617, Tokens/sec: 54314.02\n",
      "Step: 146, Training Loss: 5.89337, LR: 0.0000621, Tokens/sec: 56210.27\n",
      "Step: 147, Training Loss: 5.83835, LR: 0.0000625, Tokens/sec: 56371.31\n",
      "Step: 148, Training Loss: 6.02689, LR: 0.0000629, Tokens/sec: 54028.52\n",
      "Step: 149, Training Loss: 6.01940, LR: 0.0000633, Tokens/sec: 55752.01\n",
      "Step: 150, Training Loss: 5.89630, LR: 0.0000636, Tokens/sec: 54173.26\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 150, Eval Loss: 5.92073\n",
      "Step: 151, Training Loss: 5.78455, LR: 0.0000640, Tokens/sec: 56006.19\n",
      "Step: 152, Training Loss: 5.45217, LR: 0.0000644, Tokens/sec: 54860.20\n",
      "Step: 153, Training Loss: 5.65873, LR: 0.0000648, Tokens/sec: 55752.00\n",
      "Step: 154, Training Loss: 5.82335, LR: 0.0000652, Tokens/sec: 55074.29\n",
      "Step: 155, Training Loss: 5.80035, LR: 0.0000656, Tokens/sec: 56322.16\n",
      "Step: 156, Training Loss: 5.73488, LR: 0.0000660, Tokens/sec: 53791.91\n",
      "Step: 157, Training Loss: 5.88069, LR: 0.0000664, Tokens/sec: 52784.67\n",
      "Step: 158, Training Loss: 5.71789, LR: 0.0000668, Tokens/sec: 55875.92\n",
      "Step: 159, Training Loss: 5.65564, LR: 0.0000672, Tokens/sec: 54735.97\n",
      "Step: 160, Training Loss: 5.51636, LR: 0.0000676, Tokens/sec: 56351.66\n",
      "Step: 161, Training Loss: 5.49728, LR: 0.0000679, Tokens/sec: 55752.59\n",
      "Step: 162, Training Loss: 5.72648, LR: 0.0000683, Tokens/sec: 54626.40\n",
      "Step: 163, Training Loss: 5.54391, LR: 0.0000687, Tokens/sec: 55311.85\n",
      "Step: 164, Training Loss: 5.64624, LR: 0.0000691, Tokens/sec: 53319.04\n",
      "Step: 165, Training Loss: 5.65894, LR: 0.0000695, Tokens/sec: 55972.74\n",
      "Step: 166, Training Loss: 5.54090, LR: 0.0000699, Tokens/sec: 53953.58\n",
      "Step: 167, Training Loss: 5.45411, LR: 0.0000703, Tokens/sec: 55049.23\n",
      "Step: 168, Training Loss: 5.61827, LR: 0.0000707, Tokens/sec: 55400.33\n",
      "Step: 169, Training Loss: 5.35397, LR: 0.0000711, Tokens/sec: 55119.71\n",
      "Step: 170, Training Loss: 5.66470, LR: 0.0000715, Tokens/sec: 55363.01\n",
      "Step: 171, Training Loss: 5.59042, LR: 0.0000719, Tokens/sec: 57129.63\n",
      "Step: 172, Training Loss: 5.70861, LR: 0.0000722, Tokens/sec: 54782.00\n",
      "Step: 173, Training Loss: 5.63505, LR: 0.0000726, Tokens/sec: 56245.06\n",
      "Step: 174, Training Loss: 5.35613, LR: 0.0000730, Tokens/sec: 54502.24\n",
      "Step: 175, Training Loss: 5.29875, LR: 0.0000734, Tokens/sec: 55070.78\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 175, Eval Loss: 5.51917\n",
      "Step: 176, Training Loss: 5.48359, LR: 0.0000738, Tokens/sec: 55796.07\n",
      "Step: 177, Training Loss: 5.29218, LR: 0.0000742, Tokens/sec: 56686.84\n",
      "Step: 178, Training Loss: 5.31239, LR: 0.0000746, Tokens/sec: 53083.32\n",
      "Step: 179, Training Loss: 5.39666, LR: 0.0000750, Tokens/sec: 55743.31\n",
      "Step: 180, Training Loss: 5.27315, LR: 0.0000754, Tokens/sec: 53987.45\n",
      "Step: 181, Training Loss: 5.23292, LR: 0.0000758, Tokens/sec: 56220.51\n",
      "Step: 182, Training Loss: 5.28526, LR: 0.0000762, Tokens/sec: 56373.62\n",
      "Step: 183, Training Loss: 5.62668, LR: 0.0000765, Tokens/sec: 54540.55\n",
      "Step: 184, Training Loss: 5.38133, LR: 0.0000769, Tokens/sec: 55594.94\n",
      "Step: 185, Training Loss: 5.29539, LR: 0.0000773, Tokens/sec: 53981.51\n",
      "Step: 186, Training Loss: 5.21198, LR: 0.0000777, Tokens/sec: 56755.07\n",
      "Step: 187, Training Loss: 5.16989, LR: 0.0000781, Tokens/sec: 56973.23\n",
      "Step: 188, Training Loss: 5.12696, LR: 0.0000785, Tokens/sec: 53945.07\n",
      "Step: 189, Training Loss: 5.37557, LR: 0.0000789, Tokens/sec: 55202.10\n",
      "Step: 190, Training Loss: 5.20493, LR: 0.0000793, Tokens/sec: 55802.77\n",
      "Step: 191, Training Loss: 5.20174, LR: 0.0000797, Tokens/sec: 55269.53\n",
      "Step: 192, Training Loss: 5.30798, LR: 0.0000801, Tokens/sec: 56195.40\n",
      "Step: 193, Training Loss: 5.32985, LR: 0.0000805, Tokens/sec: 54561.64\n",
      "Step: 194, Training Loss: 5.40312, LR: 0.0000808, Tokens/sec: 54891.82\n",
      "Step: 195, Training Loss: 5.24591, LR: 0.0000812, Tokens/sec: 57284.53\n",
      "Step: 196, Training Loss: 4.93585, LR: 0.0000816, Tokens/sec: 53891.23\n",
      "Step: 197, Training Loss: 5.20820, LR: 0.0000820, Tokens/sec: 56617.67\n",
      "Step: 198, Training Loss: 5.01445, LR: 0.0000824, Tokens/sec: 53144.79\n",
      "Step: 199, Training Loss: 5.15892, LR: 0.0000828, Tokens/sec: 56188.30\n",
      "Step: 200, Training Loss: 5.15121, LR: 0.0000832, Tokens/sec: 54634.83\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 200, Eval Loss: 5.13227\n",
      "Step: 201, Training Loss: 5.23990, LR: 0.0000836, Tokens/sec: 55321.96\n",
      "Step: 202, Training Loss: 4.92258, LR: 0.0000840, Tokens/sec: 55310.65\n",
      "Step: 203, Training Loss: 4.92684, LR: 0.0000844, Tokens/sec: 55076.61\n",
      "Step: 204, Training Loss: 5.04367, LR: 0.0000848, Tokens/sec: 54469.41\n",
      "Step: 205, Training Loss: 4.96209, LR: 0.0000851, Tokens/sec: 55135.72\n",
      "Step: 206, Training Loss: 5.06466, LR: 0.0000855, Tokens/sec: 55652.20\n",
      "Step: 207, Training Loss: 4.92285, LR: 0.0000859, Tokens/sec: 56131.78\n",
      "Step: 208, Training Loss: 5.01546, LR: 0.0000863, Tokens/sec: 55890.57\n",
      "Step: 209, Training Loss: 5.61505, LR: 0.0000867, Tokens/sec: 54540.48\n",
      "Step: 210, Training Loss: 5.13051, LR: 0.0000871, Tokens/sec: 56041.44\n",
      "Step: 211, Training Loss: 4.73158, LR: 0.0000875, Tokens/sec: 55981.59\n",
      "Step: 212, Training Loss: 5.00195, LR: 0.0000879, Tokens/sec: 56414.70\n",
      "Step: 213, Training Loss: 4.82625, LR: 0.0000883, Tokens/sec: 55926.42\n",
      "Step: 214, Training Loss: 4.91444, LR: 0.0000887, Tokens/sec: 54082.63\n",
      "Step: 215, Training Loss: 4.89300, LR: 0.0000891, Tokens/sec: 56251.93\n",
      "Step: 216, Training Loss: 4.94547, LR: 0.0000894, Tokens/sec: 52875.41\n",
      "Step: 217, Training Loss: 4.66906, LR: 0.0000898, Tokens/sec: 51881.87\n",
      "Step: 218, Training Loss: 4.89636, LR: 0.0000902, Tokens/sec: 53679.24\n",
      "Step: 219, Training Loss: 5.13575, LR: 0.0000906, Tokens/sec: 54063.78\n",
      "Step: 220, Training Loss: 4.77057, LR: 0.0000910, Tokens/sec: 55035.59\n",
      "Step: 221, Training Loss: 4.98855, LR: 0.0000914, Tokens/sec: 55921.51\n",
      "Step: 222, Training Loss: 4.98786, LR: 0.0000918, Tokens/sec: 54236.41\n",
      "Step: 223, Training Loss: 4.90420, LR: 0.0000922, Tokens/sec: 56593.39\n",
      "Step: 224, Training Loss: 4.96681, LR: 0.0000926, Tokens/sec: 54117.71\n",
      "Step: 225, Training Loss: 4.86177, LR: 0.0000930, Tokens/sec: 55288.29\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 225, Eval Loss: 4.72202\n",
      "Step: 226, Training Loss: 4.86051, LR: 0.0000934, Tokens/sec: 55836.79\n",
      "Step: 227, Training Loss: 4.77144, LR: 0.0000937, Tokens/sec: 54256.18\n",
      "Step: 228, Training Loss: 4.89606, LR: 0.0000941, Tokens/sec: 54775.55\n",
      "Step: 229, Training Loss: 4.69127, LR: 0.0000945, Tokens/sec: 55490.95\n",
      "Step: 230, Training Loss: 4.86406, LR: 0.0000949, Tokens/sec: 55193.86\n",
      "Step: 231, Training Loss: 4.69954, LR: 0.0000953, Tokens/sec: 53615.83\n",
      "Step: 232, Training Loss: 4.58448, LR: 0.0000957, Tokens/sec: 54998.54\n",
      "Step: 233, Training Loss: 4.67438, LR: 0.0000961, Tokens/sec: 54602.12\n",
      "Step: 234, Training Loss: 4.85664, LR: 0.0000965, Tokens/sec: 54850.60\n",
      "Step: 235, Training Loss: 4.82210, LR: 0.0000969, Tokens/sec: 55409.73\n",
      "Step: 236, Training Loss: 4.45954, LR: 0.0000973, Tokens/sec: 54138.64\n",
      "Step: 237, Training Loss: 4.68977, LR: 0.0000977, Tokens/sec: 55589.79\n",
      "Step: 238, Training Loss: 4.68550, LR: 0.0000980, Tokens/sec: 52747.14\n",
      "Step: 239, Training Loss: 4.43551, LR: 0.0000984, Tokens/sec: 55379.41\n",
      "Step: 240, Training Loss: 4.78900, LR: 0.0000988, Tokens/sec: 55822.00\n",
      "Step: 241, Training Loss: 4.62820, LR: 0.0000992, Tokens/sec: 55700.34\n",
      "Step: 242, Training Loss: 4.62848, LR: 0.0000996, Tokens/sec: 55058.62\n",
      "Step: 243, Training Loss: 4.63160, LR: 0.0001000, Tokens/sec: 53710.85\n",
      "Step: 244, Training Loss: 4.88272, LR: 0.0001000, Tokens/sec: 56558.18\n",
      "Step: 245, Training Loss: 4.49082, LR: 0.0001000, Tokens/sec: 56036.25\n",
      "Step: 246, Training Loss: 4.70519, LR: 0.0001000, Tokens/sec: 53796.29\n",
      "Step: 247, Training Loss: 4.68866, LR: 0.0001000, Tokens/sec: 54677.70\n",
      "Step: 248, Training Loss: 4.54332, LR: 0.0001000, Tokens/sec: 52695.30\n",
      "Step: 249, Training Loss: 4.54570, LR: 0.0001000, Tokens/sec: 55023.28\n",
      "Step: 250, Training Loss: 4.42312, LR: 0.0001000, Tokens/sec: 54805.65\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 250, Eval Loss: 4.55422\n",
      "Step: 251, Training Loss: 4.52721, LR: 0.0001000, Tokens/sec: 54279.19\n",
      "Step: 252, Training Loss: 4.79367, LR: 0.0001000, Tokens/sec: 54913.26\n",
      "Step: 253, Training Loss: 4.71270, LR: 0.0001000, Tokens/sec: 53703.55\n",
      "Step: 254, Training Loss: 4.60556, LR: 0.0001000, Tokens/sec: 53647.38\n",
      "Step: 255, Training Loss: 4.50909, LR: 0.0001000, Tokens/sec: 53582.35\n",
      "Step: 256, Training Loss: 4.37083, LR: 0.0001000, Tokens/sec: 54398.62\n",
      "Step: 257, Training Loss: 4.30401, LR: 0.0001000, Tokens/sec: 54379.63\n",
      "Step: 258, Training Loss: 4.39048, LR: 0.0001000, Tokens/sec: 54500.58\n",
      "Step: 259, Training Loss: 4.34137, LR: 0.0001000, Tokens/sec: 52075.91\n",
      "Step: 260, Training Loss: 4.62483, LR: 0.0001000, Tokens/sec: 55645.07\n",
      "Step: 261, Training Loss: 4.15410, LR: 0.0001000, Tokens/sec: 54006.01\n",
      "Step: 262, Training Loss: 4.42402, LR: 0.0001000, Tokens/sec: 55602.03\n",
      "Step: 263, Training Loss: 4.40962, LR: 0.0001000, Tokens/sec: 53728.15\n",
      "Step: 264, Training Loss: 4.55776, LR: 0.0001000, Tokens/sec: 53902.42\n",
      "Step: 265, Training Loss: 4.55818, LR: 0.0001000, Tokens/sec: 53830.12\n",
      "Step: 266, Training Loss: 4.13800, LR: 0.0001000, Tokens/sec: 55412.57\n",
      "Step: 267, Training Loss: 4.29472, LR: 0.0001000, Tokens/sec: 53378.23\n",
      "Step: 268, Training Loss: 4.17896, LR: 0.0001000, Tokens/sec: 55265.74\n",
      "Step: 269, Training Loss: 4.17981, LR: 0.0001000, Tokens/sec: 54155.43\n",
      "Step: 270, Training Loss: 4.49176, LR: 0.0001000, Tokens/sec: 55463.57\n",
      "Step: 271, Training Loss: 4.36118, LR: 0.0001000, Tokens/sec: 56142.90\n",
      "Step: 272, Training Loss: 4.45485, LR: 0.0001000, Tokens/sec: 53680.77\n",
      "Step: 273, Training Loss: 4.49321, LR: 0.0001000, Tokens/sec: 55665.69\n",
      "Step: 274, Training Loss: 4.49372, LR: 0.0001000, Tokens/sec: 53753.66\n",
      "Step: 275, Training Loss: 4.19609, LR: 0.0001000, Tokens/sec: 56121.22\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 275, Eval Loss: 4.14013\n",
      "Step: 276, Training Loss: 4.38217, LR: 0.0000999, Tokens/sec: 54556.38\n",
      "Step: 277, Training Loss: 4.48540, LR: 0.0000999, Tokens/sec: 55604.05\n",
      "Step: 278, Training Loss: 4.41173, LR: 0.0000999, Tokens/sec: 53812.98\n",
      "Step: 279, Training Loss: 4.17703, LR: 0.0000999, Tokens/sec: 55222.76\n",
      "Step: 280, Training Loss: 4.10179, LR: 0.0000999, Tokens/sec: 52769.01\n",
      "Step: 281, Training Loss: 4.36279, LR: 0.0000999, Tokens/sec: 55851.53\n",
      "Step: 282, Training Loss: 4.24767, LR: 0.0000999, Tokens/sec: 54658.71\n",
      "Step: 283, Training Loss: 4.24401, LR: 0.0000999, Tokens/sec: 54745.52\n",
      "Step: 284, Training Loss: 4.37122, LR: 0.0000999, Tokens/sec: 55511.39\n",
      "Step: 285, Training Loss: 4.19722, LR: 0.0000999, Tokens/sec: 55025.29\n",
      "Step: 286, Training Loss: 4.32402, LR: 0.0000999, Tokens/sec: 56476.25\n",
      "Step: 287, Training Loss: 4.07153, LR: 0.0000999, Tokens/sec: 55755.69\n",
      "Step: 288, Training Loss: 4.11963, LR: 0.0000999, Tokens/sec: 55094.68\n",
      "Step: 289, Training Loss: 4.20300, LR: 0.0000999, Tokens/sec: 54198.38\n",
      "Step: 290, Training Loss: 4.56435, LR: 0.0000999, Tokens/sec: 55155.06\n",
      "Step: 291, Training Loss: 4.20007, LR: 0.0000999, Tokens/sec: 54626.39\n",
      "Step: 292, Training Loss: 4.36356, LR: 0.0000999, Tokens/sec: 55415.36\n",
      "Step: 293, Training Loss: 4.38986, LR: 0.0000999, Tokens/sec: 54021.00\n",
      "Step: 294, Training Loss: 4.18870, LR: 0.0000999, Tokens/sec: 54644.78\n",
      "Step: 295, Training Loss: 4.42336, LR: 0.0000999, Tokens/sec: 52334.25\n",
      "Step: 296, Training Loss: 4.20354, LR: 0.0000999, Tokens/sec: 55309.66\n",
      "Step: 297, Training Loss: 4.28084, LR: 0.0000999, Tokens/sec: 54379.36\n",
      "Step: 298, Training Loss: 4.14256, LR: 0.0000999, Tokens/sec: 55197.40\n",
      "Step: 299, Training Loss: 4.21597, LR: 0.0000999, Tokens/sec: 55639.22\n",
      "Step: 300, Training Loss: 4.19310, LR: 0.0000998, Tokens/sec: 53046.66\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 300, Eval Loss: 3.99808\n",
      "Step: 301, Training Loss: 3.97407, LR: 0.0000998, Tokens/sec: 53097.12\n",
      "Step: 302, Training Loss: 4.08325, LR: 0.0000998, Tokens/sec: 54697.58\n",
      "Step: 303, Training Loss: 4.28067, LR: 0.0000998, Tokens/sec: 55549.49\n",
      "Step: 304, Training Loss: 4.23175, LR: 0.0000998, Tokens/sec: 45137.10\n",
      "Step: 305, Training Loss: 3.90851, LR: 0.0000998, Tokens/sec: 43538.36\n",
      "Step: 306, Training Loss: 3.92075, LR: 0.0000998, Tokens/sec: 53979.15\n",
      "Step: 307, Training Loss: 4.03093, LR: 0.0000998, Tokens/sec: 46935.07\n",
      "Step: 308, Training Loss: 4.11885, LR: 0.0000998, Tokens/sec: 44473.33\n",
      "Step: 309, Training Loss: 4.12080, LR: 0.0000998, Tokens/sec: 37054.00\n",
      "Step: 310, Training Loss: 3.59996, LR: 0.0000998, Tokens/sec: 54376.35\n",
      "Step: 311, Training Loss: 3.69938, LR: 0.0000998, Tokens/sec: 53531.53\n",
      "Step: 312, Training Loss: 3.85167, LR: 0.0000998, Tokens/sec: 53150.22\n",
      "Step: 313, Training Loss: 3.93900, LR: 0.0000998, Tokens/sec: 54394.93\n",
      "Step: 314, Training Loss: 3.47104, LR: 0.0000998, Tokens/sec: 54374.71\n",
      "Step: 315, Training Loss: 4.12540, LR: 0.0000998, Tokens/sec: 44193.14\n",
      "Step: 316, Training Loss: 3.79306, LR: 0.0000998, Tokens/sec: 52460.63\n",
      "Step: 317, Training Loss: 4.00179, LR: 0.0000997, Tokens/sec: 50615.81\n",
      "Step: 318, Training Loss: 4.21779, LR: 0.0000997, Tokens/sec: 53214.94\n",
      "Step: 319, Training Loss: 4.08631, LR: 0.0000997, Tokens/sec: 42062.83\n",
      "Step: 320, Training Loss: 3.76589, LR: 0.0000997, Tokens/sec: 47360.09\n",
      "Step: 321, Training Loss: 3.79158, LR: 0.0000997, Tokens/sec: 55469.79\n",
      "Step: 322, Training Loss: 3.91909, LR: 0.0000997, Tokens/sec: 54373.72\n",
      "Step: 323, Training Loss: 3.97141, LR: 0.0000997, Tokens/sec: 47493.32\n",
      "Step: 324, Training Loss: 4.21371, LR: 0.0000997, Tokens/sec: 45729.08\n",
      "Step: 325, Training Loss: 4.07576, LR: 0.0000997, Tokens/sec: 45467.19\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 325, Eval Loss: 3.90534\n",
      "Step: 326, Training Loss: 3.88870, LR: 0.0000997, Tokens/sec: 40885.37\n",
      "Step: 327, Training Loss: 3.43871, LR: 0.0000997, Tokens/sec: 53047.39\n",
      "Step: 328, Training Loss: 3.53666, LR: 0.0000997, Tokens/sec: 51846.70\n",
      "Step: 329, Training Loss: 3.77781, LR: 0.0000997, Tokens/sec: 52442.30\n",
      "Step: 330, Training Loss: 3.47584, LR: 0.0000996, Tokens/sec: 49346.03\n",
      "Step: 331, Training Loss: 3.65669, LR: 0.0000996, Tokens/sec: 50162.53\n",
      "Step: 332, Training Loss: 3.52989, LR: 0.0000996, Tokens/sec: 50829.69\n",
      "Step: 333, Training Loss: 3.45480, LR: 0.0000996, Tokens/sec: 52872.79\n",
      "Step: 334, Training Loss: 3.56372, LR: 0.0000996, Tokens/sec: 54677.91\n",
      "Step: 335, Training Loss: 3.81431, LR: 0.0000996, Tokens/sec: 49815.30\n",
      "Step: 336, Training Loss: 3.93499, LR: 0.0000996, Tokens/sec: 45927.01\n",
      "Step: 337, Training Loss: 3.83236, LR: 0.0000996, Tokens/sec: 49437.07\n",
      "Step: 338, Training Loss: 3.68327, LR: 0.0000996, Tokens/sec: 46688.33\n",
      "Step: 339, Training Loss: 3.60901, LR: 0.0000996, Tokens/sec: 53775.17\n",
      "Step: 340, Training Loss: 3.95344, LR: 0.0000996, Tokens/sec: 55461.65\n",
      "Step: 341, Training Loss: 3.77541, LR: 0.0000996, Tokens/sec: 53939.33\n",
      "Step: 342, Training Loss: 3.68855, LR: 0.0000995, Tokens/sec: 55058.69\n",
      "Step: 343, Training Loss: 3.91617, LR: 0.0000995, Tokens/sec: 55064.14\n",
      "Step: 344, Training Loss: 3.63426, LR: 0.0000995, Tokens/sec: 55317.48\n",
      "Step: 345, Training Loss: 4.01111, LR: 0.0000995, Tokens/sec: 53648.27\n",
      "Step: 346, Training Loss: 3.59021, LR: 0.0000995, Tokens/sec: 52489.67\n",
      "Step: 347, Training Loss: 3.30035, LR: 0.0000995, Tokens/sec: 54598.68\n",
      "Step: 348, Training Loss: 3.43854, LR: 0.0000995, Tokens/sec: 55068.25\n",
      "Step: 349, Training Loss: 3.62608, LR: 0.0000995, Tokens/sec: 54627.63\n",
      "Step: 350, Training Loss: 3.58032, LR: 0.0000995, Tokens/sec: 53173.37\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 350, Eval Loss: 3.65410\n",
      "Step: 351, Training Loss: 3.79538, LR: 0.0000995, Tokens/sec: 55138.05\n",
      "Step: 352, Training Loss: 3.70365, LR: 0.0000995, Tokens/sec: 52822.25\n",
      "Step: 353, Training Loss: 3.69005, LR: 0.0000994, Tokens/sec: 54150.85\n",
      "Step: 354, Training Loss: 3.73685, LR: 0.0000994, Tokens/sec: 54978.79\n",
      "Step: 355, Training Loss: 3.41398, LR: 0.0000994, Tokens/sec: 55405.87\n",
      "Step: 356, Training Loss: 3.38398, LR: 0.0000994, Tokens/sec: 53195.65\n",
      "Step: 357, Training Loss: 3.45229, LR: 0.0000994, Tokens/sec: 53453.76\n",
      "Step: 358, Training Loss: 3.45560, LR: 0.0000994, Tokens/sec: 55205.66\n",
      "Step: 359, Training Loss: 3.43730, LR: 0.0000994, Tokens/sec: 54953.64\n",
      "Step: 360, Training Loss: 3.24680, LR: 0.0000994, Tokens/sec: 54803.09\n",
      "Step: 361, Training Loss: 3.36172, LR: 0.0000994, Tokens/sec: 54395.25\n",
      "Step: 362, Training Loss: 3.39947, LR: 0.0000993, Tokens/sec: 54358.87\n",
      "Step: 363, Training Loss: 3.55391, LR: 0.0000993, Tokens/sec: 55686.01\n",
      "Step: 364, Training Loss: 3.82328, LR: 0.0000993, Tokens/sec: 54677.08\n",
      "Step: 365, Training Loss: 3.58612, LR: 0.0000993, Tokens/sec: 54867.07\n",
      "Step: 366, Training Loss: 3.31541, LR: 0.0000993, Tokens/sec: 54301.07\n",
      "Step: 367, Training Loss: 3.57518, LR: 0.0000993, Tokens/sec: 53029.59\n",
      "Step: 368, Training Loss: 3.37290, LR: 0.0000993, Tokens/sec: 54510.90\n",
      "Step: 369, Training Loss: 3.56741, LR: 0.0000993, Tokens/sec: 53514.65\n",
      "Step: 370, Training Loss: 3.39973, LR: 0.0000993, Tokens/sec: 49154.62\n",
      "Step: 371, Training Loss: 3.45410, LR: 0.0000992, Tokens/sec: 45445.93\n",
      "Step: 372, Training Loss: 3.48374, LR: 0.0000992, Tokens/sec: 43534.83\n",
      "Step: 373, Training Loss: 3.57297, LR: 0.0000992, Tokens/sec: 43149.44\n",
      "Step: 374, Training Loss: 3.58225, LR: 0.0000992, Tokens/sec: 45605.18\n",
      "Step: 375, Training Loss: 3.18448, LR: 0.0000992, Tokens/sec: 44537.83\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 375, Eval Loss: 3.47163\n",
      "Step: 376, Training Loss: 3.09966, LR: 0.0000992, Tokens/sec: 49841.52\n",
      "Step: 377, Training Loss: 3.22356, LR: 0.0000992, Tokens/sec: 42507.59\n",
      "Step: 378, Training Loss: 3.29626, LR: 0.0000992, Tokens/sec: 43051.34\n",
      "Step: 379, Training Loss: 3.35178, LR: 0.0000991, Tokens/sec: 45522.91\n",
      "Step: 380, Training Loss: 3.18813, LR: 0.0000991, Tokens/sec: 49038.73\n",
      "Step: 381, Training Loss: 3.28658, LR: 0.0000991, Tokens/sec: 49658.35\n",
      "Step: 382, Training Loss: 3.05916, LR: 0.0000991, Tokens/sec: 46927.84\n",
      "Step: 383, Training Loss: 3.10555, LR: 0.0000991, Tokens/sec: 47132.94\n",
      "Step: 384, Training Loss: 2.88499, LR: 0.0000991, Tokens/sec: 50757.45\n",
      "Step: 385, Training Loss: 3.16893, LR: 0.0000991, Tokens/sec: 44189.00\n",
      "Step: 386, Training Loss: 3.24942, LR: 0.0000991, Tokens/sec: 41828.38\n",
      "Step: 387, Training Loss: 2.70060, LR: 0.0000990, Tokens/sec: 50497.45\n",
      "Step: 388, Training Loss: 2.95246, LR: 0.0000990, Tokens/sec: 51657.50\n",
      "Step: 389, Training Loss: 3.11696, LR: 0.0000990, Tokens/sec: 43460.19\n",
      "Step: 390, Training Loss: 3.01027, LR: 0.0000990, Tokens/sec: 51480.38\n",
      "Step: 391, Training Loss: 3.22917, LR: 0.0000990, Tokens/sec: 51954.12\n",
      "Step: 392, Training Loss: 3.30196, LR: 0.0000990, Tokens/sec: 50155.37\n",
      "Step: 393, Training Loss: 3.07808, LR: 0.0000990, Tokens/sec: 51853.56\n",
      "Step: 394, Training Loss: 2.95763, LR: 0.0000989, Tokens/sec: 51259.64\n",
      "Step: 395, Training Loss: 3.22853, LR: 0.0000989, Tokens/sec: 51831.85\n",
      "Step: 396, Training Loss: 3.11315, LR: 0.0000989, Tokens/sec: 48983.05\n",
      "Step: 397, Training Loss: 3.22029, LR: 0.0000989, Tokens/sec: 52602.12\n",
      "Step: 398, Training Loss: 3.13137, LR: 0.0000989, Tokens/sec: 50076.83\n",
      "Step: 399, Training Loss: 3.32570, LR: 0.0000989, Tokens/sec: 52908.59\n",
      "Step: 400, Training Loss: 2.93103, LR: 0.0000989, Tokens/sec: 44399.03\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 400, Eval Loss: 3.29266\n",
      "Step: 401, Training Loss: 2.83582, LR: 0.0000988, Tokens/sec: 53680.11\n",
      "Step: 402, Training Loss: 3.06506, LR: 0.0000988, Tokens/sec: 45998.87\n",
      "Step: 403, Training Loss: 3.20499, LR: 0.0000988, Tokens/sec: 42338.09\n",
      "Step: 404, Training Loss: 3.36641, LR: 0.0000988, Tokens/sec: 51770.97\n",
      "Step: 405, Training Loss: 2.85266, LR: 0.0000988, Tokens/sec: 52666.68\n",
      "Step: 406, Training Loss: 2.94197, LR: 0.0000988, Tokens/sec: 51250.61\n",
      "Step: 407, Training Loss: 3.15316, LR: 0.0000988, Tokens/sec: 50698.08\n",
      "Step: 408, Training Loss: 3.30720, LR: 0.0000987, Tokens/sec: 50534.58\n",
      "Step: 409, Training Loss: 2.74313, LR: 0.0000987, Tokens/sec: 50970.21\n",
      "Step: 410, Training Loss: 2.73743, LR: 0.0000987, Tokens/sec: 51014.73\n",
      "Step: 411, Training Loss: 2.82739, LR: 0.0000987, Tokens/sec: 51411.67\n",
      "Step: 412, Training Loss: 3.04426, LR: 0.0000987, Tokens/sec: 51532.54\n",
      "Step: 413, Training Loss: 2.95330, LR: 0.0000987, Tokens/sec: 52181.78\n",
      "Step: 414, Training Loss: 2.82698, LR: 0.0000987, Tokens/sec: 51356.97\n",
      "Step: 415, Training Loss: 3.04606, LR: 0.0000986, Tokens/sec: 49073.44\n",
      "Step: 416, Training Loss: 3.09497, LR: 0.0000986, Tokens/sec: 50567.17\n",
      "Step: 417, Training Loss: 2.85911, LR: 0.0000986, Tokens/sec: 50663.13\n",
      "Step: 418, Training Loss: 2.96430, LR: 0.0000986, Tokens/sec: 51485.69\n",
      "Step: 419, Training Loss: 3.09859, LR: 0.0000986, Tokens/sec: 51549.02\n",
      "Step: 420, Training Loss: 3.09351, LR: 0.0000986, Tokens/sec: 52343.07\n",
      "Step: 421, Training Loss: 2.89352, LR: 0.0000985, Tokens/sec: 51386.02\n",
      "Step: 422, Training Loss: 2.73240, LR: 0.0000985, Tokens/sec: 49375.91\n",
      "Step: 423, Training Loss: 2.70255, LR: 0.0000985, Tokens/sec: 52535.66\n",
      "Step: 424, Training Loss: 2.92437, LR: 0.0000985, Tokens/sec: 48742.22\n",
      "Step: 425, Training Loss: 2.85194, LR: 0.0000985, Tokens/sec: 51981.20\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 425, Eval Loss: 2.71161\n",
      "Step: 426, Training Loss: 2.58096, LR: 0.0000985, Tokens/sec: 52410.63\n",
      "Step: 427, Training Loss: 2.58113, LR: 0.0000984, Tokens/sec: 51917.51\n",
      "Step: 428, Training Loss: 2.52583, LR: 0.0000984, Tokens/sec: 51432.48\n",
      "Step: 429, Training Loss: 2.64938, LR: 0.0000984, Tokens/sec: 50907.41\n",
      "Step: 430, Training Loss: 2.76221, LR: 0.0000984, Tokens/sec: 50646.01\n",
      "Step: 431, Training Loss: 2.82759, LR: 0.0000984, Tokens/sec: 51449.37\n",
      "Step: 432, Training Loss: 2.84316, LR: 0.0000984, Tokens/sec: 49874.85\n",
      "Step: 433, Training Loss: 2.93299, LR: 0.0000983, Tokens/sec: 51355.47\n",
      "Step: 434, Training Loss: 2.71367, LR: 0.0000983, Tokens/sec: 51081.09\n",
      "Step: 435, Training Loss: 2.51338, LR: 0.0000983, Tokens/sec: 51595.04\n",
      "Step: 436, Training Loss: 2.63449, LR: 0.0000983, Tokens/sec: 52136.81\n",
      "Step: 437, Training Loss: 2.54505, LR: 0.0000983, Tokens/sec: 49002.10\n",
      "Step: 438, Training Loss: 2.74063, LR: 0.0000982, Tokens/sec: 50281.52\n",
      "Step: 439, Training Loss: 2.44437, LR: 0.0000982, Tokens/sec: 48980.30\n",
      "Step: 440, Training Loss: 2.98041, LR: 0.0000982, Tokens/sec: 52078.53\n",
      "Step: 441, Training Loss: 2.93845, LR: 0.0000982, Tokens/sec: 51371.21\n",
      "Step: 442, Training Loss: 2.58123, LR: 0.0000982, Tokens/sec: 52510.72\n",
      "Step: 443, Training Loss: 2.80306, LR: 0.0000982, Tokens/sec: 50595.82\n",
      "Step: 444, Training Loss: 3.28791, LR: 0.0000981, Tokens/sec: 52663.49\n",
      "Step: 445, Training Loss: 2.90720, LR: 0.0000981, Tokens/sec: 52793.70\n",
      "Step: 446, Training Loss: 2.88767, LR: 0.0000981, Tokens/sec: 50765.29\n",
      "Step: 447, Training Loss: 2.51234, LR: 0.0000981, Tokens/sec: 53375.36\n",
      "Step: 448, Training Loss: 2.72689, LR: 0.0000981, Tokens/sec: 50636.25\n",
      "Step: 449, Training Loss: 2.82526, LR: 0.0000980, Tokens/sec: 50933.70\n",
      "Step: 450, Training Loss: 2.54533, LR: 0.0000980, Tokens/sec: 51525.38\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 450, Eval Loss: 2.98798\n",
      "Step: 451, Training Loss: 2.47402, LR: 0.0000980, Tokens/sec: 51337.52\n",
      "Step: 452, Training Loss: 2.18054, LR: 0.0000980, Tokens/sec: 50989.69\n",
      "Step: 453, Training Loss: 2.55114, LR: 0.0000980, Tokens/sec: 49106.89\n",
      "Step: 454, Training Loss: 2.67148, LR: 0.0000980, Tokens/sec: 50307.08\n",
      "Step: 455, Training Loss: 2.50019, LR: 0.0000979, Tokens/sec: 51142.07\n",
      "Step: 456, Training Loss: 2.54858, LR: 0.0000979, Tokens/sec: 49793.03\n",
      "Step: 457, Training Loss: 2.67787, LR: 0.0000979, Tokens/sec: 49597.78\n",
      "Step: 458, Training Loss: 2.52352, LR: 0.0000979, Tokens/sec: 51897.89\n",
      "Step: 459, Training Loss: 2.49732, LR: 0.0000979, Tokens/sec: 51380.12\n",
      "Step: 460, Training Loss: 2.22268, LR: 0.0000978, Tokens/sec: 51159.68\n",
      "Step: 461, Training Loss: 2.28163, LR: 0.0000978, Tokens/sec: 49556.22\n",
      "Step: 462, Training Loss: 2.48452, LR: 0.0000978, Tokens/sec: 51901.72\n",
      "Step: 463, Training Loss: 2.51549, LR: 0.0000978, Tokens/sec: 48344.64\n",
      "Step: 464, Training Loss: 2.25357, LR: 0.0000978, Tokens/sec: 52401.42\n",
      "Step: 465, Training Loss: 2.49720, LR: 0.0000977, Tokens/sec: 51175.46\n",
      "Step: 466, Training Loss: 2.62261, LR: 0.0000977, Tokens/sec: 53173.42\n",
      "Step: 467, Training Loss: 2.21196, LR: 0.0000977, Tokens/sec: 49903.19\n",
      "Step: 468, Training Loss: 2.78330, LR: 0.0000977, Tokens/sec: 48579.77\n",
      "Step: 469, Training Loss: 2.38874, LR: 0.0000977, Tokens/sec: 50380.54\n",
      "Step: 470, Training Loss: 2.09462, LR: 0.0000976, Tokens/sec: 49852.42\n",
      "Step: 471, Training Loss: 1.99997, LR: 0.0000976, Tokens/sec: 51174.02\n",
      "Step: 472, Training Loss: 2.50077, LR: 0.0000976, Tokens/sec: 51816.68\n",
      "Step: 473, Training Loss: 2.24727, LR: 0.0000976, Tokens/sec: 52823.48\n",
      "Step: 474, Training Loss: 2.76338, LR: 0.0000975, Tokens/sec: 50332.18\n",
      "Step: 475, Training Loss: 2.80516, LR: 0.0000975, Tokens/sec: 50945.42\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 475, Eval Loss: 2.58022\n",
      "Step: 476, Training Loss: 2.20578, LR: 0.0000975, Tokens/sec: 48027.50\n",
      "Step: 477, Training Loss: 1.77237, LR: 0.0000975, Tokens/sec: 50940.80\n",
      "Step: 478, Training Loss: 2.71526, LR: 0.0000975, Tokens/sec: 48853.37\n",
      "Step: 479, Training Loss: 2.25223, LR: 0.0000974, Tokens/sec: 51690.61\n",
      "Step: 480, Training Loss: 2.40857, LR: 0.0000974, Tokens/sec: 51951.53\n",
      "Step: 481, Training Loss: 2.24573, LR: 0.0000974, Tokens/sec: 50391.20\n",
      "Step: 482, Training Loss: 1.89932, LR: 0.0000974, Tokens/sec: 49853.77\n",
      "Step: 483, Training Loss: 2.55339, LR: 0.0000974, Tokens/sec: 50479.69\n",
      "Step: 484, Training Loss: 2.30666, LR: 0.0000973, Tokens/sec: 49835.35\n",
      "Step: 485, Training Loss: 2.44115, LR: 0.0000973, Tokens/sec: 49028.98\n",
      "Step: 486, Training Loss: 2.18218, LR: 0.0000973, Tokens/sec: 49998.65\n",
      "Step: 487, Training Loss: 2.31199, LR: 0.0000973, Tokens/sec: 52765.77\n",
      "Step: 488, Training Loss: 2.51417, LR: 0.0000972, Tokens/sec: 52087.01\n",
      "Step: 489, Training Loss: 2.34527, LR: 0.0000972, Tokens/sec: 50984.42\n",
      "Step: 490, Training Loss: 2.31864, LR: 0.0000972, Tokens/sec: 49155.86\n",
      "Step: 491, Training Loss: 2.45829, LR: 0.0000972, Tokens/sec: 52309.41\n",
      "Step: 492, Training Loss: 2.59029, LR: 0.0000972, Tokens/sec: 49409.88\n",
      "Step: 493, Training Loss: 2.01750, LR: 0.0000971, Tokens/sec: 52171.36\n",
      "Step: 494, Training Loss: 2.03718, LR: 0.0000971, Tokens/sec: 52861.91\n",
      "Step: 495, Training Loss: 2.22421, LR: 0.0000971, Tokens/sec: 50468.55\n",
      "Step: 496, Training Loss: 1.93878, LR: 0.0000971, Tokens/sec: 48936.56\n",
      "Step: 497, Training Loss: 2.50971, LR: 0.0000970, Tokens/sec: 49896.89\n",
      "Step: 498, Training Loss: 2.27352, LR: 0.0000970, Tokens/sec: 50634.01\n",
      "Step: 499, Training Loss: 2.20190, LR: 0.0000970, Tokens/sec: 50069.02\n",
      "Step: 500, Training Loss: 2.40669, LR: 0.0000970, Tokens/sec: 50252.84\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 500, Eval Loss: 2.01785\n",
      "Step: 501, Training Loss: 2.10371, LR: 0.0000970, Tokens/sec: 51854.93\n",
      "Step: 502, Training Loss: 1.96029, LR: 0.0000969, Tokens/sec: 50709.30\n",
      "Step: 503, Training Loss: 2.29079, LR: 0.0000969, Tokens/sec: 50751.90\n",
      "Step: 504, Training Loss: 2.05648, LR: 0.0000969, Tokens/sec: 51242.60\n",
      "Step: 505, Training Loss: 2.17712, LR: 0.0000969, Tokens/sec: 50663.50\n",
      "Step: 506, Training Loss: 2.44349, LR: 0.0000968, Tokens/sec: 51127.78\n",
      "Step: 507, Training Loss: 2.49613, LR: 0.0000968, Tokens/sec: 49463.98\n",
      "Step: 508, Training Loss: 2.12606, LR: 0.0000968, Tokens/sec: 51070.14\n",
      "Step: 509, Training Loss: 1.90731, LR: 0.0000968, Tokens/sec: 50467.04\n",
      "Step: 510, Training Loss: 1.96586, LR: 0.0000967, Tokens/sec: 52847.81\n",
      "Step: 511, Training Loss: 2.51867, LR: 0.0000967, Tokens/sec: 50427.69\n",
      "Step: 512, Training Loss: 2.07231, LR: 0.0000967, Tokens/sec: 51467.44\n",
      "Step: 513, Training Loss: 2.09685, LR: 0.0000967, Tokens/sec: 50396.52\n",
      "Step: 514, Training Loss: 1.96839, LR: 0.0000966, Tokens/sec: 49669.08\n",
      "Step: 515, Training Loss: 2.04713, LR: 0.0000966, Tokens/sec: 52093.31\n",
      "Step: 516, Training Loss: 1.86038, LR: 0.0000966, Tokens/sec: 49956.26\n",
      "Step: 517, Training Loss: 2.17202, LR: 0.0000966, Tokens/sec: 50600.04\n",
      "Step: 518, Training Loss: 2.18177, LR: 0.0000965, Tokens/sec: 50249.32\n",
      "Step: 519, Training Loss: 2.27533, LR: 0.0000965, Tokens/sec: 51309.15\n",
      "Step: 520, Training Loss: 2.15212, LR: 0.0000965, Tokens/sec: 50344.30\n",
      "Step: 521, Training Loss: 2.01212, LR: 0.0000965, Tokens/sec: 49160.00\n",
      "Step: 522, Training Loss: 2.23289, LR: 0.0000964, Tokens/sec: 51272.04\n",
      "Step: 523, Training Loss: 2.02138, LR: 0.0000964, Tokens/sec: 48164.71\n",
      "Step: 524, Training Loss: 2.04715, LR: 0.0000964, Tokens/sec: 51599.23\n",
      "Step: 525, Training Loss: 2.28471, LR: 0.0000964, Tokens/sec: 51752.34\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 525, Eval Loss: 2.18959\n",
      "Step: 526, Training Loss: 1.92341, LR: 0.0000963, Tokens/sec: 52111.98\n",
      "Step: 527, Training Loss: 1.82473, LR: 0.0000963, Tokens/sec: 50310.51\n",
      "Step: 528, Training Loss: 2.24162, LR: 0.0000963, Tokens/sec: 50930.71\n",
      "Step: 529, Training Loss: 2.19969, LR: 0.0000963, Tokens/sec: 48417.81\n",
      "Step: 530, Training Loss: 1.68611, LR: 0.0000962, Tokens/sec: 52782.47\n",
      "Step: 531, Training Loss: 2.28694, LR: 0.0000962, Tokens/sec: 50287.88\n",
      "Step: 532, Training Loss: 2.00290, LR: 0.0000962, Tokens/sec: 51948.41\n",
      "Step: 533, Training Loss: 2.34603, LR: 0.0000962, Tokens/sec: 51575.71\n",
      "Step: 534, Training Loss: 2.26907, LR: 0.0000961, Tokens/sec: 52150.23\n",
      "Step: 535, Training Loss: 2.11513, LR: 0.0000961, Tokens/sec: 51728.50\n",
      "Step: 536, Training Loss: 1.81656, LR: 0.0000961, Tokens/sec: 48402.34\n",
      "Step: 537, Training Loss: 1.93719, LR: 0.0000961, Tokens/sec: 51225.91\n",
      "Step: 538, Training Loss: 1.62391, LR: 0.0000960, Tokens/sec: 49615.41\n",
      "Step: 539, Training Loss: 1.86830, LR: 0.0000960, Tokens/sec: 51141.59\n",
      "Step: 540, Training Loss: 1.97048, LR: 0.0000960, Tokens/sec: 50771.17\n",
      "Step: 541, Training Loss: 1.98961, LR: 0.0000959, Tokens/sec: 51406.67\n",
      "Step: 542, Training Loss: 1.84521, LR: 0.0000959, Tokens/sec: 51675.22\n",
      "Step: 543, Training Loss: 1.48596, LR: 0.0000959, Tokens/sec: 51220.84\n",
      "Step: 544, Training Loss: 1.77558, LR: 0.0000959, Tokens/sec: 50727.80\n",
      "Step: 545, Training Loss: 1.80430, LR: 0.0000958, Tokens/sec: 50378.86\n",
      "Step: 546, Training Loss: 2.10688, LR: 0.0000958, Tokens/sec: 51855.29\n",
      "Step: 547, Training Loss: 1.87975, LR: 0.0000958, Tokens/sec: 51233.04\n",
      "Step: 548, Training Loss: 1.53990, LR: 0.0000958, Tokens/sec: 51543.67\n",
      "Step: 549, Training Loss: 1.69295, LR: 0.0000957, Tokens/sec: 50041.26\n",
      "Step: 550, Training Loss: 1.56096, LR: 0.0000957, Tokens/sec: 50129.70\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 550, Eval Loss: 1.91388\n",
      "Step: 551, Training Loss: 1.86333, LR: 0.0000957, Tokens/sec: 52959.21\n",
      "Step: 552, Training Loss: 1.71824, LR: 0.0000956, Tokens/sec: 50125.97\n",
      "Step: 553, Training Loss: 1.96669, LR: 0.0000956, Tokens/sec: 50575.18\n",
      "Step: 554, Training Loss: 1.89570, LR: 0.0000956, Tokens/sec: 51213.84\n",
      "Step: 555, Training Loss: 1.86401, LR: 0.0000956, Tokens/sec: 49957.17\n",
      "Step: 556, Training Loss: 1.72940, LR: 0.0000955, Tokens/sec: 53025.76\n",
      "Step: 557, Training Loss: 1.75677, LR: 0.0000955, Tokens/sec: 52528.60\n",
      "Step: 558, Training Loss: 1.73832, LR: 0.0000955, Tokens/sec: 49915.83\n",
      "Step: 559, Training Loss: 1.91593, LR: 0.0000955, Tokens/sec: 50774.52\n",
      "Step: 560, Training Loss: 1.47333, LR: 0.0000954, Tokens/sec: 48848.62\n",
      "Step: 561, Training Loss: 1.95049, LR: 0.0000954, Tokens/sec: 51772.35\n",
      "Step: 562, Training Loss: 2.11006, LR: 0.0000954, Tokens/sec: 49760.12\n",
      "Step: 563, Training Loss: 1.91148, LR: 0.0000953, Tokens/sec: 53114.40\n",
      "Step: 564, Training Loss: 1.61854, LR: 0.0000953, Tokens/sec: 51179.65\n",
      "Step: 565, Training Loss: 1.75944, LR: 0.0000953, Tokens/sec: 51627.29\n",
      "Step: 566, Training Loss: 1.57346, LR: 0.0000953, Tokens/sec: 51199.95\n",
      "Step: 567, Training Loss: 1.80074, LR: 0.0000952, Tokens/sec: 49699.18\n",
      "Step: 568, Training Loss: 1.49965, LR: 0.0000952, Tokens/sec: 50886.43\n",
      "Step: 569, Training Loss: 1.85942, LR: 0.0000952, Tokens/sec: 48887.91\n",
      "Step: 570, Training Loss: 1.78309, LR: 0.0000951, Tokens/sec: 50795.48\n",
      "Step: 571, Training Loss: 1.48553, LR: 0.0000951, Tokens/sec: 50587.06\n",
      "Step: 572, Training Loss: 1.62492, LR: 0.0000951, Tokens/sec: 51477.13\n",
      "Step: 573, Training Loss: 1.63959, LR: 0.0000950, Tokens/sec: 52161.59\n",
      "Step: 574, Training Loss: 1.69883, LR: 0.0000950, Tokens/sec: 47763.60\n",
      "Step: 575, Training Loss: 1.70864, LR: 0.0000950, Tokens/sec: 51037.14\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 575, Eval Loss: 1.62282\n",
      "Step: 576, Training Loss: 1.46898, LR: 0.0000950, Tokens/sec: 52257.48\n",
      "Step: 577, Training Loss: 1.52409, LR: 0.0000949, Tokens/sec: 49683.23\n",
      "Step: 578, Training Loss: 1.45834, LR: 0.0000949, Tokens/sec: 51726.02\n",
      "Step: 579, Training Loss: 1.36506, LR: 0.0000949, Tokens/sec: 50580.92\n",
      "Step: 580, Training Loss: 1.73090, LR: 0.0000948, Tokens/sec: 51296.98\n",
      "Step: 581, Training Loss: 1.51435, LR: 0.0000948, Tokens/sec: 50372.74\n",
      "Step: 582, Training Loss: 1.93556, LR: 0.0000948, Tokens/sec: 51029.01\n",
      "Step: 583, Training Loss: 1.45429, LR: 0.0000947, Tokens/sec: 51311.70\n",
      "Step: 584, Training Loss: 1.51242, LR: 0.0000947, Tokens/sec: 49788.76\n",
      "Step: 585, Training Loss: 1.77430, LR: 0.0000947, Tokens/sec: 51572.59\n",
      "Step: 586, Training Loss: 1.50429, LR: 0.0000947, Tokens/sec: 51010.01\n",
      "Step: 587, Training Loss: 1.52186, LR: 0.0000946, Tokens/sec: 51087.16\n",
      "Step: 588, Training Loss: 1.39375, LR: 0.0000946, Tokens/sec: 50917.96\n",
      "Step: 589, Training Loss: 1.49685, LR: 0.0000946, Tokens/sec: 49795.03\n",
      "Step: 590, Training Loss: 1.42407, LR: 0.0000945, Tokens/sec: 51986.90\n",
      "Step: 591, Training Loss: 1.76173, LR: 0.0000945, Tokens/sec: 50127.81\n",
      "Step: 592, Training Loss: 1.61882, LR: 0.0000945, Tokens/sec: 51333.78\n",
      "Step: 593, Training Loss: 1.44086, LR: 0.0000944, Tokens/sec: 50080.00\n",
      "Step: 594, Training Loss: 1.55293, LR: 0.0000944, Tokens/sec: 53198.32\n",
      "Step: 595, Training Loss: 1.64967, LR: 0.0000944, Tokens/sec: 52141.92\n",
      "Step: 596, Training Loss: 1.40903, LR: 0.0000943, Tokens/sec: 50757.91\n",
      "Step: 597, Training Loss: 1.28792, LR: 0.0000943, Tokens/sec: 51129.49\n",
      "Step: 598, Training Loss: 1.67967, LR: 0.0000943, Tokens/sec: 48852.37\n",
      "Step: 599, Training Loss: 1.50833, LR: 0.0000943, Tokens/sec: 50751.60\n",
      "Step: 600, Training Loss: 1.80344, LR: 0.0000942, Tokens/sec: 50962.67\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 600, Eval Loss: 1.49567\n",
      "Step: 601, Training Loss: 1.28521, LR: 0.0000942, Tokens/sec: 50345.27\n",
      "Step: 602, Training Loss: 1.50470, LR: 0.0000942, Tokens/sec: 52893.26\n",
      "Step: 603, Training Loss: 1.39991, LR: 0.0000941, Tokens/sec: 51879.40\n",
      "Step: 604, Training Loss: 1.37658, LR: 0.0000941, Tokens/sec: 50222.30\n",
      "Step: 605, Training Loss: 1.15578, LR: 0.0000941, Tokens/sec: 48120.46\n",
      "Step: 606, Training Loss: 1.49969, LR: 0.0000940, Tokens/sec: 50410.87\n",
      "Step: 607, Training Loss: 1.30490, LR: 0.0000940, Tokens/sec: 51282.11\n",
      "Step: 608, Training Loss: 1.25909, LR: 0.0000940, Tokens/sec: 49425.12\n",
      "Step: 609, Training Loss: 1.26467, LR: 0.0000939, Tokens/sec: 52217.99\n",
      "Step: 610, Training Loss: 1.45718, LR: 0.0000939, Tokens/sec: 52575.06\n",
      "Step: 611, Training Loss: 1.38221, LR: 0.0000939, Tokens/sec: 6420.40\n",
      "Step: 612, Training Loss: 1.29001, LR: 0.0000938, Tokens/sec: 43223.01\n",
      "Step: 613, Training Loss: 1.15962, LR: 0.0000938, Tokens/sec: 43165.18\n",
      "Step: 614, Training Loss: 1.13177, LR: 0.0000938, Tokens/sec: 43300.53\n",
      "Step: 615, Training Loss: 1.38275, LR: 0.0000937, Tokens/sec: 49774.19\n",
      "Step: 616, Training Loss: 1.57098, LR: 0.0000937, Tokens/sec: 50441.43\n",
      "Step: 617, Training Loss: 1.44128, LR: 0.0000937, Tokens/sec: 48728.79\n",
      "Step: 618, Training Loss: 1.45437, LR: 0.0000936, Tokens/sec: 52291.27\n",
      "Step: 619, Training Loss: 1.44457, LR: 0.0000936, Tokens/sec: 55028.26\n",
      "Step: 620, Training Loss: 1.14964, LR: 0.0000936, Tokens/sec: 47402.22\n",
      "Step: 621, Training Loss: 1.23525, LR: 0.0000935, Tokens/sec: 46853.40\n",
      "Step: 622, Training Loss: 1.29964, LR: 0.0000935, Tokens/sec: 43976.47\n",
      "Step: 623, Training Loss: 1.27446, LR: 0.0000935, Tokens/sec: 47630.46\n",
      "Step: 624, Training Loss: 1.46878, LR: 0.0000934, Tokens/sec: 48016.69\n",
      "Step: 625, Training Loss: 1.40448, LR: 0.0000934, Tokens/sec: 47066.35\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 625, Eval Loss: 1.21561\n",
      "Step: 626, Training Loss: 1.16918, LR: 0.0000934, Tokens/sec: 52115.06\n",
      "Step: 627, Training Loss: 1.16642, LR: 0.0000933, Tokens/sec: 45990.83\n",
      "Step: 628, Training Loss: 1.08465, LR: 0.0000933, Tokens/sec: 48904.64\n",
      "Step: 629, Training Loss: 1.28336, LR: 0.0000933, Tokens/sec: 46123.31\n",
      "Step: 630, Training Loss: 0.91927, LR: 0.0000932, Tokens/sec: 53927.15\n",
      "Step: 631, Training Loss: 1.14427, LR: 0.0000932, Tokens/sec: 53740.53\n",
      "Step: 632, Training Loss: 1.12676, LR: 0.0000932, Tokens/sec: 54272.57\n",
      "Step: 633, Training Loss: 1.30474, LR: 0.0000931, Tokens/sec: 52951.97\n",
      "Step: 634, Training Loss: 1.19775, LR: 0.0000931, Tokens/sec: 51905.33\n",
      "Step: 635, Training Loss: 1.39859, LR: 0.0000931, Tokens/sec: 53123.47\n",
      "Step: 636, Training Loss: 1.45103, LR: 0.0000930, Tokens/sec: 53047.01\n",
      "Step: 637, Training Loss: 1.17570, LR: 0.0000930, Tokens/sec: 53224.79\n",
      "Step: 638, Training Loss: 1.30615, LR: 0.0000930, Tokens/sec: 53945.76\n",
      "Step: 639, Training Loss: 1.10860, LR: 0.0000929, Tokens/sec: 51851.54\n",
      "Step: 640, Training Loss: 1.27204, LR: 0.0000929, Tokens/sec: 48634.25\n",
      "Step: 641, Training Loss: 1.09380, LR: 0.0000929, Tokens/sec: 48018.76\n",
      "Step: 642, Training Loss: 1.23280, LR: 0.0000928, Tokens/sec: 53700.14\n",
      "Step: 643, Training Loss: 1.55349, LR: 0.0000928, Tokens/sec: 53296.18\n",
      "Step: 644, Training Loss: 1.25936, LR: 0.0000928, Tokens/sec: 54154.99\n",
      "Step: 645, Training Loss: 1.10941, LR: 0.0000927, Tokens/sec: 52974.56\n",
      "Step: 646, Training Loss: 1.05911, LR: 0.0000927, Tokens/sec: 52228.00\n",
      "Step: 647, Training Loss: 0.88925, LR: 0.0000926, Tokens/sec: 52783.14\n",
      "Step: 648, Training Loss: 1.19949, LR: 0.0000926, Tokens/sec: 51974.71\n",
      "Step: 649, Training Loss: 1.07528, LR: 0.0000926, Tokens/sec: 53524.50\n",
      "Step: 650, Training Loss: 1.29296, LR: 0.0000925, Tokens/sec: 54333.74\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 650, Eval Loss: 1.11112\n",
      "Step: 651, Training Loss: 1.17662, LR: 0.0000925, Tokens/sec: 52991.71\n",
      "Step: 652, Training Loss: 1.00062, LR: 0.0000925, Tokens/sec: 52256.27\n",
      "Step: 653, Training Loss: 1.23810, LR: 0.0000924, Tokens/sec: 52842.13\n",
      "Step: 654, Training Loss: 1.20164, LR: 0.0000924, Tokens/sec: 52435.71\n",
      "Step: 655, Training Loss: 1.22293, LR: 0.0000924, Tokens/sec: 53568.57\n",
      "Step: 656, Training Loss: 0.96986, LR: 0.0000923, Tokens/sec: 51371.29\n",
      "Step: 657, Training Loss: 0.83774, LR: 0.0000923, Tokens/sec: 53079.92\n",
      "Step: 658, Training Loss: 1.06433, LR: 0.0000923, Tokens/sec: 48802.09\n",
      "Step: 659, Training Loss: 1.12757, LR: 0.0000922, Tokens/sec: 44757.21\n",
      "Step: 660, Training Loss: 1.30235, LR: 0.0000922, Tokens/sec: 46492.21\n",
      "Step: 661, Training Loss: 1.36650, LR: 0.0000921, Tokens/sec: 46918.57\n",
      "Step: 662, Training Loss: 1.12154, LR: 0.0000921, Tokens/sec: 46212.26\n",
      "Step: 663, Training Loss: 1.14659, LR: 0.0000921, Tokens/sec: 44471.23\n",
      "Step: 664, Training Loss: 1.12301, LR: 0.0000920, Tokens/sec: 46355.12\n",
      "Step: 665, Training Loss: 0.97727, LR: 0.0000920, Tokens/sec: 46048.90\n",
      "Step: 666, Training Loss: 1.08964, LR: 0.0000920, Tokens/sec: 44374.41\n",
      "Step: 667, Training Loss: 1.00816, LR: 0.0000919, Tokens/sec: 47733.16\n",
      "Step: 668, Training Loss: 1.16523, LR: 0.0000919, Tokens/sec: 42617.95\n",
      "Step: 669, Training Loss: 1.47951, LR: 0.0000918, Tokens/sec: 43705.14\n",
      "Step: 670, Training Loss: 0.99973, LR: 0.0000918, Tokens/sec: 41511.07\n",
      "Step: 671, Training Loss: 1.20319, LR: 0.0000918, Tokens/sec: 47942.72\n",
      "Step: 672, Training Loss: 0.97995, LR: 0.0000917, Tokens/sec: 49322.84\n",
      "Step: 673, Training Loss: 1.02102, LR: 0.0000917, Tokens/sec: 47274.69\n",
      "Step: 674, Training Loss: 1.06154, LR: 0.0000917, Tokens/sec: 48715.37\n",
      "Step: 675, Training Loss: 1.00250, LR: 0.0000916, Tokens/sec: 48013.36\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 675, Eval Loss: 1.16686\n",
      "Step: 676, Training Loss: 1.02105, LR: 0.0000916, Tokens/sec: 50971.02\n",
      "Step: 677, Training Loss: 0.95597, LR: 0.0000915, Tokens/sec: 50863.22\n",
      "Step: 678, Training Loss: 1.17841, LR: 0.0000915, Tokens/sec: 50503.84\n",
      "Step: 679, Training Loss: 0.78874, LR: 0.0000915, Tokens/sec: 48961.43\n",
      "Step: 680, Training Loss: 0.80398, LR: 0.0000914, Tokens/sec: 50627.15\n",
      "Step: 681, Training Loss: 1.05017, LR: 0.0000914, Tokens/sec: 48909.45\n",
      "Step: 682, Training Loss: 0.75325, LR: 0.0000914, Tokens/sec: 51346.90\n",
      "Step: 683, Training Loss: 1.11844, LR: 0.0000913, Tokens/sec: 51588.44\n",
      "Step: 684, Training Loss: 1.06073, LR: 0.0000913, Tokens/sec: 51353.61\n",
      "Step: 685, Training Loss: 0.91109, LR: 0.0000912, Tokens/sec: 49682.15\n",
      "Step: 686, Training Loss: 1.04700, LR: 0.0000912, Tokens/sec: 49099.56\n",
      "Step: 687, Training Loss: 1.01726, LR: 0.0000912, Tokens/sec: 50994.67\n",
      "Step: 688, Training Loss: 0.84807, LR: 0.0000911, Tokens/sec: 50276.17\n",
      "Step: 689, Training Loss: 0.97088, LR: 0.0000911, Tokens/sec: 49511.19\n",
      "Step: 690, Training Loss: 0.90309, LR: 0.0000911, Tokens/sec: 49683.59\n",
      "Step: 691, Training Loss: 0.90382, LR: 0.0000910, Tokens/sec: 50666.96\n",
      "Step: 692, Training Loss: 0.80423, LR: 0.0000910, Tokens/sec: 50686.87\n",
      "Step: 693, Training Loss: 0.77005, LR: 0.0000909, Tokens/sec: 51344.69\n",
      "Step: 694, Training Loss: 1.23406, LR: 0.0000909, Tokens/sec: 52512.69\n",
      "Step: 695, Training Loss: 0.87962, LR: 0.0000909, Tokens/sec: 48590.02\n",
      "Step: 696, Training Loss: 0.94719, LR: 0.0000908, Tokens/sec: 49855.40\n",
      "Step: 697, Training Loss: 0.90093, LR: 0.0000908, Tokens/sec: 49523.09\n",
      "Step: 698, Training Loss: 0.88135, LR: 0.0000907, Tokens/sec: 51755.41\n",
      "Step: 699, Training Loss: 0.90959, LR: 0.0000907, Tokens/sec: 50343.30\n",
      "Step: 700, Training Loss: 0.86516, LR: 0.0000907, Tokens/sec: 50103.26\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 700, Eval Loss: 1.00526\n",
      "Step: 701, Training Loss: 0.85388, LR: 0.0000906, Tokens/sec: 51728.90\n",
      "Step: 702, Training Loss: 0.75952, LR: 0.0000906, Tokens/sec: 49960.00\n",
      "Step: 703, Training Loss: 0.89838, LR: 0.0000905, Tokens/sec: 50811.14\n",
      "Step: 704, Training Loss: 0.99128, LR: 0.0000905, Tokens/sec: 49186.40\n",
      "Step: 705, Training Loss: 0.80639, LR: 0.0000905, Tokens/sec: 49733.18\n",
      "Step: 706, Training Loss: 0.67148, LR: 0.0000904, Tokens/sec: 50424.37\n",
      "Step: 707, Training Loss: 0.79122, LR: 0.0000904, Tokens/sec: 48784.75\n",
      "Step: 708, Training Loss: 1.10162, LR: 0.0000903, Tokens/sec: 50994.34\n",
      "Step: 709, Training Loss: 0.74554, LR: 0.0000903, Tokens/sec: 49487.94\n",
      "Step: 710, Training Loss: 0.72943, LR: 0.0000903, Tokens/sec: 49698.17\n",
      "Step: 711, Training Loss: 0.76127, LR: 0.0000902, Tokens/sec: 51389.07\n",
      "Step: 712, Training Loss: 0.76647, LR: 0.0000902, Tokens/sec: 50037.88\n",
      "Step: 713, Training Loss: 0.79208, LR: 0.0000901, Tokens/sec: 51546.40\n",
      "Step: 714, Training Loss: 1.23493, LR: 0.0000901, Tokens/sec: 49240.25\n",
      "Step: 715, Training Loss: 0.78323, LR: 0.0000901, Tokens/sec: 51765.00\n",
      "Step: 716, Training Loss: 0.86357, LR: 0.0000900, Tokens/sec: 48786.46\n",
      "Step: 717, Training Loss: 0.87343, LR: 0.0000900, Tokens/sec: 50338.86\n",
      "Step: 718, Training Loss: 0.87848, LR: 0.0000899, Tokens/sec: 50756.77\n",
      "Step: 719, Training Loss: 0.80210, LR: 0.0000899, Tokens/sec: 51023.93\n",
      "Step: 720, Training Loss: 0.69281, LR: 0.0000899, Tokens/sec: 50063.67\n",
      "Step: 721, Training Loss: 0.94707, LR: 0.0000898, Tokens/sec: 48578.20\n",
      "Step: 722, Training Loss: 0.78435, LR: 0.0000898, Tokens/sec: 49740.02\n",
      "Step: 723, Training Loss: 0.64369, LR: 0.0000897, Tokens/sec: 49858.76\n",
      "Step: 724, Training Loss: 0.70631, LR: 0.0000897, Tokens/sec: 50705.55\n",
      "Step: 725, Training Loss: 0.66934, LR: 0.0000897, Tokens/sec: 51104.50\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 725, Eval Loss: 0.77086\n",
      "Step: 726, Training Loss: 0.69887, LR: 0.0000896, Tokens/sec: 51173.85\n",
      "Step: 727, Training Loss: 0.75741, LR: 0.0000896, Tokens/sec: 51459.25\n",
      "Step: 728, Training Loss: 0.66692, LR: 0.0000895, Tokens/sec: 49923.66\n",
      "Step: 729, Training Loss: 0.81891, LR: 0.0000895, Tokens/sec: 50559.38\n",
      "Step: 730, Training Loss: 0.91470, LR: 0.0000894, Tokens/sec: 50388.00\n",
      "Step: 731, Training Loss: 0.77447, LR: 0.0000894, Tokens/sec: 50244.17\n",
      "Step: 732, Training Loss: 0.80310, LR: 0.0000894, Tokens/sec: 49845.86\n",
      "Step: 733, Training Loss: 0.59060, LR: 0.0000893, Tokens/sec: 49932.84\n",
      "Step: 734, Training Loss: 0.81544, LR: 0.0000893, Tokens/sec: 50566.74\n",
      "Step: 735, Training Loss: 0.64161, LR: 0.0000892, Tokens/sec: 51407.14\n",
      "Step: 736, Training Loss: 0.79711, LR: 0.0000892, Tokens/sec: 50252.43\n",
      "Step: 737, Training Loss: 0.89244, LR: 0.0000892, Tokens/sec: 52065.66\n",
      "Step: 738, Training Loss: 0.80701, LR: 0.0000891, Tokens/sec: 49283.67\n",
      "Step: 739, Training Loss: 0.86372, LR: 0.0000891, Tokens/sec: 51491.46\n",
      "Step: 740, Training Loss: 0.97842, LR: 0.0000890, Tokens/sec: 49400.88\n",
      "Step: 741, Training Loss: 0.74839, LR: 0.0000890, Tokens/sec: 52061.37\n",
      "Step: 742, Training Loss: 0.79062, LR: 0.0000889, Tokens/sec: 50175.73\n",
      "Step: 743, Training Loss: 0.73552, LR: 0.0000889, Tokens/sec: 50323.39\n",
      "Step: 744, Training Loss: 0.80627, LR: 0.0000889, Tokens/sec: 50649.95\n",
      "Step: 745, Training Loss: 0.72213, LR: 0.0000888, Tokens/sec: 51626.41\n",
      "Step: 746, Training Loss: 0.77363, LR: 0.0000888, Tokens/sec: 49838.00\n",
      "Step: 747, Training Loss: 0.66152, LR: 0.0000887, Tokens/sec: 49061.74\n",
      "Step: 748, Training Loss: 0.86277, LR: 0.0000887, Tokens/sec: 50860.21\n",
      "Step: 749, Training Loss: 0.65714, LR: 0.0000886, Tokens/sec: 50525.71\n",
      "Step: 750, Training Loss: 0.88491, LR: 0.0000886, Tokens/sec: 48694.69\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 750, Eval Loss: 0.67464\n",
      "Step: 751, Training Loss: 0.58317, LR: 0.0000886, Tokens/sec: 51020.10\n",
      "Step: 752, Training Loss: 0.80824, LR: 0.0000885, Tokens/sec: 51390.18\n",
      "Step: 753, Training Loss: 0.56461, LR: 0.0000885, Tokens/sec: 51363.00\n",
      "Step: 754, Training Loss: 0.61272, LR: 0.0000884, Tokens/sec: 50642.20\n",
      "Step: 755, Training Loss: 0.72606, LR: 0.0000884, Tokens/sec: 50359.43\n",
      "Step: 756, Training Loss: 0.67795, LR: 0.0000883, Tokens/sec: 51152.71\n",
      "Step: 757, Training Loss: 0.56127, LR: 0.0000883, Tokens/sec: 49264.01\n",
      "Step: 758, Training Loss: 0.64255, LR: 0.0000883, Tokens/sec: 51924.64\n",
      "Step: 759, Training Loss: 0.70256, LR: 0.0000882, Tokens/sec: 49106.12\n",
      "Step: 760, Training Loss: 0.54251, LR: 0.0000882, Tokens/sec: 51076.69\n",
      "Step: 761, Training Loss: 0.69580, LR: 0.0000881, Tokens/sec: 50195.58\n",
      "Step: 762, Training Loss: 0.60486, LR: 0.0000881, Tokens/sec: 50966.43\n",
      "Step: 763, Training Loss: 0.53019, LR: 0.0000880, Tokens/sec: 47398.87\n",
      "Step: 764, Training Loss: 0.60239, LR: 0.0000880, Tokens/sec: 48890.88\n",
      "Step: 765, Training Loss: 0.87878, LR: 0.0000880, Tokens/sec: 49833.14\n",
      "Step: 766, Training Loss: 0.54490, LR: 0.0000879, Tokens/sec: 49525.25\n",
      "Step: 767, Training Loss: 0.58551, LR: 0.0000879, Tokens/sec: 51329.78\n",
      "Step: 768, Training Loss: 0.81571, LR: 0.0000878, Tokens/sec: 50748.43\n",
      "Step: 769, Training Loss: 0.65284, LR: 0.0000878, Tokens/sec: 50625.87\n",
      "Step: 770, Training Loss: 0.60198, LR: 0.0000877, Tokens/sec: 50965.79\n",
      "Step: 771, Training Loss: 0.62044, LR: 0.0000877, Tokens/sec: 51453.54\n",
      "Step: 772, Training Loss: 0.71648, LR: 0.0000876, Tokens/sec: 50129.96\n",
      "Step: 773, Training Loss: 0.75344, LR: 0.0000876, Tokens/sec: 50260.05\n",
      "Step: 774, Training Loss: 0.66045, LR: 0.0000876, Tokens/sec: 51656.32\n",
      "Step: 775, Training Loss: 0.46683, LR: 0.0000875, Tokens/sec: 51049.68\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 775, Eval Loss: 0.53498\n",
      "Step: 776, Training Loss: 0.65130, LR: 0.0000875, Tokens/sec: 51325.97\n",
      "Step: 777, Training Loss: 0.60615, LR: 0.0000874, Tokens/sec: 49841.02\n",
      "Step: 778, Training Loss: 0.74227, LR: 0.0000874, Tokens/sec: 52379.89\n",
      "Step: 779, Training Loss: 0.49800, LR: 0.0000873, Tokens/sec: 50365.11\n",
      "Step: 780, Training Loss: 0.48858, LR: 0.0000873, Tokens/sec: 51042.92\n",
      "Step: 781, Training Loss: 0.52121, LR: 0.0000872, Tokens/sec: 47999.48\n",
      "Step: 782, Training Loss: 0.82438, LR: 0.0000872, Tokens/sec: 51492.54\n",
      "Step: 783, Training Loss: 0.56681, LR: 0.0000871, Tokens/sec: 49666.42\n",
      "Step: 784, Training Loss: 0.58649, LR: 0.0000871, Tokens/sec: 51800.12\n",
      "Step: 785, Training Loss: 0.73907, LR: 0.0000871, Tokens/sec: 51176.57\n",
      "Step: 786, Training Loss: 0.69065, LR: 0.0000870, Tokens/sec: 50797.13\n",
      "Step: 787, Training Loss: 0.60222, LR: 0.0000870, Tokens/sec: 50314.93\n",
      "Step: 788, Training Loss: 0.49800, LR: 0.0000869, Tokens/sec: 50563.32\n",
      "Step: 789, Training Loss: 0.44052, LR: 0.0000869, Tokens/sec: 50173.58\n",
      "Step: 790, Training Loss: 0.57192, LR: 0.0000868, Tokens/sec: 50866.61\n",
      "Step: 791, Training Loss: 0.69093, LR: 0.0000868, Tokens/sec: 51375.35\n",
      "Step: 792, Training Loss: 0.49938, LR: 0.0000867, Tokens/sec: 49981.66\n",
      "Step: 793, Training Loss: 0.54037, LR: 0.0000867, Tokens/sec: 50542.63\n",
      "Step: 794, Training Loss: 0.56713, LR: 0.0000866, Tokens/sec: 51821.15\n",
      "Step: 795, Training Loss: 0.46207, LR: 0.0000866, Tokens/sec: 49775.14\n",
      "Step: 796, Training Loss: 0.64915, LR: 0.0000866, Tokens/sec: 50419.62\n",
      "Step: 797, Training Loss: 0.57728, LR: 0.0000865, Tokens/sec: 48118.71\n",
      "Step: 798, Training Loss: 0.42983, LR: 0.0000865, Tokens/sec: 51833.40\n",
      "Step: 799, Training Loss: 0.55277, LR: 0.0000864, Tokens/sec: 48896.92\n",
      "Step: 800, Training Loss: 0.48145, LR: 0.0000864, Tokens/sec: 51974.94\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 800, Eval Loss: 0.52290\n",
      "Step: 801, Training Loss: 0.52657, LR: 0.0000863, Tokens/sec: 52996.38\n",
      "Step: 802, Training Loss: 0.53324, LR: 0.0000863, Tokens/sec: 52262.56\n",
      "Step: 803, Training Loss: 0.44634, LR: 0.0000862, Tokens/sec: 50942.13\n",
      "Step: 804, Training Loss: 0.46402, LR: 0.0000862, Tokens/sec: 49677.38\n",
      "Step: 805, Training Loss: 0.51118, LR: 0.0000861, Tokens/sec: 50603.59\n",
      "Step: 806, Training Loss: 0.37178, LR: 0.0000861, Tokens/sec: 50558.80\n",
      "Step: 807, Training Loss: 0.39811, LR: 0.0000860, Tokens/sec: 48963.28\n",
      "Step: 808, Training Loss: 0.41442, LR: 0.0000860, Tokens/sec: 49737.68\n",
      "Step: 809, Training Loss: 0.39786, LR: 0.0000860, Tokens/sec: 51205.99\n",
      "Step: 810, Training Loss: 0.49148, LR: 0.0000859, Tokens/sec: 50051.68\n",
      "Step: 811, Training Loss: 0.49649, LR: 0.0000859, Tokens/sec: 50368.71\n",
      "Step: 812, Training Loss: 0.38603, LR: 0.0000858, Tokens/sec: 47641.98\n",
      "Step: 813, Training Loss: 0.62689, LR: 0.0000858, Tokens/sec: 49931.28\n",
      "Step: 814, Training Loss: 0.42635, LR: 0.0000857, Tokens/sec: 49425.73\n",
      "Step: 815, Training Loss: 0.50958, LR: 0.0000857, Tokens/sec: 50863.70\n",
      "Step: 816, Training Loss: 0.45345, LR: 0.0000856, Tokens/sec: 51316.89\n",
      "Step: 817, Training Loss: 0.58461, LR: 0.0000856, Tokens/sec: 51049.63\n",
      "Step: 818, Training Loss: 0.41956, LR: 0.0000855, Tokens/sec: 48529.63\n",
      "Step: 819, Training Loss: 0.72181, LR: 0.0000855, Tokens/sec: 51673.10\n",
      "Step: 820, Training Loss: 0.49583, LR: 0.0000854, Tokens/sec: 49542.66\n",
      "Step: 821, Training Loss: 0.43663, LR: 0.0000854, Tokens/sec: 50216.98\n",
      "Step: 822, Training Loss: 0.50214, LR: 0.0000853, Tokens/sec: 52286.81\n",
      "Step: 823, Training Loss: 0.47688, LR: 0.0000853, Tokens/sec: 49277.35\n",
      "Step: 824, Training Loss: 0.42860, LR: 0.0000852, Tokens/sec: 51027.26\n",
      "Step: 825, Training Loss: 0.57147, LR: 0.0000852, Tokens/sec: 51306.98\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 825, Eval Loss: 0.54385\n",
      "Step: 826, Training Loss: 0.48861, LR: 0.0000851, Tokens/sec: 52773.34\n",
      "Step: 827, Training Loss: 0.43350, LR: 0.0000851, Tokens/sec: 51273.34\n",
      "Step: 828, Training Loss: 0.52240, LR: 0.0000851, Tokens/sec: 49568.15\n",
      "Step: 829, Training Loss: 0.44011, LR: 0.0000850, Tokens/sec: 48841.63\n",
      "Step: 830, Training Loss: 0.39340, LR: 0.0000850, Tokens/sec: 52249.51\n",
      "Step: 831, Training Loss: 0.52270, LR: 0.0000849, Tokens/sec: 50835.57\n",
      "Step: 832, Training Loss: 0.34431, LR: 0.0000849, Tokens/sec: 50586.56\n",
      "Step: 833, Training Loss: 0.36827, LR: 0.0000848, Tokens/sec: 49811.08\n",
      "Step: 834, Training Loss: 0.38834, LR: 0.0000848, Tokens/sec: 51772.93\n",
      "Step: 835, Training Loss: 0.40929, LR: 0.0000847, Tokens/sec: 50771.33\n",
      "Step: 836, Training Loss: 0.33622, LR: 0.0000847, Tokens/sec: 51558.89\n",
      "Step: 837, Training Loss: 0.28062, LR: 0.0000846, Tokens/sec: 51157.26\n",
      "Step: 838, Training Loss: 0.41394, LR: 0.0000846, Tokens/sec: 49276.62\n",
      "Step: 839, Training Loss: 0.42468, LR: 0.0000845, Tokens/sec: 50239.14\n",
      "Step: 840, Training Loss: 0.32357, LR: 0.0000845, Tokens/sec: 50047.59\n",
      "Step: 841, Training Loss: 0.35868, LR: 0.0000844, Tokens/sec: 50092.17\n",
      "Step: 842, Training Loss: 0.35685, LR: 0.0000844, Tokens/sec: 51044.86\n",
      "Step: 843, Training Loss: 0.30069, LR: 0.0000843, Tokens/sec: 52003.43\n",
      "Step: 844, Training Loss: 0.39436, LR: 0.0000843, Tokens/sec: 51820.87\n",
      "Step: 845, Training Loss: 0.49187, LR: 0.0000842, Tokens/sec: 49107.89\n",
      "Step: 846, Training Loss: 0.28741, LR: 0.0000842, Tokens/sec: 51816.88\n",
      "Step: 847, Training Loss: 0.37982, LR: 0.0000841, Tokens/sec: 50806.90\n",
      "Step: 848, Training Loss: 0.50603, LR: 0.0000841, Tokens/sec: 51675.57\n",
      "Step: 849, Training Loss: 0.30639, LR: 0.0000840, Tokens/sec: 51447.52\n",
      "Step: 850, Training Loss: 0.34962, LR: 0.0000840, Tokens/sec: 51731.63\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 850, Eval Loss: 0.39895\n",
      "Step: 851, Training Loss: 0.28956, LR: 0.0000839, Tokens/sec: 51922.75\n",
      "Step: 852, Training Loss: 0.34411, LR: 0.0000839, Tokens/sec: 51445.90\n",
      "Step: 853, Training Loss: 0.35169, LR: 0.0000838, Tokens/sec: 48717.64\n",
      "Step: 854, Training Loss: 0.32138, LR: 0.0000838, Tokens/sec: 52226.89\n",
      "Step: 855, Training Loss: 0.28557, LR: 0.0000837, Tokens/sec: 49408.74\n",
      "Step: 856, Training Loss: 0.34251, LR: 0.0000837, Tokens/sec: 50678.77\n",
      "Step: 857, Training Loss: 0.58775, LR: 0.0000836, Tokens/sec: 50647.22\n",
      "Step: 858, Training Loss: 0.34489, LR: 0.0000836, Tokens/sec: 51051.42\n",
      "Step: 859, Training Loss: 0.51420, LR: 0.0000835, Tokens/sec: 49929.17\n",
      "Step: 860, Training Loss: 0.36570, LR: 0.0000835, Tokens/sec: 51172.29\n",
      "Step: 861, Training Loss: 0.30309, LR: 0.0000834, Tokens/sec: 50070.05\n",
      "Step: 862, Training Loss: 0.50339, LR: 0.0000834, Tokens/sec: 48668.10\n",
      "Step: 863, Training Loss: 0.41625, LR: 0.0000833, Tokens/sec: 50913.76\n",
      "Step: 864, Training Loss: 0.31818, LR: 0.0000833, Tokens/sec: 51159.05\n",
      "Step: 865, Training Loss: 0.29611, LR: 0.0000832, Tokens/sec: 50250.15\n",
      "Step: 866, Training Loss: 0.33570, LR: 0.0000832, Tokens/sec: 50791.73\n",
      "Step: 867, Training Loss: 0.36836, LR: 0.0000831, Tokens/sec: 50826.94\n",
      "Step: 868, Training Loss: 0.23992, LR: 0.0000831, Tokens/sec: 50863.16\n",
      "Step: 869, Training Loss: 0.34778, LR: 0.0000830, Tokens/sec: 49675.38\n",
      "Step: 870, Training Loss: 0.35748, LR: 0.0000830, Tokens/sec: 51114.63\n",
      "Step: 871, Training Loss: 0.31267, LR: 0.0000829, Tokens/sec: 48580.24\n",
      "Step: 872, Training Loss: 0.27240, LR: 0.0000829, Tokens/sec: 49734.04\n",
      "Step: 873, Training Loss: 0.29922, LR: 0.0000828, Tokens/sec: 47813.44\n",
      "Step: 874, Training Loss: 0.32681, LR: 0.0000828, Tokens/sec: 51279.18\n",
      "Step: 875, Training Loss: 0.31974, LR: 0.0000827, Tokens/sec: 50131.98\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 875, Eval Loss: 0.26338\n",
      "Step: 876, Training Loss: 0.22986, LR: 0.0000827, Tokens/sec: 51815.26\n",
      "Step: 877, Training Loss: 0.25934, LR: 0.0000826, Tokens/sec: 49725.69\n",
      "Step: 878, Training Loss: 0.20821, LR: 0.0000826, Tokens/sec: 51568.70\n",
      "Step: 879, Training Loss: 0.24644, LR: 0.0000825, Tokens/sec: 49470.28\n",
      "Step: 880, Training Loss: 0.24776, LR: 0.0000825, Tokens/sec: 51753.50\n",
      "Step: 881, Training Loss: 0.39455, LR: 0.0000824, Tokens/sec: 51874.43\n",
      "Step: 882, Training Loss: 0.24138, LR: 0.0000824, Tokens/sec: 52685.56\n",
      "Step: 883, Training Loss: 0.26179, LR: 0.0000823, Tokens/sec: 49941.72\n",
      "Step: 884, Training Loss: 0.36225, LR: 0.0000823, Tokens/sec: 50847.28\n",
      "Step: 885, Training Loss: 0.30575, LR: 0.0000822, Tokens/sec: 51695.17\n",
      "Step: 886, Training Loss: 0.30209, LR: 0.0000822, Tokens/sec: 49213.27\n",
      "Step: 887, Training Loss: 0.36938, LR: 0.0000821, Tokens/sec: 50791.37\n",
      "Step: 888, Training Loss: 0.36817, LR: 0.0000821, Tokens/sec: 48475.56\n",
      "Step: 889, Training Loss: 0.28316, LR: 0.0000820, Tokens/sec: 50901.62\n",
      "Step: 890, Training Loss: 0.30123, LR: 0.0000820, Tokens/sec: 50926.05\n",
      "Step: 891, Training Loss: 0.27204, LR: 0.0000819, Tokens/sec: 51491.39\n",
      "Step: 892, Training Loss: 0.27569, LR: 0.0000819, Tokens/sec: 50521.93\n",
      "Step: 893, Training Loss: 0.25748, LR: 0.0000818, Tokens/sec: 48569.52\n",
      "Step: 894, Training Loss: 0.29363, LR: 0.0000817, Tokens/sec: 50284.45\n",
      "Step: 895, Training Loss: 0.30804, LR: 0.0000817, Tokens/sec: 49428.48\n",
      "Step: 896, Training Loss: 0.27201, LR: 0.0000816, Tokens/sec: 50675.68\n",
      "Step: 897, Training Loss: 0.26991, LR: 0.0000816, Tokens/sec: 50543.87\n",
      "Step: 898, Training Loss: 0.28540, LR: 0.0000815, Tokens/sec: 51785.37\n",
      "Step: 899, Training Loss: 0.32286, LR: 0.0000815, Tokens/sec: 50573.59\n",
      "Step: 900, Training Loss: 0.32791, LR: 0.0000814, Tokens/sec: 51497.86\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 900, Eval Loss: 0.29009\n",
      "Step: 901, Training Loss: 0.26931, LR: 0.0000814, Tokens/sec: 51508.53\n",
      "Step: 902, Training Loss: 0.24296, LR: 0.0000813, Tokens/sec: 50036.49\n",
      "Step: 903, Training Loss: 0.18764, LR: 0.0000813, Tokens/sec: 49982.21\n",
      "Step: 904, Training Loss: 0.35635, LR: 0.0000812, Tokens/sec: 50563.67\n",
      "Step: 905, Training Loss: 0.27752, LR: 0.0000812, Tokens/sec: 49742.46\n",
      "Step: 906, Training Loss: 0.27825, LR: 0.0000811, Tokens/sec: 50648.42\n",
      "Step: 907, Training Loss: 0.22112, LR: 0.0000811, Tokens/sec: 51350.79\n",
      "Step: 908, Training Loss: 0.29992, LR: 0.0000810, Tokens/sec: 49141.49\n",
      "Step: 909, Training Loss: 0.22441, LR: 0.0000810, Tokens/sec: 50625.04\n",
      "Step: 910, Training Loss: 0.23807, LR: 0.0000809, Tokens/sec: 47918.24\n",
      "Step: 911, Training Loss: 0.24318, LR: 0.0000809, Tokens/sec: 50256.95\n",
      "Step: 912, Training Loss: 0.20817, LR: 0.0000808, Tokens/sec: 47948.20\n",
      "Step: 913, Training Loss: 0.23691, LR: 0.0000808, Tokens/sec: 50487.30\n",
      "Step: 914, Training Loss: 0.20488, LR: 0.0000807, Tokens/sec: 50109.12\n",
      "Step: 915, Training Loss: 0.18010, LR: 0.0000806, Tokens/sec: 51785.59\n",
      "Step: 916, Training Loss: 0.18133, LR: 0.0000806, Tokens/sec: 52201.14\n",
      "Step: 917, Training Loss: 0.21165, LR: 0.0000805, Tokens/sec: 50280.57\n",
      "Step: 918, Training Loss: 0.20396, LR: 0.0000805, Tokens/sec: 50483.04\n",
      "Step: 919, Training Loss: 0.15653, LR: 0.0000804, Tokens/sec: 50112.97\n",
      "Step: 920, Training Loss: 0.17152, LR: 0.0000804, Tokens/sec: 50484.26\n",
      "Step: 921, Training Loss: 0.21290, LR: 0.0000803, Tokens/sec: 48661.03\n",
      "Step: 922, Training Loss: 0.15138, LR: 0.0000803, Tokens/sec: 48980.25\n",
      "Step: 923, Training Loss: 0.21469, LR: 0.0000802, Tokens/sec: 51641.39\n",
      "Step: 924, Training Loss: 0.20735, LR: 0.0000802, Tokens/sec: 51547.29\n",
      "Step: 925, Training Loss: 0.18813, LR: 0.0000801, Tokens/sec: 50766.80\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 925, Eval Loss: 0.22248\n",
      "Step: 926, Training Loss: 0.29933, LR: 0.0000801, Tokens/sec: 47158.34\n",
      "Step: 927, Training Loss: 0.22615, LR: 0.0000800, Tokens/sec: 49926.79\n",
      "Step: 928, Training Loss: 0.26385, LR: 0.0000800, Tokens/sec: 50422.15\n",
      "Step: 929, Training Loss: 0.27503, LR: 0.0000799, Tokens/sec: 49099.53\n",
      "Step: 930, Training Loss: 0.30041, LR: 0.0000798, Tokens/sec: 49872.33\n",
      "Step: 931, Training Loss: 0.16241, LR: 0.0000798, Tokens/sec: 50050.77\n",
      "Step: 932, Training Loss: 0.23417, LR: 0.0000797, Tokens/sec: 50965.01\n",
      "Step: 933, Training Loss: 0.17767, LR: 0.0000797, Tokens/sec: 52328.42\n",
      "Step: 934, Training Loss: 0.20999, LR: 0.0000796, Tokens/sec: 49676.10\n",
      "Step: 935, Training Loss: 0.23984, LR: 0.0000796, Tokens/sec: 50404.79\n",
      "Step: 936, Training Loss: 0.18762, LR: 0.0000795, Tokens/sec: 49981.83\n",
      "Step: 937, Training Loss: 0.18246, LR: 0.0000795, Tokens/sec: 52113.96\n",
      "Step: 938, Training Loss: 0.26462, LR: 0.0000794, Tokens/sec: 49429.92\n",
      "Step: 939, Training Loss: 0.27392, LR: 0.0000794, Tokens/sec: 49641.45\n",
      "Step: 940, Training Loss: 0.16759, LR: 0.0000793, Tokens/sec: 51605.76\n",
      "Step: 941, Training Loss: 0.21719, LR: 0.0000792, Tokens/sec: 52016.95\n",
      "Step: 942, Training Loss: 0.15166, LR: 0.0000792, Tokens/sec: 49162.39\n",
      "Step: 943, Training Loss: 0.18426, LR: 0.0000791, Tokens/sec: 48967.54\n",
      "Step: 944, Training Loss: 0.25362, LR: 0.0000791, Tokens/sec: 49830.80\n",
      "Step: 945, Training Loss: 0.22803, LR: 0.0000790, Tokens/sec: 49852.08\n",
      "Step: 946, Training Loss: 0.18667, LR: 0.0000790, Tokens/sec: 49586.29\n",
      "Step: 947, Training Loss: 0.21017, LR: 0.0000789, Tokens/sec: 50682.23\n",
      "Step: 948, Training Loss: 0.16438, LR: 0.0000789, Tokens/sec: 50280.39\n",
      "Step: 949, Training Loss: 0.17982, LR: 0.0000788, Tokens/sec: 50800.58\n",
      "Step: 950, Training Loss: 0.15082, LR: 0.0000788, Tokens/sec: 51922.06\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 950, Eval Loss: 0.16118\n",
      "Step: 951, Training Loss: 0.16662, LR: 0.0000787, Tokens/sec: 51290.07\n",
      "Step: 952, Training Loss: 0.18107, LR: 0.0000786, Tokens/sec: 50510.64\n",
      "Step: 953, Training Loss: 0.15056, LR: 0.0000786, Tokens/sec: 48945.59\n",
      "Step: 954, Training Loss: 0.18011, LR: 0.0000785, Tokens/sec: 50742.33\n",
      "Step: 955, Training Loss: 0.16202, LR: 0.0000785, Tokens/sec: 49059.62\n",
      "Step: 956, Training Loss: 0.16095, LR: 0.0000784, Tokens/sec: 52880.76\n",
      "Step: 957, Training Loss: 0.17369, LR: 0.0000784, Tokens/sec: 51552.78\n",
      "Step: 958, Training Loss: 0.13470, LR: 0.0000783, Tokens/sec: 51438.81\n",
      "Step: 959, Training Loss: 0.26764, LR: 0.0000783, Tokens/sec: 51035.71\n",
      "Step: 960, Training Loss: 0.07990, LR: 0.0000782, Tokens/sec: 48641.57\n",
      "Step: 961, Training Loss: 0.17493, LR: 0.0000781, Tokens/sec: 49873.15\n",
      "Step: 962, Training Loss: 0.13035, LR: 0.0000781, Tokens/sec: 48778.30\n",
      "Step: 963, Training Loss: 0.19166, LR: 0.0000780, Tokens/sec: 50586.04\n",
      "Step: 964, Training Loss: 0.13347, LR: 0.0000780, Tokens/sec: 51476.42\n",
      "Step: 965, Training Loss: 0.12787, LR: 0.0000779, Tokens/sec: 50483.26\n",
      "Step: 966, Training Loss: 0.12124, LR: 0.0000779, Tokens/sec: 50470.75\n",
      "Step: 967, Training Loss: 0.12828, LR: 0.0000778, Tokens/sec: 49560.04\n",
      "Step: 968, Training Loss: 0.20507, LR: 0.0000778, Tokens/sec: 52031.99\n",
      "Step: 969, Training Loss: 0.20354, LR: 0.0000777, Tokens/sec: 48887.10\n",
      "Step: 970, Training Loss: 0.12683, LR: 0.0000776, Tokens/sec: 50841.28\n",
      "Step: 971, Training Loss: 0.17750, LR: 0.0000776, Tokens/sec: 48932.41\n",
      "Step: 972, Training Loss: 0.14239, LR: 0.0000775, Tokens/sec: 51810.42\n",
      "Step: 973, Training Loss: 0.18526, LR: 0.0000775, Tokens/sec: 50917.20\n",
      "Step: 974, Training Loss: 0.12954, LR: 0.0000774, Tokens/sec: 51191.80\n",
      "Step: 975, Training Loss: 0.17525, LR: 0.0000774, Tokens/sec: 49055.80\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 975, Eval Loss: 0.18236\n",
      "Step: 976, Training Loss: 0.12428, LR: 0.0000773, Tokens/sec: 49799.47\n",
      "Step: 977, Training Loss: 0.17013, LR: 0.0000773, Tokens/sec: 50023.41\n",
      "Step: 978, Training Loss: 0.09382, LR: 0.0000772, Tokens/sec: 51141.41\n",
      "Step: 979, Training Loss: 0.16243, LR: 0.0000771, Tokens/sec: 48621.35\n",
      "Step: 980, Training Loss: 0.08286, LR: 0.0000771, Tokens/sec: 48916.82\n",
      "Step: 981, Training Loss: 0.10970, LR: 0.0000770, Tokens/sec: 50669.12\n",
      "Step: 982, Training Loss: 0.15331, LR: 0.0000770, Tokens/sec: 50458.94\n",
      "Step: 983, Training Loss: 0.09949, LR: 0.0000769, Tokens/sec: 51360.86\n",
      "Step: 984, Training Loss: 0.20556, LR: 0.0000769, Tokens/sec: 51245.19\n",
      "Step: 985, Training Loss: 0.15617, LR: 0.0000768, Tokens/sec: 49969.83\n",
      "Step: 986, Training Loss: 0.16503, LR: 0.0000768, Tokens/sec: 48832.03\n",
      "Step: 987, Training Loss: 0.07957, LR: 0.0000767, Tokens/sec: 49532.85\n",
      "Step: 988, Training Loss: 0.12969, LR: 0.0000766, Tokens/sec: 49272.59\n",
      "Step: 989, Training Loss: 0.21356, LR: 0.0000766, Tokens/sec: 50864.86\n",
      "Step: 990, Training Loss: 0.08985, LR: 0.0000765, Tokens/sec: 50988.47\n",
      "Step: 991, Training Loss: 0.14418, LR: 0.0000765, Tokens/sec: 49250.85\n",
      "Step: 992, Training Loss: 0.08029, LR: 0.0000764, Tokens/sec: 49959.53\n",
      "Step: 993, Training Loss: 0.13895, LR: 0.0000764, Tokens/sec: 49305.29\n",
      "Step: 994, Training Loss: 0.15244, LR: 0.0000763, Tokens/sec: 50746.49\n",
      "Step: 995, Training Loss: 0.18133, LR: 0.0000762, Tokens/sec: 49735.96\n",
      "Step: 996, Training Loss: 0.09532, LR: 0.0000762, Tokens/sec: 49880.28\n",
      "Step: 997, Training Loss: 0.10988, LR: 0.0000761, Tokens/sec: 50839.41\n",
      "Step: 998, Training Loss: 0.13555, LR: 0.0000761, Tokens/sec: 50929.04\n",
      "Step: 999, Training Loss: 0.13065, LR: 0.0000760, Tokens/sec: 51485.16\n",
      "Step: 1000, Training Loss: 0.16541, LR: 0.0000760, Tokens/sec: 51957.08\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 1000, Eval Loss: 0.11967\n",
      "Step: 1001, Training Loss: 0.17572, LR: 0.0000759, Tokens/sec: 51983.36\n",
      "Step: 1002, Training Loss: 0.14166, LR: 0.0000758, Tokens/sec: 50194.45\n",
      "Step: 1003, Training Loss: 0.16523, LR: 0.0000758, Tokens/sec: 49233.97\n",
      "Step: 1004, Training Loss: 0.05396, LR: 0.0000757, Tokens/sec: 50238.26\n",
      "Step: 1005, Training Loss: 0.14481, LR: 0.0000757, Tokens/sec: 49829.28\n",
      "Step: 1006, Training Loss: 0.09672, LR: 0.0000756, Tokens/sec: 50490.85\n",
      "Step: 1007, Training Loss: 0.12890, LR: 0.0000756, Tokens/sec: 50500.30\n",
      "Step: 1008, Training Loss: 0.10381, LR: 0.0000755, Tokens/sec: 50853.15\n",
      "Step: 1009, Training Loss: 0.05576, LR: 0.0000754, Tokens/sec: 49566.54\n",
      "Step: 1010, Training Loss: 0.09157, LR: 0.0000754, Tokens/sec: 44305.49\n",
      "Step: 1011, Training Loss: 0.09818, LR: 0.0000753, Tokens/sec: 49810.69\n",
      "Step: 1012, Training Loss: 0.23479, LR: 0.0000753, Tokens/sec: 49596.71\n",
      "Step: 1013, Training Loss: 0.08865, LR: 0.0000752, Tokens/sec: 49536.41\n",
      "Step: 1014, Training Loss: 0.14243, LR: 0.0000751, Tokens/sec: 50142.40\n",
      "Step: 1015, Training Loss: 0.06643, LR: 0.0000751, Tokens/sec: 51170.80\n",
      "Step: 1016, Training Loss: 0.15267, LR: 0.0000750, Tokens/sec: 49849.27\n",
      "Step: 1017, Training Loss: 0.15975, LR: 0.0000750, Tokens/sec: 51185.46\n",
      "Step: 1018, Training Loss: 0.09829, LR: 0.0000749, Tokens/sec: 51663.39\n",
      "Step: 1019, Training Loss: 0.12215, LR: 0.0000749, Tokens/sec: 49486.46\n",
      "Step: 1020, Training Loss: 0.10802, LR: 0.0000748, Tokens/sec: 51506.81\n",
      "Step: 1021, Training Loss: 0.11250, LR: 0.0000747, Tokens/sec: 48848.01\n",
      "Step: 1022, Training Loss: 0.14000, LR: 0.0000747, Tokens/sec: 49673.70\n",
      "Step: 1023, Training Loss: 0.16352, LR: 0.0000746, Tokens/sec: 49807.15\n",
      "Step: 1024, Training Loss: 0.14398, LR: 0.0000746, Tokens/sec: 51663.00\n",
      "Step: 1025, Training Loss: 0.12444, LR: 0.0000745, Tokens/sec: 51680.33\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 1025, Eval Loss: 0.09185\n",
      "Step: 1026, Training Loss: 0.09947, LR: 0.0000745, Tokens/sec: 50438.70\n",
      "Step: 1027, Training Loss: 0.12328, LR: 0.0000744, Tokens/sec: 49919.39\n",
      "Step: 1028, Training Loss: 0.06384, LR: 0.0000743, Tokens/sec: 49962.80\n",
      "Step: 1029, Training Loss: 0.11066, LR: 0.0000743, Tokens/sec: 49375.01\n",
      "Step: 1030, Training Loss: 0.09067, LR: 0.0000742, Tokens/sec: 49901.50\n",
      "Step: 1031, Training Loss: 0.07938, LR: 0.0000742, Tokens/sec: 49705.01\n",
      "Step: 1032, Training Loss: 0.09999, LR: 0.0000741, Tokens/sec: 50180.96\n",
      "Step: 1033, Training Loss: 0.08573, LR: 0.0000740, Tokens/sec: 50699.70\n",
      "Step: 1034, Training Loss: 0.11083, LR: 0.0000740, Tokens/sec: 46856.33\n",
      "Step: 1035, Training Loss: 0.12202, LR: 0.0000739, Tokens/sec: 48909.04\n",
      "Step: 1036, Training Loss: 0.10380, LR: 0.0000739, Tokens/sec: 49679.58\n",
      "Step: 1037, Training Loss: 0.13227, LR: 0.0000738, Tokens/sec: 50267.20\n",
      "Step: 1038, Training Loss: 0.11166, LR: 0.0000738, Tokens/sec: 49471.91\n",
      "Step: 1039, Training Loss: 0.11377, LR: 0.0000737, Tokens/sec: 51051.50\n",
      "Step: 1040, Training Loss: 0.08743, LR: 0.0000736, Tokens/sec: 48417.11\n",
      "Step: 1041, Training Loss: 0.12874, LR: 0.0000736, Tokens/sec: 49592.34\n",
      "Step: 1042, Training Loss: 0.11612, LR: 0.0000735, Tokens/sec: 50306.78\n",
      "Step: 1043, Training Loss: 0.08558, LR: 0.0000735, Tokens/sec: 49684.65\n",
      "Step: 1044, Training Loss: 0.08643, LR: 0.0000734, Tokens/sec: 49748.85\n",
      "Step: 1045, Training Loss: 0.14827, LR: 0.0000733, Tokens/sec: 50979.41\n",
      "Step: 1046, Training Loss: 0.11775, LR: 0.0000733, Tokens/sec: 49914.30\n",
      "Step: 1047, Training Loss: 0.10152, LR: 0.0000732, Tokens/sec: 48411.25\n",
      "Step: 1048, Training Loss: 0.14140, LR: 0.0000732, Tokens/sec: 50190.50\n",
      "Step: 1049, Training Loss: 0.07486, LR: 0.0000731, Tokens/sec: 51422.24\n",
      "Step: 1050, Training Loss: 0.11383, LR: 0.0000730, Tokens/sec: 50635.68\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 1050, Eval Loss: 0.09098\n",
      "Step: 1051, Training Loss: 0.07561, LR: 0.0000730, Tokens/sec: 50343.18\n",
      "Step: 1052, Training Loss: 0.06002, LR: 0.0000729, Tokens/sec: 49372.46\n",
      "Step: 1053, Training Loss: 0.08141, LR: 0.0000729, Tokens/sec: 50207.97\n",
      "Step: 1054, Training Loss: 0.07280, LR: 0.0000728, Tokens/sec: 50134.16\n",
      "Step: 1055, Training Loss: 0.10327, LR: 0.0000727, Tokens/sec: 49219.33\n",
      "Step: 1056, Training Loss: 0.08610, LR: 0.0000727, Tokens/sec: 50162.15\n",
      "Step: 1057, Training Loss: 0.06031, LR: 0.0000726, Tokens/sec: 48608.51\n",
      "Step: 1058, Training Loss: 0.08538, LR: 0.0000726, Tokens/sec: 51268.45\n",
      "Step: 1059, Training Loss: 0.09152, LR: 0.0000725, Tokens/sec: 51497.49\n",
      "Step: 1060, Training Loss: 0.05857, LR: 0.0000725, Tokens/sec: 49076.18\n",
      "Step: 1061, Training Loss: 0.06923, LR: 0.0000724, Tokens/sec: 50407.30\n",
      "Step: 1062, Training Loss: 0.06739, LR: 0.0000723, Tokens/sec: 49564.37\n",
      "Step: 1063, Training Loss: 0.08922, LR: 0.0000723, Tokens/sec: 50372.98\n",
      "Step: 1064, Training Loss: 0.07557, LR: 0.0000722, Tokens/sec: 46672.08\n",
      "Step: 1065, Training Loss: 0.04315, LR: 0.0000722, Tokens/sec: 50292.89\n",
      "Step: 1066, Training Loss: 0.10181, LR: 0.0000721, Tokens/sec: 49491.85\n",
      "Step: 1067, Training Loss: 0.08976, LR: 0.0000720, Tokens/sec: 50604.71\n",
      "Step: 1068, Training Loss: 0.06198, LR: 0.0000720, Tokens/sec: 51637.27\n",
      "Step: 1069, Training Loss: 0.08210, LR: 0.0000719, Tokens/sec: 50600.19\n",
      "Step: 1070, Training Loss: 0.06887, LR: 0.0000719, Tokens/sec: 49730.63\n",
      "Step: 1071, Training Loss: 0.07271, LR: 0.0000718, Tokens/sec: 49767.50\n",
      "Step: 1072, Training Loss: 0.08666, LR: 0.0000717, Tokens/sec: 49148.93\n",
      "Step: 1073, Training Loss: 0.11520, LR: 0.0000717, Tokens/sec: 49719.75\n",
      "Step: 1074, Training Loss: 0.06957, LR: 0.0000716, Tokens/sec: 50276.05\n",
      "Step: 1075, Training Loss: 0.06914, LR: 0.0000716, Tokens/sec: 48747.80\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 1075, Eval Loss: 0.06818\n",
      "Step: 1076, Training Loss: 0.07998, LR: 0.0000715, Tokens/sec: 49713.11\n",
      "Step: 1077, Training Loss: 0.05044, LR: 0.0000714, Tokens/sec: 50604.82\n",
      "Step: 1078, Training Loss: 0.07450, LR: 0.0000714, Tokens/sec: 49528.99\n",
      "Step: 1079, Training Loss: 0.04765, LR: 0.0000713, Tokens/sec: 50014.39\n",
      "Step: 1080, Training Loss: 0.08376, LR: 0.0000713, Tokens/sec: 51870.77\n",
      "Step: 1081, Training Loss: 0.05062, LR: 0.0000712, Tokens/sec: 49972.22\n",
      "Step: 1082, Training Loss: 0.06799, LR: 0.0000711, Tokens/sec: 49643.13\n",
      "Step: 1083, Training Loss: 0.05511, LR: 0.0000711, Tokens/sec: 48893.18\n",
      "Step: 1084, Training Loss: 0.08661, LR: 0.0000710, Tokens/sec: 51345.24\n",
      "Step: 1085, Training Loss: 0.06609, LR: 0.0000710, Tokens/sec: 48889.23\n",
      "Step: 1086, Training Loss: 0.07506, LR: 0.0000709, Tokens/sec: 51894.52\n",
      "Step: 1087, Training Loss: 0.06090, LR: 0.0000708, Tokens/sec: 51850.22\n",
      "Step: 1088, Training Loss: 0.09809, LR: 0.0000708, Tokens/sec: 48869.93\n",
      "Step: 1089, Training Loss: 0.05915, LR: 0.0000707, Tokens/sec: 48697.40\n",
      "Step: 1090, Training Loss: 0.06355, LR: 0.0000707, Tokens/sec: 47617.42\n",
      "Step: 1091, Training Loss: 0.05924, LR: 0.0000706, Tokens/sec: 48719.45\n",
      "Step: 1092, Training Loss: 0.07075, LR: 0.0000705, Tokens/sec: 49658.77\n",
      "Step: 1093, Training Loss: 0.07442, LR: 0.0000705, Tokens/sec: 50514.18\n",
      "Step: 1094, Training Loss: 0.07750, LR: 0.0000704, Tokens/sec: 50738.66\n",
      "Step: 1095, Training Loss: 0.05849, LR: 0.0000703, Tokens/sec: 50476.43\n",
      "Step: 1096, Training Loss: 0.06246, LR: 0.0000703, Tokens/sec: 51393.92\n",
      "Step: 1097, Training Loss: 0.07715, LR: 0.0000702, Tokens/sec: 51269.49\n",
      "Step: 1098, Training Loss: 0.10086, LR: 0.0000702, Tokens/sec: 48276.99\n",
      "Step: 1099, Training Loss: 0.09271, LR: 0.0000701, Tokens/sec: 49700.25\n",
      "Step: 1100, Training Loss: 0.12207, LR: 0.0000700, Tokens/sec: 50036.58\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 1100, Eval Loss: 0.07097\n",
      "Step: 1101, Training Loss: 0.07904, LR: 0.0000700, Tokens/sec: 50802.75\n",
      "Step: 1102, Training Loss: 0.05472, LR: 0.0000699, Tokens/sec: 48893.60\n",
      "Step: 1103, Training Loss: 0.05977, LR: 0.0000699, Tokens/sec: 49623.42\n",
      "Step: 1104, Training Loss: 0.05721, LR: 0.0000698, Tokens/sec: 50310.44\n",
      "Step: 1105, Training Loss: 0.05967, LR: 0.0000697, Tokens/sec: 49301.72\n",
      "Step: 1106, Training Loss: 0.07930, LR: 0.0000697, Tokens/sec: 51704.32\n",
      "Step: 1107, Training Loss: 0.06163, LR: 0.0000696, Tokens/sec: 49527.20\n",
      "Step: 1108, Training Loss: 0.08109, LR: 0.0000696, Tokens/sec: 50146.19\n",
      "Step: 1109, Training Loss: 0.04905, LR: 0.0000695, Tokens/sec: 48075.22\n",
      "Step: 1110, Training Loss: 0.07543, LR: 0.0000694, Tokens/sec: 50151.72\n",
      "Step: 1111, Training Loss: 0.05839, LR: 0.0000694, Tokens/sec: 49292.59\n",
      "Step: 1112, Training Loss: 0.07073, LR: 0.0000693, Tokens/sec: 49765.97\n",
      "Step: 1113, Training Loss: 0.06554, LR: 0.0000692, Tokens/sec: 51205.58\n",
      "Step: 1114, Training Loss: 0.05924, LR: 0.0000692, Tokens/sec: 50301.94\n",
      "Step: 1115, Training Loss: 0.06126, LR: 0.0000691, Tokens/sec: 50034.83\n",
      "Step: 1116, Training Loss: 0.07154, LR: 0.0000691, Tokens/sec: 49609.46\n",
      "Step: 1117, Training Loss: 0.06085, LR: 0.0000690, Tokens/sec: 49115.72\n",
      "Step: 1118, Training Loss: 0.07539, LR: 0.0000689, Tokens/sec: 48400.86\n",
      "Step: 1119, Training Loss: 0.06725, LR: 0.0000689, Tokens/sec: 51034.55\n",
      "Step: 1120, Training Loss: 0.04785, LR: 0.0000688, Tokens/sec: 50563.40\n",
      "Step: 1121, Training Loss: 0.06854, LR: 0.0000688, Tokens/sec: 49536.38\n",
      "Step: 1122, Training Loss: 0.08332, LR: 0.0000687, Tokens/sec: 51535.72\n",
      "Step: 1123, Training Loss: 0.07021, LR: 0.0000686, Tokens/sec: 50062.28\n",
      "Step: 1124, Training Loss: 0.08628, LR: 0.0000686, Tokens/sec: 49894.19\n",
      "Step: 1125, Training Loss: 0.06236, LR: 0.0000685, Tokens/sec: 49285.77\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 1125, Eval Loss: 0.06989\n",
      "Step: 1126, Training Loss: 0.03855, LR: 0.0000685, Tokens/sec: 50143.67\n",
      "Step: 1127, Training Loss: 0.04671, LR: 0.0000684, Tokens/sec: 50077.39\n",
      "Step: 1128, Training Loss: 0.07083, LR: 0.0000683, Tokens/sec: 48164.83\n",
      "Step: 1129, Training Loss: 0.10237, LR: 0.0000683, Tokens/sec: 50907.64\n",
      "Step: 1130, Training Loss: 0.04987, LR: 0.0000682, Tokens/sec: 51698.70\n",
      "Step: 1131, Training Loss: 0.05810, LR: 0.0000681, Tokens/sec: 51058.09\n",
      "Step: 1132, Training Loss: 0.03430, LR: 0.0000681, Tokens/sec: 51368.30\n",
      "Step: 1133, Training Loss: 0.06173, LR: 0.0000680, Tokens/sec: 49388.33\n",
      "Step: 1134, Training Loss: 0.06340, LR: 0.0000680, Tokens/sec: 52852.87\n",
      "Step: 1135, Training Loss: 0.05092, LR: 0.0000679, Tokens/sec: 48270.71\n",
      "Step: 1136, Training Loss: 0.04290, LR: 0.0000678, Tokens/sec: 50146.05\n",
      "Step: 1137, Training Loss: 0.04441, LR: 0.0000678, Tokens/sec: 49588.53\n",
      "Step: 1138, Training Loss: 0.05998, LR: 0.0000677, Tokens/sec: 50920.63\n",
      "Step: 1139, Training Loss: 0.06863, LR: 0.0000676, Tokens/sec: 49923.32\n",
      "Step: 1140, Training Loss: 0.08491, LR: 0.0000676, Tokens/sec: 49456.80\n",
      "Step: 1141, Training Loss: 0.04628, LR: 0.0000675, Tokens/sec: 50499.88\n",
      "Step: 1142, Training Loss: 0.05143, LR: 0.0000675, Tokens/sec: 49658.81\n",
      "Step: 1143, Training Loss: 0.04229, LR: 0.0000674, Tokens/sec: 50060.81\n",
      "Step: 1144, Training Loss: 0.05827, LR: 0.0000673, Tokens/sec: 49580.78\n",
      "Step: 1145, Training Loss: 0.06272, LR: 0.0000673, Tokens/sec: 49843.72\n",
      "Step: 1146, Training Loss: 0.05047, LR: 0.0000672, Tokens/sec: 50324.84\n",
      "Step: 1147, Training Loss: 0.05340, LR: 0.0000672, Tokens/sec: 51562.22\n",
      "Step: 1148, Training Loss: 0.04349, LR: 0.0000671, Tokens/sec: 51635.10\n",
      "Step: 1149, Training Loss: 0.04637, LR: 0.0000670, Tokens/sec: 47110.42\n",
      "Step: 1150, Training Loss: 0.04377, LR: 0.0000670, Tokens/sec: 50382.86\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 1150, Eval Loss: 0.05223\n",
      "Step: 1151, Training Loss: 0.07019, LR: 0.0000669, Tokens/sec: 51056.00\n",
      "Step: 1152, Training Loss: 0.07442, LR: 0.0000668, Tokens/sec: 48610.66\n",
      "Step: 1153, Training Loss: 0.05461, LR: 0.0000668, Tokens/sec: 51023.43\n",
      "Step: 1154, Training Loss: 0.04359, LR: 0.0000667, Tokens/sec: 49073.27\n",
      "Step: 1155, Training Loss: 0.05009, LR: 0.0000667, Tokens/sec: 50979.23\n",
      "Step: 1156, Training Loss: 0.04304, LR: 0.0000666, Tokens/sec: 50174.91\n",
      "Step: 1157, Training Loss: 0.03745, LR: 0.0000665, Tokens/sec: 51859.52\n",
      "Step: 1158, Training Loss: 0.06384, LR: 0.0000665, Tokens/sec: 50192.09\n",
      "Step: 1159, Training Loss: 0.03996, LR: 0.0000664, Tokens/sec: 47908.01\n",
      "Step: 1160, Training Loss: 0.04211, LR: 0.0000663, Tokens/sec: 50247.92\n",
      "Step: 1161, Training Loss: 0.04917, LR: 0.0000663, Tokens/sec: 49344.87\n",
      "Step: 1162, Training Loss: 0.05540, LR: 0.0000662, Tokens/sec: 51176.16\n",
      "Step: 1163, Training Loss: 0.05269, LR: 0.0000662, Tokens/sec: 51235.66\n",
      "Step: 1164, Training Loss: 0.08690, LR: 0.0000661, Tokens/sec: 51394.67\n",
      "Step: 1165, Training Loss: 0.04667, LR: 0.0000660, Tokens/sec: 50330.82\n",
      "Step: 1166, Training Loss: 0.04490, LR: 0.0000660, Tokens/sec: 50433.38\n",
      "Step: 1167, Training Loss: 0.04434, LR: 0.0000659, Tokens/sec: 50606.48\n",
      "Step: 1168, Training Loss: 0.03737, LR: 0.0000658, Tokens/sec: 49039.98\n",
      "Step: 1169, Training Loss: 0.04423, LR: 0.0000658, Tokens/sec: 49947.27\n",
      "Step: 1170, Training Loss: 0.05538, LR: 0.0000657, Tokens/sec: 49790.62\n",
      "Step: 1171, Training Loss: 0.05237, LR: 0.0000657, Tokens/sec: 50722.26\n",
      "Step: 1172, Training Loss: 0.04845, LR: 0.0000656, Tokens/sec: 50244.68\n",
      "Step: 1173, Training Loss: 0.05726, LR: 0.0000655, Tokens/sec: 51392.61\n",
      "Step: 1174, Training Loss: 0.06113, LR: 0.0000655, Tokens/sec: 51324.71\n",
      "Step: 1175, Training Loss: 0.07808, LR: 0.0000654, Tokens/sec: 48905.70\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 1175, Eval Loss: 0.04839\n",
      "Step: 1176, Training Loss: 0.03206, LR: 0.0000653, Tokens/sec: 49595.33\n",
      "Step: 1177, Training Loss: 0.04596, LR: 0.0000653, Tokens/sec: 51717.87\n",
      "Step: 1178, Training Loss: 0.05049, LR: 0.0000652, Tokens/sec: 49337.95\n",
      "Step: 1179, Training Loss: 0.03819, LR: 0.0000651, Tokens/sec: 50758.57\n",
      "Step: 1180, Training Loss: 0.04175, LR: 0.0000651, Tokens/sec: 49150.00\n",
      "Step: 1181, Training Loss: 0.04212, LR: 0.0000650, Tokens/sec: 50828.75\n",
      "Step: 1182, Training Loss: 0.03561, LR: 0.0000650, Tokens/sec: 50483.26\n",
      "Step: 1183, Training Loss: 0.03713, LR: 0.0000649, Tokens/sec: 50032.48\n",
      "Step: 1184, Training Loss: 0.04072, LR: 0.0000648, Tokens/sec: 48671.28\n",
      "Step: 1185, Training Loss: 0.04626, LR: 0.0000648, Tokens/sec: 49414.81\n",
      "Step: 1186, Training Loss: 0.04381, LR: 0.0000647, Tokens/sec: 50225.23\n",
      "Step: 1187, Training Loss: 0.03318, LR: 0.0000646, Tokens/sec: 48663.38\n",
      "Step: 1188, Training Loss: 0.03983, LR: 0.0000646, Tokens/sec: 50612.21\n",
      "Step: 1189, Training Loss: 0.04737, LR: 0.0000645, Tokens/sec: 50415.15\n",
      "Step: 1190, Training Loss: 0.05067, LR: 0.0000645, Tokens/sec: 50237.15\n",
      "Step: 1191, Training Loss: 0.04572, LR: 0.0000644, Tokens/sec: 50254.18\n",
      "Step: 1192, Training Loss: 0.03883, LR: 0.0000643, Tokens/sec: 49509.57\n",
      "Step: 1193, Training Loss: 0.03799, LR: 0.0000643, Tokens/sec: 51246.94\n",
      "Step: 1194, Training Loss: 0.04759, LR: 0.0000642, Tokens/sec: 48469.43\n",
      "Step: 1195, Training Loss: 0.05226, LR: 0.0000641, Tokens/sec: 50929.13\n",
      "Step: 1196, Training Loss: 0.03022, LR: 0.0000641, Tokens/sec: 48926.29\n",
      "Step: 1197, Training Loss: 0.03996, LR: 0.0000640, Tokens/sec: 50745.86\n",
      "Step: 1198, Training Loss: 0.04637, LR: 0.0000639, Tokens/sec: 48602.09\n",
      "Step: 1199, Training Loss: 0.03691, LR: 0.0000639, Tokens/sec: 49886.85\n",
      "Step: 1200, Training Loss: 0.06950, LR: 0.0000638, Tokens/sec: 50500.70\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 1200, Eval Loss: 0.03776\n",
      "Step: 1201, Training Loss: 0.03069, LR: 0.0000638, Tokens/sec: 52443.47\n",
      "Step: 1202, Training Loss: 0.02637, LR: 0.0000637, Tokens/sec: 48048.25\n",
      "Step: 1203, Training Loss: 0.03769, LR: 0.0000636, Tokens/sec: 51248.66\n",
      "Step: 1204, Training Loss: 0.04061, LR: 0.0000636, Tokens/sec: 49970.27\n",
      "Step: 1205, Training Loss: 0.03726, LR: 0.0000635, Tokens/sec: 51585.54\n",
      "Step: 1206, Training Loss: 0.04706, LR: 0.0000634, Tokens/sec: 51492.67\n",
      "Step: 1207, Training Loss: 0.04879, LR: 0.0000634, Tokens/sec: 52650.40\n",
      "Step: 1208, Training Loss: 0.03110, LR: 0.0000633, Tokens/sec: 50192.41\n",
      "Step: 1209, Training Loss: 0.03875, LR: 0.0000633, Tokens/sec: 50342.90\n",
      "Step: 1210, Training Loss: 0.03514, LR: 0.0000632, Tokens/sec: 51370.97\n",
      "Step: 1211, Training Loss: 0.03420, LR: 0.0000631, Tokens/sec: 49349.73\n",
      "Step: 1212, Training Loss: 0.02936, LR: 0.0000631, Tokens/sec: 50486.69\n",
      "Step: 1213, Training Loss: 0.04955, LR: 0.0000630, Tokens/sec: 50247.53\n",
      "Step: 1214, Training Loss: 0.04866, LR: 0.0000629, Tokens/sec: 50435.52\n",
      "Step: 1215, Training Loss: 0.02866, LR: 0.0000629, Tokens/sec: 50145.59\n",
      "Step: 1216, Training Loss: 0.03595, LR: 0.0000628, Tokens/sec: 51545.99\n",
      "Step: 1217, Training Loss: 0.03227, LR: 0.0000627, Tokens/sec: 50141.21\n",
      "Step: 1218, Training Loss: 0.04375, LR: 0.0000627, Tokens/sec: 48245.47\n",
      "Step: 1219, Training Loss: 0.02599, LR: 0.0000626, Tokens/sec: 50993.26\n",
      "Step: 1220, Training Loss: 0.04174, LR: 0.0000626, Tokens/sec: 48741.12\n",
      "Step: 1221, Training Loss: 0.04747, LR: 0.0000625, Tokens/sec: 50697.30\n",
      "Step: 1222, Training Loss: 0.03932, LR: 0.0000624, Tokens/sec: 49922.29\n",
      "Step: 1223, Training Loss: 0.03884, LR: 0.0000624, Tokens/sec: 51671.10\n",
      "Step: 1224, Training Loss: 0.05320, LR: 0.0000623, Tokens/sec: 49246.16\n",
      "Step: 1225, Training Loss: 0.03841, LR: 0.0000622, Tokens/sec: 50209.74\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 1225, Eval Loss: 0.03989\n",
      "Step: 1226, Training Loss: 0.03165, LR: 0.0000622, Tokens/sec: 51499.12\n",
      "Step: 1227, Training Loss: 0.03132, LR: 0.0000621, Tokens/sec: 50729.34\n",
      "Step: 1228, Training Loss: 0.03098, LR: 0.0000620, Tokens/sec: 47683.67\n",
      "Step: 1229, Training Loss: 0.03606, LR: 0.0000620, Tokens/sec: 50009.24\n",
      "Step: 1230, Training Loss: 0.04020, LR: 0.0000619, Tokens/sec: 49445.83\n",
      "Step: 1231, Training Loss: 0.03583, LR: 0.0000619, Tokens/sec: 50450.19\n",
      "Step: 1232, Training Loss: 0.04173, LR: 0.0000618, Tokens/sec: 50103.61\n",
      "Step: 1233, Training Loss: 0.04931, LR: 0.0000617, Tokens/sec: 49417.95\n",
      "Step: 1234, Training Loss: 0.03798, LR: 0.0000617, Tokens/sec: 51808.75\n",
      "Step: 1235, Training Loss: 0.06780, LR: 0.0000616, Tokens/sec: 52157.68\n",
      "Step: 1236, Training Loss: 0.03668, LR: 0.0000615, Tokens/sec: 50500.81\n",
      "Step: 1237, Training Loss: 0.03674, LR: 0.0000615, Tokens/sec: 49492.42\n",
      "Step: 1238, Training Loss: 0.04788, LR: 0.0000614, Tokens/sec: 49746.97\n",
      "Step: 1239, Training Loss: 0.03985, LR: 0.0000613, Tokens/sec: 50036.55\n",
      "Step: 1240, Training Loss: 0.06298, LR: 0.0000613, Tokens/sec: 50432.55\n",
      "Step: 1241, Training Loss: 0.04002, LR: 0.0000612, Tokens/sec: 50214.01\n",
      "Step: 1242, Training Loss: 0.04030, LR: 0.0000611, Tokens/sec: 51599.58\n",
      "Step: 1243, Training Loss: 0.04405, LR: 0.0000611, Tokens/sec: 51423.39\n",
      "Step: 1244, Training Loss: 0.04254, LR: 0.0000610, Tokens/sec: 48796.48\n",
      "Step: 1245, Training Loss: 0.03625, LR: 0.0000610, Tokens/sec: 49372.15\n",
      "Step: 1246, Training Loss: 0.02839, LR: 0.0000609, Tokens/sec: 48828.71\n",
      "Step: 1247, Training Loss: 0.04451, LR: 0.0000608, Tokens/sec: 49886.20\n",
      "Step: 1248, Training Loss: 0.04667, LR: 0.0000608, Tokens/sec: 51223.40\n",
      "Step: 1249, Training Loss: 0.03365, LR: 0.0000607, Tokens/sec: 50950.84\n",
      "Step: 1250, Training Loss: 0.03960, LR: 0.0000606, Tokens/sec: 49894.05\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 1250, Eval Loss: 0.03277\n",
      "Step: 1251, Training Loss: 0.03004, LR: 0.0000606, Tokens/sec: 47822.94\n",
      "Step: 1252, Training Loss: 0.03605, LR: 0.0000605, Tokens/sec: 49461.98\n",
      "Step: 1253, Training Loss: 0.02572, LR: 0.0000604, Tokens/sec: 50603.86\n",
      "Step: 1254, Training Loss: 0.02966, LR: 0.0000604, Tokens/sec: 49228.74\n",
      "Step: 1255, Training Loss: 0.01816, LR: 0.0000603, Tokens/sec: 48975.20\n",
      "Step: 1256, Training Loss: 0.03191, LR: 0.0000603, Tokens/sec: 50065.16\n",
      "Step: 1257, Training Loss: 0.04255, LR: 0.0000602, Tokens/sec: 50774.05\n",
      "Step: 1258, Training Loss: 0.03386, LR: 0.0000601, Tokens/sec: 49410.22\n",
      "Step: 1259, Training Loss: 0.02891, LR: 0.0000601, Tokens/sec: 51288.28\n",
      "Step: 1260, Training Loss: 0.03178, LR: 0.0000600, Tokens/sec: 51078.80\n",
      "Step: 1261, Training Loss: 0.04240, LR: 0.0000599, Tokens/sec: 48480.08\n",
      "Step: 1262, Training Loss: 0.03012, LR: 0.0000599, Tokens/sec: 50903.98\n",
      "Step: 1263, Training Loss: 0.04191, LR: 0.0000598, Tokens/sec: 48599.25\n",
      "Step: 1264, Training Loss: 0.02974, LR: 0.0000597, Tokens/sec: 50568.34\n",
      "Step: 1265, Training Loss: 0.04304, LR: 0.0000597, Tokens/sec: 49774.42\n",
      "Step: 1266, Training Loss: 0.04129, LR: 0.0000596, Tokens/sec: 50937.25\n",
      "Step: 1267, Training Loss: 0.03147, LR: 0.0000595, Tokens/sec: 50663.41\n",
      "Step: 1268, Training Loss: 0.04200, LR: 0.0000595, Tokens/sec: 52087.07\n",
      "Step: 1269, Training Loss: 0.03535, LR: 0.0000594, Tokens/sec: 49205.16\n",
      "Step: 1270, Training Loss: 0.03887, LR: 0.0000594, Tokens/sec: 48722.97\n",
      "Step: 1271, Training Loss: 0.03338, LR: 0.0000593, Tokens/sec: 50548.72\n",
      "Step: 1272, Training Loss: 0.02565, LR: 0.0000592, Tokens/sec: 49162.70\n",
      "Step: 1273, Training Loss: 0.02933, LR: 0.0000592, Tokens/sec: 49686.69\n",
      "Step: 1274, Training Loss: 0.04273, LR: 0.0000591, Tokens/sec: 51474.43\n",
      "Step: 1275, Training Loss: 0.02986, LR: 0.0000590, Tokens/sec: 49656.08\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 1275, Eval Loss: 0.03875\n",
      "Step: 1276, Training Loss: 0.02729, LR: 0.0000590, Tokens/sec: 52304.45\n",
      "Step: 1277, Training Loss: 0.02041, LR: 0.0000589, Tokens/sec: 50271.40\n",
      "Step: 1278, Training Loss: 0.02714, LR: 0.0000588, Tokens/sec: 49665.41\n",
      "Step: 1279, Training Loss: 0.03159, LR: 0.0000588, Tokens/sec: 51005.27\n",
      "Step: 1280, Training Loss: 0.02379, LR: 0.0000587, Tokens/sec: 50470.45\n",
      "Step: 1281, Training Loss: 0.03280, LR: 0.0000586, Tokens/sec: 49131.51\n",
      "Step: 1282, Training Loss: 0.03331, LR: 0.0000586, Tokens/sec: 48779.48\n",
      "Step: 1283, Training Loss: 0.03160, LR: 0.0000585, Tokens/sec: 50639.81\n",
      "Step: 1284, Training Loss: 0.04051, LR: 0.0000585, Tokens/sec: 50512.84\n",
      "Step: 1285, Training Loss: 0.03106, LR: 0.0000584, Tokens/sec: 50619.02\n",
      "Step: 1286, Training Loss: 0.03607, LR: 0.0000583, Tokens/sec: 50124.08\n",
      "Step: 1287, Training Loss: 0.03455, LR: 0.0000583, Tokens/sec: 48298.37\n",
      "Step: 1288, Training Loss: 0.02782, LR: 0.0000582, Tokens/sec: 50377.21\n",
      "Step: 1289, Training Loss: 0.03185, LR: 0.0000581, Tokens/sec: 49479.12\n",
      "Step: 1290, Training Loss: 0.02498, LR: 0.0000581, Tokens/sec: 51615.78\n",
      "Step: 1291, Training Loss: 0.03310, LR: 0.0000580, Tokens/sec: 48963.73\n",
      "Step: 1292, Training Loss: 0.02667, LR: 0.0000579, Tokens/sec: 49802.68\n",
      "Step: 1293, Training Loss: 0.02091, LR: 0.0000579, Tokens/sec: 50971.58\n",
      "Step: 1294, Training Loss: 0.03272, LR: 0.0000578, Tokens/sec: 51450.62\n",
      "Step: 1295, Training Loss: 0.02952, LR: 0.0000577, Tokens/sec: 52481.00\n",
      "Step: 1296, Training Loss: 0.02597, LR: 0.0000577, Tokens/sec: 48538.16\n",
      "Step: 1297, Training Loss: 0.03177, LR: 0.0000576, Tokens/sec: 50171.43\n",
      "Step: 1298, Training Loss: 0.04129, LR: 0.0000575, Tokens/sec: 49185.71\n",
      "Step: 1299, Training Loss: 0.03149, LR: 0.0000575, Tokens/sec: 48745.28\n",
      "Step: 1300, Training Loss: 0.04566, LR: 0.0000574, Tokens/sec: 49788.79\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 1300, Eval Loss: 0.03215\n",
      "Step: 1301, Training Loss: 0.02963, LR: 0.0000574, Tokens/sec: 52917.60\n",
      "Step: 1302, Training Loss: 0.02737, LR: 0.0000573, Tokens/sec: 49981.04\n",
      "Step: 1303, Training Loss: 0.02542, LR: 0.0000572, Tokens/sec: 50049.26\n",
      "Step: 1304, Training Loss: 0.02983, LR: 0.0000572, Tokens/sec: 49284.83\n",
      "Step: 1305, Training Loss: 0.02961, LR: 0.0000571, Tokens/sec: 47603.33\n",
      "Step: 1306, Training Loss: 0.02842, LR: 0.0000570, Tokens/sec: 49222.35\n",
      "Step: 1307, Training Loss: 0.03609, LR: 0.0000570, Tokens/sec: 51302.60\n",
      "Step: 1308, Training Loss: 0.02432, LR: 0.0000569, Tokens/sec: 48691.90\n",
      "Step: 1309, Training Loss: 0.03294, LR: 0.0000568, Tokens/sec: 50698.23\n",
      "Step: 1310, Training Loss: 0.03065, LR: 0.0000568, Tokens/sec: 51307.33\n",
      "Step: 1311, Training Loss: 0.04042, LR: 0.0000567, Tokens/sec: 51040.71\n",
      "Step: 1312, Training Loss: 0.03880, LR: 0.0000566, Tokens/sec: 51830.02\n",
      "Step: 1313, Training Loss: 0.03595, LR: 0.0000566, Tokens/sec: 48957.38\n",
      "Step: 1314, Training Loss: 0.02427, LR: 0.0000565, Tokens/sec: 49556.68\n",
      "Step: 1315, Training Loss: 0.02866, LR: 0.0000565, Tokens/sec: 49767.94\n",
      "Step: 1316, Training Loss: 0.02958, LR: 0.0000564, Tokens/sec: 51769.96\n",
      "Step: 1317, Training Loss: 0.03240, LR: 0.0000563, Tokens/sec: 48400.86\n",
      "Step: 1318, Training Loss: 0.02701, LR: 0.0000563, Tokens/sec: 49992.82\n",
      "Step: 1319, Training Loss: 0.03131, LR: 0.0000562, Tokens/sec: 51828.25\n",
      "Step: 1320, Training Loss: 0.02393, LR: 0.0000561, Tokens/sec: 52160.96\n",
      "Step: 1321, Training Loss: 0.02771, LR: 0.0000561, Tokens/sec: 49342.23\n",
      "Step: 1322, Training Loss: 0.03615, LR: 0.0000560, Tokens/sec: 42291.79\n",
      "Step: 1323, Training Loss: 0.03796, LR: 0.0000559, Tokens/sec: 42018.67\n",
      "Step: 1324, Training Loss: 0.02007, LR: 0.0000559, Tokens/sec: 48754.93\n",
      "Step: 1325, Training Loss: 0.03922, LR: 0.0000558, Tokens/sec: 49833.97\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 1325, Eval Loss: 0.02597\n",
      "Step: 1326, Training Loss: 0.02697, LR: 0.0000557, Tokens/sec: 49800.60\n",
      "Step: 1327, Training Loss: 0.02102, LR: 0.0000557, Tokens/sec: 41473.05\n",
      "Step: 1328, Training Loss: 0.02133, LR: 0.0000556, Tokens/sec: 53603.58\n",
      "Step: 1329, Training Loss: 0.03289, LR: 0.0000555, Tokens/sec: 52734.62\n",
      "Step: 1330, Training Loss: 0.02737, LR: 0.0000555, Tokens/sec: 54814.70\n",
      "Step: 1331, Training Loss: 0.02673, LR: 0.0000554, Tokens/sec: 53703.57\n",
      "Step: 1332, Training Loss: 0.02821, LR: 0.0000554, Tokens/sec: 54385.72\n",
      "Step: 1333, Training Loss: 0.02770, LR: 0.0000553, Tokens/sec: 53204.92\n",
      "Step: 1334, Training Loss: 0.02131, LR: 0.0000552, Tokens/sec: 40208.25\n",
      "Step: 1335, Training Loss: 0.02307, LR: 0.0000552, Tokens/sec: 54000.19\n",
      "Step: 1336, Training Loss: 0.03656, LR: 0.0000551, Tokens/sec: 51967.81\n",
      "Step: 1337, Training Loss: 0.03631, LR: 0.0000550, Tokens/sec: 54592.30\n",
      "Step: 1338, Training Loss: 0.02417, LR: 0.0000550, Tokens/sec: 53082.76\n",
      "Step: 1339, Training Loss: 0.02977, LR: 0.0000549, Tokens/sec: 54599.04\n",
      "Step: 1340, Training Loss: 0.03519, LR: 0.0000548, Tokens/sec: 41333.84\n",
      "Step: 1341, Training Loss: 0.02560, LR: 0.0000548, Tokens/sec: 42216.00\n",
      "Step: 1342, Training Loss: 0.02952, LR: 0.0000547, Tokens/sec: 53073.52\n",
      "Step: 1343, Training Loss: 0.02662, LR: 0.0000546, Tokens/sec: 45375.72\n",
      "Step: 1344, Training Loss: 0.02072, LR: 0.0000546, Tokens/sec: 51651.11\n",
      "Step: 1345, Training Loss: 0.03437, LR: 0.0000545, Tokens/sec: 41137.32\n",
      "Step: 1346, Training Loss: 0.03087, LR: 0.0000545, Tokens/sec: 47197.88\n",
      "Step: 1347, Training Loss: 0.01909, LR: 0.0000544, Tokens/sec: 49395.83\n",
      "Step: 1348, Training Loss: 0.02395, LR: 0.0000543, Tokens/sec: 50355.84\n",
      "Step: 1349, Training Loss: 0.03249, LR: 0.0000543, Tokens/sec: 47439.21\n",
      "Step: 1350, Training Loss: 0.02871, LR: 0.0000542, Tokens/sec: 50419.07\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 1350, Eval Loss: 0.02362\n",
      "Step: 1351, Training Loss: 0.03180, LR: 0.0000541, Tokens/sec: 44640.41\n",
      "Step: 1352, Training Loss: 0.02201, LR: 0.0000541, Tokens/sec: 42945.71\n",
      "Step: 1353, Training Loss: 0.02119, LR: 0.0000540, Tokens/sec: 50080.74\n",
      "Step: 1354, Training Loss: 0.01920, LR: 0.0000539, Tokens/sec: 51450.81\n",
      "Step: 1355, Training Loss: 0.02467, LR: 0.0000539, Tokens/sec: 49335.66\n",
      "Step: 1356, Training Loss: 0.02609, LR: 0.0000538, Tokens/sec: 41148.38\n",
      "Step: 1357, Training Loss: 0.01788, LR: 0.0000537, Tokens/sec: 44409.48\n",
      "Step: 1358, Training Loss: 0.02458, LR: 0.0000537, Tokens/sec: 48054.24\n",
      "Step: 1359, Training Loss: 0.02995, LR: 0.0000536, Tokens/sec: 43853.27\n",
      "Step: 1360, Training Loss: 0.02285, LR: 0.0000535, Tokens/sec: 47855.76\n",
      "Step: 1361, Training Loss: 0.01803, LR: 0.0000535, Tokens/sec: 43803.71\n",
      "Step: 1362, Training Loss: 0.04471, LR: 0.0000534, Tokens/sec: 51149.51\n",
      "Step: 1363, Training Loss: 0.03014, LR: 0.0000534, Tokens/sec: 48735.85\n",
      "Step: 1364, Training Loss: 0.02154, LR: 0.0000533, Tokens/sec: 49387.79\n",
      "Step: 1365, Training Loss: 0.01774, LR: 0.0000532, Tokens/sec: 41573.52\n",
      "Step: 1366, Training Loss: 0.02522, LR: 0.0000532, Tokens/sec: 41728.50\n",
      "Step: 1367, Training Loss: 0.02705, LR: 0.0000531, Tokens/sec: 42967.74\n",
      "Step: 1368, Training Loss: 0.02694, LR: 0.0000530, Tokens/sec: 54027.24\n",
      "Step: 1369, Training Loss: 0.02578, LR: 0.0000530, Tokens/sec: 53927.28\n",
      "Step: 1370, Training Loss: 0.02543, LR: 0.0000529, Tokens/sec: 40473.95\n",
      "Step: 1371, Training Loss: 0.02158, LR: 0.0000528, Tokens/sec: 52569.71\n",
      "Step: 1372, Training Loss: 0.03084, LR: 0.0000528, Tokens/sec: 43923.66\n",
      "Step: 1373, Training Loss: 0.02432, LR: 0.0000527, Tokens/sec: 55877.48\n",
      "Step: 1374, Training Loss: 0.02681, LR: 0.0000526, Tokens/sec: 47327.43\n",
      "Step: 1375, Training Loss: 0.02089, LR: 0.0000526, Tokens/sec: 48723.52\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 1375, Eval Loss: 0.03539\n",
      "Step: 1376, Training Loss: 0.02006, LR: 0.0000525, Tokens/sec: 50205.43\n",
      "Step: 1377, Training Loss: 0.02183, LR: 0.0000525, Tokens/sec: 43270.03\n",
      "Step: 1378, Training Loss: 0.02777, LR: 0.0000524, Tokens/sec: 45774.04\n",
      "Step: 1379, Training Loss: 0.01929, LR: 0.0000523, Tokens/sec: 48134.26\n",
      "Step: 1380, Training Loss: 0.02324, LR: 0.0000523, Tokens/sec: 48589.93\n",
      "Step: 1381, Training Loss: 0.02751, LR: 0.0000522, Tokens/sec: 42572.61\n",
      "Step: 1382, Training Loss: 0.02192, LR: 0.0000521, Tokens/sec: 46192.56\n",
      "Step: 1383, Training Loss: 0.02059, LR: 0.0000521, Tokens/sec: 46947.58\n",
      "Step: 1384, Training Loss: 0.04401, LR: 0.0000520, Tokens/sec: 48080.45\n",
      "Step: 1385, Training Loss: 0.03568, LR: 0.0000519, Tokens/sec: 46765.31\n",
      "Step: 1386, Training Loss: 0.03033, LR: 0.0000519, Tokens/sec: 48783.74\n",
      "Step: 1387, Training Loss: 0.02723, LR: 0.0000518, Tokens/sec: 47370.06\n",
      "Step: 1388, Training Loss: 0.03135, LR: 0.0000517, Tokens/sec: 44136.58\n",
      "Step: 1389, Training Loss: 0.01951, LR: 0.0000517, Tokens/sec: 50017.43\n",
      "Step: 1390, Training Loss: 0.03511, LR: 0.0000516, Tokens/sec: 49386.75\n",
      "Step: 1391, Training Loss: 0.01958, LR: 0.0000515, Tokens/sec: 49555.38\n",
      "Step: 1392, Training Loss: 0.02459, LR: 0.0000515, Tokens/sec: 51325.57\n",
      "Step: 1393, Training Loss: 0.02471, LR: 0.0000514, Tokens/sec: 50276.33\n",
      "Step: 1394, Training Loss: 0.02192, LR: 0.0000514, Tokens/sec: 48832.19\n",
      "Step: 1395, Training Loss: 0.02684, LR: 0.0000513, Tokens/sec: 49455.81\n",
      "Step: 1396, Training Loss: 0.03384, LR: 0.0000512, Tokens/sec: 48885.55\n",
      "Step: 1397, Training Loss: 0.02007, LR: 0.0000512, Tokens/sec: 51019.79\n",
      "Step: 1398, Training Loss: 0.02368, LR: 0.0000511, Tokens/sec: 50265.76\n",
      "Step: 1399, Training Loss: 0.02395, LR: 0.0000510, Tokens/sec: 51207.58\n",
      "Step: 1400, Training Loss: 0.02774, LR: 0.0000510, Tokens/sec: 50206.66\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 1400, Eval Loss: 0.02152\n",
      "Step: 1401, Training Loss: 0.01844, LR: 0.0000509, Tokens/sec: 48873.52\n",
      "Step: 1402, Training Loss: 0.03371, LR: 0.0000508, Tokens/sec: 49411.30\n",
      "Step: 1403, Training Loss: 0.04141, LR: 0.0000508, Tokens/sec: 49711.06\n",
      "Step: 1404, Training Loss: 0.02703, LR: 0.0000507, Tokens/sec: 49119.91\n",
      "Step: 1405, Training Loss: 0.03407, LR: 0.0000506, Tokens/sec: 49435.42\n",
      "Step: 1406, Training Loss: 0.02293, LR: 0.0000506, Tokens/sec: 47673.33\n",
      "Step: 1407, Training Loss: 0.02158, LR: 0.0000505, Tokens/sec: 49667.47\n",
      "Step: 1408, Training Loss: 0.01915, LR: 0.0000505, Tokens/sec: 49775.38\n",
      "Step: 1409, Training Loss: 0.02076, LR: 0.0000504, Tokens/sec: 46888.30\n",
      "Step: 1410, Training Loss: 0.02027, LR: 0.0000503, Tokens/sec: 47690.56\n",
      "Step: 1411, Training Loss: 0.02246, LR: 0.0000503, Tokens/sec: 50937.84\n",
      "Step: 1412, Training Loss: 0.02757, LR: 0.0000502, Tokens/sec: 50484.33\n",
      "Step: 1413, Training Loss: 0.02068, LR: 0.0000501, Tokens/sec: 47877.44\n",
      "Step: 1414, Training Loss: 0.02812, LR: 0.0000501, Tokens/sec: 49098.52\n",
      "Step: 1415, Training Loss: 0.02173, LR: 0.0000500, Tokens/sec: 47848.90\n",
      "Step: 1416, Training Loss: 0.02244, LR: 0.0000499, Tokens/sec: 51353.79\n",
      "Step: 1417, Training Loss: 0.02420, LR: 0.0000499, Tokens/sec: 50565.69\n",
      "Step: 1418, Training Loss: 0.02828, LR: 0.0000498, Tokens/sec: 50583.97\n",
      "Step: 1419, Training Loss: 0.01892, LR: 0.0000497, Tokens/sec: 49037.81\n",
      "Step: 1420, Training Loss: 0.02579, LR: 0.0000497, Tokens/sec: 51391.42\n",
      "Step: 1421, Training Loss: 0.02104, LR: 0.0000496, Tokens/sec: 49920.65\n",
      "Step: 1422, Training Loss: 0.01792, LR: 0.0000496, Tokens/sec: 47728.45\n",
      "Step: 1423, Training Loss: 0.03782, LR: 0.0000495, Tokens/sec: 48883.70\n",
      "Step: 1424, Training Loss: 0.02183, LR: 0.0000494, Tokens/sec: 47407.85\n",
      "Step: 1425, Training Loss: 0.02282, LR: 0.0000494, Tokens/sec: 50352.94\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 1425, Eval Loss: 0.01900\n",
      "Step: 1426, Training Loss: 0.01709, LR: 0.0000493, Tokens/sec: 49727.84\n",
      "Step: 1427, Training Loss: 0.01284, LR: 0.0000492, Tokens/sec: 49541.59\n",
      "Step: 1428, Training Loss: 0.02620, LR: 0.0000492, Tokens/sec: 49816.25\n",
      "Step: 1429, Training Loss: 0.01524, LR: 0.0000491, Tokens/sec: 50649.25\n",
      "Step: 1430, Training Loss: 0.01433, LR: 0.0000490, Tokens/sec: 49336.64\n",
      "Step: 1431, Training Loss: 0.01299, LR: 0.0000490, Tokens/sec: 50576.94\n",
      "Step: 1432, Training Loss: 0.02682, LR: 0.0000489, Tokens/sec: 49067.76\n",
      "Step: 1433, Training Loss: 0.01464, LR: 0.0000489, Tokens/sec: 47819.13\n",
      "Step: 1434, Training Loss: 0.02435, LR: 0.0000488, Tokens/sec: 49771.22\n",
      "Step: 1435, Training Loss: 0.02319, LR: 0.0000487, Tokens/sec: 50156.84\n",
      "Step: 1436, Training Loss: 0.03264, LR: 0.0000487, Tokens/sec: 51354.34\n",
      "Step: 1437, Training Loss: 0.01534, LR: 0.0000486, Tokens/sec: 50237.17\n",
      "Step: 1438, Training Loss: 0.02390, LR: 0.0000485, Tokens/sec: 48887.28\n",
      "Step: 1439, Training Loss: 0.01998, LR: 0.0000485, Tokens/sec: 48889.67\n",
      "Step: 1440, Training Loss: 0.02460, LR: 0.0000484, Tokens/sec: 50797.61\n",
      "Step: 1441, Training Loss: 0.03118, LR: 0.0000483, Tokens/sec: 47735.94\n",
      "Step: 1442, Training Loss: 0.02312, LR: 0.0000483, Tokens/sec: 48794.47\n",
      "Step: 1443, Training Loss: 0.03583, LR: 0.0000482, Tokens/sec: 47897.03\n",
      "Step: 1444, Training Loss: 0.01753, LR: 0.0000481, Tokens/sec: 50852.11\n",
      "Step: 1445, Training Loss: 0.02156, LR: 0.0000481, Tokens/sec: 50635.35\n",
      "Step: 1446, Training Loss: 0.02364, LR: 0.0000480, Tokens/sec: 51742.48\n",
      "Step: 1447, Training Loss: 0.01976, LR: 0.0000480, Tokens/sec: 50114.67\n",
      "Step: 1448, Training Loss: 0.02238, LR: 0.0000479, Tokens/sec: 48456.68\n",
      "Step: 1449, Training Loss: 0.01352, LR: 0.0000478, Tokens/sec: 50261.50\n",
      "Step: 1450, Training Loss: 0.01817, LR: 0.0000478, Tokens/sec: 48428.60\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 1450, Eval Loss: 0.02605\n",
      "Step: 1451, Training Loss: 0.02208, LR: 0.0000477, Tokens/sec: 48433.24\n",
      "Step: 1452, Training Loss: 0.02238, LR: 0.0000476, Tokens/sec: 52621.84\n",
      "Step: 1453, Training Loss: 0.01430, LR: 0.0000476, Tokens/sec: 49940.78\n",
      "Step: 1454, Training Loss: 0.01874, LR: 0.0000475, Tokens/sec: 50214.22\n",
      "Step: 1455, Training Loss: 0.01785, LR: 0.0000474, Tokens/sec: 50523.34\n",
      "Step: 1456, Training Loss: 0.01856, LR: 0.0000474, Tokens/sec: 50858.59\n",
      "Step: 1457, Training Loss: 0.02502, LR: 0.0000473, Tokens/sec: 50028.08\n",
      "Step: 1458, Training Loss: 0.01788, LR: 0.0000473, Tokens/sec: 50096.37\n",
      "Step: 1459, Training Loss: 0.02144, LR: 0.0000472, Tokens/sec: 50275.13\n",
      "Step: 1460, Training Loss: 0.02169, LR: 0.0000471, Tokens/sec: 50059.06\n",
      "Step: 1461, Training Loss: 0.01573, LR: 0.0000471, Tokens/sec: 49288.84\n",
      "Step: 1462, Training Loss: 0.02162, LR: 0.0000470, Tokens/sec: 51678.34\n",
      "Step: 1463, Training Loss: 0.02842, LR: 0.0000469, Tokens/sec: 47535.44\n",
      "Step: 1464, Training Loss: 0.01767, LR: 0.0000469, Tokens/sec: 46936.71\n",
      "Step: 1465, Training Loss: 0.01847, LR: 0.0000468, Tokens/sec: 50748.09\n",
      "Step: 1466, Training Loss: 0.01861, LR: 0.0000467, Tokens/sec: 51716.99\n",
      "Step: 1467, Training Loss: 0.02100, LR: 0.0000467, Tokens/sec: 48086.84\n",
      "Step: 1468, Training Loss: 0.01887, LR: 0.0000466, Tokens/sec: 51128.33\n",
      "Step: 1469, Training Loss: 0.01659, LR: 0.0000466, Tokens/sec: 48185.82\n",
      "Step: 1470, Training Loss: 0.02081, LR: 0.0000465, Tokens/sec: 50849.96\n",
      "Step: 1471, Training Loss: 0.01497, LR: 0.0000464, Tokens/sec: 49725.75\n",
      "Step: 1472, Training Loss: 0.01834, LR: 0.0000464, Tokens/sec: 50411.82\n",
      "Step: 1473, Training Loss: 0.01812, LR: 0.0000463, Tokens/sec: 50264.54\n",
      "Step: 1474, Training Loss: 0.01904, LR: 0.0000462, Tokens/sec: 51078.31\n",
      "Step: 1475, Training Loss: 0.01642, LR: 0.0000462, Tokens/sec: 49184.23\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 1475, Eval Loss: 0.02091\n",
      "Step: 1476, Training Loss: 0.01783, LR: 0.0000461, Tokens/sec: 51288.82\n",
      "Step: 1477, Training Loss: 0.01548, LR: 0.0000461, Tokens/sec: 49801.71\n",
      "Step: 1478, Training Loss: 0.01623, LR: 0.0000460, Tokens/sec: 49592.77\n",
      "Step: 1479, Training Loss: 0.02280, LR: 0.0000459, Tokens/sec: 48723.36\n",
      "Step: 1480, Training Loss: 0.02505, LR: 0.0000459, Tokens/sec: 49752.97\n",
      "Step: 1481, Training Loss: 0.01348, LR: 0.0000458, Tokens/sec: 51155.08\n",
      "Step: 1482, Training Loss: 0.02105, LR: 0.0000457, Tokens/sec: 50405.77\n",
      "Step: 1483, Training Loss: 0.02415, LR: 0.0000457, Tokens/sec: 50419.71\n",
      "Step: 1484, Training Loss: 0.01245, LR: 0.0000456, Tokens/sec: 47496.14\n",
      "Step: 1485, Training Loss: 0.01961, LR: 0.0000455, Tokens/sec: 50837.87\n",
      "Step: 1486, Training Loss: 0.01701, LR: 0.0000455, Tokens/sec: 48766.33\n",
      "Step: 1487, Training Loss: 0.01930, LR: 0.0000454, Tokens/sec: 50589.87\n",
      "Step: 1488, Training Loss: 0.01692, LR: 0.0000454, Tokens/sec: 50755.69\n",
      "Step: 1489, Training Loss: 0.01175, LR: 0.0000453, Tokens/sec: 50790.00\n",
      "Step: 1490, Training Loss: 0.01833, LR: 0.0000452, Tokens/sec: 50697.61\n",
      "Step: 1491, Training Loss: 0.02179, LR: 0.0000452, Tokens/sec: 51535.16\n",
      "Step: 1492, Training Loss: 0.01737, LR: 0.0000451, Tokens/sec: 50495.04\n",
      "Step: 1493, Training Loss: 0.01461, LR: 0.0000450, Tokens/sec: 48448.74\n",
      "Step: 1494, Training Loss: 0.02207, LR: 0.0000450, Tokens/sec: 49751.47\n",
      "Step: 1495, Training Loss: 0.01945, LR: 0.0000449, Tokens/sec: 49482.91\n",
      "Step: 1496, Training Loss: 0.01677, LR: 0.0000449, Tokens/sec: 49572.98\n",
      "Step: 1497, Training Loss: 0.01767, LR: 0.0000448, Tokens/sec: 51252.11\n",
      "Step: 1498, Training Loss: 0.01475, LR: 0.0000447, Tokens/sec: 51421.57\n",
      "Step: 1499, Training Loss: 0.01709, LR: 0.0000447, Tokens/sec: 49770.04\n",
      "Step: 1500, Training Loss: 0.01473, LR: 0.0000446, Tokens/sec: 49512.59\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 1500, Eval Loss: 0.01914\n",
      "Step: 1501, Training Loss: 0.01976, LR: 0.0000445, Tokens/sec: 52160.25\n",
      "Step: 1502, Training Loss: 0.01388, LR: 0.0000445, Tokens/sec: 48413.84\n",
      "Step: 1503, Training Loss: 0.01477, LR: 0.0000444, Tokens/sec: 47839.94\n",
      "Step: 1504, Training Loss: 0.01676, LR: 0.0000443, Tokens/sec: 49408.84\n",
      "Step: 1505, Training Loss: 0.01264, LR: 0.0000443, Tokens/sec: 49322.50\n",
      "Step: 1506, Training Loss: 0.01911, LR: 0.0000442, Tokens/sec: 51156.64\n",
      "Step: 1507, Training Loss: 0.01497, LR: 0.0000442, Tokens/sec: 50944.14\n",
      "Step: 1508, Training Loss: 0.01724, LR: 0.0000441, Tokens/sec: 49570.61\n",
      "Step: 1509, Training Loss: 0.01756, LR: 0.0000440, Tokens/sec: 50742.21\n",
      "Step: 1510, Training Loss: 0.01830, LR: 0.0000440, Tokens/sec: 49542.47\n",
      "Step: 1511, Training Loss: 0.02206, LR: 0.0000439, Tokens/sec: 50623.14\n",
      "Step: 1512, Training Loss: 0.01694, LR: 0.0000438, Tokens/sec: 47819.69\n",
      "Step: 1513, Training Loss: 0.01563, LR: 0.0000438, Tokens/sec: 51395.63\n",
      "Step: 1514, Training Loss: 0.02357, LR: 0.0000437, Tokens/sec: 48803.49\n",
      "Step: 1515, Training Loss: 0.02001, LR: 0.0000437, Tokens/sec: 49903.39\n",
      "Step: 1516, Training Loss: 0.02131, LR: 0.0000436, Tokens/sec: 50669.08\n",
      "Step: 1517, Training Loss: 0.02356, LR: 0.0000435, Tokens/sec: 48506.07\n",
      "Step: 1518, Training Loss: 0.01475, LR: 0.0000435, Tokens/sec: 46350.46\n",
      "Step: 1519, Training Loss: 0.01574, LR: 0.0000434, Tokens/sec: 47400.21\n",
      "Step: 1520, Training Loss: 0.01314, LR: 0.0000433, Tokens/sec: 50578.91\n",
      "Step: 1521, Training Loss: 0.01170, LR: 0.0000433, Tokens/sec: 49529.57\n",
      "Step: 1522, Training Loss: 0.01704, LR: 0.0000432, Tokens/sec: 51051.69\n",
      "Step: 1523, Training Loss: 0.01078, LR: 0.0000432, Tokens/sec: 48885.53\n",
      "Step: 1524, Training Loss: 0.01050, LR: 0.0000431, Tokens/sec: 50931.00\n",
      "Step: 1525, Training Loss: 0.01662, LR: 0.0000430, Tokens/sec: 50855.56\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 1525, Eval Loss: 0.01318\n",
      "Step: 1526, Training Loss: 0.01101, LR: 0.0000430, Tokens/sec: 50727.41\n",
      "Step: 1527, Training Loss: 0.01536, LR: 0.0000429, Tokens/sec: 51565.31\n",
      "Step: 1528, Training Loss: 0.01046, LR: 0.0000428, Tokens/sec: 50379.63\n",
      "Step: 1529, Training Loss: 0.01249, LR: 0.0000428, Tokens/sec: 49920.44\n",
      "Step: 1530, Training Loss: 0.01181, LR: 0.0000427, Tokens/sec: 51783.18\n",
      "Step: 1531, Training Loss: 0.01886, LR: 0.0000427, Tokens/sec: 49226.42\n",
      "Step: 1532, Training Loss: 0.01858, LR: 0.0000426, Tokens/sec: 48311.09\n",
      "Step: 1533, Training Loss: 0.01220, LR: 0.0000425, Tokens/sec: 50130.87\n",
      "Step: 1534, Training Loss: 0.01572, LR: 0.0000425, Tokens/sec: 50296.76\n",
      "Step: 1535, Training Loss: 0.02081, LR: 0.0000424, Tokens/sec: 51231.20\n",
      "Step: 1536, Training Loss: 0.01290, LR: 0.0000424, Tokens/sec: 50599.10\n",
      "Step: 1537, Training Loss: 0.01146, LR: 0.0000423, Tokens/sec: 49454.83\n",
      "Step: 1538, Training Loss: 0.01683, LR: 0.0000422, Tokens/sec: 50038.08\n",
      "Step: 1539, Training Loss: 0.01784, LR: 0.0000422, Tokens/sec: 50118.25\n",
      "Step: 1540, Training Loss: 0.01487, LR: 0.0000421, Tokens/sec: 49185.58\n",
      "Step: 1541, Training Loss: 0.01640, LR: 0.0000420, Tokens/sec: 50731.88\n",
      "Step: 1542, Training Loss: 0.01014, LR: 0.0000420, Tokens/sec: 50140.95\n",
      "Step: 1543, Training Loss: 0.02032, LR: 0.0000419, Tokens/sec: 49940.17\n",
      "Step: 1544, Training Loss: 0.02074, LR: 0.0000419, Tokens/sec: 49691.44\n",
      "Step: 1545, Training Loss: 0.01684, LR: 0.0000418, Tokens/sec: 50845.65\n",
      "Step: 1546, Training Loss: 0.01751, LR: 0.0000417, Tokens/sec: 49836.25\n",
      "Step: 1547, Training Loss: 0.01367, LR: 0.0000417, Tokens/sec: 48266.46\n",
      "Step: 1548, Training Loss: 0.01079, LR: 0.0000416, Tokens/sec: 50779.60\n",
      "Step: 1549, Training Loss: 0.01172, LR: 0.0000415, Tokens/sec: 49167.77\n",
      "Step: 1550, Training Loss: 0.01905, LR: 0.0000415, Tokens/sec: 51211.11\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 1550, Eval Loss: 0.01532\n",
      "Step: 1551, Training Loss: 0.00969, LR: 0.0000414, Tokens/sec: 50207.02\n",
      "Step: 1552, Training Loss: 0.01461, LR: 0.0000414, Tokens/sec: 51713.84\n",
      "Step: 1553, Training Loss: 0.01692, LR: 0.0000413, Tokens/sec: 51599.11\n",
      "Step: 1554, Training Loss: 0.01337, LR: 0.0000412, Tokens/sec: 49511.54\n",
      "Step: 1555, Training Loss: 0.02225, LR: 0.0000412, Tokens/sec: 47969.37\n",
      "Step: 1556, Training Loss: 0.01667, LR: 0.0000411, Tokens/sec: 51519.99\n",
      "Step: 1557, Training Loss: 0.01585, LR: 0.0000411, Tokens/sec: 49286.80\n",
      "Step: 1558, Training Loss: 0.01202, LR: 0.0000410, Tokens/sec: 50553.11\n",
      "Step: 1559, Training Loss: 0.01056, LR: 0.0000409, Tokens/sec: 49391.52\n",
      "Step: 1560, Training Loss: 0.02154, LR: 0.0000409, Tokens/sec: 49162.08\n",
      "Step: 1561, Training Loss: 0.01855, LR: 0.0000408, Tokens/sec: 51148.04\n",
      "Step: 1562, Training Loss: 0.02074, LR: 0.0000408, Tokens/sec: 51007.89\n",
      "Step: 1563, Training Loss: 0.00907, LR: 0.0000407, Tokens/sec: 49516.15\n",
      "Step: 1564, Training Loss: 0.01199, LR: 0.0000406, Tokens/sec: 49223.04\n",
      "Step: 1565, Training Loss: 0.01970, LR: 0.0000406, Tokens/sec: 51737.92\n",
      "Step: 1566, Training Loss: 0.01017, LR: 0.0000405, Tokens/sec: 49103.16\n",
      "Step: 1567, Training Loss: 0.01163, LR: 0.0000404, Tokens/sec: 49006.15\n",
      "Step: 1568, Training Loss: 0.01272, LR: 0.0000404, Tokens/sec: 50062.87\n",
      "Step: 1569, Training Loss: 0.01862, LR: 0.0000403, Tokens/sec: 49979.44\n",
      "Step: 1570, Training Loss: 0.01088, LR: 0.0000403, Tokens/sec: 50320.73\n",
      "Step: 1571, Training Loss: 0.01509, LR: 0.0000402, Tokens/sec: 46952.38\n",
      "Step: 1572, Training Loss: 0.01611, LR: 0.0000401, Tokens/sec: 48665.78\n",
      "Step: 1573, Training Loss: 0.03392, LR: 0.0000401, Tokens/sec: 49370.05\n",
      "Step: 1574, Training Loss: 0.01516, LR: 0.0000400, Tokens/sec: 50843.57\n",
      "Step: 1575, Training Loss: 0.01238, LR: 0.0000400, Tokens/sec: 50032.33\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 1575, Eval Loss: 0.01380\n",
      "Step: 1576, Training Loss: 0.01457, LR: 0.0000399, Tokens/sec: 49349.41\n",
      "Step: 1577, Training Loss: 0.00900, LR: 0.0000398, Tokens/sec: 50436.12\n",
      "Step: 1578, Training Loss: 0.01907, LR: 0.0000398, Tokens/sec: 50015.87\n",
      "Step: 1579, Training Loss: 0.01365, LR: 0.0000397, Tokens/sec: 50234.02\n",
      "Step: 1580, Training Loss: 0.01771, LR: 0.0000397, Tokens/sec: 50785.72\n",
      "Step: 1581, Training Loss: 0.01360, LR: 0.0000396, Tokens/sec: 51275.97\n",
      "Step: 1582, Training Loss: 0.01711, LR: 0.0000395, Tokens/sec: 50477.81\n",
      "Step: 1583, Training Loss: 0.02094, LR: 0.0000395, Tokens/sec: 49393.53\n",
      "Step: 1584, Training Loss: 0.01519, LR: 0.0000394, Tokens/sec: 50470.23\n",
      "Step: 1585, Training Loss: 0.01826, LR: 0.0000393, Tokens/sec: 50214.03\n",
      "Step: 1586, Training Loss: 0.01388, LR: 0.0000393, Tokens/sec: 50909.20\n",
      "Step: 1587, Training Loss: 0.01433, LR: 0.0000392, Tokens/sec: 50580.85\n",
      "Step: 1588, Training Loss: 0.00946, LR: 0.0000392, Tokens/sec: 49185.72\n",
      "Step: 1589, Training Loss: 0.01597, LR: 0.0000391, Tokens/sec: 50635.26\n",
      "Step: 1590, Training Loss: 0.01141, LR: 0.0000390, Tokens/sec: 48610.89\n",
      "Step: 1591, Training Loss: 0.01532, LR: 0.0000390, Tokens/sec: 49280.91\n",
      "Step: 1592, Training Loss: 0.02355, LR: 0.0000389, Tokens/sec: 49463.42\n",
      "Step: 1593, Training Loss: 0.01082, LR: 0.0000389, Tokens/sec: 49446.69\n",
      "Step: 1594, Training Loss: 0.01423, LR: 0.0000388, Tokens/sec: 49355.66\n",
      "Step: 1595, Training Loss: 0.01269, LR: 0.0000387, Tokens/sec: 50655.36\n",
      "Step: 1596, Training Loss: 0.01347, LR: 0.0000387, Tokens/sec: 52351.27\n",
      "Step: 1597, Training Loss: 0.01517, LR: 0.0000386, Tokens/sec: 52319.99\n",
      "Step: 1598, Training Loss: 0.01567, LR: 0.0000386, Tokens/sec: 50385.06\n",
      "Step: 1599, Training Loss: 0.01112, LR: 0.0000385, Tokens/sec: 49226.66\n",
      "Step: 1600, Training Loss: 0.01175, LR: 0.0000384, Tokens/sec: 50591.63\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 1600, Eval Loss: 0.01144\n",
      "Step: 1601, Training Loss: 0.00973, LR: 0.0000384, Tokens/sec: 51016.80\n",
      "Step: 1602, Training Loss: 0.01104, LR: 0.0000383, Tokens/sec: 49450.65\n",
      "Step: 1603, Training Loss: 0.01422, LR: 0.0000383, Tokens/sec: 50924.61\n",
      "Step: 1604, Training Loss: 0.01032, LR: 0.0000382, Tokens/sec: 50347.40\n",
      "Step: 1605, Training Loss: 0.01046, LR: 0.0000381, Tokens/sec: 49907.46\n",
      "Step: 1606, Training Loss: 0.01034, LR: 0.0000381, Tokens/sec: 50430.31\n",
      "Step: 1607, Training Loss: 0.01290, LR: 0.0000380, Tokens/sec: 50324.42\n",
      "Step: 1608, Training Loss: 0.01275, LR: 0.0000380, Tokens/sec: 49610.59\n",
      "Step: 1609, Training Loss: 0.01904, LR: 0.0000379, Tokens/sec: 48590.26\n",
      "Step: 1610, Training Loss: 0.00949, LR: 0.0000378, Tokens/sec: 49879.27\n",
      "Step: 1611, Training Loss: 0.01344, LR: 0.0000378, Tokens/sec: 47669.55\n",
      "Step: 1612, Training Loss: 0.00960, LR: 0.0000377, Tokens/sec: 50673.89\n",
      "Step: 1613, Training Loss: 0.01741, LR: 0.0000377, Tokens/sec: 51156.87\n",
      "Step: 1614, Training Loss: 0.01359, LR: 0.0000376, Tokens/sec: 51459.24\n",
      "Step: 1615, Training Loss: 0.01274, LR: 0.0000375, Tokens/sec: 49020.78\n",
      "Step: 1616, Training Loss: 0.01741, LR: 0.0000375, Tokens/sec: 50462.52\n",
      "Step: 1617, Training Loss: 0.00956, LR: 0.0000374, Tokens/sec: 51868.31\n",
      "Step: 1618, Training Loss: 0.01473, LR: 0.0000374, Tokens/sec: 48306.30\n",
      "Step: 1619, Training Loss: 0.01143, LR: 0.0000373, Tokens/sec: 50494.28\n",
      "Step: 1620, Training Loss: 0.01004, LR: 0.0000373, Tokens/sec: 48310.60\n",
      "Step: 1621, Training Loss: 0.01003, LR: 0.0000372, Tokens/sec: 49655.52\n",
      "Step: 1622, Training Loss: 0.01038, LR: 0.0000371, Tokens/sec: 50921.76\n",
      "Step: 1623, Training Loss: 0.01155, LR: 0.0000371, Tokens/sec: 50960.95\n",
      "Step: 1624, Training Loss: 0.01541, LR: 0.0000370, Tokens/sec: 50265.28\n",
      "Step: 1625, Training Loss: 0.01249, LR: 0.0000370, Tokens/sec: 46867.64\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 1625, Eval Loss: 0.01367\n",
      "Step: 1626, Training Loss: 0.01107, LR: 0.0000369, Tokens/sec: 49316.32\n",
      "Step: 1627, Training Loss: 0.00966, LR: 0.0000368, Tokens/sec: 50898.63\n",
      "Step: 1628, Training Loss: 0.01211, LR: 0.0000368, Tokens/sec: 48567.72\n",
      "Step: 1629, Training Loss: 0.00900, LR: 0.0000367, Tokens/sec: 48864.94\n",
      "Step: 1630, Training Loss: 0.01139, LR: 0.0000367, Tokens/sec: 48732.55\n",
      "Step: 1631, Training Loss: 0.01108, LR: 0.0000366, Tokens/sec: 48924.11\n",
      "Step: 1632, Training Loss: 0.01234, LR: 0.0000365, Tokens/sec: 51086.48\n",
      "Step: 1633, Training Loss: 0.00819, LR: 0.0000365, Tokens/sec: 49519.68\n",
      "Step: 1634, Training Loss: 0.00780, LR: 0.0000364, Tokens/sec: 51245.85\n",
      "Step: 1635, Training Loss: 0.01308, LR: 0.0000364, Tokens/sec: 49372.24\n",
      "Step: 1636, Training Loss: 0.00798, LR: 0.0000363, Tokens/sec: 50431.20\n",
      "Step: 1637, Training Loss: 0.01663, LR: 0.0000362, Tokens/sec: 47918.33\n",
      "Step: 1638, Training Loss: 0.01007, LR: 0.0000362, Tokens/sec: 49806.52\n",
      "Step: 1639, Training Loss: 0.01195, LR: 0.0000361, Tokens/sec: 47568.74\n",
      "Step: 1640, Training Loss: 0.00901, LR: 0.0000361, Tokens/sec: 51004.77\n",
      "Step: 1641, Training Loss: 0.00892, LR: 0.0000360, Tokens/sec: 51401.53\n",
      "Step: 1642, Training Loss: 0.01442, LR: 0.0000360, Tokens/sec: 50479.23\n",
      "Step: 1643, Training Loss: 0.01564, LR: 0.0000359, Tokens/sec: 49099.53\n",
      "Step: 1644, Training Loss: 0.00824, LR: 0.0000358, Tokens/sec: 48229.43\n",
      "Step: 1645, Training Loss: 0.00894, LR: 0.0000358, Tokens/sec: 50003.38\n",
      "Step: 1646, Training Loss: 0.01786, LR: 0.0000357, Tokens/sec: 48789.98\n",
      "Step: 1647, Training Loss: 0.00822, LR: 0.0000357, Tokens/sec: 50346.00\n",
      "Step: 1648, Training Loss: 0.01040, LR: 0.0000356, Tokens/sec: 50874.48\n",
      "Step: 1649, Training Loss: 0.00824, LR: 0.0000355, Tokens/sec: 49652.44\n",
      "Step: 1650, Training Loss: 0.01765, LR: 0.0000355, Tokens/sec: 50540.02\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 1650, Eval Loss: 0.00824\n",
      "Step: 1651, Training Loss: 0.00932, LR: 0.0000354, Tokens/sec: 52107.89\n",
      "Step: 1652, Training Loss: 0.00823, LR: 0.0000354, Tokens/sec: 52085.40\n",
      "Step: 1653, Training Loss: 0.02376, LR: 0.0000353, Tokens/sec: 49322.84\n",
      "Step: 1654, Training Loss: 0.00728, LR: 0.0000353, Tokens/sec: 48958.04\n",
      "Step: 1655, Training Loss: 0.00805, LR: 0.0000352, Tokens/sec: 50876.01\n",
      "Step: 1656, Training Loss: 0.00939, LR: 0.0000351, Tokens/sec: 49217.51\n",
      "Step: 1657, Training Loss: 0.01508, LR: 0.0000351, Tokens/sec: 48986.51\n",
      "Step: 1658, Training Loss: 0.00620, LR: 0.0000350, Tokens/sec: 51319.60\n",
      "Step: 1659, Training Loss: 0.00794, LR: 0.0000350, Tokens/sec: 50512.30\n",
      "Step: 1660, Training Loss: 0.00719, LR: 0.0000349, Tokens/sec: 50853.65\n",
      "Step: 1661, Training Loss: 0.00784, LR: 0.0000349, Tokens/sec: 47641.82\n",
      "Step: 1662, Training Loss: 0.00752, LR: 0.0000348, Tokens/sec: 50531.46\n",
      "Step: 1663, Training Loss: 0.00961, LR: 0.0000347, Tokens/sec: 48707.58\n",
      "Step: 1664, Training Loss: 0.01001, LR: 0.0000347, Tokens/sec: 50953.49\n",
      "Step: 1665, Training Loss: 0.00990, LR: 0.0000346, Tokens/sec: 49674.27\n",
      "Step: 1666, Training Loss: 0.00877, LR: 0.0000346, Tokens/sec: 50655.40\n",
      "Step: 1667, Training Loss: 0.00707, LR: 0.0000345, Tokens/sec: 50273.23\n",
      "Step: 1668, Training Loss: 0.00861, LR: 0.0000344, Tokens/sec: 50853.88\n",
      "Step: 1669, Training Loss: 0.00742, LR: 0.0000344, Tokens/sec: 51340.41\n",
      "Step: 1670, Training Loss: 0.01030, LR: 0.0000343, Tokens/sec: 48859.15\n",
      "Step: 1671, Training Loss: 0.01371, LR: 0.0000343, Tokens/sec: 49839.52\n",
      "Step: 1672, Training Loss: 0.00934, LR: 0.0000342, Tokens/sec: 49328.33\n",
      "Step: 1673, Training Loss: 0.00768, LR: 0.0000342, Tokens/sec: 48799.22\n",
      "Step: 1674, Training Loss: 0.01888, LR: 0.0000341, Tokens/sec: 50458.38\n",
      "Step: 1675, Training Loss: 0.00983, LR: 0.0000340, Tokens/sec: 49917.50\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 1675, Eval Loss: 0.01144\n",
      "Step: 1676, Training Loss: 0.00944, LR: 0.0000340, Tokens/sec: 50317.57\n",
      "Step: 1677, Training Loss: 0.01286, LR: 0.0000339, Tokens/sec: 50657.65\n",
      "Step: 1678, Training Loss: 0.00673, LR: 0.0000339, Tokens/sec: 46444.65\n",
      "Step: 1679, Training Loss: 0.00987, LR: 0.0000338, Tokens/sec: 47029.23\n",
      "Step: 1680, Training Loss: 0.01270, LR: 0.0000338, Tokens/sec: 48562.31\n",
      "Step: 1681, Training Loss: 0.01042, LR: 0.0000337, Tokens/sec: 50738.69\n",
      "Step: 1682, Training Loss: 0.00633, LR: 0.0000336, Tokens/sec: 46295.35\n",
      "Step: 1683, Training Loss: 0.01128, LR: 0.0000336, Tokens/sec: 46720.83\n",
      "Step: 1684, Training Loss: 0.01230, LR: 0.0000335, Tokens/sec: 48686.99\n",
      "Step: 1685, Training Loss: 0.02528, LR: 0.0000335, Tokens/sec: 50873.76\n",
      "Step: 1686, Training Loss: 0.00975, LR: 0.0000334, Tokens/sec: 51090.00\n",
      "Step: 1687, Training Loss: 0.01032, LR: 0.0000334, Tokens/sec: 50068.12\n",
      "Step: 1688, Training Loss: 0.00759, LR: 0.0000333, Tokens/sec: 50142.69\n",
      "Step: 1689, Training Loss: 0.01031, LR: 0.0000332, Tokens/sec: 48937.95\n",
      "Step: 1690, Training Loss: 0.01177, LR: 0.0000332, Tokens/sec: 50681.44\n",
      "Step: 1691, Training Loss: 0.00797, LR: 0.0000331, Tokens/sec: 49983.77\n",
      "Step: 1692, Training Loss: 0.00791, LR: 0.0000331, Tokens/sec: 50220.98\n",
      "Step: 1693, Training Loss: 0.00928, LR: 0.0000330, Tokens/sec: 50695.86\n",
      "Step: 1694, Training Loss: 0.00779, LR: 0.0000330, Tokens/sec: 50907.13\n",
      "Step: 1695, Training Loss: 0.00783, LR: 0.0000329, Tokens/sec: 50932.48\n",
      "Step: 1696, Training Loss: 0.00726, LR: 0.0000329, Tokens/sec: 51040.15\n",
      "Step: 1697, Training Loss: 0.00881, LR: 0.0000328, Tokens/sec: 48520.02\n",
      "Step: 1698, Training Loss: 0.00844, LR: 0.0000327, Tokens/sec: 48687.95\n",
      "Step: 1699, Training Loss: 0.00740, LR: 0.0000327, Tokens/sec: 49895.73\n",
      "Step: 1700, Training Loss: 0.01152, LR: 0.0000326, Tokens/sec: 49404.72\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 1700, Eval Loss: 0.01113\n",
      "Step: 1701, Training Loss: 0.00628, LR: 0.0000326, Tokens/sec: 48480.08\n",
      "Step: 1702, Training Loss: 0.00816, LR: 0.0000325, Tokens/sec: 50680.56\n",
      "Step: 1703, Training Loss: 0.01394, LR: 0.0000325, Tokens/sec: 51489.27\n",
      "Step: 1704, Training Loss: 0.01019, LR: 0.0000324, Tokens/sec: 50103.39\n",
      "Step: 1705, Training Loss: 0.00637, LR: 0.0000324, Tokens/sec: 51094.18\n",
      "Step: 1706, Training Loss: 0.00753, LR: 0.0000323, Tokens/sec: 51013.76\n",
      "Step: 1707, Training Loss: 0.00562, LR: 0.0000322, Tokens/sec: 50441.14\n",
      "Step: 1708, Training Loss: 0.00755, LR: 0.0000322, Tokens/sec: 48218.02\n",
      "Step: 1709, Training Loss: 0.00745, LR: 0.0000321, Tokens/sec: 50734.76\n",
      "Step: 1710, Training Loss: 0.00684, LR: 0.0000321, Tokens/sec: 49958.28\n",
      "Step: 1711, Training Loss: 0.01243, LR: 0.0000320, Tokens/sec: 50894.86\n",
      "Step: 1712, Training Loss: 0.00506, LR: 0.0000320, Tokens/sec: 49910.26\n",
      "Step: 1713, Training Loss: 0.01465, LR: 0.0000319, Tokens/sec: 50125.44\n",
      "Step: 1714, Training Loss: 0.00594, LR: 0.0000319, Tokens/sec: 51921.29\n",
      "Step: 1715, Training Loss: 0.00828, LR: 0.0000318, Tokens/sec: 47481.59\n",
      "Step: 1716, Training Loss: 0.00914, LR: 0.0000317, Tokens/sec: 49730.44\n",
      "Step: 1717, Training Loss: 0.00783, LR: 0.0000317, Tokens/sec: 49233.85\n",
      "Step: 1718, Training Loss: 0.00732, LR: 0.0000316, Tokens/sec: 50099.26\n",
      "Step: 1719, Training Loss: 0.00647, LR: 0.0000316, Tokens/sec: 49122.59\n",
      "Step: 1720, Training Loss: 0.01595, LR: 0.0000315, Tokens/sec: 50782.72\n",
      "Step: 1721, Training Loss: 0.00978, LR: 0.0000315, Tokens/sec: 49740.65\n",
      "Step: 1722, Training Loss: 0.00776, LR: 0.0000314, Tokens/sec: 49747.54\n",
      "Step: 1723, Training Loss: 0.01165, LR: 0.0000314, Tokens/sec: 50176.82\n",
      "Step: 1724, Training Loss: 0.00823, LR: 0.0000313, Tokens/sec: 48456.59\n",
      "Step: 1725, Training Loss: 0.00590, LR: 0.0000312, Tokens/sec: 50962.07\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 1725, Eval Loss: 0.00751\n",
      "Step: 1726, Training Loss: 0.00908, LR: 0.0000312, Tokens/sec: 49843.05\n",
      "Step: 1727, Training Loss: 0.00531, LR: 0.0000311, Tokens/sec: 48842.03\n",
      "Step: 1728, Training Loss: 0.00915, LR: 0.0000311, Tokens/sec: 52056.46\n",
      "Step: 1729, Training Loss: 0.00466, LR: 0.0000310, Tokens/sec: 48757.30\n",
      "Step: 1730, Training Loss: 0.00518, LR: 0.0000310, Tokens/sec: 49969.77\n",
      "Step: 1731, Training Loss: 0.00589, LR: 0.0000309, Tokens/sec: 50924.44\n",
      "Step: 1732, Training Loss: 0.00708, LR: 0.0000309, Tokens/sec: 47611.52\n",
      "Step: 1733, Training Loss: 0.00720, LR: 0.0000308, Tokens/sec: 46147.37\n",
      "Step: 1734, Training Loss: 0.00770, LR: 0.0000308, Tokens/sec: 49197.34\n",
      "Step: 1735, Training Loss: 0.00613, LR: 0.0000307, Tokens/sec: 48484.50\n",
      "Step: 1736, Training Loss: 0.00659, LR: 0.0000306, Tokens/sec: 47859.71\n",
      "Step: 1737, Training Loss: 0.00425, LR: 0.0000306, Tokens/sec: 51333.32\n",
      "Step: 1738, Training Loss: 0.00891, LR: 0.0000305, Tokens/sec: 48745.66\n",
      "Step: 1739, Training Loss: 0.00677, LR: 0.0000305, Tokens/sec: 51011.80\n",
      "Step: 1740, Training Loss: 0.00666, LR: 0.0000304, Tokens/sec: 52406.15\n",
      "Step: 1741, Training Loss: 0.00550, LR: 0.0000304, Tokens/sec: 49816.79\n",
      "Step: 1742, Training Loss: 0.01013, LR: 0.0000303, Tokens/sec: 49789.27\n",
      "Step: 1743, Training Loss: 0.01228, LR: 0.0000303, Tokens/sec: 47206.49\n",
      "Step: 1744, Training Loss: 0.01083, LR: 0.0000302, Tokens/sec: 50686.27\n",
      "Step: 1745, Training Loss: 0.00677, LR: 0.0000302, Tokens/sec: 49386.38\n",
      "Step: 1746, Training Loss: 0.00899, LR: 0.0000301, Tokens/sec: 50123.14\n",
      "Step: 1747, Training Loss: 0.00918, LR: 0.0000300, Tokens/sec: 48107.63\n",
      "Step: 1748, Training Loss: 0.00644, LR: 0.0000300, Tokens/sec: 50009.47\n",
      "Step: 1749, Training Loss: 0.00908, LR: 0.0000299, Tokens/sec: 51274.88\n",
      "Step: 1750, Training Loss: 0.00566, LR: 0.0000299, Tokens/sec: 50634.58\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 1750, Eval Loss: 0.00785\n",
      "Step: 1751, Training Loss: 0.00822, LR: 0.0000298, Tokens/sec: 51694.78\n",
      "Step: 1752, Training Loss: 0.00478, LR: 0.0000298, Tokens/sec: 48956.51\n",
      "Step: 1753, Training Loss: 0.01191, LR: 0.0000297, Tokens/sec: 49586.93\n",
      "Step: 1754, Training Loss: 0.01200, LR: 0.0000297, Tokens/sec: 49603.49\n",
      "Step: 1755, Training Loss: 0.00492, LR: 0.0000296, Tokens/sec: 48722.21\n",
      "Step: 1756, Training Loss: 0.00573, LR: 0.0000296, Tokens/sec: 51007.70\n",
      "Step: 1757, Training Loss: 0.00715, LR: 0.0000295, Tokens/sec: 51878.08\n",
      "Step: 1758, Training Loss: 0.00593, LR: 0.0000295, Tokens/sec: 51782.48\n",
      "Step: 1759, Training Loss: 0.00671, LR: 0.0000294, Tokens/sec: 50590.88\n",
      "Step: 1760, Training Loss: 0.00490, LR: 0.0000294, Tokens/sec: 50868.18\n",
      "Step: 1761, Training Loss: 0.01359, LR: 0.0000293, Tokens/sec: 50942.26\n",
      "Step: 1762, Training Loss: 0.00405, LR: 0.0000292, Tokens/sec: 47892.60\n",
      "Step: 1763, Training Loss: 0.01030, LR: 0.0000292, Tokens/sec: 50974.93\n",
      "Step: 1764, Training Loss: 0.01321, LR: 0.0000291, Tokens/sec: 48512.33\n",
      "Step: 1765, Training Loss: 0.01679, LR: 0.0000291, Tokens/sec: 50342.85\n",
      "Step: 1766, Training Loss: 0.00771, LR: 0.0000290, Tokens/sec: 52037.06\n",
      "Step: 1767, Training Loss: 0.00509, LR: 0.0000290, Tokens/sec: 51653.29\n",
      "Step: 1768, Training Loss: 0.01219, LR: 0.0000289, Tokens/sec: 49867.70\n",
      "Step: 1769, Training Loss: 0.01072, LR: 0.0000289, Tokens/sec: 47783.49\n",
      "Step: 1770, Training Loss: 0.01212, LR: 0.0000288, Tokens/sec: 49022.01\n",
      "Step: 1771, Training Loss: 0.00757, LR: 0.0000288, Tokens/sec: 49630.70\n",
      "Step: 1772, Training Loss: 0.00796, LR: 0.0000287, Tokens/sec: 50716.01\n",
      "Step: 1773, Training Loss: 0.00625, LR: 0.0000287, Tokens/sec: 48436.95\n",
      "Step: 1774, Training Loss: 0.00793, LR: 0.0000286, Tokens/sec: 48886.70\n",
      "Step: 1775, Training Loss: 0.00531, LR: 0.0000286, Tokens/sec: 50994.63\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 1775, Eval Loss: 0.00732\n",
      "Step: 1776, Training Loss: 0.01373, LR: 0.0000285, Tokens/sec: 50374.45\n",
      "Step: 1777, Training Loss: 0.00566, LR: 0.0000285, Tokens/sec: 51187.46\n",
      "Step: 1778, Training Loss: 0.00990, LR: 0.0000284, Tokens/sec: 48156.74\n",
      "Step: 1779, Training Loss: 0.00946, LR: 0.0000284, Tokens/sec: 48982.99\n",
      "Step: 1780, Training Loss: 0.00763, LR: 0.0000283, Tokens/sec: 49371.81\n",
      "Step: 1781, Training Loss: 0.00612, LR: 0.0000283, Tokens/sec: 49335.20\n",
      "Step: 1782, Training Loss: 0.00423, LR: 0.0000282, Tokens/sec: 49849.48\n",
      "Step: 1783, Training Loss: 0.00760, LR: 0.0000281, Tokens/sec: 48621.62\n",
      "Step: 1784, Training Loss: 0.00616, LR: 0.0000281, Tokens/sec: 49629.60\n",
      "Step: 1785, Training Loss: 0.00475, LR: 0.0000280, Tokens/sec: 49985.03\n",
      "Step: 1786, Training Loss: 0.00528, LR: 0.0000280, Tokens/sec: 46084.07\n",
      "Step: 1787, Training Loss: 0.00526, LR: 0.0000279, Tokens/sec: 46647.35\n",
      "Step: 1788, Training Loss: 0.00797, LR: 0.0000279, Tokens/sec: 49503.16\n",
      "Step: 1789, Training Loss: 0.00690, LR: 0.0000278, Tokens/sec: 50463.62\n",
      "Step: 1790, Training Loss: 0.01594, LR: 0.0000278, Tokens/sec: 48515.39\n",
      "Step: 1791, Training Loss: 0.00634, LR: 0.0000277, Tokens/sec: 49763.04\n",
      "Step: 1792, Training Loss: 0.00632, LR: 0.0000277, Tokens/sec: 48648.21\n",
      "Step: 1793, Training Loss: 0.00506, LR: 0.0000276, Tokens/sec: 50725.93\n",
      "Step: 1794, Training Loss: 0.01663, LR: 0.0000276, Tokens/sec: 50472.12\n",
      "Step: 1795, Training Loss: 0.00464, LR: 0.0000275, Tokens/sec: 50487.96\n",
      "Step: 1796, Training Loss: 0.00649, LR: 0.0000275, Tokens/sec: 48801.89\n",
      "Step: 1797, Training Loss: 0.00521, LR: 0.0000274, Tokens/sec: 51403.31\n",
      "Step: 1798, Training Loss: 0.01064, LR: 0.0000274, Tokens/sec: 48583.32\n",
      "Step: 1799, Training Loss: 0.00557, LR: 0.0000273, Tokens/sec: 49676.07\n",
      "Step: 1800, Training Loss: 0.00520, LR: 0.0000273, Tokens/sec: 50182.52\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 1800, Eval Loss: 0.00753\n",
      "Step: 1801, Training Loss: 0.00616, LR: 0.0000272, Tokens/sec: 49691.73\n",
      "Step: 1802, Training Loss: 0.00456, LR: 0.0000272, Tokens/sec: 48778.91\n",
      "Step: 1803, Training Loss: 0.00499, LR: 0.0000271, Tokens/sec: 51488.40\n",
      "Step: 1804, Training Loss: 0.00712, LR: 0.0000271, Tokens/sec: 51144.85\n",
      "Step: 1805, Training Loss: 0.00460, LR: 0.0000270, Tokens/sec: 50883.06\n",
      "Step: 1806, Training Loss: 0.00597, LR: 0.0000270, Tokens/sec: 49030.19\n",
      "Step: 1807, Training Loss: 0.00414, LR: 0.0000269, Tokens/sec: 50368.87\n",
      "Step: 1808, Training Loss: 0.00932, LR: 0.0000269, Tokens/sec: 50560.48\n",
      "Step: 1809, Training Loss: 0.00640, LR: 0.0000268, Tokens/sec: 48528.03\n",
      "Step: 1810, Training Loss: 0.00887, LR: 0.0000268, Tokens/sec: 49919.10\n",
      "Step: 1811, Training Loss: 0.01040, LR: 0.0000267, Tokens/sec: 50127.64\n",
      "Step: 1812, Training Loss: 0.00449, LR: 0.0000267, Tokens/sec: 49394.30\n",
      "Step: 1813, Training Loss: 0.01799, LR: 0.0000266, Tokens/sec: 50449.93\n",
      "Step: 1814, Training Loss: 0.00579, LR: 0.0000266, Tokens/sec: 50186.17\n",
      "Step: 1815, Training Loss: 0.00510, LR: 0.0000265, Tokens/sec: 50579.12\n",
      "Step: 1816, Training Loss: 0.00620, LR: 0.0000265, Tokens/sec: 49993.89\n",
      "Step: 1817, Training Loss: 0.00534, LR: 0.0000264, Tokens/sec: 50986.68\n",
      "Step: 1818, Training Loss: 0.00551, LR: 0.0000264, Tokens/sec: 48673.64\n",
      "Step: 1819, Training Loss: 0.00901, LR: 0.0000263, Tokens/sec: 51095.72\n",
      "Step: 1820, Training Loss: 0.00476, LR: 0.0000263, Tokens/sec: 48363.96\n",
      "Step: 1821, Training Loss: 0.00391, LR: 0.0000262, Tokens/sec: 50458.01\n",
      "Step: 1822, Training Loss: 0.00779, LR: 0.0000262, Tokens/sec: 49568.91\n",
      "Step: 1823, Training Loss: 0.00642, LR: 0.0000261, Tokens/sec: 50834.99\n",
      "Step: 1824, Training Loss: 0.00457, LR: 0.0000261, Tokens/sec: 50051.19\n",
      "Step: 1825, Training Loss: 0.00620, LR: 0.0000260, Tokens/sec: 50097.65\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 1825, Eval Loss: 0.00565\n",
      "Step: 1826, Training Loss: 0.00690, LR: 0.0000260, Tokens/sec: 51032.68\n",
      "Step: 1827, Training Loss: 0.00573, LR: 0.0000259, Tokens/sec: 50263.83\n",
      "Step: 1828, Training Loss: 0.00946, LR: 0.0000259, Tokens/sec: 49380.28\n",
      "Step: 1829, Training Loss: 0.00487, LR: 0.0000258, Tokens/sec: 50561.87\n",
      "Step: 1830, Training Loss: 0.00679, LR: 0.0000258, Tokens/sec: 49243.72\n",
      "Step: 1831, Training Loss: 0.00588, LR: 0.0000257, Tokens/sec: 51090.06\n",
      "Step: 1832, Training Loss: 0.00475, LR: 0.0000257, Tokens/sec: 51745.53\n",
      "Step: 1833, Training Loss: 0.00510, LR: 0.0000256, Tokens/sec: 51696.97\n",
      "Step: 1834, Training Loss: 0.00569, LR: 0.0000256, Tokens/sec: 49906.32\n",
      "Step: 1835, Training Loss: 0.00352, LR: 0.0000255, Tokens/sec: 49318.20\n",
      "Step: 1836, Training Loss: 0.00506, LR: 0.0000255, Tokens/sec: 51253.07\n",
      "Step: 1837, Training Loss: 0.00842, LR: 0.0000254, Tokens/sec: 50415.67\n",
      "Step: 1838, Training Loss: 0.00751, LR: 0.0000254, Tokens/sec: 50070.87\n",
      "Step: 1839, Training Loss: 0.00508, LR: 0.0000253, Tokens/sec: 51303.41\n",
      "Step: 1840, Training Loss: 0.00517, LR: 0.0000253, Tokens/sec: 48861.42\n",
      "Step: 1841, Training Loss: 0.00714, LR: 0.0000252, Tokens/sec: 48343.35\n",
      "Step: 1842, Training Loss: 0.00748, LR: 0.0000252, Tokens/sec: 51134.57\n",
      "Step: 1843, Training Loss: 0.00515, LR: 0.0000251, Tokens/sec: 50688.27\n",
      "Step: 1844, Training Loss: 0.00870, LR: 0.0000251, Tokens/sec: 48821.04\n",
      "Step: 1845, Training Loss: 0.01398, LR: 0.0000250, Tokens/sec: 50599.98\n",
      "Step: 1846, Training Loss: 0.00717, LR: 0.0000250, Tokens/sec: 49060.90\n",
      "Step: 1847, Training Loss: 0.00791, LR: 0.0000249, Tokens/sec: 50331.88\n",
      "Step: 1848, Training Loss: 0.00501, LR: 0.0000249, Tokens/sec: 50631.57\n",
      "Step: 1849, Training Loss: 0.01288, LR: 0.0000249, Tokens/sec: 51479.82\n",
      "Step: 1850, Training Loss: 0.00606, LR: 0.0000248, Tokens/sec: 51288.43\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 1850, Eval Loss: 0.00604\n",
      "Step: 1851, Training Loss: 0.00589, LR: 0.0000248, Tokens/sec: 50284.71\n",
      "Step: 1852, Training Loss: 0.00612, LR: 0.0000247, Tokens/sec: 48193.28\n",
      "Step: 1853, Training Loss: 0.01254, LR: 0.0000247, Tokens/sec: 49095.02\n",
      "Step: 1854, Training Loss: 0.00833, LR: 0.0000246, Tokens/sec: 48471.90\n",
      "Step: 1855, Training Loss: 0.00547, LR: 0.0000246, Tokens/sec: 50646.49\n",
      "Step: 1856, Training Loss: 0.00470, LR: 0.0000245, Tokens/sec: 50565.32\n",
      "Step: 1857, Training Loss: 0.00490, LR: 0.0000245, Tokens/sec: 48755.93\n",
      "Step: 1858, Training Loss: 0.00395, LR: 0.0000244, Tokens/sec: 51682.28\n",
      "Step: 1859, Training Loss: 0.00444, LR: 0.0000244, Tokens/sec: 50201.48\n",
      "Step: 1860, Training Loss: 0.00397, LR: 0.0000243, Tokens/sec: 49677.69\n",
      "Step: 1861, Training Loss: 0.00545, LR: 0.0000243, Tokens/sec: 46548.64\n",
      "Step: 1862, Training Loss: 0.00605, LR: 0.0000242, Tokens/sec: 51357.46\n",
      "Step: 1863, Training Loss: 0.00414, LR: 0.0000242, Tokens/sec: 48663.26\n",
      "Step: 1864, Training Loss: 0.00942, LR: 0.0000241, Tokens/sec: 50096.85\n",
      "Step: 1865, Training Loss: 0.00332, LR: 0.0000241, Tokens/sec: 51511.77\n",
      "Step: 1866, Training Loss: 0.00373, LR: 0.0000240, Tokens/sec: 51301.07\n",
      "Step: 1867, Training Loss: 0.00680, LR: 0.0000240, Tokens/sec: 50644.08\n",
      "Step: 1868, Training Loss: 0.00731, LR: 0.0000240, Tokens/sec: 50414.67\n",
      "Step: 1869, Training Loss: 0.00505, LR: 0.0000239, Tokens/sec: 51263.14\n",
      "Step: 1870, Training Loss: 0.00479, LR: 0.0000239, Tokens/sec: 49566.45\n",
      "Step: 1871, Training Loss: 0.00501, LR: 0.0000238, Tokens/sec: 52162.02\n",
      "Step: 1872, Training Loss: 0.00611, LR: 0.0000238, Tokens/sec: 48472.07\n",
      "Step: 1873, Training Loss: 0.00404, LR: 0.0000237, Tokens/sec: 51539.26\n",
      "Step: 1874, Training Loss: 0.00562, LR: 0.0000237, Tokens/sec: 51271.89\n",
      "Step: 1875, Training Loss: 0.01150, LR: 0.0000236, Tokens/sec: 50684.85\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 1875, Eval Loss: 0.00552\n",
      "Step: 1876, Training Loss: 0.00440, LR: 0.0000236, Tokens/sec: 51230.98\n",
      "Step: 1877, Training Loss: 0.00676, LR: 0.0000235, Tokens/sec: 49837.46\n",
      "Step: 1878, Training Loss: 0.00411, LR: 0.0000235, Tokens/sec: 49015.36\n",
      "Step: 1879, Training Loss: 0.00473, LR: 0.0000234, Tokens/sec: 49348.48\n",
      "Step: 1880, Training Loss: 0.00662, LR: 0.0000234, Tokens/sec: 48164.32\n",
      "Step: 1881, Training Loss: 0.00559, LR: 0.0000234, Tokens/sec: 50595.31\n",
      "Step: 1882, Training Loss: 0.00364, LR: 0.0000233, Tokens/sec: 50884.34\n",
      "Step: 1883, Training Loss: 0.00496, LR: 0.0000233, Tokens/sec: 50887.74\n",
      "Step: 1884, Training Loss: 0.00492, LR: 0.0000232, Tokens/sec: 50001.72\n",
      "Step: 1885, Training Loss: 0.00530, LR: 0.0000232, Tokens/sec: 50492.23\n",
      "Step: 1886, Training Loss: 0.00665, LR: 0.0000231, Tokens/sec: 50347.56\n",
      "Step: 1887, Training Loss: 0.00837, LR: 0.0000231, Tokens/sec: 49390.70\n",
      "Step: 1888, Training Loss: 0.00759, LR: 0.0000230, Tokens/sec: 49541.65\n",
      "Step: 1889, Training Loss: 0.00269, LR: 0.0000230, Tokens/sec: 48825.55\n",
      "Step: 1890, Training Loss: 0.00549, LR: 0.0000229, Tokens/sec: 50229.30\n",
      "Step: 1891, Training Loss: 0.00774, LR: 0.0000229, Tokens/sec: 51526.34\n",
      "Step: 1892, Training Loss: 0.00499, LR: 0.0000229, Tokens/sec: 51133.69\n",
      "Step: 1893, Training Loss: 0.00407, LR: 0.0000228, Tokens/sec: 50099.03\n",
      "Step: 1894, Training Loss: 0.00798, LR: 0.0000228, Tokens/sec: 46311.06\n",
      "Step: 1895, Training Loss: 0.00583, LR: 0.0000227, Tokens/sec: 50588.94\n",
      "Step: 1896, Training Loss: 0.00329, LR: 0.0000227, Tokens/sec: 48602.38\n",
      "Step: 1897, Training Loss: 0.00552, LR: 0.0000226, Tokens/sec: 50515.03\n",
      "Step: 1898, Training Loss: 0.00413, LR: 0.0000226, Tokens/sec: 48445.18\n",
      "Step: 1899, Training Loss: 0.00604, LR: 0.0000225, Tokens/sec: 50263.37\n",
      "Step: 1900, Training Loss: 0.00457, LR: 0.0000225, Tokens/sec: 51759.58\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 1900, Eval Loss: 0.00415\n",
      "Step: 1901, Training Loss: 0.00404, LR: 0.0000224, Tokens/sec: 50770.10\n",
      "Step: 1902, Training Loss: 0.00364, LR: 0.0000224, Tokens/sec: 50641.88\n",
      "Step: 1903, Training Loss: 0.00436, LR: 0.0000224, Tokens/sec: 49587.88\n",
      "Step: 1904, Training Loss: 0.00433, LR: 0.0000223, Tokens/sec: 49977.06\n",
      "Step: 1905, Training Loss: 0.00411, LR: 0.0000223, Tokens/sec: 50788.29\n",
      "Step: 1906, Training Loss: 0.00246, LR: 0.0000222, Tokens/sec: 48940.96\n",
      "Step: 1907, Training Loss: 0.00505, LR: 0.0000222, Tokens/sec: 49603.51\n",
      "Step: 1908, Training Loss: 0.00598, LR: 0.0000221, Tokens/sec: 49979.05\n",
      "Step: 1909, Training Loss: 0.00386, LR: 0.0000221, Tokens/sec: 50546.10\n",
      "Step: 1910, Training Loss: 0.00412, LR: 0.0000220, Tokens/sec: 50412.26\n",
      "Step: 1911, Training Loss: 0.00406, LR: 0.0000220, Tokens/sec: 52035.17\n",
      "Step: 1912, Training Loss: 0.00528, LR: 0.0000220, Tokens/sec: 50866.17\n",
      "Step: 1913, Training Loss: 0.00506, LR: 0.0000219, Tokens/sec: 47126.02\n",
      "Step: 1914, Training Loss: 0.00522, LR: 0.0000219, Tokens/sec: 50645.21\n",
      "Step: 1915, Training Loss: 0.00610, LR: 0.0000218, Tokens/sec: 49180.15\n",
      "Step: 1916, Training Loss: 0.00540, LR: 0.0000218, Tokens/sec: 50930.40\n",
      "Step: 1917, Training Loss: 0.00370, LR: 0.0000217, Tokens/sec: 51213.96\n",
      "Step: 1918, Training Loss: 0.01056, LR: 0.0000217, Tokens/sec: 50788.78\n",
      "Step: 1919, Training Loss: 0.00401, LR: 0.0000217, Tokens/sec: 49396.25\n",
      "Step: 1920, Training Loss: 0.00491, LR: 0.0000216, Tokens/sec: 44824.31\n",
      "Step: 1921, Training Loss: 0.00348, LR: 0.0000216, Tokens/sec: 49409.38\n",
      "Step: 1922, Training Loss: 0.00396, LR: 0.0000215, Tokens/sec: 49133.54\n",
      "Step: 1923, Training Loss: 0.00751, LR: 0.0000215, Tokens/sec: 49614.93\n",
      "Step: 1924, Training Loss: 0.02060, LR: 0.0000214, Tokens/sec: 48894.12\n",
      "Step: 1925, Training Loss: 0.00356, LR: 0.0000214, Tokens/sec: 50210.71\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 1925, Eval Loss: 0.00759\n",
      "Step: 1926, Training Loss: 0.00421, LR: 0.0000214, Tokens/sec: 51038.02\n",
      "Step: 1927, Training Loss: 0.00377, LR: 0.0000213, Tokens/sec: 49613.06\n",
      "Step: 1928, Training Loss: 0.00259, LR: 0.0000213, Tokens/sec: 50646.88\n",
      "Step: 1929, Training Loss: 0.00297, LR: 0.0000212, Tokens/sec: 50813.62\n",
      "Step: 1930, Training Loss: 0.00493, LR: 0.0000212, Tokens/sec: 50984.58\n",
      "Step: 1931, Training Loss: 0.00628, LR: 0.0000211, Tokens/sec: 49660.23\n",
      "Step: 1932, Training Loss: 0.00378, LR: 0.0000211, Tokens/sec: 49265.61\n",
      "Step: 1933, Training Loss: 0.00730, LR: 0.0000211, Tokens/sec: 51436.76\n",
      "Step: 1934, Training Loss: 0.00551, LR: 0.0000210, Tokens/sec: 48874.56\n",
      "Step: 1935, Training Loss: 0.00384, LR: 0.0000210, Tokens/sec: 50979.97\n",
      "Step: 1936, Training Loss: 0.00341, LR: 0.0000209, Tokens/sec: 50545.84\n",
      "Step: 1937, Training Loss: 0.00357, LR: 0.0000209, Tokens/sec: 49656.80\n",
      "Step: 1938, Training Loss: 0.00529, LR: 0.0000208, Tokens/sec: 51828.51\n",
      "Step: 1939, Training Loss: 0.00554, LR: 0.0000208, Tokens/sec: 50266.25\n",
      "Step: 1940, Training Loss: 0.00315, LR: 0.0000208, Tokens/sec: 49648.02\n",
      "Step: 1941, Training Loss: 0.00303, LR: 0.0000207, Tokens/sec: 49464.75\n",
      "Step: 1942, Training Loss: 0.00283, LR: 0.0000207, Tokens/sec: 52252.38\n",
      "Step: 1943, Training Loss: 0.00509, LR: 0.0000206, Tokens/sec: 48488.50\n",
      "Step: 1944, Training Loss: 0.00992, LR: 0.0000206, Tokens/sec: 49879.08\n",
      "Step: 1945, Training Loss: 0.00317, LR: 0.0000206, Tokens/sec: 52058.35\n",
      "Step: 1946, Training Loss: 0.00922, LR: 0.0000205, Tokens/sec: 49972.17\n",
      "Step: 1947, Training Loss: 0.00797, LR: 0.0000205, Tokens/sec: 50038.18\n",
      "Step: 1948, Training Loss: 0.01380, LR: 0.0000204, Tokens/sec: 47933.95\n",
      "Step: 1949, Training Loss: 0.00722, LR: 0.0000204, Tokens/sec: 50193.80\n",
      "Step: 1950, Training Loss: 0.00541, LR: 0.0000203, Tokens/sec: 48895.42\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 1950, Eval Loss: 0.00534\n",
      "Step: 1951, Training Loss: 0.00370, LR: 0.0000203, Tokens/sec: 49094.68\n",
      "Step: 1952, Training Loss: 0.00321, LR: 0.0000203, Tokens/sec: 50415.77\n",
      "Step: 1953, Training Loss: 0.00828, LR: 0.0000202, Tokens/sec: 47991.09\n",
      "Step: 1954, Training Loss: 0.00719, LR: 0.0000202, Tokens/sec: 48981.11\n",
      "Step: 1955, Training Loss: 0.00328, LR: 0.0000201, Tokens/sec: 50544.00\n",
      "Step: 1956, Training Loss: 0.00290, LR: 0.0000201, Tokens/sec: 49524.93\n",
      "Step: 1957, Training Loss: 0.00346, LR: 0.0000201, Tokens/sec: 50472.01\n",
      "Step: 1958, Training Loss: 0.00435, LR: 0.0000200, Tokens/sec: 48605.39\n",
      "Step: 1959, Training Loss: 0.00555, LR: 0.0000200, Tokens/sec: 51770.79\n",
      "Step: 1960, Training Loss: 0.00634, LR: 0.0000199, Tokens/sec: 48288.38\n",
      "Step: 1961, Training Loss: 0.00687, LR: 0.0000199, Tokens/sec: 50805.18\n",
      "Step: 1962, Training Loss: 0.01126, LR: 0.0000199, Tokens/sec: 49114.29\n",
      "Step: 1963, Training Loss: 0.00338, LR: 0.0000198, Tokens/sec: 50273.42\n",
      "Step: 1964, Training Loss: 0.00289, LR: 0.0000198, Tokens/sec: 50387.05\n",
      "Step: 1965, Training Loss: 0.00726, LR: 0.0000197, Tokens/sec: 50205.81\n",
      "Step: 1966, Training Loss: 0.00590, LR: 0.0000197, Tokens/sec: 49416.82\n",
      "Step: 1967, Training Loss: 0.00420, LR: 0.0000197, Tokens/sec: 49328.75\n",
      "Step: 1968, Training Loss: 0.00994, LR: 0.0000196, Tokens/sec: 50098.68\n",
      "Step: 1969, Training Loss: 0.00718, LR: 0.0000196, Tokens/sec: 47916.46\n",
      "Step: 1970, Training Loss: 0.00284, LR: 0.0000195, Tokens/sec: 49668.58\n",
      "Step: 1971, Training Loss: 0.00677, LR: 0.0000195, Tokens/sec: 49899.81\n",
      "Step: 1972, Training Loss: 0.00748, LR: 0.0000195, Tokens/sec: 50196.06\n",
      "Step: 1973, Training Loss: 0.00549, LR: 0.0000194, Tokens/sec: 50784.28\n",
      "Step: 1974, Training Loss: 0.00269, LR: 0.0000194, Tokens/sec: 51243.65\n",
      "Step: 1975, Training Loss: 0.00494, LR: 0.0000193, Tokens/sec: 49438.25\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 1975, Eval Loss: 0.00334\n",
      "Step: 1976, Training Loss: 0.00793, LR: 0.0000193, Tokens/sec: 48915.08\n",
      "Step: 1977, Training Loss: 0.00348, LR: 0.0000193, Tokens/sec: 48871.93\n",
      "Step: 1978, Training Loss: 0.00398, LR: 0.0000192, Tokens/sec: 49516.32\n",
      "Step: 1979, Training Loss: 0.01208, LR: 0.0000192, Tokens/sec: 47793.48\n",
      "Step: 1980, Training Loss: 0.00265, LR: 0.0000191, Tokens/sec: 44854.12\n",
      "Step: 1981, Training Loss: 0.00586, LR: 0.0000191, Tokens/sec: 47123.32\n",
      "Step: 1982, Training Loss: 0.00342, LR: 0.0000191, Tokens/sec: 46785.80\n",
      "Step: 1983, Training Loss: 0.00362, LR: 0.0000190, Tokens/sec: 48905.27\n",
      "Step: 1984, Training Loss: 0.00453, LR: 0.0000190, Tokens/sec: 43255.75\n",
      "Step: 1985, Training Loss: 0.00353, LR: 0.0000189, Tokens/sec: 43583.15\n",
      "Step: 1986, Training Loss: 0.00412, LR: 0.0000189, Tokens/sec: 42926.48\n",
      "Step: 1987, Training Loss: 0.00468, LR: 0.0000189, Tokens/sec: 42669.74\n",
      "Step: 1988, Training Loss: 0.01091, LR: 0.0000188, Tokens/sec: 50974.32\n",
      "Step: 1989, Training Loss: 0.00316, LR: 0.0000188, Tokens/sec: 47892.33\n",
      "Step: 1990, Training Loss: 0.00314, LR: 0.0000188, Tokens/sec: 55902.48\n",
      "Step: 1991, Training Loss: 0.00279, LR: 0.0000187, Tokens/sec: 54579.31\n",
      "Step: 1992, Training Loss: 0.00398, LR: 0.0000187, Tokens/sec: 47681.25\n",
      "Step: 1993, Training Loss: 0.00646, LR: 0.0000186, Tokens/sec: 48752.13\n",
      "Step: 1994, Training Loss: 0.00203, LR: 0.0000186, Tokens/sec: 44480.31\n",
      "Step: 1995, Training Loss: 0.00192, LR: 0.0000186, Tokens/sec: 45985.23\n",
      "Step: 1996, Training Loss: 0.00396, LR: 0.0000185, Tokens/sec: 45998.00\n",
      "Step: 1997, Training Loss: 0.00261, LR: 0.0000185, Tokens/sec: 48396.75\n",
      "Step: 1998, Training Loss: 0.00344, LR: 0.0000185, Tokens/sec: 43733.61\n",
      "Step: 1999, Training Loss: 0.00411, LR: 0.0000184, Tokens/sec: 50899.80\n",
      "Step: 2000, Training Loss: 0.00614, LR: 0.0000184, Tokens/sec: 54724.91\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 2000, Eval Loss: 0.00445\n",
      "Step: 2001, Training Loss: 0.00262, LR: 0.0000183, Tokens/sec: 50303.15\n",
      "Step: 2002, Training Loss: 0.00503, LR: 0.0000183, Tokens/sec: 51720.33\n",
      "Step: 2003, Training Loss: 0.00426, LR: 0.0000183, Tokens/sec: 48791.47\n",
      "Step: 2004, Training Loss: 0.00287, LR: 0.0000182, Tokens/sec: 53081.82\n",
      "Step: 2005, Training Loss: 0.00581, LR: 0.0000182, Tokens/sec: 53148.07\n",
      "Step: 2006, Training Loss: 0.00541, LR: 0.0000182, Tokens/sec: 53185.01\n",
      "Step: 2007, Training Loss: 0.00697, LR: 0.0000181, Tokens/sec: 49151.09\n",
      "Step: 2008, Training Loss: 0.00203, LR: 0.0000181, Tokens/sec: 50207.02\n",
      "Step: 2009, Training Loss: 0.00231, LR: 0.0000180, Tokens/sec: 55485.76\n",
      "Step: 2010, Training Loss: 0.00407, LR: 0.0000180, Tokens/sec: 55669.93\n",
      "Step: 2011, Training Loss: 0.00236, LR: 0.0000180, Tokens/sec: 52668.80\n",
      "Step: 2012, Training Loss: 0.00899, LR: 0.0000179, Tokens/sec: 54155.51\n",
      "Step: 2013, Training Loss: 0.00322, LR: 0.0000179, Tokens/sec: 53935.30\n",
      "Step: 2014, Training Loss: 0.00274, LR: 0.0000179, Tokens/sec: 53382.22\n",
      "Step: 2015, Training Loss: 0.00202, LR: 0.0000178, Tokens/sec: 53689.84\n",
      "Step: 2016, Training Loss: 0.00408, LR: 0.0000178, Tokens/sec: 51882.98\n",
      "Step: 2017, Training Loss: 0.00356, LR: 0.0000177, Tokens/sec: 55414.20\n",
      "Step: 2018, Training Loss: 0.00860, LR: 0.0000177, Tokens/sec: 54900.18\n",
      "Step: 2019, Training Loss: 0.00329, LR: 0.0000177, Tokens/sec: 55285.80\n",
      "Step: 2020, Training Loss: 0.00265, LR: 0.0000176, Tokens/sec: 53876.93\n",
      "Step: 2021, Training Loss: 0.00344, LR: 0.0000176, Tokens/sec: 51861.35\n",
      "Step: 2022, Training Loss: 0.00335, LR: 0.0000176, Tokens/sec: 53776.90\n",
      "Step: 2023, Training Loss: 0.00296, LR: 0.0000175, Tokens/sec: 51992.20\n",
      "Step: 2024, Training Loss: 0.00333, LR: 0.0000175, Tokens/sec: 53460.84\n",
      "Step: 2025, Training Loss: 0.00381, LR: 0.0000175, Tokens/sec: 54324.42\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 2025, Eval Loss: 0.00415\n",
      "Step: 2026, Training Loss: 0.00228, LR: 0.0000174, Tokens/sec: 54968.05\n",
      "Step: 2027, Training Loss: 0.00266, LR: 0.0000174, Tokens/sec: 51605.30\n",
      "Step: 2028, Training Loss: 0.00198, LR: 0.0000174, Tokens/sec: 53765.62\n",
      "Step: 2029, Training Loss: 0.00228, LR: 0.0000173, Tokens/sec: 53031.91\n",
      "Step: 2030, Training Loss: 0.00231, LR: 0.0000173, Tokens/sec: 55062.61\n",
      "Step: 2031, Training Loss: 0.00323, LR: 0.0000172, Tokens/sec: 53282.86\n",
      "Step: 2032, Training Loss: 0.00229, LR: 0.0000172, Tokens/sec: 53574.92\n",
      "Step: 2033, Training Loss: 0.00191, LR: 0.0000172, Tokens/sec: 53541.28\n",
      "Step: 2034, Training Loss: 0.00400, LR: 0.0000171, Tokens/sec: 52477.16\n",
      "Step: 2035, Training Loss: 0.00422, LR: 0.0000171, Tokens/sec: 48611.27\n",
      "Step: 2036, Training Loss: 0.00324, LR: 0.0000171, Tokens/sec: 49452.43\n",
      "Step: 2037, Training Loss: 0.00243, LR: 0.0000170, Tokens/sec: 53756.49\n",
      "Step: 2038, Training Loss: 0.00329, LR: 0.0000170, Tokens/sec: 54292.21\n",
      "Step: 2039, Training Loss: 0.00352, LR: 0.0000170, Tokens/sec: 47582.85\n",
      "Step: 2040, Training Loss: 0.00437, LR: 0.0000169, Tokens/sec: 54003.32\n",
      "Step: 2041, Training Loss: 0.00253, LR: 0.0000169, Tokens/sec: 51511.33\n",
      "Step: 2042, Training Loss: 0.00370, LR: 0.0000169, Tokens/sec: 54742.28\n",
      "Step: 2043, Training Loss: 0.00345, LR: 0.0000168, Tokens/sec: 54858.84\n",
      "Step: 2044, Training Loss: 0.00557, LR: 0.0000168, Tokens/sec: 53214.84\n",
      "Step: 2045, Training Loss: 0.00348, LR: 0.0000168, Tokens/sec: 53876.72\n",
      "Step: 2046, Training Loss: 0.00351, LR: 0.0000167, Tokens/sec: 52410.56\n",
      "Step: 2047, Training Loss: 0.00499, LR: 0.0000167, Tokens/sec: 51460.89\n",
      "Step: 2048, Training Loss: 0.00561, LR: 0.0000167, Tokens/sec: 41815.60\n",
      "Step: 2049, Training Loss: 0.00584, LR: 0.0000166, Tokens/sec: 46057.27\n",
      "Step: 2050, Training Loss: 0.00223, LR: 0.0000166, Tokens/sec: 50076.05\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 2050, Eval Loss: 0.00437\n",
      "Step: 2051, Training Loss: 0.00330, LR: 0.0000166, Tokens/sec: 45515.46\n",
      "Step: 2052, Training Loss: 0.00315, LR: 0.0000165, Tokens/sec: 45707.20\n",
      "Step: 2053, Training Loss: 0.00458, LR: 0.0000165, Tokens/sec: 48803.82\n",
      "Step: 2054, Training Loss: 0.00578, LR: 0.0000165, Tokens/sec: 54180.00\n",
      "Step: 2055, Training Loss: 0.00466, LR: 0.0000164, Tokens/sec: 45932.07\n",
      "Step: 2056, Training Loss: 0.00249, LR: 0.0000164, Tokens/sec: 49877.77\n",
      "Step: 2057, Training Loss: 0.00245, LR: 0.0000164, Tokens/sec: 50503.98\n",
      "Step: 2058, Training Loss: 0.00198, LR: 0.0000163, Tokens/sec: 50737.72\n",
      "Step: 2059, Training Loss: 0.00287, LR: 0.0000163, Tokens/sec: 48770.83\n",
      "Step: 2060, Training Loss: 0.00455, LR: 0.0000163, Tokens/sec: 43578.50\n",
      "Step: 2061, Training Loss: 0.00298, LR: 0.0000162, Tokens/sec: 52292.26\n",
      "Step: 2062, Training Loss: 0.00362, LR: 0.0000162, Tokens/sec: 48884.92\n",
      "Step: 2063, Training Loss: 0.00218, LR: 0.0000162, Tokens/sec: 52152.02\n",
      "Step: 2064, Training Loss: 0.00317, LR: 0.0000161, Tokens/sec: 52669.96\n",
      "Step: 2065, Training Loss: 0.00330, LR: 0.0000161, Tokens/sec: 49006.57\n",
      "Step: 2066, Training Loss: 0.00270, LR: 0.0000161, Tokens/sec: 53549.39\n",
      "Step: 2067, Training Loss: 0.00295, LR: 0.0000160, Tokens/sec: 52385.25\n",
      "Step: 2068, Training Loss: 0.00371, LR: 0.0000160, Tokens/sec: 51748.76\n",
      "Step: 2069, Training Loss: 0.00549, LR: 0.0000160, Tokens/sec: 51930.20\n",
      "Step: 2070, Training Loss: 0.00228, LR: 0.0000159, Tokens/sec: 47377.39\n",
      "Step: 2071, Training Loss: 0.00160, LR: 0.0000159, Tokens/sec: 51662.11\n",
      "Step: 2072, Training Loss: 0.00315, LR: 0.0000159, Tokens/sec: 50197.21\n",
      "Step: 2073, Training Loss: 0.00249, LR: 0.0000158, Tokens/sec: 43952.57\n",
      "Step: 2074, Training Loss: 0.00219, LR: 0.0000158, Tokens/sec: 44889.88\n",
      "Step: 2075, Training Loss: 0.00419, LR: 0.0000158, Tokens/sec: 45497.94\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 2075, Eval Loss: 0.00483\n",
      "Step: 2076, Training Loss: 0.00259, LR: 0.0000157, Tokens/sec: 42701.09\n",
      "Step: 2077, Training Loss: 0.00177, LR: 0.0000157, Tokens/sec: 45676.50\n",
      "Step: 2078, Training Loss: 0.00487, LR: 0.0000157, Tokens/sec: 46019.72\n",
      "Step: 2079, Training Loss: 0.00404, LR: 0.0000157, Tokens/sec: 44779.00\n",
      "Step: 2080, Training Loss: 0.00364, LR: 0.0000156, Tokens/sec: 46314.17\n",
      "Step: 2081, Training Loss: 0.00822, LR: 0.0000156, Tokens/sec: 48384.66\n",
      "Step: 2082, Training Loss: 0.00247, LR: 0.0000156, Tokens/sec: 45038.59\n",
      "Step: 2083, Training Loss: 0.00416, LR: 0.0000155, Tokens/sec: 43589.07\n",
      "Step: 2084, Training Loss: 0.00272, LR: 0.0000155, Tokens/sec: 53187.23\n",
      "Step: 2085, Training Loss: 0.00232, LR: 0.0000155, Tokens/sec: 45730.65\n",
      "Step: 2086, Training Loss: 0.00744, LR: 0.0000154, Tokens/sec: 49614.15\n",
      "Step: 2087, Training Loss: 0.00519, LR: 0.0000154, Tokens/sec: 53216.47\n",
      "Step: 2088, Training Loss: 0.00377, LR: 0.0000154, Tokens/sec: 54393.49\n",
      "Step: 2089, Training Loss: 0.00240, LR: 0.0000153, Tokens/sec: 47228.44\n",
      "Step: 2090, Training Loss: 0.00308, LR: 0.0000153, Tokens/sec: 47385.14\n",
      "Step: 2091, Training Loss: 0.00464, LR: 0.0000153, Tokens/sec: 52549.54\n",
      "Step: 2092, Training Loss: 0.00252, LR: 0.0000153, Tokens/sec: 50996.35\n",
      "Step: 2093, Training Loss: 0.00217, LR: 0.0000152, Tokens/sec: 53016.33\n",
      "Step: 2094, Training Loss: 0.00286, LR: 0.0000152, Tokens/sec: 54173.10\n",
      "Step: 2095, Training Loss: 0.00495, LR: 0.0000152, Tokens/sec: 52944.80\n",
      "Step: 2096, Training Loss: 0.00316, LR: 0.0000151, Tokens/sec: 54032.09\n",
      "Step: 2097, Training Loss: 0.00601, LR: 0.0000151, Tokens/sec: 50612.71\n",
      "Step: 2098, Training Loss: 0.00363, LR: 0.0000151, Tokens/sec: 53544.98\n",
      "Step: 2099, Training Loss: 0.00351, LR: 0.0000150, Tokens/sec: 53264.50\n",
      "Step: 2100, Training Loss: 0.00176, LR: 0.0000150, Tokens/sec: 54683.94\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 2100, Eval Loss: 0.00590\n",
      "Step: 2101, Training Loss: 0.00400, LR: 0.0000150, Tokens/sec: 55319.82\n",
      "Step: 2102, Training Loss: 0.00348, LR: 0.0000150, Tokens/sec: 53859.66\n",
      "Step: 2103, Training Loss: 0.00324, LR: 0.0000149, Tokens/sec: 52755.23\n",
      "Step: 2104, Training Loss: 0.00244, LR: 0.0000149, Tokens/sec: 54336.40\n",
      "Step: 2105, Training Loss: 0.00401, LR: 0.0000149, Tokens/sec: 55933.21\n",
      "Step: 2106, Training Loss: 0.00278, LR: 0.0000148, Tokens/sec: 53463.48\n",
      "Step: 2107, Training Loss: 0.01212, LR: 0.0000148, Tokens/sec: 45285.26\n",
      "Step: 2108, Training Loss: 0.00314, LR: 0.0000148, Tokens/sec: 47770.10\n",
      "Step: 2109, Training Loss: 0.00217, LR: 0.0000147, Tokens/sec: 45651.67\n",
      "Step: 2110, Training Loss: 0.00408, LR: 0.0000147, Tokens/sec: 47600.26\n",
      "Step: 2111, Training Loss: 0.00178, LR: 0.0000147, Tokens/sec: 53065.62\n",
      "Step: 2112, Training Loss: 0.00380, LR: 0.0000147, Tokens/sec: 44572.75\n",
      "Step: 2113, Training Loss: 0.00467, LR: 0.0000146, Tokens/sec: 53562.20\n",
      "Step: 2114, Training Loss: 0.00420, LR: 0.0000146, Tokens/sec: 41458.04\n",
      "Step: 2115, Training Loss: 0.00273, LR: 0.0000146, Tokens/sec: 53618.15\n",
      "Step: 2116, Training Loss: 0.00288, LR: 0.0000145, Tokens/sec: 52500.37\n",
      "Step: 2117, Training Loss: 0.00539, LR: 0.0000145, Tokens/sec: 46891.56\n",
      "Step: 2118, Training Loss: 0.00279, LR: 0.0000145, Tokens/sec: 52595.44\n",
      "Step: 2119, Training Loss: 0.00190, LR: 0.0000145, Tokens/sec: 43801.47\n",
      "Step: 2120, Training Loss: 0.00247, LR: 0.0000144, Tokens/sec: 41525.42\n",
      "Step: 2121, Training Loss: 0.00214, LR: 0.0000144, Tokens/sec: 51588.39\n",
      "Step: 2122, Training Loss: 0.00315, LR: 0.0000144, Tokens/sec: 46396.54\n",
      "Step: 2123, Training Loss: 0.00308, LR: 0.0000144, Tokens/sec: 49698.22\n",
      "Step: 2124, Training Loss: 0.00369, LR: 0.0000143, Tokens/sec: 48169.63\n",
      "Step: 2125, Training Loss: 0.00210, LR: 0.0000143, Tokens/sec: 46642.28\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 2125, Eval Loss: 0.00249\n",
      "Step: 2126, Training Loss: 0.00335, LR: 0.0000143, Tokens/sec: 44929.53\n",
      "Step: 2127, Training Loss: 0.00202, LR: 0.0000142, Tokens/sec: 39624.34\n",
      "Step: 2128, Training Loss: 0.00307, LR: 0.0000142, Tokens/sec: 47172.35\n",
      "Step: 2129, Training Loss: 0.00274, LR: 0.0000142, Tokens/sec: 49144.27\n",
      "Step: 2130, Training Loss: 0.00213, LR: 0.0000142, Tokens/sec: 43880.33\n",
      "Step: 2131, Training Loss: 0.00244, LR: 0.0000141, Tokens/sec: 44473.02\n",
      "Step: 2132, Training Loss: 0.00280, LR: 0.0000141, Tokens/sec: 42616.59\n",
      "Step: 2133, Training Loss: 0.00224, LR: 0.0000141, Tokens/sec: 43156.59\n",
      "Step: 2134, Training Loss: 0.00188, LR: 0.0000141, Tokens/sec: 44306.51\n",
      "Step: 2135, Training Loss: 0.00223, LR: 0.0000140, Tokens/sec: 45842.60\n",
      "Step: 2136, Training Loss: 0.00263, LR: 0.0000140, Tokens/sec: 43298.15\n",
      "Step: 2137, Training Loss: 0.00246, LR: 0.0000140, Tokens/sec: 44091.03\n",
      "Step: 2138, Training Loss: 0.00329, LR: 0.0000139, Tokens/sec: 50050.67\n",
      "Step: 2139, Training Loss: 0.00089, LR: 0.0000139, Tokens/sec: 48649.95\n",
      "Step: 2140, Training Loss: 0.00254, LR: 0.0000139, Tokens/sec: 49428.91\n",
      "Step: 2141, Training Loss: 0.00252, LR: 0.0000139, Tokens/sec: 47332.55\n",
      "Step: 2142, Training Loss: 0.00525, LR: 0.0000138, Tokens/sec: 49110.25\n",
      "Step: 2143, Training Loss: 0.00140, LR: 0.0000138, Tokens/sec: 44332.81\n",
      "Step: 2144, Training Loss: 0.00173, LR: 0.0000138, Tokens/sec: 49011.52\n",
      "Step: 2145, Training Loss: 0.00417, LR: 0.0000138, Tokens/sec: 49985.89\n",
      "Step: 2146, Training Loss: 0.00472, LR: 0.0000137, Tokens/sec: 49386.27\n",
      "Step: 2147, Training Loss: 0.00321, LR: 0.0000137, Tokens/sec: 45224.40\n",
      "Step: 2148, Training Loss: 0.00302, LR: 0.0000137, Tokens/sec: 50583.33\n",
      "Step: 2149, Training Loss: 0.00193, LR: 0.0000137, Tokens/sec: 48839.26\n",
      "Step: 2150, Training Loss: 0.00378, LR: 0.0000136, Tokens/sec: 50911.40\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 2150, Eval Loss: 0.00298\n",
      "Step: 2151, Training Loss: 0.00185, LR: 0.0000136, Tokens/sec: 41623.99\n",
      "Step: 2152, Training Loss: 0.00296, LR: 0.0000136, Tokens/sec: 48019.96\n",
      "Step: 2153, Training Loss: 0.00276, LR: 0.0000136, Tokens/sec: 49483.01\n",
      "Step: 2154, Training Loss: 0.00178, LR: 0.0000135, Tokens/sec: 46992.15\n",
      "Step: 2155, Training Loss: 0.00138, LR: 0.0000135, Tokens/sec: 45578.93\n",
      "Step: 2156, Training Loss: 0.00233, LR: 0.0000135, Tokens/sec: 44484.44\n",
      "Step: 2157, Training Loss: 0.00381, LR: 0.0000135, Tokens/sec: 48817.23\n",
      "Step: 2158, Training Loss: 0.00248, LR: 0.0000134, Tokens/sec: 47222.17\n",
      "Step: 2159, Training Loss: 0.00229, LR: 0.0000134, Tokens/sec: 48813.84\n",
      "Step: 2160, Training Loss: 0.00325, LR: 0.0000134, Tokens/sec: 47376.30\n",
      "Step: 2161, Training Loss: 0.00290, LR: 0.0000134, Tokens/sec: 46711.10\n",
      "Step: 2162, Training Loss: 0.00232, LR: 0.0000133, Tokens/sec: 48969.97\n",
      "Step: 2163, Training Loss: 0.00509, LR: 0.0000133, Tokens/sec: 49134.04\n",
      "Step: 2164, Training Loss: 0.00312, LR: 0.0000133, Tokens/sec: 49577.82\n",
      "Step: 2165, Training Loss: 0.00237, LR: 0.0000133, Tokens/sec: 48723.89\n",
      "Step: 2166, Training Loss: 0.00145, LR: 0.0000132, Tokens/sec: 48316.15\n",
      "Step: 2167, Training Loss: 0.00248, LR: 0.0000132, Tokens/sec: 47338.34\n",
      "Step: 2168, Training Loss: 0.00206, LR: 0.0000132, Tokens/sec: 51279.19\n",
      "Step: 2169, Training Loss: 0.00252, LR: 0.0000132, Tokens/sec: 47394.16\n",
      "Step: 2170, Training Loss: 0.00258, LR: 0.0000131, Tokens/sec: 48462.01\n",
      "Step: 2171, Training Loss: 0.00188, LR: 0.0000131, Tokens/sec: 50922.04\n",
      "Step: 2172, Training Loss: 0.00263, LR: 0.0000131, Tokens/sec: 40560.49\n",
      "Step: 2173, Training Loss: 0.00306, LR: 0.0000131, Tokens/sec: 46586.44\n",
      "Step: 2174, Training Loss: 0.00264, LR: 0.0000130, Tokens/sec: 52828.24\n",
      "Step: 2175, Training Loss: 0.00277, LR: 0.0000130, Tokens/sec: 49915.24\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 2175, Eval Loss: 0.00170\n",
      "Step: 2176, Training Loss: 0.00231, LR: 0.0000130, Tokens/sec: 47171.74\n",
      "Step: 2177, Training Loss: 0.00236, LR: 0.0000130, Tokens/sec: 52680.48\n",
      "Step: 2178, Training Loss: 0.00241, LR: 0.0000130, Tokens/sec: 52373.74\n",
      "Step: 2179, Training Loss: 0.00122, LR: 0.0000129, Tokens/sec: 49142.73\n",
      "Step: 2180, Training Loss: 0.00308, LR: 0.0000129, Tokens/sec: 42155.49\n",
      "Step: 2181, Training Loss: 0.00186, LR: 0.0000129, Tokens/sec: 47481.35\n",
      "Step: 2182, Training Loss: 0.00343, LR: 0.0000129, Tokens/sec: 52770.98\n",
      "Step: 2183, Training Loss: 0.00280, LR: 0.0000128, Tokens/sec: 50724.07\n",
      "Step: 2184, Training Loss: 0.00137, LR: 0.0000128, Tokens/sec: 49205.26\n",
      "Step: 2185, Training Loss: 0.00123, LR: 0.0000128, Tokens/sec: 48166.24\n",
      "Step: 2186, Training Loss: 0.00368, LR: 0.0000128, Tokens/sec: 43593.06\n",
      "Step: 2187, Training Loss: 0.00239, LR: 0.0000128, Tokens/sec: 52515.48\n",
      "Step: 2188, Training Loss: 0.00299, LR: 0.0000127, Tokens/sec: 50912.59\n",
      "Step: 2189, Training Loss: 0.00326, LR: 0.0000127, Tokens/sec: 46563.94\n",
      "Step: 2190, Training Loss: 0.00232, LR: 0.0000127, Tokens/sec: 45652.65\n",
      "Step: 2191, Training Loss: 0.00243, LR: 0.0000127, Tokens/sec: 51483.18\n",
      "Step: 2192, Training Loss: 0.00482, LR: 0.0000126, Tokens/sec: 48306.98\n",
      "Step: 2193, Training Loss: 0.00268, LR: 0.0000126, Tokens/sec: 47990.50\n",
      "Step: 2194, Training Loss: 0.00244, LR: 0.0000126, Tokens/sec: 45746.79\n",
      "Step: 2195, Training Loss: 0.00150, LR: 0.0000126, Tokens/sec: 51931.51\n",
      "Step: 2196, Training Loss: 0.00247, LR: 0.0000126, Tokens/sec: 49513.60\n",
      "Step: 2197, Training Loss: 0.00256, LR: 0.0000125, Tokens/sec: 52880.22\n",
      "Step: 2198, Training Loss: 0.00335, LR: 0.0000125, Tokens/sec: 50177.16\n",
      "Step: 2199, Training Loss: 0.00307, LR: 0.0000125, Tokens/sec: 50112.68\n",
      "Step: 2200, Training Loss: 0.00296, LR: 0.0000125, Tokens/sec: 38280.84\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 2200, Eval Loss: 0.00325\n",
      "Step: 2201, Training Loss: 0.00181, LR: 0.0000125, Tokens/sec: 53414.56\n",
      "Step: 2202, Training Loss: 0.00154, LR: 0.0000124, Tokens/sec: 39837.03\n",
      "Step: 2203, Training Loss: 0.00193, LR: 0.0000124, Tokens/sec: 44572.40\n",
      "Step: 2204, Training Loss: 0.00156, LR: 0.0000124, Tokens/sec: 43161.52\n",
      "Step: 2205, Training Loss: 0.00394, LR: 0.0000124, Tokens/sec: 51464.28\n",
      "Step: 2206, Training Loss: 0.00158, LR: 0.0000123, Tokens/sec: 43835.57\n",
      "Step: 2207, Training Loss: 0.00305, LR: 0.0000123, Tokens/sec: 47670.68\n",
      "Step: 2208, Training Loss: 0.00269, LR: 0.0000123, Tokens/sec: 37635.67\n",
      "Step: 2209, Training Loss: 0.00230, LR: 0.0000123, Tokens/sec: 50800.30\n",
      "Step: 2210, Training Loss: 0.00440, LR: 0.0000123, Tokens/sec: 49719.75\n",
      "Step: 2211, Training Loss: 0.00680, LR: 0.0000122, Tokens/sec: 50990.00\n",
      "Step: 2212, Training Loss: 0.00282, LR: 0.0000122, Tokens/sec: 49971.06\n",
      "Step: 2213, Training Loss: 0.00233, LR: 0.0000122, Tokens/sec: 44391.13\n",
      "Step: 2214, Training Loss: 0.00266, LR: 0.0000122, Tokens/sec: 48293.85\n",
      "Step: 2215, Training Loss: 0.00152, LR: 0.0000122, Tokens/sec: 51158.22\n",
      "Step: 2216, Training Loss: 0.00289, LR: 0.0000121, Tokens/sec: 50510.92\n",
      "Step: 2217, Training Loss: 0.00290, LR: 0.0000121, Tokens/sec: 49616.18\n",
      "Step: 2218, Training Loss: 0.00234, LR: 0.0000121, Tokens/sec: 50759.84\n",
      "Step: 2219, Training Loss: 0.00287, LR: 0.0000121, Tokens/sec: 49880.14\n",
      "Step: 2220, Training Loss: 0.00300, LR: 0.0000121, Tokens/sec: 51241.66\n",
      "Step: 2221, Training Loss: 0.00308, LR: 0.0000120, Tokens/sec: 51169.92\n",
      "Step: 2222, Training Loss: 0.00172, LR: 0.0000120, Tokens/sec: 50481.22\n",
      "Step: 2223, Training Loss: 0.00145, LR: 0.0000120, Tokens/sec: 51568.91\n",
      "Step: 2224, Training Loss: 0.00243, LR: 0.0000120, Tokens/sec: 48765.05\n",
      "Step: 2225, Training Loss: 0.00185, LR: 0.0000120, Tokens/sec: 52451.11\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 2225, Eval Loss: 0.00279\n",
      "Step: 2226, Training Loss: 0.00178, LR: 0.0000120, Tokens/sec: 49913.71\n",
      "Step: 2227, Training Loss: 0.00322, LR: 0.0000119, Tokens/sec: 49595.76\n",
      "Step: 2228, Training Loss: 0.00138, LR: 0.0000119, Tokens/sec: 50781.46\n",
      "Step: 2229, Training Loss: 0.00130, LR: 0.0000119, Tokens/sec: 52802.03\n",
      "Step: 2230, Training Loss: 0.00104, LR: 0.0000119, Tokens/sec: 51520.66\n",
      "Step: 2231, Training Loss: 0.00699, LR: 0.0000119, Tokens/sec: 50101.05\n",
      "Step: 2232, Training Loss: 0.00229, LR: 0.0000118, Tokens/sec: 49111.47\n",
      "Step: 2233, Training Loss: 0.00175, LR: 0.0000118, Tokens/sec: 52182.41\n",
      "Step: 2234, Training Loss: 0.00254, LR: 0.0000118, Tokens/sec: 48978.46\n",
      "Step: 2235, Training Loss: 0.00267, LR: 0.0000118, Tokens/sec: 45475.46\n",
      "Step: 2236, Training Loss: 0.00128, LR: 0.0000118, Tokens/sec: 47177.84\n",
      "Step: 2237, Training Loss: 0.00181, LR: 0.0000118, Tokens/sec: 50642.22\n",
      "Step: 2238, Training Loss: 0.00097, LR: 0.0000117, Tokens/sec: 53404.62\n",
      "Step: 2239, Training Loss: 0.00186, LR: 0.0000117, Tokens/sec: 46598.29\n",
      "Step: 2240, Training Loss: 0.00376, LR: 0.0000117, Tokens/sec: 48488.20\n",
      "Step: 2241, Training Loss: 0.00211, LR: 0.0000117, Tokens/sec: 49234.70\n",
      "Step: 2242, Training Loss: 0.00200, LR: 0.0000117, Tokens/sec: 48761.89\n",
      "Step: 2243, Training Loss: 0.00188, LR: 0.0000116, Tokens/sec: 48813.51\n",
      "Step: 2244, Training Loss: 0.00082, LR: 0.0000116, Tokens/sec: 46558.73\n",
      "Step: 2245, Training Loss: 0.00515, LR: 0.0000116, Tokens/sec: 48338.09\n",
      "Step: 2246, Training Loss: 0.00165, LR: 0.0000116, Tokens/sec: 48809.70\n",
      "Step: 2247, Training Loss: 0.00462, LR: 0.0000116, Tokens/sec: 47819.93\n",
      "Step: 2248, Training Loss: 0.00208, LR: 0.0000116, Tokens/sec: 49612.50\n",
      "Step: 2249, Training Loss: 0.00219, LR: 0.0000115, Tokens/sec: 50861.99\n",
      "Step: 2250, Training Loss: 0.00236, LR: 0.0000115, Tokens/sec: 50498.44\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 2250, Eval Loss: 0.00203\n",
      "Step: 2251, Training Loss: 0.00207, LR: 0.0000115, Tokens/sec: 49060.49\n",
      "Step: 2252, Training Loss: 0.00158, LR: 0.0000115, Tokens/sec: 49881.95\n",
      "Step: 2253, Training Loss: 0.00262, LR: 0.0000115, Tokens/sec: 48719.51\n",
      "Step: 2254, Training Loss: 0.00098, LR: 0.0000115, Tokens/sec: 49931.55\n",
      "Step: 2255, Training Loss: 0.00225, LR: 0.0000114, Tokens/sec: 48349.41\n",
      "Step: 2256, Training Loss: 0.00113, LR: 0.0000114, Tokens/sec: 49529.88\n",
      "Step: 2257, Training Loss: 0.00197, LR: 0.0000114, Tokens/sec: 48648.99\n",
      "Step: 2258, Training Loss: 0.00178, LR: 0.0000114, Tokens/sec: 49643.55\n",
      "Step: 2259, Training Loss: 0.00232, LR: 0.0000114, Tokens/sec: 50180.64\n",
      "Step: 2260, Training Loss: 0.00150, LR: 0.0000114, Tokens/sec: 49897.99\n",
      "Step: 2261, Training Loss: 0.00155, LR: 0.0000113, Tokens/sec: 50644.47\n",
      "Step: 2262, Training Loss: 0.00127, LR: 0.0000113, Tokens/sec: 46802.70\n",
      "Step: 2263, Training Loss: 0.00249, LR: 0.0000113, Tokens/sec: 50240.58\n",
      "Step: 2264, Training Loss: 0.00340, LR: 0.0000113, Tokens/sec: 44604.60\n",
      "Step: 2265, Training Loss: 0.00232, LR: 0.0000113, Tokens/sec: 45521.97\n",
      "Step: 2266, Training Loss: 0.00170, LR: 0.0000113, Tokens/sec: 42912.67\n",
      "Step: 2267, Training Loss: 0.00202, LR: 0.0000113, Tokens/sec: 50627.15\n",
      "Step: 2268, Training Loss: 0.00092, LR: 0.0000112, Tokens/sec: 40598.64\n",
      "Step: 2269, Training Loss: 0.00271, LR: 0.0000112, Tokens/sec: 46229.25\n",
      "Step: 2270, Training Loss: 0.00226, LR: 0.0000112, Tokens/sec: 46816.57\n",
      "Step: 2271, Training Loss: 0.00140, LR: 0.0000112, Tokens/sec: 53771.63\n",
      "Step: 2272, Training Loss: 0.00328, LR: 0.0000112, Tokens/sec: 44980.63\n",
      "Step: 2273, Training Loss: 0.00135, LR: 0.0000112, Tokens/sec: 51019.98\n",
      "Step: 2274, Training Loss: 0.00235, LR: 0.0000112, Tokens/sec: 54175.09\n",
      "Step: 2275, Training Loss: 0.00206, LR: 0.0000111, Tokens/sec: 52819.13\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 2275, Eval Loss: 0.00224\n",
      "Step: 2276, Training Loss: 0.00100, LR: 0.0000111, Tokens/sec: 54286.25\n",
      "Step: 2277, Training Loss: 0.00161, LR: 0.0000111, Tokens/sec: 52278.86\n",
      "Step: 2278, Training Loss: 0.00229, LR: 0.0000111, Tokens/sec: 54722.01\n",
      "Step: 2279, Training Loss: 0.00172, LR: 0.0000111, Tokens/sec: 48833.59\n",
      "Step: 2280, Training Loss: 0.00235, LR: 0.0000111, Tokens/sec: 47124.67\n",
      "Step: 2281, Training Loss: 0.00247, LR: 0.0000111, Tokens/sec: 55085.47\n",
      "Step: 2282, Training Loss: 0.00059, LR: 0.0000110, Tokens/sec: 52445.76\n",
      "Step: 2283, Training Loss: 0.00130, LR: 0.0000110, Tokens/sec: 54597.63\n",
      "Step: 2284, Training Loss: 0.00233, LR: 0.0000110, Tokens/sec: 51921.15\n",
      "Step: 2285, Training Loss: 0.00284, LR: 0.0000110, Tokens/sec: 51406.41\n",
      "Step: 2286, Training Loss: 0.00182, LR: 0.0000110, Tokens/sec: 52578.98\n",
      "Step: 2287, Training Loss: 0.00254, LR: 0.0000110, Tokens/sec: 51830.72\n",
      "Step: 2288, Training Loss: 0.00343, LR: 0.0000110, Tokens/sec: 52796.00\n",
      "Step: 2289, Training Loss: 0.00093, LR: 0.0000109, Tokens/sec: 53525.10\n",
      "Step: 2290, Training Loss: 0.00221, LR: 0.0000109, Tokens/sec: 53400.45\n",
      "Step: 2291, Training Loss: 0.00194, LR: 0.0000109, Tokens/sec: 53213.66\n",
      "Step: 2292, Training Loss: 0.00168, LR: 0.0000109, Tokens/sec: 41969.49\n",
      "Step: 2293, Training Loss: 0.00190, LR: 0.0000109, Tokens/sec: 50017.48\n",
      "Step: 2294, Training Loss: 0.00289, LR: 0.0000109, Tokens/sec: 49601.93\n",
      "Step: 2295, Training Loss: 0.00199, LR: 0.0000109, Tokens/sec: 51041.40\n",
      "Step: 2296, Training Loss: 0.00287, LR: 0.0000109, Tokens/sec: 44563.22\n",
      "Step: 2297, Training Loss: 0.00129, LR: 0.0000108, Tokens/sec: 47140.73\n",
      "Step: 2298, Training Loss: 0.00130, LR: 0.0000108, Tokens/sec: 50188.91\n",
      "Step: 2299, Training Loss: 0.00179, LR: 0.0000108, Tokens/sec: 50418.06\n",
      "Step: 2300, Training Loss: 0.00280, LR: 0.0000108, Tokens/sec: 49881.28\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 2300, Eval Loss: 0.00251\n",
      "Step: 2301, Training Loss: 0.00308, LR: 0.0000108, Tokens/sec: 50877.83\n",
      "Step: 2302, Training Loss: 0.00169, LR: 0.0000108, Tokens/sec: 51518.90\n",
      "Step: 2303, Training Loss: 0.00324, LR: 0.0000108, Tokens/sec: 49853.75\n",
      "Step: 2304, Training Loss: 0.00346, LR: 0.0000108, Tokens/sec: 47067.85\n",
      "Step: 2305, Training Loss: 0.00231, LR: 0.0000107, Tokens/sec: 45653.05\n",
      "Step: 2306, Training Loss: 0.00228, LR: 0.0000107, Tokens/sec: 48757.48\n",
      "Step: 2307, Training Loss: 0.00106, LR: 0.0000107, Tokens/sec: 49439.03\n",
      "Step: 2308, Training Loss: 0.00209, LR: 0.0000107, Tokens/sec: 49846.50\n",
      "Step: 2309, Training Loss: 0.00119, LR: 0.0000107, Tokens/sec: 50754.92\n",
      "Step: 2310, Training Loss: 0.00233, LR: 0.0000107, Tokens/sec: 49725.31\n",
      "Step: 2311, Training Loss: 0.00166, LR: 0.0000107, Tokens/sec: 50925.34\n",
      "Step: 2312, Training Loss: 0.00155, LR: 0.0000107, Tokens/sec: 49869.01\n",
      "Step: 2313, Training Loss: 0.00603, LR: 0.0000107, Tokens/sec: 50828.42\n",
      "Step: 2314, Training Loss: 0.00379, LR: 0.0000106, Tokens/sec: 50982.15\n",
      "Step: 2315, Training Loss: 0.00165, LR: 0.0000106, Tokens/sec: 49981.92\n",
      "Step: 2316, Training Loss: 0.00130, LR: 0.0000106, Tokens/sec: 50805.94\n",
      "Step: 2317, Training Loss: 0.00204, LR: 0.0000106, Tokens/sec: 47981.54\n",
      "Step: 2318, Training Loss: 0.00102, LR: 0.0000106, Tokens/sec: 51383.81\n",
      "Step: 2319, Training Loss: 0.00170, LR: 0.0000106, Tokens/sec: 50335.52\n",
      "Step: 2320, Training Loss: 0.00170, LR: 0.0000106, Tokens/sec: 49668.93\n",
      "Step: 2321, Training Loss: 0.00246, LR: 0.0000106, Tokens/sec: 46120.55\n",
      "Step: 2322, Training Loss: 0.00229, LR: 0.0000106, Tokens/sec: 49705.93\n",
      "Step: 2323, Training Loss: 0.00243, LR: 0.0000105, Tokens/sec: 50204.92\n",
      "Step: 2324, Training Loss: 0.00099, LR: 0.0000105, Tokens/sec: 48797.53\n",
      "Step: 2325, Training Loss: 0.00226, LR: 0.0000105, Tokens/sec: 50055.14\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 2325, Eval Loss: 0.00221\n",
      "Step: 2326, Training Loss: 0.00225, LR: 0.0000105, Tokens/sec: 52055.50\n",
      "Step: 2327, Training Loss: 0.00212, LR: 0.0000105, Tokens/sec: 52901.32\n",
      "Step: 2328, Training Loss: 0.00196, LR: 0.0000105, Tokens/sec: 51371.19\n",
      "Step: 2329, Training Loss: 0.00170, LR: 0.0000105, Tokens/sec: 51764.37\n",
      "Step: 2330, Training Loss: 0.00088, LR: 0.0000105, Tokens/sec: 50629.25\n",
      "Step: 2331, Training Loss: 0.00222, LR: 0.0000105, Tokens/sec: 51734.32\n",
      "Step: 2332, Training Loss: 0.00189, LR: 0.0000105, Tokens/sec: 48558.00\n",
      "Step: 2333, Training Loss: 0.00104, LR: 0.0000105, Tokens/sec: 50392.53\n",
      "Step: 2334, Training Loss: 0.00111, LR: 0.0000104, Tokens/sec: 48046.61\n",
      "Step: 2335, Training Loss: 0.00189, LR: 0.0000104, Tokens/sec: 50420.57\n",
      "Step: 2336, Training Loss: 0.00199, LR: 0.0000104, Tokens/sec: 51002.38\n",
      "Step: 2337, Training Loss: 0.00153, LR: 0.0000104, Tokens/sec: 51922.80\n",
      "Step: 2338, Training Loss: 0.00255, LR: 0.0000104, Tokens/sec: 50233.57\n",
      "Step: 2339, Training Loss: 0.00262, LR: 0.0000104, Tokens/sec: 49043.73\n",
      "Step: 2340, Training Loss: 0.00156, LR: 0.0000104, Tokens/sec: 49539.38\n",
      "Step: 2341, Training Loss: 0.00352, LR: 0.0000104, Tokens/sec: 49239.98\n",
      "Step: 2342, Training Loss: 0.00154, LR: 0.0000104, Tokens/sec: 50537.02\n",
      "Step: 2343, Training Loss: 0.00191, LR: 0.0000104, Tokens/sec: 47294.99\n",
      "Step: 2344, Training Loss: 0.00313, LR: 0.0000104, Tokens/sec: 46306.99\n",
      "Step: 2345, Training Loss: 0.00149, LR: 0.0000104, Tokens/sec: 51018.97\n",
      "Step: 2346, Training Loss: 0.00123, LR: 0.0000103, Tokens/sec: 50647.81\n",
      "Step: 2347, Training Loss: 0.00180, LR: 0.0000103, Tokens/sec: 49140.15\n",
      "Step: 2348, Training Loss: 0.00379, LR: 0.0000103, Tokens/sec: 48847.45\n",
      "Step: 2349, Training Loss: 0.00323, LR: 0.0000103, Tokens/sec: 48144.67\n",
      "Step: 2350, Training Loss: 0.00355, LR: 0.0000103, Tokens/sec: 44788.34\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 2350, Eval Loss: 0.00248\n",
      "Step: 2351, Training Loss: 0.00220, LR: 0.0000103, Tokens/sec: 44683.10\n",
      "Step: 2352, Training Loss: 0.00095, LR: 0.0000103, Tokens/sec: 42775.39\n",
      "Step: 2353, Training Loss: 0.00197, LR: 0.0000103, Tokens/sec: 47456.36\n",
      "Step: 2354, Training Loss: 0.00600, LR: 0.0000103, Tokens/sec: 47350.54\n",
      "Step: 2355, Training Loss: 0.00193, LR: 0.0000103, Tokens/sec: 46474.05\n",
      "Step: 2356, Training Loss: 0.00200, LR: 0.0000103, Tokens/sec: 45426.43\n",
      "Step: 2357, Training Loss: 0.00173, LR: 0.0000103, Tokens/sec: 47080.32\n",
      "Step: 2358, Training Loss: 0.00238, LR: 0.0000103, Tokens/sec: 48399.47\n",
      "Step: 2359, Training Loss: 0.00276, LR: 0.0000102, Tokens/sec: 48227.17\n",
      "Step: 2360, Training Loss: 0.00332, LR: 0.0000102, Tokens/sec: 48972.46\n",
      "Step: 2361, Training Loss: 0.00129, LR: 0.0000102, Tokens/sec: 48099.03\n",
      "Step: 2362, Training Loss: 0.00255, LR: 0.0000102, Tokens/sec: 48961.49\n",
      "Step: 2363, Training Loss: 0.00095, LR: 0.0000102, Tokens/sec: 47756.85\n",
      "Step: 2364, Training Loss: 0.00270, LR: 0.0000102, Tokens/sec: 48999.59\n",
      "Step: 2365, Training Loss: 0.00255, LR: 0.0000102, Tokens/sec: 49618.15\n",
      "Step: 2366, Training Loss: 0.00138, LR: 0.0000102, Tokens/sec: 47248.63\n",
      "Step: 2367, Training Loss: 0.00165, LR: 0.0000102, Tokens/sec: 48254.84\n",
      "Step: 2368, Training Loss: 0.00306, LR: 0.0000102, Tokens/sec: 48312.18\n",
      "Step: 2369, Training Loss: 0.00270, LR: 0.0000102, Tokens/sec: 48300.76\n",
      "Step: 2370, Training Loss: 0.00083, LR: 0.0000102, Tokens/sec: 50637.27\n",
      "Step: 2371, Training Loss: 0.00118, LR: 0.0000102, Tokens/sec: 49576.35\n",
      "Step: 2372, Training Loss: 0.00182, LR: 0.0000102, Tokens/sec: 49003.29\n",
      "Step: 2373, Training Loss: 0.00060, LR: 0.0000102, Tokens/sec: 47956.64\n",
      "Step: 2374, Training Loss: 0.00124, LR: 0.0000102, Tokens/sec: 46890.49\n",
      "Step: 2375, Training Loss: 0.00228, LR: 0.0000102, Tokens/sec: 47979.84\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 2375, Eval Loss: 0.00152\n",
      "Step: 2376, Training Loss: 0.00516, LR: 0.0000101, Tokens/sec: 50346.04\n",
      "Step: 2377, Training Loss: 0.00108, LR: 0.0000101, Tokens/sec: 49007.62\n",
      "Step: 2378, Training Loss: 0.00214, LR: 0.0000101, Tokens/sec: 44199.69\n",
      "Step: 2379, Training Loss: 0.00341, LR: 0.0000101, Tokens/sec: 42668.89\n",
      "Step: 2380, Training Loss: 0.00110, LR: 0.0000101, Tokens/sec: 46646.67\n",
      "Step: 2381, Training Loss: 0.00134, LR: 0.0000101, Tokens/sec: 43983.25\n",
      "Step: 2382, Training Loss: 0.00225, LR: 0.0000101, Tokens/sec: 43489.52\n",
      "Step: 2383, Training Loss: 0.00173, LR: 0.0000101, Tokens/sec: 43794.62\n",
      "Step: 2384, Training Loss: 0.00118, LR: 0.0000101, Tokens/sec: 45385.52\n",
      "Step: 2385, Training Loss: 0.00195, LR: 0.0000101, Tokens/sec: 46851.64\n",
      "Step: 2386, Training Loss: 0.00139, LR: 0.0000101, Tokens/sec: 45905.24\n",
      "Step: 2387, Training Loss: 0.00114, LR: 0.0000101, Tokens/sec: 47753.28\n",
      "Step: 2388, Training Loss: 0.00158, LR: 0.0000101, Tokens/sec: 46625.24\n",
      "Step: 2389, Training Loss: 0.00176, LR: 0.0000101, Tokens/sec: 55786.42\n",
      "Step: 2390, Training Loss: 0.00141, LR: 0.0000101, Tokens/sec: 47208.93\n",
      "Step: 2391, Training Loss: 0.00233, LR: 0.0000101, Tokens/sec: 54667.51\n",
      "Step: 2392, Training Loss: 0.00230, LR: 0.0000101, Tokens/sec: 54067.79\n",
      "Step: 2393, Training Loss: 0.00194, LR: 0.0000101, Tokens/sec: 53616.34\n",
      "Step: 2394, Training Loss: 0.00166, LR: 0.0000101, Tokens/sec: 54300.05\n",
      "Step: 2395, Training Loss: 0.00176, LR: 0.0000101, Tokens/sec: 48509.39\n",
      "Step: 2396, Training Loss: 0.00200, LR: 0.0000101, Tokens/sec: 48107.07\n",
      "Step: 2397, Training Loss: 0.00223, LR: 0.0000101, Tokens/sec: 55857.42\n",
      "Step: 2398, Training Loss: 0.00245, LR: 0.0000101, Tokens/sec: 55411.19\n",
      "Step: 2399, Training Loss: 0.00119, LR: 0.0000101, Tokens/sec: 55773.88\n",
      "Step: 2400, Training Loss: 0.00237, LR: 0.0000100, Tokens/sec: 53449.88\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 2400, Eval Loss: 0.00192\n",
      "Step: 2401, Training Loss: 0.00355, LR: 0.0000100, Tokens/sec: 46840.17\n",
      "Step: 2402, Training Loss: 0.00206, LR: 0.0000100, Tokens/sec: 46458.86\n",
      "Step: 2403, Training Loss: 0.00330, LR: 0.0000100, Tokens/sec: 44720.83\n",
      "Step: 2404, Training Loss: 0.00240, LR: 0.0000100, Tokens/sec: 53879.86\n",
      "Step: 2405, Training Loss: 0.00083, LR: 0.0000100, Tokens/sec: 55698.45\n",
      "Step: 2406, Training Loss: 0.00180, LR: 0.0000100, Tokens/sec: 55314.27\n",
      "Step: 2407, Training Loss: 0.00176, LR: 0.0000100, Tokens/sec: 48525.24\n",
      "Step: 2408, Training Loss: 0.00204, LR: 0.0000100, Tokens/sec: 46648.33\n",
      "Step: 2409, Training Loss: 0.00186, LR: 0.0000100, Tokens/sec: 51100.74\n",
      "Step: 2410, Training Loss: 0.00420, LR: 0.0000100, Tokens/sec: 54332.91\n",
      "Step: 2411, Training Loss: 0.00301, LR: 0.0000100, Tokens/sec: 54572.30\n",
      "Step: 2412, Training Loss: 0.00088, LR: 0.0000100, Tokens/sec: 54510.32\n",
      "Step: 2413, Training Loss: 0.00547, LR: 0.0000100, Tokens/sec: 52266.40\n",
      "Step: 2414, Training Loss: 0.00213, LR: 0.0000100, Tokens/sec: 53997.77\n",
      "Step: 2415, Training Loss: 0.00153, LR: 0.0000100, Tokens/sec: 51912.75\n",
      "Step: 2416, Training Loss: 0.00151, LR: 0.0000100, Tokens/sec: 56666.93\n",
      "Step: 2417, Training Loss: 0.00111, LR: 0.0000100, Tokens/sec: 55698.10\n",
      "Step: 2418, Training Loss: 0.00291, LR: 0.0000100, Tokens/sec: 53072.32\n",
      "Step: 2419, Training Loss: 0.00436, LR: 0.0000100, Tokens/sec: 54531.72\n",
      "Step: 2420, Training Loss: 0.00225, LR: 0.0000100, Tokens/sec: 52957.11\n",
      "Step: 2421, Training Loss: 0.00278, LR: 0.0000100, Tokens/sec: 52206.93\n",
      "Step: 2422, Training Loss: 0.00156, LR: 0.0000100, Tokens/sec: 49889.80\n",
      "Step: 2423, Training Loss: 0.00267, LR: 0.0000100, Tokens/sec: 43652.84\n",
      "Step: 2424, Training Loss: 0.00213, LR: 0.0000100, Tokens/sec: 44963.12\n",
      "Step: 2425, Training Loss: 0.00175, LR: 0.0000100, Tokens/sec: 42388.52\n",
      "Computing Eval loss, steps: 5\n",
      "Step: 2425, Eval Loss: 0.00290\n",
      "Step: 2426, Training Loss: 0.00202, LR: 0.0000100, Tokens/sec: 41318.40\n",
      "Step: 2427, Training Loss: 0.00381, LR: 0.0000100, Tokens/sec: 48609.55\n",
      "Step: 2428, Training Loss: 0.00258, LR: 0.0000100, Tokens/sec: 51131.58\n",
      "Step: 2429, Training Loss: 0.00236, LR: 0.0000100, Tokens/sec: 50052.68\n",
      "Step: 2430, Training Loss: 0.00410, LR: 0.0000100, Tokens/sec: 50573.50\n",
      "Step: 2431, Training Loss: 0.00175, LR: 0.0000100, Tokens/sec: 50098.99\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "8b5596eda083de0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T20:57:15.806240Z",
     "start_time": "2025-02-17T20:57:13.722624Z"
    }
   },
   "source": [
    "input_text = \"\"\"\n",
    "All:\n",
    "Content, content.\n",
    "\n",
    "MENENIUS:\n",
    "O sir, you are not right: have you not known\n",
    "The worthiest men have done't?\n",
    "\n",
    "CORIOLANUS:\n",
    "\"\"\".strip()\n",
    "\n",
    "input_ids = tokenizer([input_text], return_tensors=\"pt\")['input_ids'].to(trainer.device)\n",
    "idx = model.generate(input_ids, temperature=0.25, top_k=50, max_new_tokens=128)\n",
    "print(tokenizer.batch_decode(idx)[0])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All:\n",
      "Content, content.\n",
      "\n",
      "MENENIUS:\n",
      "O sir, you are not right: have you not known\n",
      "The worthiest men have done't?\n",
      "\n",
      "CORIOLANUS:\n",
      "Away for them.\n",
      "\n",
      "COR:\n",
      "How my I will so.\n",
      "\n",
      "CORIOLANUS:\n",
      "We wouldst; I will be you, sir.\n",
      "\n",
      "CORIOLANUS:\n",
      "Your away is a heads.\n",
      "\n",
      "CORIOLANUS:\n",
      "Pray you home.\n",
      "\n",
      "CORIOLANUS:\n",
      "My son, leave the senate in a C was,\n",
      "E night; he shall never.\n",
      "\n",
      "BRUTUS:\n",
      "When we will, sir.\n",
      "\n",
      "MENENIUS:\n",
      "That a day yourI have done King me.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a94862-9944-4f3c-9c4c-d5d9b6e62e55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
