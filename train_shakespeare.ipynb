{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-16T06:53:18.835207Z",
     "start_time": "2024-12-16T06:53:17.896732Z"
    }
   },
   "source": [
    "from model import ModelConfig, LlamaModel\n",
    "from train import TrainerConfig, DataLoader, Trainer\n",
    "\n",
    "from transformers import AutoTokenizer"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T06:53:18.839295Z",
     "start_time": "2024-12-16T06:53:18.837689Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer_id = \"HuggingFaceTB/SmolLM2-135M\"",
   "id": "2f28fa23c987e72b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T06:53:19.078218Z",
     "start_time": "2024-12-16T06:53:18.884341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ],
   "id": "9bb4e51aa142abee",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T06:53:19.085484Z",
     "start_time": "2024-12-16T06:53:19.083703Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_config = ModelConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=576,\n",
    "    d_head=64,\n",
    "    d_mlp_proj=1536,\n",
    "    n_layers=30,\n",
    "    n_kv_heads=3,\n",
    "    n_attn_heads=9,\n",
    "    rms_norm_eps=1e-5,\n",
    "    initializer_range=0.041666666666666664,\n",
    "    rope_theta=100000.0,\n",
    "    padding_idx=tokenizer.pad_token_id\n",
    ")"
   ],
   "id": "cde027092af8291e",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T06:53:19.131109Z",
     "start_time": "2024-12-16T06:53:19.129110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_config = TrainerConfig(\n",
    "    per_device_train_batch_size=8,\n",
    "    max_seq_len=2048,\n",
    "    num_epochs=50,\n",
    "    learning_rate=1e-3,\n",
    "    log_dir=\"runs/shakespeare\"\n",
    ")"
   ],
   "id": "809773e662327a12",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T06:53:19.177890Z",
     "start_time": "2024-12-16T06:53:19.176055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"data/tiny_shakespeare.txt\") as f:\n",
    "    text = f.read()"
   ],
   "id": "374f398bb34f7ac1",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T06:53:21.708719Z",
     "start_time": "2024-12-16T06:53:19.219995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = LlamaModel(model_config)\n",
    "dataloader = DataLoader(train_config, tokenizer, text=text)\n",
    "trainer = Trainer(train_config, model)"
   ],
   "id": "9a912a0ec92039d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train tokens             | 342,016\n",
      "Num Trainable Params           | 162,826,560\n",
      "Train device                   | cuda, NVIDIA GeForce RTX 3090, N=1\n",
      "Training precision             | torch.bfloat16\n",
      "Flash Attention                | True\n",
      "torch.compile()                | True\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T06:55:13.537288Z",
     "start_time": "2024-12-16T06:53:21.715200Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train(dataloader)",
   "id": "ee8c2059258a0195",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training steps                 | 210 \n",
      "Step: 0, Training Loss: 11.39692, Tokens/sec: 1496.3612595824752\n",
      "Step: 1, Training Loss: 9.98063, Tokens/sec: 1642.3986627833508\n",
      "Step: 2, Training Loss: 12.49225, Tokens/sec: 82497.22358627409\n",
      "Step: 3, Training Loss: 8.97854, Tokens/sec: 82210.1207514448\n",
      "Step: 4, Training Loss: 8.53118, Tokens/sec: 87240.43839728912\n",
      "Step: 5, Training Loss: 7.97263, Tokens/sec: 81196.92795908762\n",
      "Step: 6, Training Loss: 7.82696, Tokens/sec: 80414.78203718596\n",
      "Step: 7, Training Loss: 7.15195, Tokens/sec: 81210.9544627696\n",
      "Step: 8, Training Loss: 6.98927, Tokens/sec: 80305.29658373963\n",
      "Step: 9, Training Loss: 6.85874, Tokens/sec: 84987.0998709118\n",
      "Step: 10, Training Loss: 6.70656, Tokens/sec: 86289.6941476574\n",
      "Step: 11, Training Loss: 6.63914, Tokens/sec: 88933.12336839839\n",
      "Step: 12, Training Loss: 6.70161, Tokens/sec: 85474.9501900465\n",
      "Step: 13, Training Loss: 6.65715, Tokens/sec: 87528.98235635644\n",
      "Step: 14, Training Loss: 6.73809, Tokens/sec: 88767.38323373672\n",
      "Step: 15, Training Loss: 6.70044, Tokens/sec: 86646.44440244997\n",
      "Step: 16, Training Loss: 6.75428, Tokens/sec: 88440.6437607579\n",
      "Step: 17, Training Loss: 6.65803, Tokens/sec: 87296.84442252692\n",
      "Step: 18, Training Loss: 6.55745, Tokens/sec: 81058.68580472318\n",
      "Step: 19, Training Loss: 6.70945, Tokens/sec: 76942.6536037194\n",
      "Step: 20, Training Loss: 6.63050, Tokens/sec: 1460.8502494423508\n",
      "Step: 21, Training Loss: 6.55971, Tokens/sec: 84334.75710145824\n",
      "Step: 22, Training Loss: 6.43009, Tokens/sec: 84352.5499810952\n",
      "Step: 23, Training Loss: 6.50863, Tokens/sec: 70897.72250725585\n",
      "Step: 24, Training Loss: 6.43808, Tokens/sec: 83393.45009877722\n",
      "Step: 25, Training Loss: 6.52374, Tokens/sec: 88465.98911284898\n",
      "Step: 26, Training Loss: 6.58859, Tokens/sec: 80361.5370595742\n",
      "Step: 27, Training Loss: 6.70140, Tokens/sec: 81404.38705460065\n",
      "Step: 28, Training Loss: 6.41398, Tokens/sec: 81561.7973067908\n",
      "Step: 29, Training Loss: 6.45616, Tokens/sec: 83217.74072407563\n",
      "Step: 30, Training Loss: 6.50090, Tokens/sec: 81724.91771807533\n",
      "Step: 31, Training Loss: 6.47812, Tokens/sec: 68206.92478848818\n",
      "Step: 32, Training Loss: 6.39200, Tokens/sec: 80543.73510129114\n",
      "Step: 33, Training Loss: 6.44018, Tokens/sec: 82200.29354864903\n",
      "Step: 34, Training Loss: 6.44556, Tokens/sec: 80720.64140855704\n",
      "Step: 35, Training Loss: 6.52575, Tokens/sec: 83916.18864343222\n",
      "Step: 36, Training Loss: 6.50181, Tokens/sec: 88952.43351169431\n",
      "Step: 37, Training Loss: 6.57068, Tokens/sec: 82954.64307607847\n",
      "Step: 38, Training Loss: 6.51201, Tokens/sec: 77755.3326344608\n",
      "Step: 39, Training Loss: 6.41622, Tokens/sec: 78602.97489582123\n",
      "Step: 40, Training Loss: 6.57259, Tokens/sec: 83312.9462243484\n",
      "Step: 41, Training Loss: 6.51128, Tokens/sec: 66219.4326789517\n",
      "Step: 42, Training Loss: 6.46535, Tokens/sec: 82797.16642789157\n",
      "Step: 43, Training Loss: 6.34304, Tokens/sec: 85283.65871178164\n",
      "Step: 44, Training Loss: 6.45130, Tokens/sec: 84695.92547395188\n",
      "Step: 45, Training Loss: 6.40039, Tokens/sec: 71489.93921591823\n",
      "Step: 46, Training Loss: 6.49813, Tokens/sec: 86658.95675018276\n",
      "Step: 47, Training Loss: 6.56509, Tokens/sec: 83185.11330764276\n",
      "Step: 48, Training Loss: 6.51986, Tokens/sec: 78213.84889291908\n",
      "Step: 49, Training Loss: 6.40556, Tokens/sec: 86774.23533039617\n",
      "Step: 50, Training Loss: 6.44021, Tokens/sec: 87103.45982373363\n",
      "Step: 51, Training Loss: 6.47707, Tokens/sec: 89064.50474520445\n",
      "Step: 52, Training Loss: 6.45705, Tokens/sec: 88759.96398058116\n",
      "Step: 53, Training Loss: 6.35917, Tokens/sec: 88654.90285943508\n",
      "Step: 54, Training Loss: 6.41471, Tokens/sec: 78414.10760350425\n",
      "Step: 55, Training Loss: 6.41139, Tokens/sec: 79843.48649180528\n",
      "Step: 56, Training Loss: 6.47791, Tokens/sec: 81122.40773206203\n",
      "Step: 57, Training Loss: 6.44566, Tokens/sec: 81455.01850614433\n",
      "Step: 58, Training Loss: 6.51280, Tokens/sec: 77394.79995969015\n",
      "Step: 59, Training Loss: 6.46369, Tokens/sec: 74805.0695352662\n",
      "Step: 60, Training Loss: 6.36679, Tokens/sec: 89218.15820026828\n",
      "Step: 61, Training Loss: 6.51542, Tokens/sec: 86836.03130716794\n",
      "Step: 62, Training Loss: 6.46105, Tokens/sec: 69835.0487973985\n",
      "Step: 63, Training Loss: 6.43799, Tokens/sec: 88593.83373832615\n",
      "Step: 64, Training Loss: 6.30881, Tokens/sec: 85792.59314881025\n",
      "Step: 65, Training Loss: 6.43023, Tokens/sec: 89344.23377343957\n",
      "Step: 66, Training Loss: 6.37329, Tokens/sec: 86227.89973994653\n",
      "Step: 67, Training Loss: 6.45301, Tokens/sec: 78998.13889074142\n",
      "Step: 68, Training Loss: 6.51815, Tokens/sec: 81461.85040725606\n",
      "Step: 69, Training Loss: 6.53059, Tokens/sec: 80376.62611178272\n",
      "Step: 70, Training Loss: 6.37064, Tokens/sec: 68961.88696556647\n",
      "Step: 71, Training Loss: 6.41157, Tokens/sec: 70440.96711854439\n",
      "Step: 72, Training Loss: 6.44509, Tokens/sec: 77465.63972813966\n",
      "Step: 73, Training Loss: 6.42448, Tokens/sec: 88613.82354031377\n",
      "Step: 74, Training Loss: 6.34180, Tokens/sec: 88268.85715777958\n",
      "Step: 75, Training Loss: 6.38654, Tokens/sec: 86038.80086259422\n",
      "Step: 76, Training Loss: 6.38753, Tokens/sec: 88726.29237345823\n",
      "Step: 77, Training Loss: 6.45744, Tokens/sec: 84257.95346161579\n",
      "Step: 78, Training Loss: 6.42461, Tokens/sec: 86686.25428188009\n",
      "Step: 79, Training Loss: 6.49416, Tokens/sec: 72329.14426029162\n",
      "Step: 80, Training Loss: 6.44693, Tokens/sec: 74262.38345978384\n",
      "Step: 81, Training Loss: 6.35278, Tokens/sec: 81651.64086043644\n",
      "Step: 82, Training Loss: 6.49839, Tokens/sec: 85215.1563313903\n",
      "Step: 83, Training Loss: 6.44956, Tokens/sec: 76313.37454913088\n",
      "Step: 84, Training Loss: 6.42710, Tokens/sec: 72005.87378793806\n",
      "Step: 85, Training Loss: 6.29750, Tokens/sec: 87720.79221303349\n",
      "Step: 86, Training Loss: 6.42744, Tokens/sec: 86396.45885898714\n",
      "Step: 87, Training Loss: 6.36847, Tokens/sec: 88403.42640397976\n",
      "Step: 88, Training Loss: 6.44728, Tokens/sec: 84727.89164974315\n",
      "Step: 89, Training Loss: 6.51160, Tokens/sec: 77042.01038026581\n",
      "Step: 90, Training Loss: 6.48905, Tokens/sec: 85598.03768612049\n",
      "Step: 91, Training Loss: 6.36116, Tokens/sec: 86219.39569198963\n",
      "Step: 92, Training Loss: 6.40350, Tokens/sec: 89063.62987639909\n",
      "Step: 93, Training Loss: 6.43293, Tokens/sec: 75980.59428552848\n",
      "Step: 94, Training Loss: 6.41556, Tokens/sec: 76163.76326881115\n",
      "Step: 95, Training Loss: 6.32947, Tokens/sec: 82092.06537849492\n",
      "Step: 96, Training Loss: 6.37195, Tokens/sec: 71821.59061015847\n",
      "Step: 97, Training Loss: 6.37386, Tokens/sec: 69922.15825117168\n",
      "Step: 98, Training Loss: 6.43999, Tokens/sec: 70942.91051901886\n",
      "Step: 99, Training Loss: 6.40344, Tokens/sec: 87708.00894587021\n",
      "Step: 100, Training Loss: 6.47555, Tokens/sec: 86563.38185641431\n",
      "Step: 101, Training Loss: 6.42341, Tokens/sec: 86352.34360649416\n",
      "Step: 102, Training Loss: 6.32880, Tokens/sec: 87964.5330260053\n",
      "Step: 103, Training Loss: 6.46732, Tokens/sec: 84290.40638000947\n",
      "Step: 104, Training Loss: 6.41335, Tokens/sec: 85886.85879838624\n",
      "Step: 105, Training Loss: 6.40806, Tokens/sec: 77077.17989637371\n",
      "Step: 106, Training Loss: 6.27979, Tokens/sec: 75519.92221408358\n",
      "Step: 107, Training Loss: 6.40247, Tokens/sec: 79620.32882900907\n",
      "Step: 108, Training Loss: 6.33430, Tokens/sec: 82415.05528520136\n",
      "Step: 109, Training Loss: 6.39877, Tokens/sec: 81927.23581264993\n",
      "Step: 110, Training Loss: 6.46211, Tokens/sec: 80651.25573932118\n",
      "Step: 111, Training Loss: 6.44966, Tokens/sec: 85422.97629390936\n",
      "Step: 112, Training Loss: 6.29687, Tokens/sec: 88138.69622201654\n",
      "Step: 113, Training Loss: 6.34337, Tokens/sec: 87731.58026189641\n",
      "Step: 114, Training Loss: 6.35697, Tokens/sec: 85334.03378622213\n",
      "Step: 115, Training Loss: 6.33773, Tokens/sec: 88974.82402990563\n",
      "Step: 116, Training Loss: 6.23422, Tokens/sec: 85829.18237727099\n",
      "Step: 117, Training Loss: 6.25442, Tokens/sec: 88376.38258747799\n",
      "Step: 118, Training Loss: 6.25784, Tokens/sec: 88047.63084455025\n",
      "Step: 119, Training Loss: 6.31396, Tokens/sec: 77379.2151024766\n",
      "Step: 120, Training Loss: 6.25338, Tokens/sec: 81031.10979508677\n",
      "Step: 121, Training Loss: 6.32137, Tokens/sec: 84368.04730737813\n",
      "Step: 122, Training Loss: 6.25512, Tokens/sec: 80402.01715179725\n",
      "Step: 123, Training Loss: 6.15308, Tokens/sec: 81659.29738494202\n",
      "Step: 124, Training Loss: 6.27075, Tokens/sec: 76415.95656521176\n",
      "Step: 125, Training Loss: 6.19138, Tokens/sec: 82711.67359931984\n",
      "Step: 126, Training Loss: 6.38496, Tokens/sec: 87876.06527137705\n",
      "Step: 127, Training Loss: 6.13711, Tokens/sec: 80624.54139478922\n",
      "Step: 128, Training Loss: 6.16889, Tokens/sec: 88580.17888957223\n",
      "Step: 129, Training Loss: 6.15471, Tokens/sec: 72541.37418478868\n",
      "Step: 130, Training Loss: 6.22125, Tokens/sec: 80995.69855064459\n",
      "Step: 131, Training Loss: 6.25625, Tokens/sec: 74951.96536941899\n",
      "Step: 132, Training Loss: 6.22061, Tokens/sec: 77443.37779988025\n",
      "Step: 133, Training Loss: 6.05055, Tokens/sec: 72000.78993286447\n",
      "Step: 134, Training Loss: 6.15174, Tokens/sec: 81281.41862889279\n",
      "Step: 135, Training Loss: 6.13549, Tokens/sec: 78506.63438296653\n",
      "Step: 136, Training Loss: 6.10720, Tokens/sec: 72184.8434271797\n",
      "Step: 137, Training Loss: 5.98553, Tokens/sec: 76080.74323373438\n",
      "Step: 138, Training Loss: 6.00058, Tokens/sec: 84004.91892366453\n",
      "Step: 139, Training Loss: 6.03512, Tokens/sec: 88124.69071347624\n",
      "Step: 140, Training Loss: 6.08810, Tokens/sec: 71059.04678805261\n",
      "Step: 141, Training Loss: 6.04090, Tokens/sec: 88388.00959399695\n",
      "Step: 142, Training Loss: 6.13228, Tokens/sec: 89126.84916422538\n",
      "Step: 143, Training Loss: 6.07678, Tokens/sec: 83820.55780593646\n",
      "Step: 144, Training Loss: 5.95700, Tokens/sec: 69212.75192126406\n",
      "Step: 145, Training Loss: 6.08227, Tokens/sec: 81749.11655111413\n",
      "Step: 146, Training Loss: 6.01104, Tokens/sec: 77730.0614221993\n",
      "Step: 147, Training Loss: 6.01576, Tokens/sec: 84289.86649135356\n",
      "Step: 148, Training Loss: 5.87266, Tokens/sec: 79520.6917490549\n",
      "Step: 149, Training Loss: 5.99455, Tokens/sec: 76231.44527277631\n",
      "Step: 150, Training Loss: 5.91993, Tokens/sec: 88263.30879897393\n",
      "Step: 151, Training Loss: 5.98729, Tokens/sec: 86176.55023136853\n",
      "Step: 152, Training Loss: 6.08572, Tokens/sec: 88758.30457979174\n",
      "Step: 153, Training Loss: 6.06561, Tokens/sec: 77376.22729328342\n",
      "Step: 154, Training Loss: 5.89354, Tokens/sec: 83933.93733410232\n",
      "Step: 155, Training Loss: 5.97556, Tokens/sec: 87607.77911677946\n",
      "Step: 156, Training Loss: 5.95589, Tokens/sec: 84758.35122074859\n",
      "Step: 157, Training Loss: 5.94413, Tokens/sec: 86196.69786737839\n",
      "Step: 158, Training Loss: 5.81568, Tokens/sec: 79168.34312603365\n",
      "Step: 159, Training Loss: 5.83981, Tokens/sec: 78331.64796420732\n",
      "Step: 160, Training Loss: 5.87778, Tokens/sec: 81646.62017114113\n",
      "Step: 161, Training Loss: 5.93013, Tokens/sec: 74325.36676001642\n",
      "Step: 162, Training Loss: 5.85595, Tokens/sec: 80487.27815769902\n",
      "Step: 163, Training Loss: 5.94873, Tokens/sec: 87375.82469828638\n",
      "Step: 164, Training Loss: 5.89581, Tokens/sec: 72256.60058219929\n",
      "Step: 165, Training Loss: 5.75778, Tokens/sec: 70886.20401697082\n",
      "Step: 166, Training Loss: 5.90069, Tokens/sec: 87908.51577912107\n",
      "Step: 167, Training Loss: 5.80663, Tokens/sec: 73677.60430829908\n",
      "Step: 168, Training Loss: 5.80451, Tokens/sec: 81401.00671681619\n",
      "Step: 169, Training Loss: 5.65593, Tokens/sec: 88355.54343956483\n",
      "Step: 170, Training Loss: 5.76512, Tokens/sec: 69685.49143785745\n",
      "Step: 171, Training Loss: 5.69507, Tokens/sec: 78577.57552866482\n",
      "Step: 172, Training Loss: 5.74643, Tokens/sec: 81445.73982915428\n",
      "Step: 173, Training Loss: 5.86694, Tokens/sec: 79163.5455312829\n",
      "Step: 174, Training Loss: 5.94199, Tokens/sec: 78910.42960124601\n",
      "Step: 175, Training Loss: 5.65005, Tokens/sec: 86473.08233577016\n",
      "Step: 176, Training Loss: 5.74942, Tokens/sec: 74014.1376952682\n",
      "Step: 177, Training Loss: 5.71216, Tokens/sec: 74084.9109480255\n",
      "Step: 178, Training Loss: 5.72843, Tokens/sec: 79130.75254872485\n",
      "Step: 179, Training Loss: 5.56858, Tokens/sec: 85281.6362272319\n",
      "Step: 180, Training Loss: 5.56957, Tokens/sec: 76170.09367523235\n",
      "Step: 181, Training Loss: 5.64722, Tokens/sec: 80551.06525507498\n",
      "Step: 182, Training Loss: 5.69712, Tokens/sec: 77983.91363769866\n",
      "Step: 183, Training Loss: 5.60584, Tokens/sec: 74415.06709742476\n",
      "Step: 184, Training Loss: 5.70860, Tokens/sec: 68238.78571967376\n",
      "Step: 185, Training Loss: 5.64532, Tokens/sec: 79952.91483779704\n",
      "Step: 186, Training Loss: 5.51576, Tokens/sec: 77515.37119121097\n",
      "Step: 187, Training Loss: 5.67818, Tokens/sec: 70636.68155610665\n",
      "Step: 188, Training Loss: 5.56272, Tokens/sec: 70630.39733250077\n",
      "Step: 189, Training Loss: 5.54513, Tokens/sec: 87997.16896527386\n",
      "Step: 190, Training Loss: 5.43820, Tokens/sec: 86011.54512016366\n",
      "Step: 191, Training Loss: 5.50189, Tokens/sec: 87940.65182572197\n",
      "Step: 192, Training Loss: 5.45401, Tokens/sec: 85038.2784114976\n",
      "Step: 193, Training Loss: 5.52683, Tokens/sec: 72927.60513051631\n",
      "Step: 194, Training Loss: 5.66340, Tokens/sec: 73611.48758964885\n",
      "Step: 195, Training Loss: 5.60638, Tokens/sec: 67598.55661410309\n",
      "Step: 196, Training Loss: 5.42492, Tokens/sec: 73249.15801989652\n",
      "Step: 197, Training Loss: 5.52287, Tokens/sec: 84255.84544746415\n",
      "Step: 198, Training Loss: 5.49001, Tokens/sec: 82252.05359718768\n",
      "Step: 199, Training Loss: 5.47623, Tokens/sec: 81593.9704830066\n",
      "Step: 200, Training Loss: 5.35848, Tokens/sec: 87272.28068466077\n",
      "Step: 201, Training Loss: 5.33455, Tokens/sec: 85391.57527819037\n",
      "Step: 202, Training Loss: 5.42554, Tokens/sec: 88189.83326953041\n",
      "Step: 203, Training Loss: 5.46213, Tokens/sec: 87844.60234370647\n",
      "Step: 204, Training Loss: 5.37819, Tokens/sec: 84525.05544757191\n",
      "Step: 205, Training Loss: 5.47808, Tokens/sec: 86791.67118025398\n",
      "Step: 206, Training Loss: 5.39898, Tokens/sec: 71303.72431884827\n",
      "Step: 207, Training Loss: 5.28382, Tokens/sec: 74665.24583747293\n",
      "Step: 208, Training Loss: 5.43204, Tokens/sec: 72490.86491810804\n",
      "Step: 209, Training Loss: 5.31488, Tokens/sec: 78063.00377906772\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T06:55:14.427133Z",
     "start_time": "2024-12-16T06:55:13.549970Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_ids = tokenizer([\"HAMLET:\\nTo be or\"], return_tensors=\"pt\")['input_ids'].to(trainer.device)\n",
    "idx = model.generate(input_ids, temperature=0.25, top_k=50, max_new_tokens=64)\n",
    "print(tokenizer.batch_decode(idx)[0])"
   ],
   "id": "8b5596eda083de0a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAMLET:\n",
      "To be or the be\n",
      "' the the the a the the a have\n",
      "I,\n",
      "\n",
      "\n",
      "I,\n",
      "\n",
      "And this,\n",
      "\n",
      "\n",
      "Ber,\n",
      "I,\n",
      "\n",
      "\n",
      "\n",
      "I,\n",
      "\n",
      "\n",
      "\n",
      "I,\n",
      "\n",
      "\n",
      "And the a the the a the be the be the the me,\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T06:55:14.430636Z",
     "start_time": "2024-12-16T06:55:14.429460Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "bc3bcc343073d67a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
