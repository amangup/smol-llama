{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-15T23:13:45.176814Z",
     "start_time": "2024-12-15T23:13:44.281489Z"
    }
   },
   "source": [
    "from model import ModelConfig, LlamaModel\n",
    "from train import TrainerConfig, DataLoader, Trainer\n",
    "\n",
    "from transformers import AutoTokenizer"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T23:13:45.200878Z",
     "start_time": "2024-12-15T23:13:45.199352Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer_id = \"HuggingFaceTB/SmolLM2-135M\"",
   "id": "2f28fa23c987e72b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T23:13:45.426948Z",
     "start_time": "2024-12-15T23:13:45.240476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ],
   "id": "9bb4e51aa142abee",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T23:13:45.435383Z",
     "start_time": "2024-12-15T23:13:45.433706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_config = ModelConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=576,\n",
    "    d_head=64,\n",
    "    d_mlp_proj=1536,\n",
    "    n_layers=30,\n",
    "    n_kv_heads=3,\n",
    "    n_attn_heads=9,\n",
    "    rms_norm_eps=1e-5,\n",
    "    initializer_range=0.041666666666666664,\n",
    "    rope_theta=100000.0,\n",
    "    padding_idx=tokenizer.pad_token_id\n",
    ")"
   ],
   "id": "cde027092af8291e",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T23:13:45.474947Z",
     "start_time": "2024-12-15T23:13:45.472992Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_config = TrainerConfig(\n",
    "    per_device_train_batch_size=8,\n",
    "    max_seq_len=2048,\n",
    "    num_epochs=20,\n",
    "    learning_rate=5e-4\n",
    ")"
   ],
   "id": "809773e662327a12",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T23:13:45.518493Z",
     "start_time": "2024-12-15T23:13:45.516429Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"data/tiny_shakespeare.txt\") as f:\n",
    "    text = f.read()"
   ],
   "id": "374f398bb34f7ac1",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T23:13:48.022465Z",
     "start_time": "2024-12-15T23:13:45.561195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = LlamaModel(model_config)\n",
    "dataloader = DataLoader(train_config, tokenizer, text)\n",
    "trainer = Trainer(train_config, model)"
   ],
   "id": "9a912a0ec92039d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Trainable Params                     | 162,826,560\n",
      "Train device                             | cuda, NVIDIA GeForce RTX 3090, N=1\n",
      "Training precision                       | torch.bfloat16\n",
      "Flash Attention                          | True\n",
      "torch.compile()                          | True\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-15T23:13:48.079157Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train(dataloader)",
   "id": "ee8c2059258a0195",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training steps                           | 420 \n",
      "Step: 0, Training Loss: 11.33942, Tokens/sec: 1508.647695708986\n",
      "Step: 1, Training Loss: 9.88539, Tokens/sec: 1637.9013713920879\n",
      "Step: 2, Training Loss: 11.45460, Tokens/sec: 88256.45896240971\n",
      "Step: 3, Training Loss: 9.63782, Tokens/sec: 85162.02327040584\n",
      "Step: 4, Training Loss: 9.43866, Tokens/sec: 84547.14615448186\n",
      "Step: 5, Training Loss: 9.14640, Tokens/sec: 82961.51713678939\n",
      "Step: 6, Training Loss: 8.82494, Tokens/sec: 84218.94041861851\n",
      "Step: 7, Training Loss: 8.43781, Tokens/sec: 83195.84066252301\n",
      "Step: 8, Training Loss: 8.00212, Tokens/sec: 83471.24335826166\n",
      "Step: 9, Training Loss: 7.66222, Tokens/sec: 84446.60527311619\n",
      "Step: 10, Training Loss: 7.35020, Tokens/sec: 82258.91867250211\n",
      "Step: 11, Training Loss: 7.26622, Tokens/sec: 85034.19896553642\n",
      "Step: 12, Training Loss: 7.17824, Tokens/sec: 85252.81205138942\n",
      "Step: 13, Training Loss: 7.04718, Tokens/sec: 83933.51551627144\n",
      "Step: 14, Training Loss: 6.95707, Tokens/sec: 85043.77654943014\n",
      "Step: 15, Training Loss: 6.80821, Tokens/sec: 85170.54093452879\n",
      "Step: 16, Training Loss: 6.76049, Tokens/sec: 84320.05397451077\n",
      "Step: 17, Training Loss: 6.76864, Tokens/sec: 85897.54689617445\n",
      "Step: 18, Training Loss: 6.81451, Tokens/sec: 82764.40257629204\n",
      "Step: 19, Training Loss: 6.64904, Tokens/sec: 83241.27079470179\n",
      "Step: 20, Training Loss: 7.22632, Tokens/sec: 1437.5248482175004\n",
      "Step: 21, Training Loss: 6.55384, Tokens/sec: 87807.41377316871\n",
      "Step: 22, Training Loss: 6.33712, Tokens/sec: 86406.3526206982\n",
      "Step: 23, Training Loss: 6.43369, Tokens/sec: 88534.45805469129\n",
      "Step: 24, Training Loss: 6.42333, Tokens/sec: 88088.70584225222\n",
      "Step: 25, Training Loss: 6.39332, Tokens/sec: 88453.72315708129\n",
      "Step: 26, Training Loss: 6.59227, Tokens/sec: 88397.84776387295\n",
      "Step: 27, Training Loss: 6.67797, Tokens/sec: 88297.15804805736\n",
      "Step: 28, Training Loss: 6.58741, Tokens/sec: 88087.70180329465\n",
      "Step: 29, Training Loss: 6.48913, Tokens/sec: 88398.52979388663\n",
      "Step: 30, Training Loss: 6.43857, Tokens/sec: 88706.83389793757\n",
      "Step: 31, Training Loss: 6.40892, Tokens/sec: 88658.65009076348\n",
      "Step: 32, Training Loss: 6.45391, Tokens/sec: 88469.29857051099\n",
      "Step: 33, Training Loss: 6.43138, Tokens/sec: 89269.2250915649\n",
      "Step: 34, Training Loss: 6.46284, Tokens/sec: 89199.7363221449\n",
      "Step: 35, Training Loss: 6.54025, Tokens/sec: 88626.77203704201\n",
      "Step: 36, Training Loss: 6.44320, Tokens/sec: 88915.20937410384\n",
      "Step: 37, Training Loss: 6.39628, Tokens/sec: 89173.58785596839\n",
      "Step: 38, Training Loss: 6.40750, Tokens/sec: 85835.07556305219\n",
      "Step: 39, Training Loss: 6.47528, Tokens/sec: 85802.1581129641\n",
      "Step: 40, Training Loss: 6.34166, Tokens/sec: 77481.46205046294\n",
      "Step: 41, Training Loss: 6.79075, Tokens/sec: 79035.98416308506\n",
      "Step: 42, Training Loss: 6.46008, Tokens/sec: 85302.06168715753\n",
      "Step: 43, Training Loss: 6.25048, Tokens/sec: 65687.84191297303\n",
      "Step: 44, Training Loss: 6.36072, Tokens/sec: 86093.5465234095\n",
      "Step: 45, Training Loss: 6.32367, Tokens/sec: 61758.605777062934\n",
      "Step: 46, Training Loss: 6.28481, Tokens/sec: 86291.12527414474\n",
      "Step: 47, Training Loss: 6.44622, Tokens/sec: 87910.99874248967\n",
      "Step: 48, Training Loss: 6.56819, Tokens/sec: 88942.03165655596\n",
      "Step: 49, Training Loss: 6.48220, Tokens/sec: 89739.97557233379\n",
      "Step: 50, Training Loss: 6.40026, Tokens/sec: 89550.01451081186\n",
      "Step: 51, Training Loss: 6.34645, Tokens/sec: 89591.80346828485\n",
      "Step: 52, Training Loss: 6.31478, Tokens/sec: 89113.76577643739\n",
      "Step: 53, Training Loss: 6.38078, Tokens/sec: 89425.25655103485\n",
      "Step: 54, Training Loss: 6.34432, Tokens/sec: 89088.75713657672\n",
      "Step: 55, Training Loss: 6.37302, Tokens/sec: 89622.43823619792\n",
      "Step: 56, Training Loss: 6.45708, Tokens/sec: 82558.47005663304\n",
      "Step: 57, Training Loss: 6.35015, Tokens/sec: 81421.71336180037\n",
      "Step: 58, Training Loss: 6.28760, Tokens/sec: 89302.92948030695\n",
      "Step: 59, Training Loss: 6.29266, Tokens/sec: 88957.82734351936\n",
      "Step: 60, Training Loss: 6.32169, Tokens/sec: 88927.60655593086\n",
      "Step: 61, Training Loss: 6.17720, Tokens/sec: 88719.32725075612\n",
      "Step: 62, Training Loss: 6.42515, Tokens/sec: 85479.81979394252\n",
      "Step: 63, Training Loss: 6.34977, Tokens/sec: 88405.58869629989\n",
      "Step: 64, Training Loss: 6.12354, Tokens/sec: 88411.36105446449\n",
      "Step: 65, Training Loss: 6.23977, Tokens/sec: 87767.0651380041\n",
      "Step: 66, Training Loss: 6.15856, Tokens/sec: 87940.13968906764\n",
      "Step: 67, Training Loss: 6.10751, Tokens/sec: 88804.10438495656\n",
      "Step: 68, Training Loss: 6.26021, Tokens/sec: 88756.65582202072\n",
      "Step: 69, Training Loss: 6.40117, Tokens/sec: 87941.01764127\n",
      "Step: 70, Training Loss: 6.31628, Tokens/sec: 88070.06803852899\n",
      "Step: 71, Training Loss: 6.19145, Tokens/sec: 88206.20720537286\n",
      "Step: 72, Training Loss: 6.16632, Tokens/sec: 87952.09269273738\n",
      "Step: 73, Training Loss: 6.09824, Tokens/sec: 88819.7355536945\n",
      "Step: 74, Training Loss: 6.13081, Tokens/sec: 89014.30092113692\n",
      "Step: 75, Training Loss: 6.07712, Tokens/sec: 89463.56805888176\n",
      "Step: 76, Training Loss: 6.19254, Tokens/sec: 88618.55038518806\n",
      "Step: 77, Training Loss: 6.29898, Tokens/sec: 87940.34265445588\n",
      "Step: 78, Training Loss: 6.15335, Tokens/sec: 89412.78318403052\n",
      "Step: 79, Training Loss: 6.04587, Tokens/sec: 88505.82366001731\n",
      "Step: 80, Training Loss: 6.01864, Tokens/sec: 88585.66226172002\n",
      "Step: 81, Training Loss: 6.01937, Tokens/sec: 88735.10592584622\n",
      "Step: 82, Training Loss: 5.85027, Tokens/sec: 88169.69359735557\n",
      "Step: 83, Training Loss: 6.04949, Tokens/sec: 85740.36940496178\n",
      "Step: 84, Training Loss: 6.13207, Tokens/sec: 89457.36930967904\n",
      "Step: 85, Training Loss: 5.86876, Tokens/sec: 88090.58516613954\n",
      "Step: 86, Training Loss: 5.99064, Tokens/sec: 89510.99760647336\n",
      "Step: 87, Training Loss: 5.90759, Tokens/sec: 88617.64303358024\n",
      "Step: 88, Training Loss: 5.84037, Tokens/sec: 88533.65289058197\n",
      "Step: 89, Training Loss: 6.00698, Tokens/sec: 88408.45092739984\n",
      "Step: 90, Training Loss: 6.15122, Tokens/sec: 88602.0263693196\n",
      "Step: 91, Training Loss: 6.05386, Tokens/sec: 87208.53624551871\n",
      "Step: 92, Training Loss: 5.93168, Tokens/sec: 88850.67326674648\n",
      "Step: 93, Training Loss: 5.87648, Tokens/sec: 89490.93292604391\n",
      "Step: 94, Training Loss: 5.81241, Tokens/sec: 88138.91670068307\n",
      "Step: 95, Training Loss: 5.87411, Tokens/sec: 88832.11624007378\n",
      "Step: 96, Training Loss: 5.79905, Tokens/sec: 89225.62218640231\n",
      "Step: 97, Training Loss: 5.91561, Tokens/sec: 88120.33300459439\n",
      "Step: 98, Training Loss: 6.07040, Tokens/sec: 88413.97556041702\n",
      "Step: 99, Training Loss: 5.90319, Tokens/sec: 87364.68977722157\n",
      "Step: 100, Training Loss: 5.76346, Tokens/sec: 86748.00856190811\n",
      "Step: 101, Training Loss: 5.73732, Tokens/sec: 89093.94998386622\n",
      "Step: 102, Training Loss: 5.77412, Tokens/sec: 89223.25001820951\n",
      "Step: 103, Training Loss: 5.60092, Tokens/sec: 87809.64348467956\n",
      "Step: 104, Training Loss: 6.01237, Tokens/sec: 85831.79843765547\n",
      "Step: 105, Training Loss: 5.87494, Tokens/sec: 88564.46292227936\n",
      "Step: 106, Training Loss: 5.59099, Tokens/sec: 89552.83287241665\n",
      "Step: 107, Training Loss: 5.71943, Tokens/sec: 85633.08615320173\n",
      "Step: 108, Training Loss: 5.65134, Tokens/sec: 88868.12695762319\n",
      "Step: 109, Training Loss: 5.56752, Tokens/sec: 88080.6107032124\n",
      "Step: 110, Training Loss: 5.72535, Tokens/sec: 88658.67263719658\n",
      "Step: 111, Training Loss: 5.90446, Tokens/sec: 88968.42469150436\n",
      "Step: 112, Training Loss: 5.79401, Tokens/sec: 88582.7593288905\n",
      "Step: 113, Training Loss: 5.68191, Tokens/sec: 88509.82080388216\n",
      "Step: 114, Training Loss: 5.63974, Tokens/sec: 89202.4739135715\n",
      "Step: 115, Training Loss: 5.55877, Tokens/sec: 88318.3657570934\n",
      "Step: 116, Training Loss: 5.59772, Tokens/sec: 89118.66438394725\n",
      "Step: 117, Training Loss: 5.52388, Tokens/sec: 88848.62019812768\n",
      "Step: 118, Training Loss: 5.68900, Tokens/sec: 87825.68492658845\n",
      "Step: 119, Training Loss: 5.83190, Tokens/sec: 89038.43016903999\n",
      "Step: 120, Training Loss: 5.66927, Tokens/sec: 88755.00183433761\n",
      "Step: 121, Training Loss: 5.48479, Tokens/sec: 88764.854067251\n",
      "Step: 122, Training Loss: 5.47557, Tokens/sec: 87975.29419063822\n",
      "Step: 123, Training Loss: 5.54410, Tokens/sec: 88465.37482499014\n",
      "Step: 124, Training Loss: 5.34105, Tokens/sec: 89360.26574671734\n",
      "Step: 125, Training Loss: 5.60980, Tokens/sec: 86242.65691341917\n",
      "Step: 126, Training Loss: 5.66152, Tokens/sec: 89159.7372277881\n",
      "Step: 127, Training Loss: 5.33882, Tokens/sec: 88339.16593310826\n",
      "Step: 128, Training Loss: 5.47535, Tokens/sec: 88355.54344129826\n",
      "Step: 129, Training Loss: 5.37240, Tokens/sec: 88340.70443102287\n",
      "Step: 130, Training Loss: 5.29707, Tokens/sec: 89155.045140215\n",
      "Step: 131, Training Loss: 5.47206, Tokens/sec: 88098.84036014415\n",
      "Step: 132, Training Loss: 5.63174, Tokens/sec: 88066.17157961392\n",
      "Step: 133, Training Loss: 5.51915, Tokens/sec: 88661.35936643614\n",
      "Step: 134, Training Loss: 5.46579, Tokens/sec: 88067.49608070392\n",
      "Step: 135, Training Loss: 5.40432, Tokens/sec: 87931.48334052127\n",
      "Step: 136, Training Loss: 5.31096, Tokens/sec: 88674.73737890326\n",
      "Step: 137, Training Loss: 5.35639, Tokens/sec: 88169.43785150375\n",
      "Step: 138, Training Loss: 5.24531, Tokens/sec: 88378.6446222673\n",
      "Step: 139, Training Loss: 5.48006, Tokens/sec: 88583.21863383379\n",
      "Step: 140, Training Loss: 5.64705, Tokens/sec: 88874.05243838743\n",
      "Step: 141, Training Loss: 5.46490, Tokens/sec: 88774.98747866745\n",
      "Step: 142, Training Loss: 5.26412, Tokens/sec: 87628.27401569614\n",
      "Step: 143, Training Loss: 5.20547, Tokens/sec: 87260.05395783692\n",
      "Step: 144, Training Loss: 5.28700, Tokens/sec: 86297.41888155638\n",
      "Step: 145, Training Loss: 5.07881, Tokens/sec: 86966.89641508884\n",
      "Step: 146, Training Loss: 5.23615, Tokens/sec: 86106.69342499242\n",
      "Step: 147, Training Loss: 5.43469, Tokens/sec: 88711.63503254781\n",
      "Step: 148, Training Loss: 5.13751, Tokens/sec: 87414.53593862121\n",
      "Step: 149, Training Loss: 5.26163, Tokens/sec: 88422.67324561405\n",
      "Step: 150, Training Loss: 5.12749, Tokens/sec: 88183.69347867272\n",
      "Step: 151, Training Loss: 5.04790, Tokens/sec: 88266.88367156609\n",
      "Step: 152, Training Loss: 5.21434, Tokens/sec: 88460.06346949378\n",
      "Step: 153, Training Loss: 5.41942, Tokens/sec: 88314.85193255312\n",
      "Step: 154, Training Loss: 5.28578, Tokens/sec: 88303.38075300425\n",
      "Step: 155, Training Loss: 5.25252, Tokens/sec: 88643.41367226608\n",
      "Step: 156, Training Loss: 5.20845, Tokens/sec: 87953.19185289243\n",
      "Step: 157, Training Loss: 5.08597, Tokens/sec: 88663.32605938578\n",
      "Step: 158, Training Loss: 5.13455, Tokens/sec: 88475.60289239745\n",
      "Step: 159, Training Loss: 5.03199, Tokens/sec: 88542.72822992865\n",
      "Step: 160, Training Loss: 5.26509, Tokens/sec: 88163.75824036344\n",
      "Step: 161, Training Loss: 5.46768, Tokens/sec: 88001.68466507741\n",
      "Step: 162, Training Loss: 5.25562, Tokens/sec: 88710.91021930065\n",
      "Step: 163, Training Loss: 5.09123, Tokens/sec: 88653.9472703305\n",
      "Step: 164, Training Loss: 5.02242, Tokens/sec: 87826.74326612099\n",
      "Step: 165, Training Loss: 5.09070, Tokens/sec: 88584.60279599413\n",
      "Step: 166, Training Loss: 4.90203, Tokens/sec: 88670.84577945394\n",
      "Step: 167, Training Loss: 5.10205, Tokens/sec: 84962.56126738593\n",
      "Step: 168, Training Loss: 5.29847, Tokens/sec: 88856.22342809134\n",
      "Step: 169, Training Loss: 4.99273, Tokens/sec: 88700.29585189835\n",
      "Step: 170, Training Loss: 5.16938, Tokens/sec: 88086.80007955412\n",
      "Step: 171, Training Loss: 4.97027, Tokens/sec: 88497.07423862522\n",
      "Step: 172, Training Loss: 4.85166, Tokens/sec: 88674.33903463777\n",
      "Step: 173, Training Loss: 5.02264, Tokens/sec: 88191.05040779125\n",
      "Step: 174, Training Loss: 5.23907, Tokens/sec: 88502.05061288393\n",
      "Step: 175, Training Loss: 5.10848, Tokens/sec: 88191.68177830758\n",
      "Step: 176, Training Loss: 5.06292, Tokens/sec: 87990.46906077748\n",
      "Step: 177, Training Loss: 5.01367, Tokens/sec: 88571.04990651239\n",
      "Step: 178, Training Loss: 4.91965, Tokens/sec: 87870.37390655963\n",
      "Step: 179, Training Loss: 4.95050, Tokens/sec: 87620.54536722833\n",
      "Step: 180, Training Loss: 4.86232, Tokens/sec: 88531.03082335288\n",
      "Step: 181, Training Loss: 5.06493, Tokens/sec: 88709.51201970213\n",
      "Step: 182, Training Loss: 5.25123, Tokens/sec: 88341.34604389091\n",
      "Step: 183, Training Loss: 5.09427, Tokens/sec: 87771.54408369439\n",
      "Step: 184, Training Loss: 4.90798, Tokens/sec: 87000.01990280155\n",
      "Step: 185, Training Loss: 4.76255, Tokens/sec: 74697.07143855894\n",
      "Step: 186, Training Loss: 4.88606, Tokens/sec: 58074.541551770715\n",
      "Step: 187, Training Loss: 4.70719, Tokens/sec: 83694.73898947054\n",
      "Step: 188, Training Loss: 4.94530, Tokens/sec: 83332.9341843216\n",
      "Step: 189, Training Loss: 5.09804, Tokens/sec: 88570.13538933841\n",
      "Step: 190, Training Loss: 4.88179, Tokens/sec: 88169.83546651634\n",
      "Step: 191, Training Loss: 4.98468, Tokens/sec: 87750.6531348767\n",
      "Step: 192, Training Loss: 4.79909, Tokens/sec: 88325.28235031836\n",
      "Step: 193, Training Loss: 4.73251, Tokens/sec: 88296.62271693221\n",
      "Step: 194, Training Loss: 4.88700, Tokens/sec: 88091.26577824054\n",
      "Step: 195, Training Loss: 5.07687, Tokens/sec: 85781.56344751354\n",
      "Step: 196, Training Loss: 4.92484, Tokens/sec: 73635.1915672473\n",
      "Step: 197, Training Loss: 4.94053, Tokens/sec: 81031.90611110313\n",
      "Step: 198, Training Loss: 4.86701, Tokens/sec: 87259.38334307208\n",
      "Step: 199, Training Loss: 4.80187, Tokens/sec: 87538.4791424227\n",
      "Step: 200, Training Loss: 4.87220, Tokens/sec: 88559.7418471988\n",
      "Step: 201, Training Loss: 4.74104, Tokens/sec: 88609.60086882112\n",
      "Step: 202, Training Loss: 4.91491, Tokens/sec: 88945.18706164457\n",
      "Step: 203, Training Loss: 5.12829, Tokens/sec: 88463.58312636413\n",
      "Step: 204, Training Loss: 4.94630, Tokens/sec: 88575.3876648823\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "input_ids = tokenizer([\"HAMLET:\\nTo be or\"], return_tensors=\"pt\")['input_ids'].to(trainer.device)\n",
    "idx = model.generate(input_ids, temperature=0.25, top_k=50, max_new_tokens=64)\n",
    "print(tokenizer.batch_decode(idx)[0])"
   ],
   "id": "8b5596eda083de0a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "bc3bcc343073d67a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
