{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-16T06:29:00.671565Z",
     "start_time": "2024-12-16T06:28:59.743817Z"
    }
   },
   "source": [
    "from model import ModelConfig, LlamaModel\n",
    "from train import TrainerConfig, DataLoader, Trainer\n",
    "\n",
    "from transformers import AutoTokenizer"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T06:29:00.675585Z",
     "start_time": "2024-12-16T06:29:00.673934Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer_id = \"HuggingFaceTB/SmolLM2-135M\"",
   "id": "2f28fa23c987e72b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T06:29:00.882270Z",
     "start_time": "2024-12-16T06:29:00.720183Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ],
   "id": "9bb4e51aa142abee",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T06:29:00.889135Z",
     "start_time": "2024-12-16T06:29:00.887549Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_config = ModelConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=576,\n",
    "    d_head=64,\n",
    "    d_mlp_proj=1536,\n",
    "    n_layers=30,\n",
    "    n_kv_heads=3,\n",
    "    n_attn_heads=9,\n",
    "    rms_norm_eps=1e-5,\n",
    "    initializer_range=0.041666666666666664,\n",
    "    rope_theta=100000.0,\n",
    "    padding_idx=tokenizer.pad_token_id\n",
    ")"
   ],
   "id": "cde027092af8291e",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T06:29:00.933820Z",
     "start_time": "2024-12-16T06:29:00.932281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_config = TrainerConfig(\n",
    "    per_device_train_batch_size=8,\n",
    "    max_seq_len=2048,\n",
    "    num_epochs=50,\n",
    "    learning_rate=1e-3\n",
    ")"
   ],
   "id": "809773e662327a12",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T06:29:00.980628Z",
     "start_time": "2024-12-16T06:29:00.976427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"data/tiny_shakespeare.txt\") as f:\n",
    "    text = f.read()"
   ],
   "id": "374f398bb34f7ac1",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T06:29:03.519423Z",
     "start_time": "2024-12-16T06:29:01.021608Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = LlamaModel(model_config)\n",
    "dataloader = DataLoader(train_config, tokenizer, text=text)\n",
    "trainer = Trainer(train_config, model)"
   ],
   "id": "9a912a0ec92039d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train tokens             | 342,016\n",
      "Num Trainable Params           | 162,826,560\n",
      "Train device                   | cuda, NVIDIA GeForce RTX 3090, N=1\n",
      "Training precision             | torch.bfloat16\n",
      "Flash Attention                | True\n",
      "torch.compile()                | True\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-16T06:29:03.526090Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train(dataloader)",
   "id": "ee8c2059258a0195",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training steps                 | 1,050 \n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T23:24:45.558867Z",
     "start_time": "2024-12-15T23:24:44.670211Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_ids = tokenizer([\"HAMLET:\\nTo be or\"], return_tensors=\"pt\")['input_ids'].to(trainer.device)\n",
    "idx = model.generate(input_ids, temperature=0.25, top_k=50, max_new_tokens=64)\n",
    "print(tokenizer.batch_decode(idx)[0])"
   ],
   "id": "8b5596eda083de0a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAMLET:\n",
      "To be or power, as I cannot pardon,\n",
      "And not my lord.\n",
      "\n",
      "LEONTES:\n",
      "Farewell, my lord!\n",
      "\n",
      "KING RICHARD III:\n",
      "Sweet I love the like to be contrary:\n",
      "But sanctuary after it was the drunk murd of marriage\n",
      "As I have strengthen for my\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T23:24:45.562699Z",
     "start_time": "2024-12-15T23:24:45.561337Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "bc3bcc343073d67a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
