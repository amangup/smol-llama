{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:15:20.436693Z",
     "start_time": "2024-12-16T09:15:19.511403Z"
    }
   },
   "outputs": [],
   "source": [
    "from model import ModelConfig, LlamaModel\n",
    "from train import TrainerConfig, SimpleDataLoader, Trainer\n",
    "\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f28fa23c987e72b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:15:20.440801Z",
     "start_time": "2024-12-16T09:15:20.439078Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer_id = \"HuggingFaceTB/SmolLM2-135M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bb4e51aa142abee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:15:20.670956Z",
     "start_time": "2024-12-16T09:15:20.484288Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cde027092af8291e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:15:20.678652Z",
     "start_time": "2024-12-16T09:15:20.677029Z"
    }
   },
   "outputs": [],
   "source": [
    "model_config = ModelConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=576,\n",
    "    d_head=64,\n",
    "    d_mlp_proj=1536,\n",
    "    n_layers=30,\n",
    "    n_kv_heads=3,\n",
    "    n_attn_heads=9,\n",
    "    rms_norm_eps=1e-5,\n",
    "    initializer_range=0.041666666666666664,\n",
    "    rope_theta=100000.0,\n",
    "    padding_idx=tokenizer.pad_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "809773e662327a12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:15:20.722233Z",
     "start_time": "2024-12-16T09:15:20.720451Z"
    }
   },
   "outputs": [],
   "source": [
    "train_config = TrainerConfig(\n",
    "    per_device_train_batch_size=96,\n",
    "    max_seq_len=192,\n",
    "    num_epochs=64,\n",
    "    eval_interval_steps=25,\n",
    "    learning_rate=1e-4,\n",
    "    grad_clip_norm=1.0,\n",
    "    val_size=0.1,\n",
    "    log_dir=\"runs/shakespeare\",\n",
    "    warmup_ratio=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "374f398bb34f7ac1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:15:20.766365Z",
     "start_time": "2024-12-16T09:15:20.764228Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"data/tiny_shakespeare.txt\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a912a0ec92039d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:15:23.391460Z",
     "start_time": "2024-12-16T09:15:20.808857Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens                   | 341,184\n",
      "Num Trainable Params           | 162,826,560\n",
      "Train device                   | cuda, NVIDIA GeForce RTX 3090, N=1\n",
      "Training precision             | torch.bfloat16\n",
      "Flash Attention                | True\n",
      "torch.compile()                | True\n",
      "DistributedDataParallel        | False\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LlamaModel(model_config)\n",
    "dataloader = SimpleDataLoader(train_config, tokenizer, text=text)\n",
    "trainer = Trainer(train_config, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee8c2059258a0195",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:18:17.472064Z",
     "start_time": "2024-12-16T09:15:23.398622Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training steps                 | 544 \n",
      "Step: 0, Training Loss: 11.35194, LR: 0.0000050, Tokens/sec: 1658.74\n",
      "Step: 1, Training Loss: 11.32047, LR: 0.0000068, Tokens/sec: 1783.98\n",
      "Step: 2, Training Loss: 11.26234, LR: 0.0000085, Tokens/sec: 87963.25\n",
      "Step: 3, Training Loss: 11.22078, LR: 0.0000103, Tokens/sec: 87143.92\n",
      "Computing Eval loss, steps: 2\n",
      "Step: 3, Eval Loss: 11.19228\n",
      "Step: 4, Training Loss: 11.19003, LR: 0.0000120, Tokens/sec: 90402.11\n",
      "Step: 5, Training Loss: 11.13039, LR: 0.0000138, Tokens/sec: 87071.38\n",
      "Step: 6, Training Loss: 11.06210, LR: 0.0000156, Tokens/sec: 95790.97\n",
      "Step: 7, Training Loss: 10.95111, LR: 0.0000173, Tokens/sec: 94131.17\n",
      "Step: 8, Training Loss: 10.80084, LR: 0.0000191, Tokens/sec: 93003.88\n",
      "Step: 9, Training Loss: 10.70723, LR: 0.0000208, Tokens/sec: 94569.32\n",
      "Step: 10, Training Loss: 10.50686, LR: 0.0000226, Tokens/sec: 95778.45\n",
      "Step: 11, Training Loss: 10.41341, LR: 0.0000244, Tokens/sec: 91992.52\n",
      "Step: 12, Training Loss: 10.22006, LR: 0.0000261, Tokens/sec: 79153.57\n",
      "Step: 13, Training Loss: 10.10920, LR: 0.0000279, Tokens/sec: 84129.63\n",
      "Step: 14, Training Loss: 9.95729, LR: 0.0000296, Tokens/sec: 85093.49\n",
      "Step: 15, Training Loss: 9.79308, LR: 0.0000314, Tokens/sec: 84570.72\n",
      "Step: 16, Training Loss: 9.56164, LR: 0.0000331, Tokens/sec: 73437.28\n",
      "Step: 17, Training Loss: 9.43904, LR: 0.0000349, Tokens/sec: 84754.43\n",
      "Step: 18, Training Loss: 9.35109, LR: 0.0000367, Tokens/sec: 83167.69\n",
      "Step: 19, Training Loss: 9.29278, LR: 0.0000384, Tokens/sec: 84591.86\n",
      "Step: 20, Training Loss: 9.14404, LR: 0.0000402, Tokens/sec: 85400.23\n",
      "Step: 21, Training Loss: 9.02379, LR: 0.0000419, Tokens/sec: 79405.16\n",
      "Step: 22, Training Loss: 9.00684, LR: 0.0000437, Tokens/sec: 83740.16\n",
      "Step: 23, Training Loss: 8.90052, LR: 0.0000455, Tokens/sec: 93036.98\n",
      "Step: 24, Training Loss: 8.80613, LR: 0.0000472, Tokens/sec: 84238.17\n",
      "Step: 25, Training Loss: 8.73043, LR: 0.0000490, Tokens/sec: 97274.81\n",
      "Computing Eval loss, steps: 2\n",
      "Step: 25, Eval Loss: 8.66778\n",
      "Step: 26, Training Loss: 8.67980, LR: 0.0000507, Tokens/sec: 82760.77\n",
      "Step: 27, Training Loss: 8.58607, LR: 0.0000525, Tokens/sec: 97163.32\n",
      "Step: 28, Training Loss: 8.51206, LR: 0.0000543, Tokens/sec: 95207.48\n",
      "Step: 29, Training Loss: 8.47389, LR: 0.0000560, Tokens/sec: 87408.96\n",
      "Step: 30, Training Loss: 8.39949, LR: 0.0000578, Tokens/sec: 96257.11\n",
      "Step: 31, Training Loss: 8.38593, LR: 0.0000595, Tokens/sec: 90862.65\n",
      "Step: 32, Training Loss: 8.29116, LR: 0.0000613, Tokens/sec: 96582.25\n",
      "Step: 33, Training Loss: 8.20223, LR: 0.0000631, Tokens/sec: 86363.43\n",
      "Step: 34, Training Loss: 8.19917, LR: 0.0000648, Tokens/sec: 82761.81\n",
      "Step: 35, Training Loss: 8.12600, LR: 0.0000666, Tokens/sec: 83074.12\n",
      "Step: 36, Training Loss: 8.12254, LR: 0.0000683, Tokens/sec: 85288.95\n",
      "Step: 37, Training Loss: 8.04268, LR: 0.0000701, Tokens/sec: 83865.10\n",
      "Step: 38, Training Loss: 7.97994, LR: 0.0000719, Tokens/sec: 85709.64\n",
      "Step: 39, Training Loss: 7.98418, LR: 0.0000736, Tokens/sec: 85837.77\n",
      "Step: 40, Training Loss: 7.87266, LR: 0.0000754, Tokens/sec: 85620.36\n",
      "Step: 41, Training Loss: 7.81527, LR: 0.0000771, Tokens/sec: 84778.21\n",
      "Step: 42, Training Loss: 7.75976, LR: 0.0000789, Tokens/sec: 83688.22\n",
      "Step: 43, Training Loss: 7.71789, LR: 0.0000806, Tokens/sec: 78406.59\n",
      "Step: 44, Training Loss: 7.61522, LR: 0.0000824, Tokens/sec: 86423.06\n",
      "Step: 45, Training Loss: 7.53384, LR: 0.0000842, Tokens/sec: 87741.44\n",
      "Step: 46, Training Loss: 7.50284, LR: 0.0000859, Tokens/sec: 89360.03\n",
      "Step: 47, Training Loss: 7.42671, LR: 0.0000877, Tokens/sec: 98735.19\n",
      "Step: 48, Training Loss: 7.41450, LR: 0.0000894, Tokens/sec: 85783.71\n",
      "Step: 49, Training Loss: 7.30447, LR: 0.0000912, Tokens/sec: 96192.30\n",
      "Step: 50, Training Loss: 7.23677, LR: 0.0000930, Tokens/sec: 88351.63\n",
      "Computing Eval loss, steps: 2\n",
      "Step: 50, Eval Loss: 7.21412\n",
      "Step: 51, Training Loss: 7.20585, LR: 0.0000947, Tokens/sec: 95519.20\n",
      "Step: 52, Training Loss: 7.12012, LR: 0.0000965, Tokens/sec: 93183.84\n",
      "Step: 53, Training Loss: 7.12740, LR: 0.0000982, Tokens/sec: 92567.17\n",
      "Step: 54, Training Loss: 7.04899, LR: 0.0001000, Tokens/sec: 89545.52\n",
      "Step: 55, Training Loss: 6.98840, LR: 0.0001000, Tokens/sec: 93103.39\n",
      "Step: 56, Training Loss: 6.96911, LR: 0.0001000, Tokens/sec: 93108.16\n",
      "Step: 57, Training Loss: 6.85700, LR: 0.0001000, Tokens/sec: 86485.32\n",
      "Step: 58, Training Loss: 6.81300, LR: 0.0001000, Tokens/sec: 96576.02\n",
      "Step: 59, Training Loss: 6.76120, LR: 0.0001000, Tokens/sec: 95368.30\n",
      "Step: 60, Training Loss: 6.73919, LR: 0.0001000, Tokens/sec: 97789.98\n",
      "Step: 61, Training Loss: 6.64405, LR: 0.0001000, Tokens/sec: 98868.89\n",
      "Step: 62, Training Loss: 6.58289, LR: 0.0000999, Tokens/sec: 97246.23\n",
      "Step: 63, Training Loss: 6.56595, LR: 0.0000999, Tokens/sec: 81454.86\n",
      "Step: 64, Training Loss: 6.50700, LR: 0.0000999, Tokens/sec: 94993.80\n",
      "Step: 65, Training Loss: 6.49559, LR: 0.0000999, Tokens/sec: 96349.19\n",
      "Step: 66, Training Loss: 6.40112, LR: 0.0000999, Tokens/sec: 91917.71\n",
      "Step: 67, Training Loss: 6.35689, LR: 0.0000998, Tokens/sec: 94973.63\n",
      "Step: 68, Training Loss: 6.34327, LR: 0.0000998, Tokens/sec: 97482.68\n",
      "Step: 69, Training Loss: 6.26975, LR: 0.0000998, Tokens/sec: 97383.30\n",
      "Step: 70, Training Loss: 6.31909, LR: 0.0000998, Tokens/sec: 95065.04\n",
      "Step: 71, Training Loss: 6.25448, LR: 0.0000997, Tokens/sec: 91436.33\n",
      "Step: 72, Training Loss: 6.21428, LR: 0.0000997, Tokens/sec: 97139.37\n",
      "Step: 73, Training Loss: 6.20511, LR: 0.0000997, Tokens/sec: 95649.62\n",
      "Step: 74, Training Loss: 6.11631, LR: 0.0000996, Tokens/sec: 94652.50\n",
      "Step: 75, Training Loss: 6.08256, LR: 0.0000996, Tokens/sec: 98347.99\n",
      "Computing Eval loss, steps: 2\n",
      "Step: 75, Eval Loss: 6.09455\n",
      "Step: 76, Training Loss: 6.06760, LR: 0.0000996, Tokens/sec: 95923.76\n",
      "Step: 77, Training Loss: 6.05193, LR: 0.0000995, Tokens/sec: 95968.47\n",
      "Step: 78, Training Loss: 5.96123, LR: 0.0000995, Tokens/sec: 96854.41\n",
      "Step: 79, Training Loss: 5.91218, LR: 0.0000994, Tokens/sec: 94093.56\n",
      "Step: 80, Training Loss: 5.92184, LR: 0.0000994, Tokens/sec: 98489.59\n",
      "Step: 81, Training Loss: 5.86492, LR: 0.0000993, Tokens/sec: 98543.68\n",
      "Step: 82, Training Loss: 5.85415, LR: 0.0000993, Tokens/sec: 95880.23\n",
      "Step: 83, Training Loss: 5.77444, LR: 0.0000992, Tokens/sec: 93941.68\n",
      "Step: 84, Training Loss: 5.74625, LR: 0.0000992, Tokens/sec: 92413.66\n",
      "Step: 85, Training Loss: 5.74921, LR: 0.0000991, Tokens/sec: 95037.45\n",
      "Step: 86, Training Loss: 5.66852, LR: 0.0000991, Tokens/sec: 97321.07\n",
      "Step: 87, Training Loss: 5.76167, LR: 0.0000990, Tokens/sec: 93359.71\n",
      "Step: 88, Training Loss: 5.70117, LR: 0.0000989, Tokens/sec: 93397.42\n",
      "Step: 89, Training Loss: 5.65011, LR: 0.0000989, Tokens/sec: 95512.87\n",
      "Step: 90, Training Loss: 5.64177, LR: 0.0000988, Tokens/sec: 96963.12\n",
      "Step: 91, Training Loss: 5.55105, LR: 0.0000987, Tokens/sec: 94711.89\n",
      "Step: 92, Training Loss: 5.53780, LR: 0.0000987, Tokens/sec: 97796.84\n",
      "Step: 93, Training Loss: 5.55373, LR: 0.0000986, Tokens/sec: 96725.85\n",
      "Step: 94, Training Loss: 5.52943, LR: 0.0000985, Tokens/sec: 94147.73\n",
      "Step: 95, Training Loss: 5.45733, LR: 0.0000985, Tokens/sec: 96091.93\n",
      "Step: 96, Training Loss: 5.40916, LR: 0.0000984, Tokens/sec: 97618.31\n",
      "Step: 97, Training Loss: 5.44483, LR: 0.0000983, Tokens/sec: 90588.65\n",
      "Step: 98, Training Loss: 5.36281, LR: 0.0000982, Tokens/sec: 88839.61\n",
      "Step: 99, Training Loss: 5.35544, LR: 0.0000981, Tokens/sec: 89784.09\n",
      "Step: 100, Training Loss: 5.28420, LR: 0.0000981, Tokens/sec: 95092.14\n",
      "Computing Eval loss, steps: 2\n",
      "Step: 100, Eval Loss: 5.45454\n",
      "Step: 101, Training Loss: 5.27426, LR: 0.0000980, Tokens/sec: 97587.65\n",
      "Step: 102, Training Loss: 5.29265, LR: 0.0000979, Tokens/sec: 86307.82\n",
      "Step: 103, Training Loss: 5.21189, LR: 0.0000978, Tokens/sec: 97470.33\n",
      "Step: 104, Training Loss: 5.36962, LR: 0.0000977, Tokens/sec: 98470.24\n",
      "Step: 105, Training Loss: 5.28782, LR: 0.0000976, Tokens/sec: 94365.76\n",
      "Step: 106, Training Loss: 5.23370, LR: 0.0000975, Tokens/sec: 96474.24\n",
      "Step: 107, Training Loss: 5.23360, LR: 0.0000974, Tokens/sec: 94359.74\n",
      "Step: 108, Training Loss: 5.15082, LR: 0.0000973, Tokens/sec: 94928.12\n",
      "Step: 109, Training Loss: 5.14647, LR: 0.0000972, Tokens/sec: 95826.22\n",
      "Step: 110, Training Loss: 5.16654, LR: 0.0000971, Tokens/sec: 94136.83\n",
      "Step: 111, Training Loss: 5.15014, LR: 0.0000970, Tokens/sec: 94488.36\n",
      "Step: 112, Training Loss: 5.09726, LR: 0.0000969, Tokens/sec: 96559.11\n",
      "Step: 113, Training Loss: 5.03743, LR: 0.0000968, Tokens/sec: 98363.25\n",
      "Step: 114, Training Loss: 5.10016, LR: 0.0000967, Tokens/sec: 95925.24\n",
      "Step: 115, Training Loss: 5.00368, LR: 0.0000966, Tokens/sec: 82160.27\n",
      "Step: 116, Training Loss: 5.00039, LR: 0.0000965, Tokens/sec: 96626.03\n",
      "Step: 117, Training Loss: 4.94658, LR: 0.0000964, Tokens/sec: 96253.46\n",
      "Step: 118, Training Loss: 4.94132, LR: 0.0000963, Tokens/sec: 96134.71\n",
      "Step: 119, Training Loss: 4.95796, LR: 0.0000961, Tokens/sec: 97046.12\n",
      "Step: 120, Training Loss: 4.89782, LR: 0.0000960, Tokens/sec: 96738.60\n",
      "Step: 121, Training Loss: 5.12951, LR: 0.0000959, Tokens/sec: 94791.94\n",
      "Step: 122, Training Loss: 4.97462, LR: 0.0000958, Tokens/sec: 95244.06\n",
      "Step: 123, Training Loss: 4.91933, LR: 0.0000957, Tokens/sec: 96739.82\n",
      "Step: 124, Training Loss: 4.92429, LR: 0.0000955, Tokens/sec: 95922.46\n",
      "Step: 125, Training Loss: 4.83933, LR: 0.0000954, Tokens/sec: 94824.70\n",
      "Computing Eval loss, steps: 2\n",
      "Step: 125, Eval Loss: 5.13915\n",
      "Step: 126, Training Loss: 4.84397, LR: 0.0000953, Tokens/sec: 93252.18\n",
      "Step: 127, Training Loss: 4.87062, LR: 0.0000952, Tokens/sec: 98254.53\n",
      "Step: 128, Training Loss: 4.85353, LR: 0.0000950, Tokens/sec: 98068.08\n",
      "Step: 129, Training Loss: 4.81053, LR: 0.0000949, Tokens/sec: 97536.76\n",
      "Step: 130, Training Loss: 4.74512, LR: 0.0000948, Tokens/sec: 96973.01\n",
      "Step: 131, Training Loss: 4.82207, LR: 0.0000946, Tokens/sec: 94752.31\n",
      "Step: 132, Training Loss: 4.72875, LR: 0.0000945, Tokens/sec: 94150.21\n",
      "Step: 133, Training Loss: 4.71739, LR: 0.0000944, Tokens/sec: 98075.69\n",
      "Step: 134, Training Loss: 4.67034, LR: 0.0000942, Tokens/sec: 95667.75\n",
      "Step: 135, Training Loss: 4.68679, LR: 0.0000941, Tokens/sec: 95921.39\n",
      "Step: 136, Training Loss: 4.67809, LR: 0.0000939, Tokens/sec: 97228.40\n",
      "Step: 137, Training Loss: 4.64264, LR: 0.0000938, Tokens/sec: 96391.42\n",
      "Step: 138, Training Loss: 4.89970, LR: 0.0000936, Tokens/sec: 94312.41\n",
      "Step: 139, Training Loss: 4.70685, LR: 0.0000935, Tokens/sec: 94055.48\n",
      "Step: 140, Training Loss: 4.65190, LR: 0.0000933, Tokens/sec: 96096.88\n",
      "Step: 141, Training Loss: 4.67110, LR: 0.0000932, Tokens/sec: 89406.25\n",
      "Step: 142, Training Loss: 4.56771, LR: 0.0000930, Tokens/sec: 83070.48\n",
      "Step: 143, Training Loss: 4.59782, LR: 0.0000929, Tokens/sec: 97858.82\n",
      "Step: 144, Training Loss: 4.61060, LR: 0.0000927, Tokens/sec: 85526.11\n",
      "Step: 145, Training Loss: 4.61166, LR: 0.0000926, Tokens/sec: 94033.19\n",
      "Step: 146, Training Loss: 4.54647, LR: 0.0000924, Tokens/sec: 98241.34\n",
      "Step: 147, Training Loss: 4.51622, LR: 0.0000922, Tokens/sec: 97548.76\n",
      "Step: 148, Training Loss: 4.57261, LR: 0.0000921, Tokens/sec: 91948.20\n",
      "Step: 149, Training Loss: 4.50044, LR: 0.0000919, Tokens/sec: 95270.70\n",
      "Step: 150, Training Loss: 4.48022, LR: 0.0000917, Tokens/sec: 96311.48\n",
      "Computing Eval loss, steps: 2\n",
      "Step: 150, Eval Loss: 5.01813\n",
      "Step: 151, Training Loss: 4.41563, LR: 0.0000916, Tokens/sec: 94277.41\n",
      "Step: 152, Training Loss: 4.45741, LR: 0.0000914, Tokens/sec: 95710.71\n",
      "Step: 153, Training Loss: 4.43727, LR: 0.0000912, Tokens/sec: 93503.59\n",
      "Step: 154, Training Loss: 4.40711, LR: 0.0000911, Tokens/sec: 92762.41\n",
      "Step: 155, Training Loss: 4.65950, LR: 0.0000909, Tokens/sec: 94851.49\n",
      "Step: 156, Training Loss: 4.45227, LR: 0.0000907, Tokens/sec: 96805.46\n",
      "Step: 157, Training Loss: 4.42546, LR: 0.0000905, Tokens/sec: 95966.71\n",
      "Step: 158, Training Loss: 4.41929, LR: 0.0000904, Tokens/sec: 93100.66\n",
      "Step: 159, Training Loss: 4.34979, LR: 0.0000902, Tokens/sec: 93017.59\n",
      "Step: 160, Training Loss: 4.34830, LR: 0.0000900, Tokens/sec: 98021.25\n",
      "Step: 161, Training Loss: 4.38070, LR: 0.0000898, Tokens/sec: 95574.70\n",
      "Step: 162, Training Loss: 4.36188, LR: 0.0000896, Tokens/sec: 96112.95\n",
      "Step: 163, Training Loss: 4.31657, LR: 0.0000895, Tokens/sec: 97953.17\n",
      "Step: 164, Training Loss: 4.27719, LR: 0.0000893, Tokens/sec: 96570.20\n",
      "Step: 165, Training Loss: 4.32628, LR: 0.0000891, Tokens/sec: 93572.86\n",
      "Step: 166, Training Loss: 4.27503, LR: 0.0000889, Tokens/sec: 97411.97\n",
      "Step: 167, Training Loss: 4.22398, LR: 0.0000887, Tokens/sec: 96654.85\n",
      "Step: 168, Training Loss: 4.17398, LR: 0.0000885, Tokens/sec: 94504.95\n",
      "Step: 169, Training Loss: 4.18907, LR: 0.0000883, Tokens/sec: 94577.18\n",
      "Step: 170, Training Loss: 4.22004, LR: 0.0000881, Tokens/sec: 97927.74\n",
      "Step: 171, Training Loss: 4.15208, LR: 0.0000879, Tokens/sec: 95380.56\n",
      "Step: 172, Training Loss: 4.42440, LR: 0.0000877, Tokens/sec: 94453.40\n",
      "Step: 173, Training Loss: 4.17054, LR: 0.0000875, Tokens/sec: 93460.29\n",
      "Step: 174, Training Loss: 4.19195, LR: 0.0000873, Tokens/sec: 93570.66\n",
      "Step: 175, Training Loss: 4.13605, LR: 0.0000871, Tokens/sec: 96200.19\n",
      "Computing Eval loss, steps: 2\n",
      "Step: 175, Eval Loss: 4.94522\n",
      "Step: 176, Training Loss: 4.11749, LR: 0.0000869, Tokens/sec: 95346.65\n",
      "Step: 177, Training Loss: 4.09857, LR: 0.0000867, Tokens/sec: 94508.67\n",
      "Step: 178, Training Loss: 4.15366, LR: 0.0000865, Tokens/sec: 93306.42\n",
      "Step: 179, Training Loss: 4.12261, LR: 0.0000863, Tokens/sec: 91597.19\n",
      "Step: 180, Training Loss: 4.09163, LR: 0.0000861, Tokens/sec: 95678.91\n",
      "Step: 181, Training Loss: 4.04348, LR: 0.0000859, Tokens/sec: 96022.79\n",
      "Step: 182, Training Loss: 4.08785, LR: 0.0000857, Tokens/sec: 94154.31\n",
      "Step: 183, Training Loss: 4.02774, LR: 0.0000855, Tokens/sec: 94827.13\n",
      "Step: 184, Training Loss: 3.98120, LR: 0.0000853, Tokens/sec: 96724.74\n",
      "Step: 185, Training Loss: 3.93079, LR: 0.0000850, Tokens/sec: 93007.23\n",
      "Step: 186, Training Loss: 3.94440, LR: 0.0000848, Tokens/sec: 96283.57\n",
      "Step: 187, Training Loss: 3.98364, LR: 0.0000846, Tokens/sec: 97957.97\n",
      "Step: 188, Training Loss: 3.92223, LR: 0.0000844, Tokens/sec: 97037.69\n",
      "Step: 189, Training Loss: 4.17729, LR: 0.0000842, Tokens/sec: 97907.55\n",
      "Step: 190, Training Loss: 3.92521, LR: 0.0000839, Tokens/sec: 85811.49\n",
      "Step: 191, Training Loss: 3.96263, LR: 0.0000837, Tokens/sec: 95789.10\n",
      "Step: 192, Training Loss: 3.91243, LR: 0.0000835, Tokens/sec: 93811.05\n",
      "Step: 193, Training Loss: 3.88588, LR: 0.0000833, Tokens/sec: 93190.14\n",
      "Step: 194, Training Loss: 3.89050, LR: 0.0000831, Tokens/sec: 96758.73\n",
      "Step: 195, Training Loss: 3.95835, LR: 0.0000828, Tokens/sec: 95167.72\n",
      "Step: 196, Training Loss: 3.91488, LR: 0.0000826, Tokens/sec: 96192.87\n",
      "Step: 197, Training Loss: 3.93590, LR: 0.0000824, Tokens/sec: 95002.40\n",
      "Step: 198, Training Loss: 3.84521, LR: 0.0000821, Tokens/sec: 94006.85\n",
      "Step: 199, Training Loss: 3.92972, LR: 0.0000819, Tokens/sec: 96102.01\n",
      "Step: 200, Training Loss: 3.83177, LR: 0.0000817, Tokens/sec: 97298.85\n",
      "Computing Eval loss, steps: 2\n",
      "Step: 200, Eval Loss: 4.91494\n",
      "Step: 201, Training Loss: 3.80154, LR: 0.0000815, Tokens/sec: 94106.18\n",
      "Step: 202, Training Loss: 3.74091, LR: 0.0000812, Tokens/sec: 96650.77\n",
      "Step: 203, Training Loss: 3.73204, LR: 0.0000810, Tokens/sec: 96951.48\n",
      "Step: 204, Training Loss: 3.77112, LR: 0.0000807, Tokens/sec: 97883.55\n",
      "Step: 205, Training Loss: 3.72097, LR: 0.0000805, Tokens/sec: 94403.21\n",
      "Step: 206, Training Loss: 3.93949, LR: 0.0000803, Tokens/sec: 92620.37\n",
      "Step: 207, Training Loss: 3.73588, LR: 0.0000800, Tokens/sec: 91193.09\n",
      "Step: 208, Training Loss: 3.75542, LR: 0.0000798, Tokens/sec: 91566.57\n",
      "Step: 209, Training Loss: 3.75116, LR: 0.0000795, Tokens/sec: 86303.18\n",
      "Step: 210, Training Loss: 3.71531, LR: 0.0000793, Tokens/sec: 96912.10\n",
      "Step: 211, Training Loss: 3.71332, LR: 0.0000791, Tokens/sec: 89386.01\n",
      "Step: 212, Training Loss: 3.79385, LR: 0.0000788, Tokens/sec: 85132.65\n",
      "Step: 213, Training Loss: 3.72874, LR: 0.0000786, Tokens/sec: 95489.94\n",
      "Step: 214, Training Loss: 3.73879, LR: 0.0000783, Tokens/sec: 94573.11\n",
      "Step: 215, Training Loss: 3.64848, LR: 0.0000781, Tokens/sec: 96013.47\n",
      "Step: 216, Training Loss: 3.73556, LR: 0.0000778, Tokens/sec: 75903.06\n",
      "Step: 217, Training Loss: 3.63184, LR: 0.0000776, Tokens/sec: 96568.15\n",
      "Step: 218, Training Loss: 3.59609, LR: 0.0000773, Tokens/sec: 93889.49\n",
      "Step: 219, Training Loss: 3.52750, LR: 0.0000771, Tokens/sec: 95245.95\n",
      "Step: 220, Training Loss: 3.52653, LR: 0.0000768, Tokens/sec: 93633.64\n",
      "Step: 221, Training Loss: 3.56828, LR: 0.0000766, Tokens/sec: 96622.02\n",
      "Step: 222, Training Loss: 3.51832, LR: 0.0000763, Tokens/sec: 94987.87\n",
      "Step: 223, Training Loss: 3.70119, LR: 0.0000761, Tokens/sec: 86799.64\n",
      "Step: 224, Training Loss: 3.53115, LR: 0.0000758, Tokens/sec: 86038.69\n",
      "Step: 225, Training Loss: 3.54074, LR: 0.0000756, Tokens/sec: 89002.99\n",
      "Computing Eval loss, steps: 2\n",
      "Step: 225, Eval Loss: 4.91742\n",
      "Step: 226, Training Loss: 3.53165, LR: 0.0000753, Tokens/sec: 79674.57\n",
      "Step: 227, Training Loss: 3.49807, LR: 0.0000750, Tokens/sec: 96150.14\n",
      "Step: 228, Training Loss: 3.49366, LR: 0.0000748, Tokens/sec: 85481.17\n",
      "Step: 229, Training Loss: 3.58145, LR: 0.0000745, Tokens/sec: 97030.13\n",
      "Step: 230, Training Loss: 3.50424, LR: 0.0000743, Tokens/sec: 96459.74\n",
      "Step: 231, Training Loss: 3.50612, LR: 0.0000740, Tokens/sec: 84962.67\n",
      "Step: 232, Training Loss: 3.44845, LR: 0.0000737, Tokens/sec: 92895.88\n",
      "Step: 233, Training Loss: 3.50683, LR: 0.0000735, Tokens/sec: 95318.79\n",
      "Step: 234, Training Loss: 3.41069, LR: 0.0000732, Tokens/sec: 95152.07\n",
      "Step: 235, Training Loss: 3.38121, LR: 0.0000730, Tokens/sec: 87628.37\n",
      "Step: 236, Training Loss: 3.28912, LR: 0.0000727, Tokens/sec: 90312.06\n",
      "Step: 237, Training Loss: 3.32201, LR: 0.0000724, Tokens/sec: 95643.54\n",
      "Step: 238, Training Loss: 3.36433, LR: 0.0000722, Tokens/sec: 95443.57\n",
      "Step: 239, Training Loss: 3.31821, LR: 0.0000719, Tokens/sec: 95153.88\n",
      "Step: 240, Training Loss: 3.49223, LR: 0.0000716, Tokens/sec: 95491.94\n",
      "Step: 241, Training Loss: 3.31407, LR: 0.0000714, Tokens/sec: 92301.51\n",
      "Step: 242, Training Loss: 3.32920, LR: 0.0000711, Tokens/sec: 96425.14\n",
      "Step: 243, Training Loss: 3.32425, LR: 0.0000708, Tokens/sec: 96826.04\n",
      "Step: 244, Training Loss: 3.27586, LR: 0.0000705, Tokens/sec: 96726.20\n",
      "Step: 245, Training Loss: 3.29278, LR: 0.0000703, Tokens/sec: 92499.03\n",
      "Step: 246, Training Loss: 3.37179, LR: 0.0000700, Tokens/sec: 95565.15\n",
      "Step: 247, Training Loss: 3.29797, LR: 0.0000697, Tokens/sec: 93491.37\n",
      "Step: 248, Training Loss: 3.30498, LR: 0.0000695, Tokens/sec: 94254.70\n",
      "Step: 249, Training Loss: 3.24951, LR: 0.0000692, Tokens/sec: 97090.66\n",
      "Step: 250, Training Loss: 3.32402, LR: 0.0000689, Tokens/sec: 95153.94\n",
      "Computing Eval loss, steps: 2\n",
      "Step: 250, Eval Loss: 4.94027\n",
      "Step: 251, Training Loss: 3.21554, LR: 0.0000686, Tokens/sec: 96304.74\n",
      "Step: 252, Training Loss: 3.19313, LR: 0.0000684, Tokens/sec: 93718.27\n",
      "Step: 253, Training Loss: 3.09765, LR: 0.0000681, Tokens/sec: 96657.10\n",
      "Step: 254, Training Loss: 3.14844, LR: 0.0000678, Tokens/sec: 97816.08\n",
      "Step: 255, Training Loss: 3.18495, LR: 0.0000675, Tokens/sec: 95410.33\n",
      "Step: 256, Training Loss: 3.15450, LR: 0.0000672, Tokens/sec: 95740.89\n",
      "Step: 257, Training Loss: 3.32231, LR: 0.0000670, Tokens/sec: 91264.68\n",
      "Step: 258, Training Loss: 3.14343, LR: 0.0000667, Tokens/sec: 95130.17\n",
      "Step: 259, Training Loss: 3.16939, LR: 0.0000664, Tokens/sec: 95858.55\n",
      "Step: 260, Training Loss: 3.17953, LR: 0.0000661, Tokens/sec: 95355.67\n",
      "Step: 261, Training Loss: 3.12593, LR: 0.0000659, Tokens/sec: 96891.75\n",
      "Step: 262, Training Loss: 3.16783, LR: 0.0000656, Tokens/sec: 94342.40\n",
      "Step: 263, Training Loss: 3.21997, LR: 0.0000653, Tokens/sec: 91515.53\n",
      "Step: 264, Training Loss: 3.16155, LR: 0.0000650, Tokens/sec: 97368.32\n",
      "Step: 265, Training Loss: 3.17131, LR: 0.0000647, Tokens/sec: 97034.36\n",
      "Step: 266, Training Loss: 3.08753, LR: 0.0000645, Tokens/sec: 96631.99\n",
      "Step: 267, Training Loss: 3.18345, LR: 0.0000642, Tokens/sec: 95221.43\n",
      "Step: 268, Training Loss: 3.07596, LR: 0.0000639, Tokens/sec: 87955.92\n",
      "Step: 269, Training Loss: 3.02793, LR: 0.0000636, Tokens/sec: 92979.22\n",
      "Step: 270, Training Loss: 2.94226, LR: 0.0000633, Tokens/sec: 92709.67\n",
      "Step: 271, Training Loss: 3.00172, LR: 0.0000630, Tokens/sec: 96522.51\n",
      "Step: 272, Training Loss: 3.00627, LR: 0.0000628, Tokens/sec: 96603.05\n",
      "Step: 273, Training Loss: 2.98503, LR: 0.0000625, Tokens/sec: 96624.21\n",
      "Step: 274, Training Loss: 3.16524, LR: 0.0000622, Tokens/sec: 92585.31\n",
      "Step: 275, Training Loss: 2.99335, LR: 0.0000619, Tokens/sec: 95052.16\n",
      "Computing Eval loss, steps: 2\n",
      "Step: 275, Eval Loss: 4.95682\n",
      "Step: 276, Training Loss: 3.00010, LR: 0.0000616, Tokens/sec: 97855.19\n",
      "Step: 277, Training Loss: 3.02252, LR: 0.0000613, Tokens/sec: 94951.22\n",
      "Step: 278, Training Loss: 2.97076, LR: 0.0000610, Tokens/sec: 97154.95\n",
      "Step: 279, Training Loss: 2.99211, LR: 0.0000608, Tokens/sec: 88293.89\n",
      "Step: 280, Training Loss: 3.05739, LR: 0.0000605, Tokens/sec: 92914.56\n",
      "Step: 281, Training Loss: 2.99300, LR: 0.0000602, Tokens/sec: 94263.16\n",
      "Step: 282, Training Loss: 2.97070, LR: 0.0000599, Tokens/sec: 97179.23\n",
      "Step: 283, Training Loss: 2.89915, LR: 0.0000596, Tokens/sec: 96864.35\n",
      "Step: 284, Training Loss: 3.01167, LR: 0.0000593, Tokens/sec: 93919.44\n",
      "Step: 285, Training Loss: 2.89493, LR: 0.0000590, Tokens/sec: 93500.45\n",
      "Step: 286, Training Loss: 2.85832, LR: 0.0000587, Tokens/sec: 95987.85\n",
      "Step: 287, Training Loss: 2.78062, LR: 0.0000585, Tokens/sec: 95256.97\n",
      "Step: 288, Training Loss: 2.81319, LR: 0.0000582, Tokens/sec: 92626.19\n",
      "Step: 289, Training Loss: 2.83293, LR: 0.0000579, Tokens/sec: 94811.32\n",
      "Step: 290, Training Loss: 2.82878, LR: 0.0000576, Tokens/sec: 97335.09\n",
      "Step: 291, Training Loss: 2.97855, LR: 0.0000573, Tokens/sec: 96575.96\n",
      "Step: 292, Training Loss: 2.80728, LR: 0.0000570, Tokens/sec: 96679.47\n",
      "Step: 293, Training Loss: 2.83068, LR: 0.0000567, Tokens/sec: 94940.19\n",
      "Step: 294, Training Loss: 2.84091, LR: 0.0000564, Tokens/sec: 93409.71\n",
      "Step: 295, Training Loss: 2.77989, LR: 0.0000562, Tokens/sec: 96153.79\n",
      "Step: 296, Training Loss: 2.81220, LR: 0.0000559, Tokens/sec: 96380.31\n",
      "Step: 297, Training Loss: 2.89502, LR: 0.0000556, Tokens/sec: 94023.58\n",
      "Step: 298, Training Loss: 2.81715, LR: 0.0000553, Tokens/sec: 93412.22\n",
      "Step: 299, Training Loss: 2.79848, LR: 0.0000550, Tokens/sec: 93974.14\n",
      "Step: 300, Training Loss: 2.73082, LR: 0.0000547, Tokens/sec: 96958.07\n",
      "Computing Eval loss, steps: 2\n",
      "Step: 300, Eval Loss: 4.97944\n",
      "Step: 301, Training Loss: 2.83834, LR: 0.0000544, Tokens/sec: 94708.88\n",
      "Step: 302, Training Loss: 2.72464, LR: 0.0000541, Tokens/sec: 85127.26\n",
      "Step: 303, Training Loss: 2.71322, LR: 0.0000538, Tokens/sec: 91442.06\n",
      "Step: 304, Training Loss: 2.63080, LR: 0.0000536, Tokens/sec: 88085.59\n",
      "Step: 305, Training Loss: 2.66735, LR: 0.0000533, Tokens/sec: 87640.79\n",
      "Step: 306, Training Loss: 2.67752, LR: 0.0000530, Tokens/sec: 96118.59\n",
      "Step: 307, Training Loss: 2.67258, LR: 0.0000527, Tokens/sec: 96762.85\n",
      "Step: 308, Training Loss: 2.83137, LR: 0.0000524, Tokens/sec: 96575.85\n",
      "Step: 309, Training Loss: 2.67243, LR: 0.0000521, Tokens/sec: 92206.35\n",
      "Step: 310, Training Loss: 2.68727, LR: 0.0000518, Tokens/sec: 96429.76\n",
      "Step: 311, Training Loss: 2.69434, LR: 0.0000515, Tokens/sec: 82924.39\n",
      "Step: 312, Training Loss: 2.64598, LR: 0.0000513, Tokens/sec: 87580.36\n",
      "Step: 313, Training Loss: 2.66970, LR: 0.0000510, Tokens/sec: 70435.95\n",
      "Step: 314, Training Loss: 2.76469, LR: 0.0000507, Tokens/sec: 73086.87\n",
      "Step: 315, Training Loss: 2.70028, LR: 0.0000504, Tokens/sec: 87533.01\n",
      "Step: 316, Training Loss: 2.67807, LR: 0.0000501, Tokens/sec: 94597.13\n",
      "Step: 317, Training Loss: 2.59895, LR: 0.0000498, Tokens/sec: 68303.03\n",
      "Step: 318, Training Loss: 2.71305, LR: 0.0000495, Tokens/sec: 93894.89\n",
      "Step: 319, Training Loss: 2.60062, LR: 0.0000492, Tokens/sec: 95119.19\n",
      "Step: 320, Training Loss: 2.57113, LR: 0.0000490, Tokens/sec: 84758.21\n",
      "Step: 321, Training Loss: 2.49149, LR: 0.0000487, Tokens/sec: 78290.16\n",
      "Step: 322, Training Loss: 2.52722, LR: 0.0000484, Tokens/sec: 96635.67\n",
      "Step: 323, Training Loss: 2.53265, LR: 0.0000481, Tokens/sec: 94087.94\n",
      "Step: 324, Training Loss: 2.53732, LR: 0.0000478, Tokens/sec: 97368.85\n",
      "Step: 325, Training Loss: 2.67943, LR: 0.0000475, Tokens/sec: 84966.38\n",
      "Computing Eval loss, steps: 2\n",
      "Step: 325, Eval Loss: 5.01892\n",
      "Step: 326, Training Loss: 2.53638, LR: 0.0000472, Tokens/sec: 96799.28\n",
      "Step: 327, Training Loss: 2.56976, LR: 0.0000470, Tokens/sec: 88810.34\n",
      "Step: 328, Training Loss: 2.56196, LR: 0.0000467, Tokens/sec: 86180.69\n",
      "Step: 329, Training Loss: 2.50800, LR: 0.0000464, Tokens/sec: 97128.36\n",
      "Step: 330, Training Loss: 2.54693, LR: 0.0000461, Tokens/sec: 96670.34\n",
      "Step: 331, Training Loss: 2.63830, LR: 0.0000458, Tokens/sec: 97419.05\n",
      "Step: 332, Training Loss: 2.54713, LR: 0.0000455, Tokens/sec: 97370.26\n",
      "Step: 333, Training Loss: 2.52128, LR: 0.0000453, Tokens/sec: 93856.40\n",
      "Step: 334, Training Loss: 2.45109, LR: 0.0000450, Tokens/sec: 97195.26\n",
      "Step: 335, Training Loss: 2.55793, LR: 0.0000447, Tokens/sec: 97386.57\n",
      "Step: 336, Training Loss: 2.46218, LR: 0.0000444, Tokens/sec: 95356.13\n",
      "Step: 337, Training Loss: 2.44243, LR: 0.0000441, Tokens/sec: 90815.14\n",
      "Step: 338, Training Loss: 2.35680, LR: 0.0000439, Tokens/sec: 86104.01\n",
      "Step: 339, Training Loss: 2.39642, LR: 0.0000436, Tokens/sec: 88676.99\n",
      "Step: 340, Training Loss: 2.41002, LR: 0.0000433, Tokens/sec: 96118.16\n",
      "Step: 341, Training Loss: 2.40740, LR: 0.0000430, Tokens/sec: 96609.57\n",
      "Step: 342, Training Loss: 2.53262, LR: 0.0000428, Tokens/sec: 97098.30\n",
      "Step: 343, Training Loss: 2.38306, LR: 0.0000425, Tokens/sec: 96685.88\n",
      "Step: 344, Training Loss: 2.41321, LR: 0.0000422, Tokens/sec: 97553.72\n",
      "Step: 345, Training Loss: 2.41745, LR: 0.0000419, Tokens/sec: 97362.98\n",
      "Step: 346, Training Loss: 2.36933, LR: 0.0000416, Tokens/sec: 95185.16\n",
      "Step: 347, Training Loss: 2.40817, LR: 0.0000414, Tokens/sec: 95796.33\n",
      "Step: 348, Training Loss: 2.50442, LR: 0.0000411, Tokens/sec: 97083.86\n",
      "Step: 349, Training Loss: 2.42383, LR: 0.0000408, Tokens/sec: 96670.59\n",
      "Step: 350, Training Loss: 2.40669, LR: 0.0000405, Tokens/sec: 93694.05\n",
      "Computing Eval loss, steps: 2\n",
      "Step: 350, Eval Loss: 5.05466\n",
      "Step: 351, Training Loss: 2.33720, LR: 0.0000403, Tokens/sec: 71936.85\n",
      "Step: 352, Training Loss: 2.42567, LR: 0.0000400, Tokens/sec: 92073.13\n",
      "Step: 353, Training Loss: 2.32764, LR: 0.0000397, Tokens/sec: 93765.54\n",
      "Step: 354, Training Loss: 2.31090, LR: 0.0000395, Tokens/sec: 96851.94\n",
      "Step: 355, Training Loss: 2.22277, LR: 0.0000392, Tokens/sec: 91551.15\n",
      "Step: 356, Training Loss: 2.26964, LR: 0.0000389, Tokens/sec: 81195.96\n",
      "Step: 357, Training Loss: 2.28315, LR: 0.0000386, Tokens/sec: 94741.35\n",
      "Step: 358, Training Loss: 2.28914, LR: 0.0000384, Tokens/sec: 97420.60\n",
      "Step: 359, Training Loss: 2.41561, LR: 0.0000381, Tokens/sec: 95909.03\n",
      "Step: 360, Training Loss: 2.27011, LR: 0.0000378, Tokens/sec: 94359.84\n",
      "Step: 361, Training Loss: 2.29910, LR: 0.0000376, Tokens/sec: 90404.59\n",
      "Step: 362, Training Loss: 2.29841, LR: 0.0000373, Tokens/sec: 67138.31\n",
      "Step: 363, Training Loss: 2.24899, LR: 0.0000370, Tokens/sec: 67873.69\n",
      "Step: 364, Training Loss: 2.28996, LR: 0.0000368, Tokens/sec: 67319.28\n",
      "Step: 365, Training Loss: 2.38115, LR: 0.0000365, Tokens/sec: 96537.49\n",
      "Step: 366, Training Loss: 2.28706, LR: 0.0000363, Tokens/sec: 97369.66\n",
      "Step: 367, Training Loss: 2.27589, LR: 0.0000360, Tokens/sec: 95093.28\n",
      "Step: 368, Training Loss: 2.22267, LR: 0.0000357, Tokens/sec: 96320.74\n",
      "Step: 369, Training Loss: 2.31503, LR: 0.0000355, Tokens/sec: 97562.34\n",
      "Step: 370, Training Loss: 2.21938, LR: 0.0000352, Tokens/sec: 94632.25\n",
      "Step: 371, Training Loss: 2.20309, LR: 0.0000350, Tokens/sec: 96520.23\n",
      "Step: 372, Training Loss: 2.11676, LR: 0.0000347, Tokens/sec: 96692.11\n",
      "Step: 373, Training Loss: 2.17788, LR: 0.0000344, Tokens/sec: 95503.01\n",
      "Step: 374, Training Loss: 2.18621, LR: 0.0000342, Tokens/sec: 96530.76\n",
      "Step: 375, Training Loss: 2.17685, LR: 0.0000339, Tokens/sec: 96766.43\n",
      "Computing Eval loss, steps: 2\n",
      "Step: 375, Eval Loss: 5.06274\n",
      "Step: 376, Training Loss: 2.29756, LR: 0.0000337, Tokens/sec: 96818.85\n",
      "Step: 377, Training Loss: 2.15937, LR: 0.0000334, Tokens/sec: 97141.42\n",
      "Step: 378, Training Loss: 2.19566, LR: 0.0000332, Tokens/sec: 95270.59\n",
      "Step: 379, Training Loss: 2.19629, LR: 0.0000329, Tokens/sec: 97147.20\n",
      "Step: 380, Training Loss: 2.14529, LR: 0.0000327, Tokens/sec: 97605.00\n",
      "Step: 381, Training Loss: 2.18400, LR: 0.0000324, Tokens/sec: 93129.95\n",
      "Step: 382, Training Loss: 2.28663, LR: 0.0000322, Tokens/sec: 97427.49\n",
      "Step: 383, Training Loss: 2.20058, LR: 0.0000319, Tokens/sec: 84345.74\n",
      "Step: 384, Training Loss: 2.19346, LR: 0.0000317, Tokens/sec: 96628.69\n",
      "Step: 385, Training Loss: 2.12085, LR: 0.0000314, Tokens/sec: 96624.46\n",
      "Step: 386, Training Loss: 2.19898, LR: 0.0000312, Tokens/sec: 94735.06\n",
      "Step: 387, Training Loss: 2.11022, LR: 0.0000309, Tokens/sec: 88279.09\n",
      "Step: 388, Training Loss: 2.10616, LR: 0.0000307, Tokens/sec: 87822.05\n",
      "Step: 389, Training Loss: 2.01580, LR: 0.0000305, Tokens/sec: 90413.90\n",
      "Step: 390, Training Loss: 2.06927, LR: 0.0000302, Tokens/sec: 82161.05\n",
      "Step: 391, Training Loss: 2.07989, LR: 0.0000300, Tokens/sec: 86524.13\n",
      "Step: 392, Training Loss: 2.07898, LR: 0.0000297, Tokens/sec: 96499.70\n",
      "Step: 393, Training Loss: 2.20754, LR: 0.0000295, Tokens/sec: 96159.32\n",
      "Step: 394, Training Loss: 2.07308, LR: 0.0000293, Tokens/sec: 95775.90\n",
      "Step: 395, Training Loss: 2.09374, LR: 0.0000290, Tokens/sec: 96509.94\n",
      "Step: 396, Training Loss: 2.09649, LR: 0.0000288, Tokens/sec: 95669.52\n",
      "Step: 397, Training Loss: 2.04352, LR: 0.0000285, Tokens/sec: 92257.27\n",
      "Step: 398, Training Loss: 2.07988, LR: 0.0000283, Tokens/sec: 80421.52\n",
      "Step: 399, Training Loss: 2.18007, LR: 0.0000281, Tokens/sec: 95121.20\n",
      "Step: 400, Training Loss: 2.08967, LR: 0.0000279, Tokens/sec: 93473.33\n",
      "Computing Eval loss, steps: 2\n",
      "Step: 400, Eval Loss: 5.09826\n",
      "Step: 401, Training Loss: 2.08957, LR: 0.0000276, Tokens/sec: 96233.92\n",
      "Step: 402, Training Loss: 2.03068, LR: 0.0000274, Tokens/sec: 86968.73\n",
      "Step: 403, Training Loss: 2.11029, LR: 0.0000272, Tokens/sec: 88907.53\n",
      "Step: 404, Training Loss: 2.01590, LR: 0.0000269, Tokens/sec: 96157.48\n",
      "Step: 405, Training Loss: 2.00463, LR: 0.0000267, Tokens/sec: 85725.54\n",
      "Step: 406, Training Loss: 1.91257, LR: 0.0000265, Tokens/sec: 96820.79\n",
      "Step: 407, Training Loss: 1.97735, LR: 0.0000263, Tokens/sec: 95346.54\n",
      "Step: 408, Training Loss: 1.98694, LR: 0.0000261, Tokens/sec: 97355.10\n",
      "Step: 409, Training Loss: 1.98256, LR: 0.0000258, Tokens/sec: 96626.69\n",
      "Step: 410, Training Loss: 2.10164, LR: 0.0000256, Tokens/sec: 94854.77\n",
      "Step: 411, Training Loss: 1.97771, LR: 0.0000254, Tokens/sec: 82236.59\n",
      "Step: 412, Training Loss: 2.00812, LR: 0.0000252, Tokens/sec: 92700.30\n",
      "Step: 413, Training Loss: 2.01594, LR: 0.0000250, Tokens/sec: 95865.45\n",
      "Step: 414, Training Loss: 1.95987, LR: 0.0000247, Tokens/sec: 94853.46\n",
      "Step: 415, Training Loss: 1.99015, LR: 0.0000245, Tokens/sec: 84585.10\n",
      "Step: 416, Training Loss: 2.08555, LR: 0.0000243, Tokens/sec: 96700.59\n",
      "Step: 417, Training Loss: 1.99847, LR: 0.0000241, Tokens/sec: 97589.85\n",
      "Step: 418, Training Loss: 1.99993, LR: 0.0000239, Tokens/sec: 91699.34\n",
      "Step: 419, Training Loss: 1.94004, LR: 0.0000237, Tokens/sec: 85876.90\n",
      "Step: 420, Training Loss: 2.02279, LR: 0.0000235, Tokens/sec: 96577.59\n",
      "Step: 421, Training Loss: 1.93337, LR: 0.0000233, Tokens/sec: 84452.39\n",
      "Step: 422, Training Loss: 1.92373, LR: 0.0000231, Tokens/sec: 84187.57\n",
      "Step: 423, Training Loss: 1.83350, LR: 0.0000229, Tokens/sec: 86708.44\n",
      "Step: 424, Training Loss: 1.90400, LR: 0.0000227, Tokens/sec: 96441.64\n",
      "Step: 425, Training Loss: 1.91745, LR: 0.0000225, Tokens/sec: 94477.02\n",
      "Computing Eval loss, steps: 2\n",
      "Step: 425, Eval Loss: 5.10417\n",
      "Step: 426, Training Loss: 1.91275, LR: 0.0000223, Tokens/sec: 87216.34\n",
      "Step: 427, Training Loss: 2.02048, LR: 0.0000221, Tokens/sec: 78542.38\n",
      "Step: 428, Training Loss: 1.89542, LR: 0.0000219, Tokens/sec: 83547.86\n",
      "Step: 429, Training Loss: 1.92617, LR: 0.0000217, Tokens/sec: 83945.31\n",
      "Step: 430, Training Loss: 1.93563, LR: 0.0000215, Tokens/sec: 95201.53\n",
      "Step: 431, Training Loss: 1.88571, LR: 0.0000213, Tokens/sec: 86054.21\n",
      "Step: 432, Training Loss: 1.91536, LR: 0.0000211, Tokens/sec: 92732.84\n",
      "Step: 433, Training Loss: 2.00979, LR: 0.0000209, Tokens/sec: 85780.53\n",
      "Step: 434, Training Loss: 1.92698, LR: 0.0000207, Tokens/sec: 83071.04\n",
      "Step: 435, Training Loss: 1.93418, LR: 0.0000205, Tokens/sec: 88189.97\n",
      "Step: 436, Training Loss: 1.87314, LR: 0.0000204, Tokens/sec: 81765.97\n",
      "Step: 437, Training Loss: 1.95014, LR: 0.0000202, Tokens/sec: 94546.46\n",
      "Step: 438, Training Loss: 1.85858, LR: 0.0000200, Tokens/sec: 91389.47\n",
      "Step: 439, Training Loss: 1.84975, LR: 0.0000198, Tokens/sec: 73765.98\n",
      "Step: 440, Training Loss: 1.76352, LR: 0.0000196, Tokens/sec: 85564.30\n",
      "Step: 441, Training Loss: 1.83138, LR: 0.0000195, Tokens/sec: 87993.24\n",
      "Step: 442, Training Loss: 1.84275, LR: 0.0000193, Tokens/sec: 95547.45\n",
      "Step: 443, Training Loss: 1.84732, LR: 0.0000191, Tokens/sec: 90486.24\n",
      "Step: 444, Training Loss: 1.95774, LR: 0.0000189, Tokens/sec: 82109.94\n",
      "Step: 445, Training Loss: 1.83479, LR: 0.0000188, Tokens/sec: 90371.20\n",
      "Step: 446, Training Loss: 1.86316, LR: 0.0000186, Tokens/sec: 91358.54\n",
      "Step: 447, Training Loss: 1.86688, LR: 0.0000184, Tokens/sec: 94515.86\n",
      "Step: 448, Training Loss: 1.81813, LR: 0.0000183, Tokens/sec: 86087.38\n",
      "Step: 449, Training Loss: 1.84784, LR: 0.0000181, Tokens/sec: 92465.17\n",
      "Step: 450, Training Loss: 1.93833, LR: 0.0000179, Tokens/sec: 94821.62\n",
      "Computing Eval loss, steps: 2\n",
      "Step: 450, Eval Loss: 5.13160\n",
      "Step: 451, Training Loss: 1.84889, LR: 0.0000178, Tokens/sec: 83301.03\n",
      "Step: 452, Training Loss: 1.85879, LR: 0.0000176, Tokens/sec: 80073.30\n",
      "Step: 453, Training Loss: 1.80118, LR: 0.0000174, Tokens/sec: 80327.27\n",
      "Step: 454, Training Loss: 1.88751, LR: 0.0000173, Tokens/sec: 74974.70\n",
      "Step: 455, Training Loss: 1.80105, LR: 0.0000171, Tokens/sec: 86511.32\n",
      "Step: 456, Training Loss: 1.79006, LR: 0.0000170, Tokens/sec: 96192.61\n",
      "Step: 457, Training Loss: 1.70251, LR: 0.0000168, Tokens/sec: 97368.83\n",
      "Step: 458, Training Loss: 1.77441, LR: 0.0000167, Tokens/sec: 97296.20\n",
      "Step: 459, Training Loss: 1.78180, LR: 0.0000165, Tokens/sec: 94654.53\n",
      "Step: 460, Training Loss: 1.78238, LR: 0.0000164, Tokens/sec: 97281.20\n",
      "Step: 461, Training Loss: 1.88440, LR: 0.0000162, Tokens/sec: 97313.13\n",
      "Step: 462, Training Loss: 1.76502, LR: 0.0000161, Tokens/sec: 83195.15\n",
      "Step: 463, Training Loss: 1.79897, LR: 0.0000159, Tokens/sec: 96870.93\n",
      "Step: 464, Training Loss: 1.80506, LR: 0.0000158, Tokens/sec: 87988.51\n",
      "Step: 465, Training Loss: 1.75505, LR: 0.0000156, Tokens/sec: 82077.61\n",
      "Step: 466, Training Loss: 1.78158, LR: 0.0000155, Tokens/sec: 85684.76\n",
      "Step: 467, Training Loss: 1.87223, LR: 0.0000154, Tokens/sec: 82589.55\n",
      "Step: 468, Training Loss: 1.78826, LR: 0.0000152, Tokens/sec: 95638.11\n",
      "Step: 469, Training Loss: 1.80349, LR: 0.0000151, Tokens/sec: 82279.95\n",
      "Step: 470, Training Loss: 1.73970, LR: 0.0000150, Tokens/sec: 83218.73\n",
      "Step: 471, Training Loss: 1.81990, LR: 0.0000148, Tokens/sec: 87090.78\n",
      "Step: 472, Training Loss: 1.73730, LR: 0.0000147, Tokens/sec: 97284.03\n",
      "Step: 473, Training Loss: 1.73338, LR: 0.0000146, Tokens/sec: 94969.85\n",
      "Step: 474, Training Loss: 1.64571, LR: 0.0000145, Tokens/sec: 83752.49\n",
      "Step: 475, Training Loss: 1.71440, LR: 0.0000143, Tokens/sec: 93537.66\n",
      "Computing Eval loss, steps: 2\n",
      "Step: 475, Eval Loss: 5.12948\n",
      "Step: 476, Training Loss: 1.71924, LR: 0.0000142, Tokens/sec: 88194.04\n",
      "Step: 477, Training Loss: 1.72470, LR: 0.0000141, Tokens/sec: 96232.98\n",
      "Step: 478, Training Loss: 1.83161, LR: 0.0000140, Tokens/sec: 95620.86\n",
      "Step: 479, Training Loss: 1.71427, LR: 0.0000139, Tokens/sec: 93397.78\n",
      "Step: 480, Training Loss: 1.74284, LR: 0.0000137, Tokens/sec: 81844.49\n",
      "Step: 481, Training Loss: 1.74562, LR: 0.0000136, Tokens/sec: 97299.10\n",
      "Step: 482, Training Loss: 1.70093, LR: 0.0000135, Tokens/sec: 97341.54\n",
      "Step: 483, Training Loss: 1.73198, LR: 0.0000134, Tokens/sec: 96006.31\n",
      "Step: 484, Training Loss: 1.81996, LR: 0.0000133, Tokens/sec: 97442.57\n",
      "Step: 485, Training Loss: 1.72944, LR: 0.0000132, Tokens/sec: 96366.95\n",
      "Step: 486, Training Loss: 1.74494, LR: 0.0000131, Tokens/sec: 75897.62\n",
      "Step: 487, Training Loss: 1.69096, LR: 0.0000130, Tokens/sec: 78571.88\n",
      "Step: 488, Training Loss: 1.77446, LR: 0.0000129, Tokens/sec: 77989.13\n",
      "Step: 489, Training Loss: 1.69106, LR: 0.0000128, Tokens/sec: 97663.15\n",
      "Step: 490, Training Loss: 1.67989, LR: 0.0000127, Tokens/sec: 83916.33\n",
      "Step: 491, Training Loss: 1.59194, LR: 0.0000126, Tokens/sec: 84443.71\n",
      "Step: 492, Training Loss: 1.66331, LR: 0.0000125, Tokens/sec: 91207.56\n",
      "Step: 493, Training Loss: 1.66841, LR: 0.0000124, Tokens/sec: 85008.72\n",
      "Step: 494, Training Loss: 1.66956, LR: 0.0000123, Tokens/sec: 88252.47\n",
      "Step: 495, Training Loss: 1.77402, LR: 0.0000122, Tokens/sec: 89389.54\n",
      "Step: 496, Training Loss: 1.66114, LR: 0.0000121, Tokens/sec: 88257.64\n",
      "Step: 497, Training Loss: 1.69544, LR: 0.0000120, Tokens/sec: 86990.22\n",
      "Step: 498, Training Loss: 1.69783, LR: 0.0000119, Tokens/sec: 95645.14\n",
      "Step: 499, Training Loss: 1.65333, LR: 0.0000119, Tokens/sec: 73161.59\n",
      "Step: 500, Training Loss: 1.67871, LR: 0.0000118, Tokens/sec: 95930.81\n",
      "Computing Eval loss, steps: 2\n",
      "Step: 500, Eval Loss: 5.14233\n",
      "Step: 501, Training Loss: 1.76870, LR: 0.0000117, Tokens/sec: 97004.47\n",
      "Step: 502, Training Loss: 1.67830, LR: 0.0000116, Tokens/sec: 84476.13\n",
      "Step: 503, Training Loss: 1.69286, LR: 0.0000115, Tokens/sec: 83622.20\n",
      "Step: 504, Training Loss: 1.63805, LR: 0.0000115, Tokens/sec: 92322.78\n",
      "Step: 505, Training Loss: 1.72195, LR: 0.0000114, Tokens/sec: 95697.81\n",
      "Step: 506, Training Loss: 1.64430, LR: 0.0000113, Tokens/sec: 84015.28\n",
      "Step: 507, Training Loss: 1.63656, LR: 0.0000113, Tokens/sec: 83898.90\n",
      "Step: 508, Training Loss: 1.54938, LR: 0.0000112, Tokens/sec: 86162.49\n",
      "Step: 509, Training Loss: 1.61978, LR: 0.0000111, Tokens/sec: 95579.70\n",
      "Step: 510, Training Loss: 1.62337, LR: 0.0000111, Tokens/sec: 94319.59\n",
      "Step: 511, Training Loss: 1.62308, LR: 0.0000110, Tokens/sec: 95877.53\n",
      "Step: 512, Training Loss: 1.72380, LR: 0.0000109, Tokens/sec: 97518.08\n",
      "Step: 513, Training Loss: 1.61234, LR: 0.0000109, Tokens/sec: 82600.01\n",
      "Step: 514, Training Loss: 1.64930, LR: 0.0000108, Tokens/sec: 85081.29\n",
      "Step: 515, Training Loss: 1.65270, LR: 0.0000108, Tokens/sec: 87157.08\n",
      "Step: 516, Training Loss: 1.61282, LR: 0.0000107, Tokens/sec: 80123.19\n",
      "Step: 517, Training Loss: 1.63825, LR: 0.0000107, Tokens/sec: 96509.29\n",
      "Step: 518, Training Loss: 1.72849, LR: 0.0000106, Tokens/sec: 84159.54\n",
      "Step: 519, Training Loss: 1.63645, LR: 0.0000106, Tokens/sec: 86258.07\n",
      "Step: 520, Training Loss: 1.65046, LR: 0.0000105, Tokens/sec: 97191.40\n",
      "Step: 521, Training Loss: 1.59425, LR: 0.0000105, Tokens/sec: 94699.43\n",
      "Step: 522, Training Loss: 1.67675, LR: 0.0000104, Tokens/sec: 85008.67\n",
      "Step: 523, Training Loss: 1.60390, LR: 0.0000104, Tokens/sec: 88523.06\n",
      "Step: 524, Training Loss: 1.59685, LR: 0.0000104, Tokens/sec: 84741.63\n",
      "Step: 525, Training Loss: 1.51186, LR: 0.0000103, Tokens/sec: 94752.99\n",
      "Computing Eval loss, steps: 2\n",
      "Step: 525, Eval Loss: 5.15222\n",
      "Step: 526, Training Loss: 1.58177, LR: 0.0000103, Tokens/sec: 84657.33\n",
      "Step: 527, Training Loss: 1.58482, LR: 0.0000103, Tokens/sec: 96326.23\n",
      "Step: 528, Training Loss: 1.58371, LR: 0.0000102, Tokens/sec: 86677.60\n",
      "Step: 529, Training Loss: 1.68111, LR: 0.0000102, Tokens/sec: 90960.22\n",
      "Step: 530, Training Loss: 1.56947, LR: 0.0000102, Tokens/sec: 95330.56\n",
      "Step: 531, Training Loss: 1.60748, LR: 0.0000102, Tokens/sec: 83601.54\n",
      "Step: 532, Training Loss: 1.61139, LR: 0.0000101, Tokens/sec: 95400.03\n",
      "Step: 533, Training Loss: 1.57302, LR: 0.0000101, Tokens/sec: 86307.55\n",
      "Step: 534, Training Loss: 1.59952, LR: 0.0000101, Tokens/sec: 86870.62\n",
      "Step: 535, Training Loss: 1.69189, LR: 0.0000101, Tokens/sec: 91818.05\n",
      "Step: 536, Training Loss: 1.59850, LR: 0.0000101, Tokens/sec: 91675.58\n",
      "Step: 537, Training Loss: 1.61270, LR: 0.0000100, Tokens/sec: 95751.74\n",
      "Step: 538, Training Loss: 1.55447, LR: 0.0000100, Tokens/sec: 96705.65\n",
      "Step: 539, Training Loss: 1.63390, LR: 0.0000100, Tokens/sec: 88465.63\n",
      "Step: 540, Training Loss: 1.56468, LR: 0.0000100, Tokens/sec: 87653.71\n",
      "Step: 541, Training Loss: 1.55820, LR: 0.0000100, Tokens/sec: 96743.95\n",
      "Step: 542, Training Loss: 1.47579, LR: 0.0000100, Tokens/sec: 97041.29\n",
      "Step: 543, Training Loss: 1.54559, LR: 0.0000100, Tokens/sec: 97595.64\n"
     ]
    }
   ],
   "source": [
    "trainer.train(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b5596eda083de0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:18:18.388416Z",
     "start_time": "2024-12-16T09:18:17.485673Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All:\n",
      "Content, content.\n",
      "\n",
      "MENENIUS:\n",
      "O sir, you are not right: have you not known\n",
      "The worthiest men have done't?\n",
      "\n",
      "CORIOLANUS:\n",
      "I am, no more.\n",
      "\n",
      "MENENIUS:\n",
      "A thousandly he dost!\n",
      "\n",
      "CORIOLANUS:\n",
      "I am, I have more by the people,\n",
      "To see my good my master.\n",
      "\n",
      "MENENIUS:\n",
      "I am it more!\n"
     ]
    }
   ],
   "source": [
    "input_text = \"\"\"\n",
    "All:\n",
    "Content, content.\n",
    "\n",
    "MENENIUS:\n",
    "O sir, you are not right: have you not known\n",
    "The worthiest men have done't?\n",
    "\n",
    "CORIOLANUS:\n",
    "\"\"\".strip()\n",
    "\n",
    "input_ids = tokenizer([input_text], return_tensors=\"pt\")['input_ids'].to(trainer.device)\n",
    "idx = model.generate(input_ids, temperature=0.01, top_k=5, max_new_tokens=64)\n",
    "print(tokenizer.batch_decode(idx)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a94862-9944-4f3c-9c4c-d5d9b6e62e55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
