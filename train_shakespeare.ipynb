{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-16T07:25:27.119619Z",
     "start_time": "2024-12-16T07:25:26.194556Z"
    }
   },
   "source": [
    "from model import ModelConfig, LlamaModel\n",
    "from train import TrainerConfig, DataLoader, Trainer\n",
    "\n",
    "from transformers import AutoTokenizer"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T07:25:27.124115Z",
     "start_time": "2024-12-16T07:25:27.122407Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer_id = \"HuggingFaceTB/SmolLM2-135M\"",
   "id": "2f28fa23c987e72b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T07:25:27.320524Z",
     "start_time": "2024-12-16T07:25:27.168484Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ],
   "id": "9bb4e51aa142abee",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T07:25:27.327312Z",
     "start_time": "2024-12-16T07:25:27.325672Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_config = ModelConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=576,\n",
    "    d_head=64,\n",
    "    d_mlp_proj=1536,\n",
    "    n_layers=30,\n",
    "    n_kv_heads=3,\n",
    "    n_attn_heads=9,\n",
    "    rms_norm_eps=1e-5,\n",
    "    initializer_range=0.041666666666666664,\n",
    "    rope_theta=100000.0,\n",
    "    padding_idx=tokenizer.pad_token_id\n",
    ")"
   ],
   "id": "cde027092af8291e",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T07:25:27.370782Z",
     "start_time": "2024-12-16T07:25:27.368804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_config = TrainerConfig(\n",
    "    per_device_train_batch_size=8,\n",
    "    max_seq_len=2048,\n",
    "    num_epochs=50,\n",
    "    learning_rate=1e-3,\n",
    "    log_dir=\"runs/shakespeare\"\n",
    ")"
   ],
   "id": "809773e662327a12",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T07:25:27.415288Z",
     "start_time": "2024-12-16T07:25:27.412679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"data/tiny_shakespeare.txt\") as f:\n",
    "    text = f.read()"
   ],
   "id": "374f398bb34f7ac1",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T07:25:30.021750Z",
     "start_time": "2024-12-16T07:25:27.460319Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = LlamaModel(model_config)\n",
    "dataloader = DataLoader(train_config, tokenizer, text=text)\n",
    "trainer = Trainer(train_config, model)"
   ],
   "id": "9a912a0ec92039d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train tokens             | 342,016\n",
      "Num Trainable Params           | 162,826,560\n",
      "Train device                   | cuda, NVIDIA GeForce RTX 3090, N=1\n",
      "Training precision             | torch.bfloat16\n",
      "Flash Attention                | True\n",
      "torch.compile()                | True\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T07:32:41.239199Z",
     "start_time": "2024-12-16T07:25:30.029554Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train(dataloader)",
   "id": "ee8c2059258a0195",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training steps                 | 950 \n",
      "Step: 0, Training Loss: 11.31190, Tokens/sec: 1490.120841474059\n",
      "Step: 1, Training Loss: 9.69302, Tokens/sec: 1605.889267044552\n",
      "Step: 2, Training Loss: 12.86033, Tokens/sec: 87127.19376181753\n",
      "Step: 3, Training Loss: 10.63981, Tokens/sec: 88562.79837798575\n",
      "Step: 4, Training Loss: 9.64350, Tokens/sec: 88759.39753710364\n",
      "Step: 5, Training Loss: 8.81989, Tokens/sec: 87127.25399751143\n",
      "Step: 6, Training Loss: 8.48675, Tokens/sec: 87472.40680398425\n",
      "Step: 7, Training Loss: 7.74859, Tokens/sec: 87093.91736807216\n",
      "Step: 8, Training Loss: 7.59799, Tokens/sec: 86772.18886528346\n",
      "Step: 9, Training Loss: 7.34216, Tokens/sec: 88012.3603651888\n",
      "Step: 10, Training Loss: 7.11566, Tokens/sec: 87378.20169779556\n",
      "Step: 11, Training Loss: 6.82298, Tokens/sec: 84790.50715805775\n",
      "Step: 12, Training Loss: 6.68290, Tokens/sec: 86732.86018722753\n",
      "Step: 13, Training Loss: 6.63552, Tokens/sec: 88123.0075860156\n",
      "Step: 14, Training Loss: 6.62263, Tokens/sec: 88060.90426346315\n",
      "Step: 15, Training Loss: 6.52662, Tokens/sec: 87218.97300406347\n",
      "Step: 16, Training Loss: 6.49206, Tokens/sec: 84492.67870710506\n",
      "Step: 17, Training Loss: 6.61108, Tokens/sec: 87360.46466460542\n",
      "Step: 18, Training Loss: 6.63953, Tokens/sec: 87326.66078859124\n",
      "Step: 19, Training Loss: 6.63248, Tokens/sec: 84473.39766486386\n",
      "Step: 20, Training Loss: 6.39430, Tokens/sec: 87250.71924110326\n",
      "Step: 21, Training Loss: 6.54488, Tokens/sec: 74915.2961959348\n",
      "Step: 22, Training Loss: 6.63745, Tokens/sec: 74458.89247437347\n",
      "Step: 23, Training Loss: 6.39822, Tokens/sec: 74417.23332761988\n",
      "Step: 24, Training Loss: 6.48351, Tokens/sec: 73548.95408074607\n",
      "Step: 25, Training Loss: 6.41250, Tokens/sec: 62134.17457116331\n",
      "Step: 26, Training Loss: 6.36517, Tokens/sec: 74685.75004349109\n",
      "Step: 27, Training Loss: 6.34765, Tokens/sec: 74131.11089528551\n",
      "Step: 28, Training Loss: 6.40377, Tokens/sec: 74417.9431494152\n",
      "Step: 29, Training Loss: 6.39587, Tokens/sec: 73703.28242341559\n",
      "Step: 30, Training Loss: 6.31495, Tokens/sec: 74284.8527770079\n",
      "Step: 31, Training Loss: 6.34644, Tokens/sec: 73263.3308404254\n",
      "Step: 32, Training Loss: 6.34250, Tokens/sec: 74326.68412252153\n",
      "Step: 33, Training Loss: 6.37455, Tokens/sec: 74443.2382751785\n",
      "Step: 34, Training Loss: 6.35056, Tokens/sec: 75066.70141965365\n",
      "Step: 35, Training Loss: 6.32313, Tokens/sec: 73157.28480338244\n",
      "Step: 36, Training Loss: 6.39141, Tokens/sec: 74112.47361346279\n",
      "Step: 37, Training Loss: 6.39172, Tokens/sec: 74243.14956795757\n",
      "Step: 38, Training Loss: 6.43536, Tokens/sec: 74663.36898700068\n",
      "Step: 39, Training Loss: 6.22230, Tokens/sec: 75088.85349953336\n",
      "Step: 40, Training Loss: 6.33574, Tokens/sec: 73340.94021728224\n",
      "Step: 41, Training Loss: 6.32545, Tokens/sec: 72997.3450819714\n",
      "Step: 42, Training Loss: 6.20526, Tokens/sec: 74599.6236075861\n",
      "Step: 43, Training Loss: 6.27892, Tokens/sec: 75018.5128679574\n",
      "Step: 44, Training Loss: 6.22645, Tokens/sec: 74381.19546504537\n",
      "Step: 45, Training Loss: 6.21010, Tokens/sec: 73226.92230368018\n",
      "Step: 46, Training Loss: 6.20971, Tokens/sec: 73660.08643233085\n",
      "Step: 47, Training Loss: 6.26837, Tokens/sec: 72746.83764792651\n",
      "Step: 48, Training Loss: 6.25518, Tokens/sec: 74789.51628479519\n",
      "Step: 49, Training Loss: 6.18736, Tokens/sec: 74792.57534220007\n",
      "Step: 50, Training Loss: 6.22329, Tokens/sec: 74384.11280360796\n",
      "Step: 51, Training Loss: 6.18967, Tokens/sec: 73311.54996398001\n",
      "Step: 52, Training Loss: 6.25581, Tokens/sec: 73985.6972499207\n",
      "Step: 53, Training Loss: 6.21655, Tokens/sec: 73717.4650351091\n",
      "Step: 54, Training Loss: 6.16101, Tokens/sec: 71241.23179888773\n",
      "Step: 55, Training Loss: 6.24004, Tokens/sec: 74015.92888382144\n",
      "Step: 56, Training Loss: 6.23757, Tokens/sec: 72904.9049708655\n",
      "Step: 57, Training Loss: 6.27673, Tokens/sec: 73649.81876525472\n",
      "Step: 58, Training Loss: 6.08645, Tokens/sec: 72458.30920744353\n",
      "Step: 59, Training Loss: 6.20293, Tokens/sec: 74673.25687268993\n",
      "Step: 60, Training Loss: 6.21688, Tokens/sec: 74758.56085288103\n",
      "Step: 61, Training Loss: 6.07625, Tokens/sec: 74297.89284947974\n",
      "Step: 62, Training Loss: 6.15099, Tokens/sec: 74398.49178134747\n",
      "Step: 63, Training Loss: 6.10634, Tokens/sec: 74361.52400819288\n",
      "Step: 64, Training Loss: 6.09321, Tokens/sec: 74687.31206893093\n",
      "Step: 65, Training Loss: 6.08492, Tokens/sec: 74956.96219683369\n",
      "Step: 66, Training Loss: 6.13879, Tokens/sec: 74977.78341417557\n",
      "Step: 67, Training Loss: 6.12543, Tokens/sec: 73768.5657408715\n",
      "Step: 68, Training Loss: 6.05409, Tokens/sec: 74144.91205947964\n",
      "Step: 69, Training Loss: 6.08149, Tokens/sec: 74677.50793556511\n",
      "Step: 70, Training Loss: 6.04075, Tokens/sec: 74131.90885292164\n",
      "Step: 71, Training Loss: 6.09757, Tokens/sec: 74181.62407939965\n",
      "Step: 72, Training Loss: 6.06232, Tokens/sec: 74139.3354878743\n",
      "Step: 73, Training Loss: 5.98506, Tokens/sec: 74152.12315274755\n",
      "Step: 74, Training Loss: 6.06049, Tokens/sec: 73759.07436524352\n",
      "Step: 75, Training Loss: 6.05568, Tokens/sec: 74430.038602078\n",
      "Step: 76, Training Loss: 6.10226, Tokens/sec: 74648.4242163329\n",
      "Step: 77, Training Loss: 5.88303, Tokens/sec: 73171.4116330477\n",
      "Step: 78, Training Loss: 6.01218, Tokens/sec: 73366.6516317555\n",
      "Step: 79, Training Loss: 5.99172, Tokens/sec: 73456.40680070582\n",
      "Step: 80, Training Loss: 5.84933, Tokens/sec: 73514.48467804359\n",
      "Step: 81, Training Loss: 5.91706, Tokens/sec: 73828.36870881294\n",
      "Step: 82, Training Loss: 5.85156, Tokens/sec: 74024.75368833248\n",
      "Step: 83, Training Loss: 5.82854, Tokens/sec: 73678.0959489023\n",
      "Step: 84, Training Loss: 5.83630, Tokens/sec: 73216.69945284212\n",
      "Step: 85, Training Loss: 5.87960, Tokens/sec: 72606.01346549948\n",
      "Step: 86, Training Loss: 5.87151, Tokens/sec: 74112.93859956923\n",
      "Step: 87, Training Loss: 5.76663, Tokens/sec: 73836.0108243875\n",
      "Step: 88, Training Loss: 5.80732, Tokens/sec: 74202.19483916921\n",
      "Step: 89, Training Loss: 5.76169, Tokens/sec: 73500.73220915346\n",
      "Step: 90, Training Loss: 5.78805, Tokens/sec: 74275.0795958295\n",
      "Step: 91, Training Loss: 5.75251, Tokens/sec: 73490.9466839033\n",
      "Step: 92, Training Loss: 5.68973, Tokens/sec: 74661.35715527576\n",
      "Step: 93, Training Loss: 5.75943, Tokens/sec: 74549.12555720125\n",
      "Step: 94, Training Loss: 5.74961, Tokens/sec: 73589.10707294532\n",
      "Step: 95, Training Loss: 5.80531, Tokens/sec: 73241.38053097874\n",
      "Step: 96, Training Loss: 5.56434, Tokens/sec: 73754.52183134762\n",
      "Step: 97, Training Loss: 5.70632, Tokens/sec: 73724.60616002319\n",
      "Step: 98, Training Loss: 5.64640, Tokens/sec: 74360.18718521165\n",
      "Step: 99, Training Loss: 5.55119, Tokens/sec: 74723.78969760041\n",
      "Step: 100, Training Loss: 5.62734, Tokens/sec: 73775.01548987954\n",
      "Step: 101, Training Loss: 5.57495, Tokens/sec: 73132.69822561865\n",
      "Step: 102, Training Loss: 5.55627, Tokens/sec: 74360.622551803\n",
      "Step: 103, Training Loss: 5.53376, Tokens/sec: 74165.31776168144\n",
      "Step: 104, Training Loss: 5.59148, Tokens/sec: 74442.6693532864\n",
      "Step: 105, Training Loss: 5.56662, Tokens/sec: 72908.54437782711\n",
      "Step: 106, Training Loss: 5.48624, Tokens/sec: 73554.26948762593\n",
      "Step: 107, Training Loss: 5.52629, Tokens/sec: 69323.19373275807\n",
      "Step: 108, Training Loss: 5.45596, Tokens/sec: 75027.6395379799\n",
      "Step: 109, Training Loss: 5.50495, Tokens/sec: 74536.28164798721\n",
      "Step: 110, Training Loss: 5.46403, Tokens/sec: 73729.12846122804\n",
      "Step: 111, Training Loss: 5.40275, Tokens/sec: 72531.01978449021\n",
      "Step: 112, Training Loss: 5.46511, Tokens/sec: 72875.95254616292\n",
      "Step: 113, Training Loss: 5.46464, Tokens/sec: 73962.37778893496\n",
      "Step: 114, Training Loss: 5.51751, Tokens/sec: 74680.0029780817\n",
      "Step: 115, Training Loss: 5.28171, Tokens/sec: 73388.5308179919\n",
      "Step: 116, Training Loss: 5.40605, Tokens/sec: 74683.71658030796\n",
      "Step: 117, Training Loss: 5.23770, Tokens/sec: 74271.87013895049\n",
      "Step: 118, Training Loss: 5.25578, Tokens/sec: 72672.7965179838\n",
      "Step: 119, Training Loss: 5.35629, Tokens/sec: 74327.26611107102\n",
      "Step: 120, Training Loss: 5.29927, Tokens/sec: 73453.29733932123\n",
      "Step: 121, Training Loss: 5.26887, Tokens/sec: 72770.10333299258\n",
      "Step: 122, Training Loss: 5.26540, Tokens/sec: 72508.70089945135\n",
      "Step: 123, Training Loss: 5.28685, Tokens/sec: 74026.93371389207\n",
      "Step: 124, Training Loss: 5.27549, Tokens/sec: 74178.26854369698\n",
      "Step: 125, Training Loss: 5.23319, Tokens/sec: 73536.20427060338\n",
      "Step: 126, Training Loss: 5.27624, Tokens/sec: 73930.5318417856\n",
      "Step: 127, Training Loss: 5.16434, Tokens/sec: 74721.20653147345\n",
      "Step: 128, Training Loss: 5.22038, Tokens/sec: 73745.13298457551\n",
      "Step: 129, Training Loss: 5.19714, Tokens/sec: 73048.25867113816\n",
      "Step: 130, Training Loss: 5.12090, Tokens/sec: 74572.12717396\n",
      "Step: 131, Training Loss: 5.18271, Tokens/sec: 74858.20209985338\n",
      "Step: 132, Training Loss: 5.20251, Tokens/sec: 73731.18194370551\n",
      "Step: 133, Training Loss: 5.23325, Tokens/sec: 74614.39423203251\n",
      "Step: 134, Training Loss: 5.01043, Tokens/sec: 73436.9113216694\n",
      "Step: 135, Training Loss: 5.16622, Tokens/sec: 72983.97591443677\n",
      "Step: 136, Training Loss: 5.47459, Tokens/sec: 74237.26186147\n",
      "Step: 137, Training Loss: 5.05421, Tokens/sec: 74014.95821410207\n",
      "Step: 138, Training Loss: 5.10173, Tokens/sec: 73503.41205208223\n",
      "Step: 139, Training Loss: 5.06332, Tokens/sec: 73631.21320432615\n",
      "Step: 140, Training Loss: 4.98906, Tokens/sec: 73796.54462863565\n",
      "Step: 141, Training Loss: 5.00450, Tokens/sec: 74003.85933941885\n",
      "Step: 142, Training Loss: 5.04033, Tokens/sec: 74516.98223335322\n",
      "Step: 143, Training Loss: 5.03280, Tokens/sec: 73436.21712869787\n",
      "Step: 144, Training Loss: 5.00910, Tokens/sec: 73908.03061367679\n",
      "Step: 145, Training Loss: 5.00398, Tokens/sec: 73448.00869749616\n",
      "Step: 146, Training Loss: 4.91133, Tokens/sec: 74173.590240854\n",
      "Step: 147, Training Loss: 5.00637, Tokens/sec: 73608.17251947033\n",
      "Step: 148, Training Loss: 4.96507, Tokens/sec: 73797.86059717814\n",
      "Step: 149, Training Loss: 4.92919, Tokens/sec: 72222.19440243545\n",
      "Step: 150, Training Loss: 4.92710, Tokens/sec: 72781.74046433617\n",
      "Step: 151, Training Loss: 4.98239, Tokens/sec: 73092.2746772486\n",
      "Step: 152, Training Loss: 4.99384, Tokens/sec: 72803.11102501073\n",
      "Step: 153, Training Loss: 4.77729, Tokens/sec: 73313.55597403186\n",
      "Step: 154, Training Loss: 4.96365, Tokens/sec: 74158.52266928679\n",
      "Step: 155, Training Loss: 4.98750, Tokens/sec: 73992.0379986297\n",
      "Step: 156, Training Loss: 4.77893, Tokens/sec: 64106.86664695598\n",
      "Step: 157, Training Loss: 4.85357, Tokens/sec: 72411.00269571702\n",
      "Step: 158, Training Loss: 4.78311, Tokens/sec: 71735.46527915997\n",
      "Step: 159, Training Loss: 4.77266, Tokens/sec: 71010.96315431848\n",
      "Step: 160, Training Loss: 4.75302, Tokens/sec: 68687.00307923021\n",
      "Step: 161, Training Loss: 4.77471, Tokens/sec: 73212.44131933007\n",
      "Step: 162, Training Loss: 4.81454, Tokens/sec: 73967.02147007671\n",
      "Step: 163, Training Loss: 4.76926, Tokens/sec: 73895.70960079368\n",
      "Step: 164, Training Loss: 4.76997, Tokens/sec: 62608.38173357723\n",
      "Step: 165, Training Loss: 4.66256, Tokens/sec: 73116.83962621725\n",
      "Step: 166, Training Loss: 4.76152, Tokens/sec: 74622.60779844776\n",
      "Step: 167, Training Loss: 4.74746, Tokens/sec: 75476.04057173716\n",
      "Step: 168, Training Loss: 4.68944, Tokens/sec: 74443.46016425976\n",
      "Step: 169, Training Loss: 4.70642, Tokens/sec: 70581.28430657662\n",
      "Step: 170, Training Loss: 4.77761, Tokens/sec: 73962.43354765722\n",
      "Step: 171, Training Loss: 4.80899, Tokens/sec: 65578.577022566\n",
      "Step: 172, Training Loss: 4.55145, Tokens/sec: 73809.00117333131\n",
      "Step: 173, Training Loss: 4.77346, Tokens/sec: 74070.70716283405\n",
      "Step: 174, Training Loss: 4.70164, Tokens/sec: 74588.07875457707\n",
      "Step: 175, Training Loss: 4.60489, Tokens/sec: 72344.41828182292\n",
      "Step: 176, Training Loss: 4.70779, Tokens/sec: 73182.10137189427\n",
      "Step: 177, Training Loss: 4.62985, Tokens/sec: 73698.27464782624\n",
      "Step: 178, Training Loss: 4.60699, Tokens/sec: 74183.68235530329\n",
      "Step: 179, Training Loss: 4.61149, Tokens/sec: 73550.56070876918\n",
      "Step: 180, Training Loss: 4.60881, Tokens/sec: 74370.9533251639\n",
      "Step: 181, Training Loss: 4.68594, Tokens/sec: 73925.05051152727\n",
      "Step: 182, Training Loss: 4.64239, Tokens/sec: 73197.38919822017\n",
      "Step: 183, Training Loss: 4.58010, Tokens/sec: 74813.59057305679\n",
      "Step: 184, Training Loss: 4.53865, Tokens/sec: 74317.65672687617\n",
      "Step: 185, Training Loss: 4.61147, Tokens/sec: 74497.47872134573\n",
      "Step: 186, Training Loss: 4.62297, Tokens/sec: 73407.45957466507\n",
      "Step: 187, Training Loss: 4.53819, Tokens/sec: 73693.70972868738\n",
      "Step: 188, Training Loss: 4.54240, Tokens/sec: 72789.11859816304\n",
      "Step: 189, Training Loss: 4.63143, Tokens/sec: 74272.58662203165\n",
      "Step: 190, Training Loss: 4.63316, Tokens/sec: 72118.79567955724\n",
      "Step: 191, Training Loss: 4.39775, Tokens/sec: 75405.757325762\n",
      "Step: 192, Training Loss: 4.64445, Tokens/sec: 74917.03089802955\n",
      "Step: 193, Training Loss: 4.40546, Tokens/sec: 73874.31176774215\n",
      "Step: 194, Training Loss: 4.43094, Tokens/sec: 74343.6828594288\n",
      "Step: 195, Training Loss: 4.53050, Tokens/sec: 73893.13971518587\n",
      "Step: 196, Training Loss: 4.47756, Tokens/sec: 74134.53361671172\n",
      "Step: 197, Training Loss: 4.42908, Tokens/sec: 74079.90376538102\n",
      "Step: 198, Training Loss: 4.41485, Tokens/sec: 74340.00636989347\n",
      "Step: 199, Training Loss: 4.45322, Tokens/sec: 72294.60390161455\n",
      "Step: 200, Training Loss: 4.47741, Tokens/sec: 74384.20263444129\n",
      "Step: 201, Training Loss: 4.44823, Tokens/sec: 73985.71896995204\n",
      "Step: 202, Training Loss: 4.40690, Tokens/sec: 74546.11894235476\n",
      "Step: 203, Training Loss: 4.31065, Tokens/sec: 73868.6166322098\n",
      "Step: 204, Training Loss: 4.38886, Tokens/sec: 74348.54458628499\n",
      "Step: 205, Training Loss: 4.38473, Tokens/sec: 72877.1862875683\n",
      "Step: 206, Training Loss: 4.31228, Tokens/sec: 74474.20795503812\n",
      "Step: 207, Training Loss: 4.36564, Tokens/sec: 65610.06878075864\n",
      "Step: 208, Training Loss: 4.39370, Tokens/sec: 74083.60147159186\n",
      "Step: 209, Training Loss: 4.40245, Tokens/sec: 74731.85590544026\n",
      "Step: 210, Training Loss: 4.20838, Tokens/sec: 74139.33749925751\n",
      "Step: 211, Training Loss: 4.41286, Tokens/sec: 72724.92292015479\n",
      "Step: 212, Training Loss: 4.19841, Tokens/sec: 65651.09924293016\n",
      "Step: 213, Training Loss: 4.22179, Tokens/sec: 69366.41040745491\n",
      "Step: 214, Training Loss: 4.28048, Tokens/sec: 74619.75906466684\n",
      "Step: 215, Training Loss: 4.23645, Tokens/sec: 73953.10519130157\n",
      "Step: 216, Training Loss: 4.22470, Tokens/sec: 74807.74218509736\n",
      "Step: 217, Training Loss: 4.20621, Tokens/sec: 73857.63784521814\n",
      "Step: 218, Training Loss: 4.26585, Tokens/sec: 73907.01242837911\n",
      "Step: 219, Training Loss: 4.27372, Tokens/sec: 73684.04208957766\n",
      "Step: 220, Training Loss: 4.27534, Tokens/sec: 73785.51016049726\n",
      "Step: 221, Training Loss: 4.22936, Tokens/sec: 73982.81876458836\n",
      "Step: 222, Training Loss: 4.15358, Tokens/sec: 73275.7731787865\n",
      "Step: 223, Training Loss: 4.22069, Tokens/sec: 73615.14661744106\n",
      "Step: 224, Training Loss: 4.20181, Tokens/sec: 73504.89863072567\n",
      "Step: 225, Training Loss: 4.14087, Tokens/sec: 74252.2991956615\n",
      "Step: 226, Training Loss: 4.17740, Tokens/sec: 74557.39804248951\n",
      "Step: 227, Training Loss: 4.21469, Tokens/sec: 74452.7384036547\n",
      "Step: 228, Training Loss: 4.23462, Tokens/sec: 74319.90123261909\n",
      "Step: 229, Training Loss: 4.05850, Tokens/sec: 73865.51913071107\n",
      "Step: 230, Training Loss: 4.26363, Tokens/sec: 75012.01936494528\n",
      "Step: 231, Training Loss: 3.95954, Tokens/sec: 73026.3065061934\n",
      "Step: 232, Training Loss: 4.06309, Tokens/sec: 73672.74308523897\n",
      "Step: 233, Training Loss: 4.08693, Tokens/sec: 73012.75525956559\n",
      "Step: 234, Training Loss: 4.05488, Tokens/sec: 73878.12222979589\n",
      "Step: 235, Training Loss: 4.03507, Tokens/sec: 72618.94516968013\n",
      "Step: 236, Training Loss: 4.06814, Tokens/sec: 74035.09070671345\n",
      "Step: 237, Training Loss: 4.05667, Tokens/sec: 74937.65479078887\n",
      "Step: 238, Training Loss: 4.12861, Tokens/sec: 73698.70793271097\n",
      "Step: 239, Training Loss: 4.14905, Tokens/sec: 73490.05170786797\n",
      "Step: 240, Training Loss: 4.08645, Tokens/sec: 73236.45496572512\n",
      "Step: 241, Training Loss: 4.00178, Tokens/sec: 73285.97254223596\n",
      "Step: 242, Training Loss: 4.08950, Tokens/sec: 74307.49038224558\n",
      "Step: 243, Training Loss: 4.14196, Tokens/sec: 74323.93986829657\n",
      "Step: 244, Training Loss: 4.01339, Tokens/sec: 74095.59886219479\n",
      "Step: 245, Training Loss: 4.07666, Tokens/sec: 73294.9359550303\n",
      "Step: 246, Training Loss: 4.11893, Tokens/sec: 73374.56780638393\n",
      "Step: 247, Training Loss: 4.09002, Tokens/sec: 73092.1491369887\n",
      "Step: 248, Training Loss: 3.98156, Tokens/sec: 73577.80940697697\n",
      "Step: 249, Training Loss: 4.11633, Tokens/sec: 73684.10770318472\n",
      "Step: 250, Training Loss: 3.87635, Tokens/sec: 73226.45854854709\n",
      "Step: 251, Training Loss: 4.00588, Tokens/sec: 74402.5031448808\n",
      "Step: 252, Training Loss: 3.98811, Tokens/sec: 72768.80243409296\n",
      "Step: 253, Training Loss: 3.96751, Tokens/sec: 72388.85003467435\n",
      "Step: 254, Training Loss: 3.94848, Tokens/sec: 73591.36563332936\n",
      "Step: 255, Training Loss: 3.96757, Tokens/sec: 74816.54842210836\n",
      "Step: 256, Training Loss: 3.93590, Tokens/sec: 73167.19797595778\n",
      "Step: 257, Training Loss: 3.98926, Tokens/sec: 74285.47317859497\n",
      "Step: 258, Training Loss: 4.04333, Tokens/sec: 74341.23014070788\n",
      "Step: 259, Training Loss: 4.01744, Tokens/sec: 73587.71029180134\n",
      "Step: 260, Training Loss: 3.89279, Tokens/sec: 73368.0321497362\n",
      "Step: 261, Training Loss: 4.04365, Tokens/sec: 73728.19714959529\n",
      "Step: 262, Training Loss: 4.04162, Tokens/sec: 74583.56183523712\n",
      "Step: 263, Training Loss: 3.96034, Tokens/sec: 73090.66290537786\n",
      "Step: 264, Training Loss: 4.02636, Tokens/sec: 73763.30964859865\n",
      "Step: 265, Training Loss: 4.02532, Tokens/sec: 72117.95730128505\n",
      "Step: 266, Training Loss: 4.07921, Tokens/sec: 73856.50851706187\n",
      "Step: 267, Training Loss: 3.93898, Tokens/sec: 73674.29019027959\n",
      "Step: 268, Training Loss: 4.03477, Tokens/sec: 69082.18619170754\n",
      "Step: 269, Training Loss: 3.79222, Tokens/sec: 71794.07908576554\n",
      "Step: 270, Training Loss: 3.95989, Tokens/sec: 71817.34775971928\n",
      "Step: 271, Training Loss: 3.87774, Tokens/sec: 73368.44972961364\n",
      "Step: 272, Training Loss: 3.88723, Tokens/sec: 74406.53724564622\n",
      "Step: 273, Training Loss: 3.92259, Tokens/sec: 71468.4206432693\n",
      "Step: 274, Training Loss: 3.89904, Tokens/sec: 72141.97929139272\n",
      "Step: 275, Training Loss: 3.87544, Tokens/sec: 73050.12327409151\n",
      "Step: 276, Training Loss: 3.90267, Tokens/sec: 73532.8926758367\n",
      "Step: 277, Training Loss: 3.93444, Tokens/sec: 74021.5206814105\n",
      "Step: 278, Training Loss: 3.87913, Tokens/sec: 73796.84577883856\n",
      "Step: 279, Training Loss: 3.78046, Tokens/sec: 73680.80365006007\n",
      "Step: 280, Training Loss: 3.87931, Tokens/sec: 74188.09788961869\n",
      "Step: 281, Training Loss: 3.91327, Tokens/sec: 73333.78459543953\n",
      "Step: 282, Training Loss: 3.86349, Tokens/sec: 73998.87012697993\n",
      "Step: 283, Training Loss: 3.87743, Tokens/sec: 73829.05536658158\n",
      "Step: 284, Training Loss: 3.92734, Tokens/sec: 73753.37440631542\n",
      "Step: 285, Training Loss: 3.96282, Tokens/sec: 73448.67908182445\n",
      "Step: 286, Training Loss: 3.80109, Tokens/sec: 73310.51304771987\n",
      "Step: 287, Training Loss: 3.95668, Tokens/sec: 72655.99096969655\n",
      "Step: 288, Training Loss: 3.70652, Tokens/sec: 73738.14817429686\n",
      "Step: 289, Training Loss: 3.80786, Tokens/sec: 73877.37536120431\n",
      "Step: 290, Training Loss: 3.79686, Tokens/sec: 67270.00050546571\n",
      "Step: 291, Training Loss: 3.81306, Tokens/sec: 72969.02212928049\n",
      "Step: 292, Training Loss: 3.75254, Tokens/sec: 62239.631759138145\n",
      "Step: 293, Training Loss: 3.78049, Tokens/sec: 55475.37141736677\n",
      "Step: 294, Training Loss: 3.80015, Tokens/sec: 73726.48521361676\n",
      "Step: 295, Training Loss: 3.82004, Tokens/sec: 73723.75192852196\n",
      "Step: 296, Training Loss: 3.83240, Tokens/sec: 71733.13358365493\n",
      "Step: 297, Training Loss: 3.76169, Tokens/sec: 74612.95146224556\n",
      "Step: 298, Training Loss: 3.71188, Tokens/sec: 74513.16760292291\n",
      "Step: 299, Training Loss: 3.79560, Tokens/sec: 73939.5945661584\n",
      "Step: 300, Training Loss: 3.79108, Tokens/sec: 71595.24943670958\n",
      "Step: 301, Training Loss: 3.71872, Tokens/sec: 71973.85394900494\n",
      "Step: 302, Training Loss: 3.77446, Tokens/sec: 73825.64216396089\n",
      "Step: 303, Training Loss: 3.83315, Tokens/sec: 71117.46167801737\n",
      "Step: 304, Training Loss: 3.83768, Tokens/sec: 72567.08687448468\n",
      "Step: 305, Training Loss: 3.68649, Tokens/sec: 74244.56999825145\n",
      "Step: 306, Training Loss: 3.87746, Tokens/sec: 74593.24214934128\n",
      "Step: 307, Training Loss: 3.60347, Tokens/sec: 73875.26043404233\n",
      "Step: 308, Training Loss: 3.68540, Tokens/sec: 72970.8027400519\n",
      "Step: 309, Training Loss: 3.67405, Tokens/sec: 73112.90957188053\n",
      "Step: 310, Training Loss: 3.66167, Tokens/sec: 73415.81649333835\n",
      "Step: 311, Training Loss: 3.62499, Tokens/sec: 74328.03929752031\n",
      "Step: 312, Training Loss: 3.67811, Tokens/sec: 73436.59006018176\n",
      "Step: 313, Training Loss: 3.65199, Tokens/sec: 73736.08070038141\n",
      "Step: 314, Training Loss: 3.67588, Tokens/sec: 55578.27934383374\n",
      "Step: 315, Training Loss: 3.71572, Tokens/sec: 73778.77086179567\n",
      "Step: 316, Training Loss: 3.62604, Tokens/sec: 73255.03971493753\n",
      "Step: 317, Training Loss: 3.55160, Tokens/sec: 73459.77639131997\n",
      "Step: 318, Training Loss: 3.64867, Tokens/sec: 73188.46466523246\n",
      "Step: 319, Training Loss: 3.69425, Tokens/sec: 72728.69902044792\n",
      "Step: 320, Training Loss: 3.62739, Tokens/sec: 72609.2414557005\n",
      "Step: 321, Training Loss: 3.64221, Tokens/sec: 74179.11285820715\n",
      "Step: 322, Training Loss: 3.66616, Tokens/sec: 73370.65665619962\n",
      "Step: 323, Training Loss: 3.69804, Tokens/sec: 72853.10181740156\n",
      "Step: 324, Training Loss: 3.57453, Tokens/sec: 72989.11437053302\n",
      "Step: 325, Training Loss: 3.71786, Tokens/sec: 72565.43164962273\n",
      "Step: 326, Training Loss: 3.46248, Tokens/sec: 67722.91822916927\n",
      "Step: 327, Training Loss: 3.56694, Tokens/sec: 71747.13671108504\n",
      "Step: 328, Training Loss: 3.54779, Tokens/sec: 73978.51481290282\n",
      "Step: 329, Training Loss: 3.52536, Tokens/sec: 58788.21422755257\n",
      "Step: 330, Training Loss: 3.50632, Tokens/sec: 56422.00016851317\n",
      "Step: 331, Training Loss: 3.54455, Tokens/sec: 61231.35355172662\n",
      "Step: 332, Training Loss: 3.54584, Tokens/sec: 69656.92484936514\n",
      "Step: 333, Training Loss: 3.59877, Tokens/sec: 73440.48717733368\n",
      "Step: 334, Training Loss: 3.59583, Tokens/sec: 74496.3934209142\n",
      "Step: 335, Training Loss: 3.48214, Tokens/sec: 73539.6052822159\n",
      "Step: 336, Training Loss: 3.48903, Tokens/sec: 74269.58476663158\n",
      "Step: 337, Training Loss: 3.50741, Tokens/sec: 74640.78643178311\n",
      "Step: 338, Training Loss: 3.57222, Tokens/sec: 74277.34274830003\n",
      "Step: 339, Training Loss: 3.51361, Tokens/sec: 73377.61966556778\n",
      "Step: 340, Training Loss: 3.53199, Tokens/sec: 73516.04163707532\n",
      "Step: 341, Training Loss: 3.55991, Tokens/sec: 72471.03575898628\n",
      "Step: 342, Training Loss: 3.61717, Tokens/sec: 72981.83803154247\n",
      "Step: 343, Training Loss: 3.48943, Tokens/sec: 73885.50076177962\n",
      "Step: 344, Training Loss: 3.60853, Tokens/sec: 74193.97948194145\n",
      "Step: 345, Training Loss: 3.35190, Tokens/sec: 65988.33738407704\n",
      "Step: 346, Training Loss: 3.50909, Tokens/sec: 73746.68080803966\n",
      "Step: 347, Training Loss: 3.45371, Tokens/sec: 67250.87332993907\n",
      "Step: 348, Training Loss: 3.45356, Tokens/sec: 74083.21490326764\n",
      "Step: 349, Training Loss: 3.44791, Tokens/sec: 72427.73543055383\n",
      "Step: 350, Training Loss: 3.50254, Tokens/sec: 58879.98488313208\n",
      "Step: 351, Training Loss: 3.46501, Tokens/sec: 74426.99222559141\n",
      "Step: 352, Training Loss: 3.47076, Tokens/sec: 62223.76573221813\n",
      "Step: 353, Training Loss: 3.50062, Tokens/sec: 73819.06548805478\n",
      "Step: 354, Training Loss: 3.41220, Tokens/sec: 72883.3089414863\n",
      "Step: 355, Training Loss: 3.35051, Tokens/sec: 73367.44340470301\n",
      "Step: 356, Training Loss: 3.41286, Tokens/sec: 73946.24112801072\n",
      "Step: 357, Training Loss: 3.47976, Tokens/sec: 74325.8812916112\n",
      "Step: 358, Training Loss: 3.41181, Tokens/sec: 73567.40478305293\n",
      "Step: 359, Training Loss: 3.42985, Tokens/sec: 73416.77282817224\n",
      "Step: 360, Training Loss: 3.49079, Tokens/sec: 73875.82571140333\n",
      "Step: 361, Training Loss: 3.52322, Tokens/sec: 73936.27022934877\n",
      "Step: 362, Training Loss: 3.38733, Tokens/sec: 72780.86170916686\n",
      "Step: 363, Training Loss: 3.51418, Tokens/sec: 73446.6429506983\n",
      "Step: 364, Training Loss: 3.23913, Tokens/sec: 73892.9277579376\n",
      "Step: 365, Training Loss: 3.35914, Tokens/sec: 73672.69902609296\n",
      "Step: 366, Training Loss: 3.34424, Tokens/sec: 73042.86602278685\n",
      "Step: 367, Training Loss: 3.35319, Tokens/sec: 73070.15168846064\n",
      "Step: 368, Training Loss: 3.32097, Tokens/sec: 72650.47439166663\n",
      "Step: 369, Training Loss: 3.38834, Tokens/sec: 73607.19796313062\n",
      "Step: 370, Training Loss: 3.39667, Tokens/sec: 73355.56897261908\n",
      "Step: 371, Training Loss: 3.44570, Tokens/sec: 73757.34074490915\n",
      "Step: 372, Training Loss: 3.44478, Tokens/sec: 72527.5621222429\n",
      "Step: 373, Training Loss: 3.31403, Tokens/sec: 72664.67427969602\n",
      "Step: 374, Training Loss: 3.24147, Tokens/sec: 71735.28091067605\n",
      "Step: 375, Training Loss: 3.30256, Tokens/sec: 73217.67089536613\n",
      "Step: 376, Training Loss: 3.36541, Tokens/sec: 73760.63174265194\n",
      "Step: 377, Training Loss: 3.31816, Tokens/sec: 72929.41521036321\n",
      "Step: 378, Training Loss: 3.31707, Tokens/sec: 73117.72162251605\n",
      "Step: 379, Training Loss: 3.37136, Tokens/sec: 73543.02443766143\n",
      "Step: 380, Training Loss: 3.44395, Tokens/sec: 67831.13173038248\n",
      "Step: 381, Training Loss: 3.30871, Tokens/sec: 72501.56558311338\n",
      "Step: 382, Training Loss: 3.44958, Tokens/sec: 73575.77965931459\n",
      "Step: 383, Training Loss: 3.22438, Tokens/sec: 73028.0622465125\n",
      "Step: 384, Training Loss: 3.33872, Tokens/sec: 73934.37846446758\n",
      "Step: 385, Training Loss: 3.24609, Tokens/sec: 73265.04000678293\n",
      "Step: 386, Training Loss: 3.23588, Tokens/sec: 72042.62316817345\n",
      "Step: 387, Training Loss: 3.23033, Tokens/sec: 73179.92211930717\n",
      "Step: 388, Training Loss: 3.31065, Tokens/sec: 72660.29160008786\n",
      "Step: 389, Training Loss: 3.28224, Tokens/sec: 72905.3101623404\n",
      "Step: 390, Training Loss: 3.31137, Tokens/sec: 73222.41853067915\n",
      "Step: 391, Training Loss: 3.34378, Tokens/sec: 73096.30262884662\n",
      "Step: 392, Training Loss: 3.22076, Tokens/sec: 73823.87114685442\n",
      "Step: 393, Training Loss: 3.16577, Tokens/sec: 59502.352236855244\n",
      "Step: 394, Training Loss: 3.21907, Tokens/sec: 72880.16708655324\n",
      "Step: 395, Training Loss: 3.28505, Tokens/sec: 73199.86938433872\n",
      "Step: 396, Training Loss: 3.21822, Tokens/sec: 73404.46146954982\n",
      "Step: 397, Training Loss: 3.22255, Tokens/sec: 73631.62684166267\n",
      "Step: 398, Training Loss: 3.26562, Tokens/sec: 73563.29534955068\n",
      "Step: 399, Training Loss: 3.31596, Tokens/sec: 71881.64869156686\n",
      "Step: 400, Training Loss: 3.22991, Tokens/sec: 72342.94696790529\n",
      "Step: 401, Training Loss: 3.32971, Tokens/sec: 73273.78529430417\n",
      "Step: 402, Training Loss: 3.07562, Tokens/sec: 73290.53757533077\n",
      "Step: 403, Training Loss: 3.23600, Tokens/sec: 70653.42899292649\n",
      "Step: 404, Training Loss: 3.15567, Tokens/sec: 71837.25039374802\n",
      "Step: 405, Training Loss: 3.14433, Tokens/sec: 70954.8585962468\n",
      "Step: 406, Training Loss: 3.09019, Tokens/sec: 72382.37207373293\n",
      "Step: 407, Training Loss: 3.16789, Tokens/sec: 72976.38952735846\n",
      "Step: 408, Training Loss: 3.13309, Tokens/sec: 73935.32566876433\n",
      "Step: 409, Training Loss: 3.19891, Tokens/sec: 73104.73693946049\n",
      "Step: 410, Training Loss: 3.24167, Tokens/sec: 72275.40004733643\n",
      "Step: 411, Training Loss: 3.12798, Tokens/sec: 71400.97085250153\n",
      "Step: 412, Training Loss: 3.04921, Tokens/sec: 73692.28212496554\n",
      "Step: 413, Training Loss: 3.08021, Tokens/sec: 72738.56225104719\n",
      "Step: 414, Training Loss: 3.18252, Tokens/sec: 72778.52718465758\n",
      "Step: 415, Training Loss: 3.12025, Tokens/sec: 73018.16786441382\n",
      "Step: 416, Training Loss: 3.10022, Tokens/sec: 72245.18109426019\n",
      "Step: 417, Training Loss: 3.16749, Tokens/sec: 72147.31788308498\n",
      "Step: 418, Training Loss: 3.18291, Tokens/sec: 72053.68970787595\n",
      "Step: 419, Training Loss: 3.09144, Tokens/sec: 72160.36510238428\n",
      "Step: 420, Training Loss: 3.17170, Tokens/sec: 73130.02804851205\n",
      "Step: 421, Training Loss: 2.92629, Tokens/sec: 72569.88228393103\n",
      "Step: 422, Training Loss: 3.07426, Tokens/sec: 71861.05135818821\n",
      "Step: 423, Training Loss: 2.99356, Tokens/sec: 72305.43017577443\n",
      "Step: 424, Training Loss: 3.01639, Tokens/sec: 71533.68736626868\n",
      "Step: 425, Training Loss: 3.01659, Tokens/sec: 73476.55649972193\n",
      "Step: 426, Training Loss: 3.06017, Tokens/sec: 72006.90482221065\n",
      "Step: 427, Training Loss: 3.02345, Tokens/sec: 72887.28276616629\n",
      "Step: 428, Training Loss: 3.05785, Tokens/sec: 71198.40888819439\n",
      "Step: 429, Training Loss: 3.08901, Tokens/sec: 70125.63291398034\n",
      "Step: 430, Training Loss: 2.96814, Tokens/sec: 72775.59801105397\n",
      "Step: 431, Training Loss: 2.91198, Tokens/sec: 72685.7504125519\n",
      "Step: 432, Training Loss: 2.95185, Tokens/sec: 72846.00058648689\n",
      "Step: 433, Training Loss: 3.01663, Tokens/sec: 72151.40980320203\n",
      "Step: 434, Training Loss: 2.94313, Tokens/sec: 72396.99453998578\n",
      "Step: 435, Training Loss: 2.98799, Tokens/sec: 72334.32568750641\n",
      "Step: 436, Training Loss: 3.03112, Tokens/sec: 71861.93988042562\n",
      "Step: 437, Training Loss: 3.05200, Tokens/sec: 71701.79604639029\n",
      "Step: 438, Training Loss: 2.97816, Tokens/sec: 69616.39336203107\n",
      "Step: 439, Training Loss: 3.07014, Tokens/sec: 72090.64049826356\n",
      "Step: 440, Training Loss: 2.80592, Tokens/sec: 73116.4252283998\n",
      "Step: 441, Training Loss: 2.93305, Tokens/sec: 71112.37656626682\n",
      "Step: 442, Training Loss: 2.82458, Tokens/sec: 71026.25662180026\n",
      "Step: 443, Training Loss: 2.84252, Tokens/sec: 72429.95976620994\n",
      "Step: 444, Training Loss: 2.82756, Tokens/sec: 73361.46349634118\n",
      "Step: 445, Training Loss: 2.89508, Tokens/sec: 72557.06862100454\n",
      "Step: 446, Training Loss: 2.87003, Tokens/sec: 72864.57302442793\n",
      "Step: 447, Training Loss: 2.92052, Tokens/sec: 72661.01921635603\n",
      "Step: 448, Training Loss: 2.93003, Tokens/sec: 72543.27370677711\n",
      "Step: 449, Training Loss: 2.81255, Tokens/sec: 72778.7337660462\n",
      "Step: 450, Training Loss: 2.79820, Tokens/sec: 73805.82289086709\n",
      "Step: 451, Training Loss: 2.80237, Tokens/sec: 73699.09679909534\n",
      "Step: 452, Training Loss: 2.88393, Tokens/sec: 73011.15544864908\n",
      "Step: 453, Training Loss: 2.80424, Tokens/sec: 72061.40620622643\n",
      "Step: 454, Training Loss: 2.81570, Tokens/sec: 72380.95485354564\n",
      "Step: 455, Training Loss: 2.87394, Tokens/sec: 73057.50868741621\n",
      "Step: 456, Training Loss: 2.89453, Tokens/sec: 72663.45545373402\n",
      "Step: 457, Training Loss: 2.83586, Tokens/sec: 73072.9534022881\n",
      "Step: 458, Training Loss: 2.92290, Tokens/sec: 72974.18838373988\n",
      "Step: 459, Training Loss: 2.67048, Tokens/sec: 72705.80598988134\n",
      "Step: 460, Training Loss: 2.79549, Tokens/sec: 72023.39241057323\n",
      "Step: 461, Training Loss: 2.70734, Tokens/sec: 73547.22305934121\n",
      "Step: 462, Training Loss: 2.71787, Tokens/sec: 72575.14391308089\n",
      "Step: 463, Training Loss: 2.68548, Tokens/sec: 71361.47435789456\n",
      "Step: 464, Training Loss: 2.79621, Tokens/sec: 73091.8670822389\n",
      "Step: 465, Training Loss: 2.73843, Tokens/sec: 69482.14298100592\n",
      "Step: 466, Training Loss: 2.79056, Tokens/sec: 71926.04380108799\n",
      "Step: 467, Training Loss: 2.81607, Tokens/sec: 70415.15933996037\n",
      "Step: 468, Training Loss: 2.69344, Tokens/sec: 72301.9387917513\n",
      "Step: 469, Training Loss: 2.67101, Tokens/sec: 73062.2779317558\n",
      "Step: 470, Training Loss: 2.66712, Tokens/sec: 72046.45293582999\n",
      "Step: 471, Training Loss: 2.78685, Tokens/sec: 72155.15899520737\n",
      "Step: 472, Training Loss: 2.69562, Tokens/sec: 71185.69419275012\n",
      "Step: 473, Training Loss: 2.68942, Tokens/sec: 72125.14558146978\n",
      "Step: 474, Training Loss: 2.73795, Tokens/sec: 71496.47462787182\n",
      "Step: 475, Training Loss: 2.79852, Tokens/sec: 69261.5230228065\n",
      "Step: 476, Training Loss: 2.71499, Tokens/sec: 72660.12307228643\n",
      "Step: 477, Training Loss: 2.79868, Tokens/sec: 73175.25712104731\n",
      "Step: 478, Training Loss: 2.51664, Tokens/sec: 73416.28528303072\n",
      "Step: 479, Training Loss: 2.65044, Tokens/sec: 72297.14706498383\n",
      "Step: 480, Training Loss: 2.59465, Tokens/sec: 72847.17566075163\n",
      "Step: 481, Training Loss: 2.55759, Tokens/sec: 71897.17629434439\n",
      "Step: 482, Training Loss: 2.53288, Tokens/sec: 72671.81563063606\n",
      "Step: 483, Training Loss: 2.64530, Tokens/sec: 71088.61514363672\n",
      "Step: 484, Training Loss: 2.60752, Tokens/sec: 72665.63047921276\n",
      "Step: 485, Training Loss: 2.65355, Tokens/sec: 72451.97291933896\n",
      "Step: 486, Training Loss: 2.68923, Tokens/sec: 72515.6899829712\n",
      "Step: 487, Training Loss: 2.55886, Tokens/sec: 73009.0013300684\n",
      "Step: 488, Training Loss: 2.51874, Tokens/sec: 72957.15415173261\n",
      "Step: 489, Training Loss: 2.48851, Tokens/sec: 72826.28421159633\n",
      "Step: 490, Training Loss: 2.62758, Tokens/sec: 73665.58616927774\n",
      "Step: 491, Training Loss: 2.57429, Tokens/sec: 72566.63208240687\n",
      "Step: 492, Training Loss: 2.59810, Tokens/sec: 73458.70432130524\n",
      "Step: 493, Training Loss: 2.63218, Tokens/sec: 70667.55016062125\n",
      "Step: 494, Training Loss: 2.64605, Tokens/sec: 69088.4184203835\n",
      "Step: 495, Training Loss: 2.60120, Tokens/sec: 59815.28853408736\n",
      "Step: 496, Training Loss: 2.70385, Tokens/sec: 72168.22462385835\n",
      "Step: 497, Training Loss: 2.42582, Tokens/sec: 72294.42238772506\n",
      "Step: 498, Training Loss: 2.52959, Tokens/sec: 72436.26754684122\n",
      "Step: 499, Training Loss: 2.44262, Tokens/sec: 69105.46243460866\n",
      "Step: 500, Training Loss: 2.41885, Tokens/sec: 69337.0428191586\n",
      "Step: 501, Training Loss: 2.37813, Tokens/sec: 67845.04813920414\n",
      "Step: 502, Training Loss: 2.45980, Tokens/sec: 68093.68600514736\n",
      "Step: 503, Training Loss: 2.44524, Tokens/sec: 69560.84118912948\n",
      "Step: 504, Training Loss: 2.48303, Tokens/sec: 71031.91545885678\n",
      "Step: 505, Training Loss: 2.52500, Tokens/sec: 71474.28610387142\n",
      "Step: 506, Training Loss: 2.39475, Tokens/sec: 72521.4580015324\n",
      "Step: 507, Training Loss: 2.34447, Tokens/sec: 71706.6717446635\n",
      "Step: 508, Training Loss: 2.31494, Tokens/sec: 72591.4560077528\n",
      "Step: 509, Training Loss: 2.43051, Tokens/sec: 54398.73160639895\n",
      "Step: 510, Training Loss: 2.40724, Tokens/sec: 70770.01652740651\n",
      "Step: 511, Training Loss: 2.40382, Tokens/sec: 71045.46189548369\n",
      "Step: 512, Training Loss: 2.45043, Tokens/sec: 73051.48864702138\n",
      "Step: 513, Training Loss: 2.50619, Tokens/sec: 71423.27484215754\n",
      "Step: 514, Training Loss: 2.46450, Tokens/sec: 71956.77309362187\n",
      "Step: 515, Training Loss: 2.57090, Tokens/sec: 70426.58185395341\n",
      "Step: 516, Training Loss: 2.26537, Tokens/sec: 71181.8066322696\n",
      "Step: 517, Training Loss: 2.42605, Tokens/sec: 69653.64516669078\n",
      "Step: 518, Training Loss: 2.32430, Tokens/sec: 71784.70589120554\n",
      "Step: 519, Training Loss: 2.29266, Tokens/sec: 72609.50950088275\n",
      "Step: 520, Training Loss: 2.20914, Tokens/sec: 71521.81707533589\n",
      "Step: 521, Training Loss: 2.30322, Tokens/sec: 71639.70360637576\n",
      "Step: 522, Training Loss: 2.27637, Tokens/sec: 71941.84709940778\n",
      "Step: 523, Training Loss: 2.30163, Tokens/sec: 71873.20920067707\n",
      "Step: 524, Training Loss: 2.36424, Tokens/sec: 72395.5191665552\n",
      "Step: 525, Training Loss: 2.23030, Tokens/sec: 72990.99189006575\n",
      "Step: 526, Training Loss: 2.24909, Tokens/sec: 72192.39177117981\n",
      "Step: 527, Training Loss: 2.16740, Tokens/sec: 71456.40838753349\n",
      "Step: 528, Training Loss: 2.28547, Tokens/sec: 69610.66648859195\n",
      "Step: 529, Training Loss: 2.24964, Tokens/sec: 71447.73974239273\n",
      "Step: 530, Training Loss: 2.24098, Tokens/sec: 70130.98643837123\n",
      "Step: 531, Training Loss: 2.29090, Tokens/sec: 72738.08948253558\n",
      "Step: 532, Training Loss: 2.34033, Tokens/sec: 70412.38644590123\n",
      "Step: 533, Training Loss: 2.29992, Tokens/sec: 70578.89082683605\n",
      "Step: 534, Training Loss: 2.39448, Tokens/sec: 69728.27532801735\n",
      "Step: 535, Training Loss: 2.13495, Tokens/sec: 72025.38363640867\n",
      "Step: 536, Training Loss: 2.28543, Tokens/sec: 71736.87837803947\n",
      "Step: 537, Training Loss: 2.15267, Tokens/sec: 72884.61329135735\n",
      "Step: 538, Training Loss: 2.13703, Tokens/sec: 72964.14222505462\n",
      "Step: 539, Training Loss: 2.13846, Tokens/sec: 72805.71468071503\n",
      "Step: 540, Training Loss: 2.22639, Tokens/sec: 71802.3008149247\n",
      "Step: 541, Training Loss: 2.13317, Tokens/sec: 71008.80203484924\n",
      "Step: 542, Training Loss: 2.13986, Tokens/sec: 71169.57456896926\n",
      "Step: 543, Training Loss: 2.18932, Tokens/sec: 71391.3124070508\n",
      "Step: 544, Training Loss: 2.07859, Tokens/sec: 72765.35115065373\n",
      "Step: 545, Training Loss: 2.09250, Tokens/sec: 73105.76967332084\n",
      "Step: 546, Training Loss: 2.06899, Tokens/sec: 71325.25435193184\n",
      "Step: 547, Training Loss: 2.16633, Tokens/sec: 72792.04142217114\n",
      "Step: 548, Training Loss: 2.09857, Tokens/sec: 71077.21616285588\n",
      "Step: 549, Training Loss: 2.09004, Tokens/sec: 72128.09153142519\n",
      "Step: 550, Training Loss: 2.12897, Tokens/sec: 72071.12730513548\n",
      "Step: 551, Training Loss: 2.20702, Tokens/sec: 71711.73391924507\n",
      "Step: 552, Training Loss: 2.13441, Tokens/sec: 70284.7108614604\n",
      "Step: 553, Training Loss: 2.23508, Tokens/sec: 71468.58930237521\n",
      "Step: 554, Training Loss: 1.97569, Tokens/sec: 66739.42420849613\n",
      "Step: 555, Training Loss: 2.08687, Tokens/sec: 71637.4542469503\n",
      "Step: 556, Training Loss: 1.98672, Tokens/sec: 71768.79172119552\n",
      "Step: 557, Training Loss: 2.01481, Tokens/sec: 72662.2302254169\n",
      "Step: 558, Training Loss: 1.95868, Tokens/sec: 62206.06221353695\n",
      "Step: 559, Training Loss: 2.06103, Tokens/sec: 64650.587448966544\n",
      "Step: 560, Training Loss: 1.99497, Tokens/sec: 72981.87476581884\n",
      "Step: 561, Training Loss: 2.00354, Tokens/sec: 62955.388613838135\n",
      "Step: 562, Training Loss: 2.04106, Tokens/sec: 59098.51314243057\n",
      "Step: 563, Training Loss: 1.90542, Tokens/sec: 57682.41952558649\n",
      "Step: 564, Training Loss: 1.85091, Tokens/sec: 71852.85965884256\n",
      "Step: 565, Training Loss: 1.82637, Tokens/sec: 71814.27919154907\n",
      "Step: 566, Training Loss: 1.95922, Tokens/sec: 72221.01902764034\n",
      "Step: 567, Training Loss: 1.94477, Tokens/sec: 72311.26213292101\n",
      "Step: 568, Training Loss: 1.92020, Tokens/sec: 71877.21648181498\n",
      "Step: 569, Training Loss: 1.91879, Tokens/sec: 72382.01904430558\n",
      "Step: 570, Training Loss: 1.99012, Tokens/sec: 73215.92532785413\n",
      "Step: 571, Training Loss: 1.88783, Tokens/sec: 73221.61876104023\n",
      "Step: 572, Training Loss: 1.95663, Tokens/sec: 70113.32485403324\n",
      "Step: 573, Training Loss: 1.74265, Tokens/sec: 72252.322750388\n",
      "Step: 574, Training Loss: 1.87931, Tokens/sec: 72766.63738481789\n",
      "Step: 575, Training Loss: 1.79245, Tokens/sec: 72778.13471994424\n",
      "Step: 576, Training Loss: 1.77871, Tokens/sec: 72423.17735873266\n",
      "Step: 577, Training Loss: 1.74799, Tokens/sec: 72740.30805696311\n",
      "Step: 578, Training Loss: 1.85030, Tokens/sec: 71205.73160040787\n",
      "Step: 579, Training Loss: 1.78624, Tokens/sec: 69828.61167242663\n",
      "Step: 580, Training Loss: 1.84344, Tokens/sec: 72674.73194291387\n",
      "Step: 581, Training Loss: 1.86150, Tokens/sec: 70851.68875917626\n",
      "Step: 582, Training Loss: 1.73872, Tokens/sec: 72172.52175094484\n",
      "Step: 583, Training Loss: 1.70390, Tokens/sec: 69522.76753282722\n",
      "Step: 584, Training Loss: 1.64762, Tokens/sec: 72525.74239971499\n",
      "Step: 585, Training Loss: 1.71615, Tokens/sec: 71428.46804337877\n",
      "Step: 586, Training Loss: 1.70727, Tokens/sec: 73279.68896887831\n",
      "Step: 587, Training Loss: 1.72934, Tokens/sec: 72928.20534309279\n",
      "Step: 588, Training Loss: 1.72251, Tokens/sec: 72990.7408546926\n",
      "Step: 589, Training Loss: 1.80540, Tokens/sec: 73551.5770171921\n",
      "Step: 590, Training Loss: 1.72118, Tokens/sec: 72078.74257529246\n",
      "Step: 591, Training Loss: 1.75391, Tokens/sec: 72192.35900744222\n",
      "Step: 592, Training Loss: 1.54471, Tokens/sec: 72774.73394642094\n",
      "Step: 593, Training Loss: 1.66219, Tokens/sec: 72505.09937667262\n",
      "Step: 594, Training Loss: 1.57047, Tokens/sec: 71626.11688189315\n",
      "Step: 595, Training Loss: 1.57682, Tokens/sec: 71613.70035021818\n",
      "Step: 596, Training Loss: 1.53758, Tokens/sec: 69674.46773790056\n",
      "Step: 597, Training Loss: 1.63832, Tokens/sec: 72596.80697271916\n",
      "Step: 598, Training Loss: 1.58677, Tokens/sec: 68393.35024618902\n",
      "Step: 599, Training Loss: 1.60244, Tokens/sec: 71905.1177562062\n",
      "Step: 600, Training Loss: 1.65058, Tokens/sec: 72658.28863630969\n",
      "Step: 601, Training Loss: 1.53655, Tokens/sec: 72335.57948006125\n",
      "Step: 602, Training Loss: 1.49392, Tokens/sec: 72880.44686585054\n",
      "Step: 603, Training Loss: 1.47961, Tokens/sec: 71962.65703665835\n",
      "Step: 604, Training Loss: 1.53619, Tokens/sec: 64411.90102365255\n",
      "Step: 605, Training Loss: 1.51491, Tokens/sec: 72463.31558070841\n",
      "Step: 606, Training Loss: 1.47573, Tokens/sec: 72299.72997029981\n",
      "Step: 607, Training Loss: 1.50837, Tokens/sec: 72182.87803421685\n",
      "Step: 608, Training Loss: 1.58658, Tokens/sec: 72145.1213556635\n",
      "Step: 609, Training Loss: 1.54548, Tokens/sec: 71829.45365767453\n",
      "Step: 610, Training Loss: 1.57369, Tokens/sec: 70988.30780905377\n",
      "Step: 611, Training Loss: 1.37210, Tokens/sec: 62876.80727583279\n",
      "Step: 612, Training Loss: 1.50830, Tokens/sec: 73969.0805449727\n",
      "Step: 613, Training Loss: 1.40152, Tokens/sec: 70210.99066813364\n",
      "Step: 614, Training Loss: 1.43705, Tokens/sec: 71999.25504832552\n",
      "Step: 615, Training Loss: 1.35019, Tokens/sec: 72746.09829805353\n",
      "Step: 616, Training Loss: 1.44916, Tokens/sec: 72065.48997116735\n",
      "Step: 617, Training Loss: 1.36298, Tokens/sec: 71305.22566115155\n",
      "Step: 618, Training Loss: 1.43561, Tokens/sec: 71842.64447342961\n",
      "Step: 619, Training Loss: 1.46474, Tokens/sec: 72257.78030130295\n",
      "Step: 620, Training Loss: 1.33403, Tokens/sec: 71805.40612173255\n",
      "Step: 621, Training Loss: 1.31987, Tokens/sec: 72765.50594819216\n",
      "Step: 622, Training Loss: 1.26452, Tokens/sec: 71490.71751219896\n",
      "Step: 623, Training Loss: 1.33253, Tokens/sec: 71724.5326873901\n",
      "Step: 624, Training Loss: 1.31577, Tokens/sec: 69969.10477957156\n",
      "Step: 625, Training Loss: 1.32357, Tokens/sec: 72001.78348557946\n",
      "Step: 626, Training Loss: 1.33116, Tokens/sec: 70450.01053683108\n",
      "Step: 627, Training Loss: 1.36471, Tokens/sec: 71845.99334248553\n",
      "Step: 628, Training Loss: 1.30401, Tokens/sec: 70496.47425877879\n",
      "Step: 629, Training Loss: 1.37032, Tokens/sec: 70746.67209747616\n",
      "Step: 630, Training Loss: 1.22752, Tokens/sec: 71902.1950381078\n",
      "Step: 631, Training Loss: 1.27100, Tokens/sec: 72123.72190596156\n",
      "Step: 632, Training Loss: 1.18671, Tokens/sec: 71847.29485392034\n",
      "Step: 633, Training Loss: 1.21442, Tokens/sec: 72592.4160755904\n",
      "Step: 634, Training Loss: 1.12715, Tokens/sec: 70359.56378271214\n",
      "Step: 635, Training Loss: 1.23760, Tokens/sec: 71950.56244650147\n",
      "Step: 636, Training Loss: 1.16553, Tokens/sec: 69056.19943238524\n",
      "Step: 637, Training Loss: 1.17384, Tokens/sec: 71811.34780185758\n",
      "Step: 638, Training Loss: 1.22477, Tokens/sec: 71176.59821598094\n",
      "Step: 639, Training Loss: 1.12898, Tokens/sec: 71339.74135819326\n",
      "Step: 640, Training Loss: 1.12412, Tokens/sec: 67864.54154692366\n",
      "Step: 641, Training Loss: 1.08369, Tokens/sec: 71915.41574444887\n",
      "Step: 642, Training Loss: 1.12973, Tokens/sec: 70670.27368589118\n",
      "Step: 643, Training Loss: 1.07064, Tokens/sec: 71935.37719941378\n",
      "Step: 644, Training Loss: 1.07932, Tokens/sec: 71919.4217337538\n",
      "Step: 645, Training Loss: 1.07889, Tokens/sec: 70540.72930714357\n",
      "Step: 646, Training Loss: 1.10219, Tokens/sec: 71344.27062839652\n",
      "Step: 647, Training Loss: 1.08429, Tokens/sec: 71876.56911961768\n",
      "Step: 648, Training Loss: 1.09986, Tokens/sec: 71897.21604935419\n",
      "Step: 649, Training Loss: 0.95731, Tokens/sec: 72331.9641605426\n",
      "Step: 650, Training Loss: 1.05553, Tokens/sec: 71071.7212129452\n",
      "Step: 651, Training Loss: 0.96522, Tokens/sec: 60470.61708077765\n",
      "Step: 652, Training Loss: 0.96460, Tokens/sec: 82636.84946457218\n",
      "Step: 653, Training Loss: 0.93093, Tokens/sec: 83327.04972658124\n",
      "Step: 654, Training Loss: 0.97255, Tokens/sec: 65252.68770942726\n",
      "Step: 655, Training Loss: 0.92568, Tokens/sec: 87984.40991699189\n",
      "Step: 656, Training Loss: 0.93160, Tokens/sec: 83425.684267572\n",
      "Step: 657, Training Loss: 0.93493, Tokens/sec: 85957.68654203883\n",
      "Step: 658, Training Loss: 0.92823, Tokens/sec: 88274.12417331962\n",
      "Step: 659, Training Loss: 0.92134, Tokens/sec: 85821.41312065862\n",
      "Step: 660, Training Loss: 0.85893, Tokens/sec: 88810.51861588543\n",
      "Step: 661, Training Loss: 0.91941, Tokens/sec: 87244.56921180688\n",
      "Step: 662, Training Loss: 0.93330, Tokens/sec: 88130.11925785507\n",
      "Step: 663, Training Loss: 0.89043, Tokens/sec: 82131.08098446259\n",
      "Step: 664, Training Loss: 0.89071, Tokens/sec: 86622.34511409067\n",
      "Step: 665, Training Loss: 0.92426, Tokens/sec: 86936.42839445463\n",
      "Step: 666, Training Loss: 0.89486, Tokens/sec: 86722.24703122291\n",
      "Step: 667, Training Loss: 0.90915, Tokens/sec: 87725.33736569434\n",
      "Step: 668, Training Loss: 0.76485, Tokens/sec: 87442.41337605433\n",
      "Step: 669, Training Loss: 0.82744, Tokens/sec: 86099.27065066449\n",
      "Step: 670, Training Loss: 0.77908, Tokens/sec: 83581.35025971619\n",
      "Step: 671, Training Loss: 0.78880, Tokens/sec: 86265.60618974605\n",
      "Step: 672, Training Loss: 0.77205, Tokens/sec: 71490.15289560954\n",
      "Step: 673, Training Loss: 0.82249, Tokens/sec: 68562.02300399788\n",
      "Step: 674, Training Loss: 0.76621, Tokens/sec: 69912.29697822568\n",
      "Step: 675, Training Loss: 0.78659, Tokens/sec: 67449.86149065552\n",
      "Step: 676, Training Loss: 0.81298, Tokens/sec: 83173.4475579663\n",
      "Step: 677, Training Loss: 0.70522, Tokens/sec: 64268.74654034204\n",
      "Step: 678, Training Loss: 0.69740, Tokens/sec: 66725.87366169263\n",
      "Step: 679, Training Loss: 0.70123, Tokens/sec: 64009.209076260064\n",
      "Step: 680, Training Loss: 0.73797, Tokens/sec: 72725.2738156764\n",
      "Step: 681, Training Loss: 0.73284, Tokens/sec: 73303.91697615385\n",
      "Step: 682, Training Loss: 0.72118, Tokens/sec: 78711.5314329439\n",
      "Step: 683, Training Loss: 0.70514, Tokens/sec: 72353.22121942275\n",
      "Step: 684, Training Loss: 0.73273, Tokens/sec: 71272.84199583868\n",
      "Step: 685, Training Loss: 0.73145, Tokens/sec: 88341.48275093031\n",
      "Step: 686, Training Loss: 0.70231, Tokens/sec: 75494.45376648009\n",
      "Step: 687, Training Loss: 0.63381, Tokens/sec: 63502.0986476578\n",
      "Step: 688, Training Loss: 0.71986, Tokens/sec: 75646.48949242834\n",
      "Step: 689, Training Loss: 0.65017, Tokens/sec: 69296.86768423412\n",
      "Step: 690, Training Loss: 0.63966, Tokens/sec: 86991.06127646107\n",
      "Step: 691, Training Loss: 0.61480, Tokens/sec: 65262.2369248493\n",
      "Step: 692, Training Loss: 0.66639, Tokens/sec: 80356.11294571144\n",
      "Step: 693, Training Loss: 0.66331, Tokens/sec: 84079.42360984755\n",
      "Step: 694, Training Loss: 0.67320, Tokens/sec: 77842.34808409322\n",
      "Step: 695, Training Loss: 0.68753, Tokens/sec: 78044.66931177283\n",
      "Step: 696, Training Loss: 0.60971, Tokens/sec: 83862.92449349849\n",
      "Step: 697, Training Loss: 0.63146, Tokens/sec: 84010.40141563435\n",
      "Step: 698, Training Loss: 0.56699, Tokens/sec: 83759.84102718784\n",
      "Step: 699, Training Loss: 0.61084, Tokens/sec: 87769.39294780658\n",
      "Step: 700, Training Loss: 0.61252, Tokens/sec: 86789.95077460239\n",
      "Step: 701, Training Loss: 0.61028, Tokens/sec: 87107.6800255549\n",
      "Step: 702, Training Loss: 0.62876, Tokens/sec: 88088.81051007369\n",
      "Step: 703, Training Loss: 0.65550, Tokens/sec: 86141.94417581779\n",
      "Step: 704, Training Loss: 0.61393, Tokens/sec: 62955.85477133519\n",
      "Step: 705, Training Loss: 0.62498, Tokens/sec: 75361.76938154749\n",
      "Step: 706, Training Loss: 0.55011, Tokens/sec: 72275.55340524226\n",
      "Step: 707, Training Loss: 0.59092, Tokens/sec: 87346.1540273939\n",
      "Step: 708, Training Loss: 0.57341, Tokens/sec: 87726.35147972712\n",
      "Step: 709, Training Loss: 0.56517, Tokens/sec: 74671.64438619906\n",
      "Step: 710, Training Loss: 0.57276, Tokens/sec: 88266.80520934494\n",
      "Step: 711, Training Loss: 0.57307, Tokens/sec: 88404.75582133613\n",
      "Step: 712, Training Loss: 0.52762, Tokens/sec: 87377.94167353425\n",
      "Step: 713, Training Loss: 0.57153, Tokens/sec: 81452.08748708961\n",
      "Step: 714, Training Loss: 0.58707, Tokens/sec: 87706.18112326843\n",
      "Step: 715, Training Loss: 0.51834, Tokens/sec: 87362.38012235759\n",
      "Step: 716, Training Loss: 0.53951, Tokens/sec: 87443.7289809538\n",
      "Step: 717, Training Loss: 0.54108, Tokens/sec: 87382.6112179608\n",
      "Step: 718, Training Loss: 0.53895, Tokens/sec: 87919.16558246964\n",
      "Step: 719, Training Loss: 0.52799, Tokens/sec: 86436.6476179469\n",
      "Step: 720, Training Loss: 0.47737, Tokens/sec: 88172.91828696369\n",
      "Step: 721, Training Loss: 0.48544, Tokens/sec: 87088.04772667112\n",
      "Step: 722, Training Loss: 0.57661, Tokens/sec: 86682.70127159095\n",
      "Step: 723, Training Loss: 0.54353, Tokens/sec: 75077.00983862903\n",
      "Step: 724, Training Loss: 0.56673, Tokens/sec: 67910.93475902743\n",
      "Step: 725, Training Loss: 0.47862, Tokens/sec: 72739.72514426758\n",
      "Step: 726, Training Loss: 0.52252, Tokens/sec: 73928.85787003789\n",
      "Step: 727, Training Loss: 0.49939, Tokens/sec: 71157.62021236934\n",
      "Step: 728, Training Loss: 0.48372, Tokens/sec: 72077.63211555016\n",
      "Step: 729, Training Loss: 0.46588, Tokens/sec: 72783.63254947177\n",
      "Step: 730, Training Loss: 0.47165, Tokens/sec: 70692.71547654108\n",
      "Step: 731, Training Loss: 0.46061, Tokens/sec: 72011.86166887547\n",
      "Step: 732, Training Loss: 0.48758, Tokens/sec: 71677.08507312863\n",
      "Step: 733, Training Loss: 0.50140, Tokens/sec: 72505.48312869215\n",
      "Step: 734, Training Loss: 0.46908, Tokens/sec: 70706.5292113671\n",
      "Step: 735, Training Loss: 0.49438, Tokens/sec: 71007.52026097386\n",
      "Step: 736, Training Loss: 0.45317, Tokens/sec: 73227.57000048642\n",
      "Step: 737, Training Loss: 0.47934, Tokens/sec: 72754.15087856195\n",
      "Step: 738, Training Loss: 0.44451, Tokens/sec: 71605.79683658977\n",
      "Step: 739, Training Loss: 0.44831, Tokens/sec: 56644.88965011852\n",
      "Step: 740, Training Loss: 0.44202, Tokens/sec: 72775.49586043411\n",
      "Step: 741, Training Loss: 0.45419, Tokens/sec: 72677.5936820892\n",
      "Step: 742, Training Loss: 0.43301, Tokens/sec: 72516.05073719201\n",
      "Step: 743, Training Loss: 0.43248, Tokens/sec: 57595.528022241255\n",
      "Step: 744, Training Loss: 0.39146, Tokens/sec: 72504.9630098928\n",
      "Step: 745, Training Loss: 0.41364, Tokens/sec: 72811.81529893124\n",
      "Step: 746, Training Loss: 0.43320, Tokens/sec: 73093.78836885667\n",
      "Step: 747, Training Loss: 0.45249, Tokens/sec: 73187.41782377225\n",
      "Step: 748, Training Loss: 0.42706, Tokens/sec: 73169.52318942355\n",
      "Step: 749, Training Loss: 0.43025, Tokens/sec: 72707.09076538184\n",
      "Step: 750, Training Loss: 0.39918, Tokens/sec: 56363.6958857269\n",
      "Step: 751, Training Loss: 0.38123, Tokens/sec: 62344.039777342805\n",
      "Step: 752, Training Loss: 0.37367, Tokens/sec: 72306.78667999188\n",
      "Step: 753, Training Loss: 0.36488, Tokens/sec: 59957.06950894507\n",
      "Step: 754, Training Loss: 0.41552, Tokens/sec: 71857.17855856863\n",
      "Step: 755, Training Loss: 0.37696, Tokens/sec: 72538.318905609\n",
      "Step: 756, Training Loss: 0.41085, Tokens/sec: 70211.72662384617\n",
      "Step: 757, Training Loss: 0.43189, Tokens/sec: 73106.48829797683\n",
      "Step: 758, Training Loss: 0.41653, Tokens/sec: 58957.79636938735\n",
      "Step: 759, Training Loss: 0.39019, Tokens/sec: 73142.36571605028\n",
      "Step: 760, Training Loss: 0.39214, Tokens/sec: 72420.09969965406\n",
      "Step: 761, Training Loss: 0.36209, Tokens/sec: 72302.99268101873\n",
      "Step: 762, Training Loss: 0.35911, Tokens/sec: 73065.52087591816\n",
      "Step: 763, Training Loss: 0.29073, Tokens/sec: 73097.0037835858\n",
      "Step: 764, Training Loss: 0.33093, Tokens/sec: 72892.11218058343\n",
      "Step: 765, Training Loss: 0.32563, Tokens/sec: 73437.81060102505\n",
      "Step: 766, Training Loss: 0.34856, Tokens/sec: 73208.0423695343\n",
      "Step: 767, Training Loss: 0.36090, Tokens/sec: 73209.1810648167\n",
      "Step: 768, Training Loss: 0.37383, Tokens/sec: 71086.56156363682\n",
      "Step: 769, Training Loss: 0.36009, Tokens/sec: 73120.08709056427\n",
      "Step: 770, Training Loss: 0.34365, Tokens/sec: 72792.9133361297\n",
      "Step: 771, Training Loss: 0.31846, Tokens/sec: 72794.26199710713\n",
      "Step: 772, Training Loss: 0.28739, Tokens/sec: 72951.22697070142\n",
      "Step: 773, Training Loss: 0.27853, Tokens/sec: 72456.61632453144\n",
      "Step: 774, Training Loss: 0.27440, Tokens/sec: 72497.66097158764\n",
      "Step: 775, Training Loss: 0.27393, Tokens/sec: 74047.67343148397\n",
      "Step: 776, Training Loss: 0.29291, Tokens/sec: 74179.61898158182\n",
      "Step: 777, Training Loss: 0.32875, Tokens/sec: 73150.89557694271\n",
      "Step: 778, Training Loss: 0.33934, Tokens/sec: 74057.11202788958\n",
      "Step: 779, Training Loss: 0.34023, Tokens/sec: 74049.77481882532\n",
      "Step: 780, Training Loss: 0.33351, Tokens/sec: 73064.73235111272\n",
      "Step: 781, Training Loss: 0.33287, Tokens/sec: 73776.61705687783\n",
      "Step: 782, Training Loss: 0.26647, Tokens/sec: 73519.9801777334\n",
      "Step: 783, Training Loss: 0.26752, Tokens/sec: 72378.180375225\n",
      "Step: 784, Training Loss: 0.23059, Tokens/sec: 73349.88358635521\n",
      "Step: 785, Training Loss: 0.22236, Tokens/sec: 73177.68972495607\n",
      "Step: 786, Training Loss: 0.23471, Tokens/sec: 73519.20127771307\n",
      "Step: 787, Training Loss: 0.24116, Tokens/sec: 70397.89004370794\n",
      "Step: 788, Training Loss: 0.26367, Tokens/sec: 73732.71524384848\n",
      "Step: 789, Training Loss: 0.29523, Tokens/sec: 73324.02640808071\n",
      "Step: 790, Training Loss: 0.27584, Tokens/sec: 72698.26119009739\n",
      "Step: 791, Training Loss: 0.24132, Tokens/sec: 72397.49135669372\n",
      "Step: 792, Training Loss: 0.22593, Tokens/sec: 69691.78290211206\n",
      "Step: 793, Training Loss: 0.20617, Tokens/sec: 73725.33932697565\n",
      "Step: 794, Training Loss: 0.18966, Tokens/sec: 70809.1885982573\n",
      "Step: 795, Training Loss: 0.16583, Tokens/sec: 74240.75393952998\n",
      "Step: 796, Training Loss: 0.17780, Tokens/sec: 73079.64846768959\n",
      "Step: 797, Training Loss: 0.18714, Tokens/sec: 72725.51527968499\n",
      "Step: 798, Training Loss: 0.21468, Tokens/sec: 73479.0055523801\n",
      "Step: 799, Training Loss: 0.21198, Tokens/sec: 73741.8609621932\n",
      "Step: 800, Training Loss: 0.22608, Tokens/sec: 73380.29743967754\n",
      "Step: 801, Training Loss: 0.20531, Tokens/sec: 73902.86030122303\n",
      "Step: 802, Training Loss: 0.20874, Tokens/sec: 73386.00298951866\n",
      "Step: 803, Training Loss: 0.17804, Tokens/sec: 73174.41491613445\n",
      "Step: 804, Training Loss: 0.17155, Tokens/sec: 73952.7997600834\n",
      "Step: 805, Training Loss: 0.15044, Tokens/sec: 73548.43506268626\n",
      "Step: 806, Training Loss: 0.16076, Tokens/sec: 74197.99435829186\n",
      "Step: 807, Training Loss: 0.15150, Tokens/sec: 73434.94661026666\n",
      "Step: 808, Training Loss: 0.15291, Tokens/sec: 72879.50218089942\n",
      "Step: 809, Training Loss: 0.15656, Tokens/sec: 73775.69019153247\n",
      "Step: 810, Training Loss: 0.15987, Tokens/sec: 72946.2861135625\n",
      "Step: 811, Training Loss: 0.15372, Tokens/sec: 73659.75791671217\n",
      "Step: 812, Training Loss: 0.13575, Tokens/sec: 73120.2567805674\n",
      "Step: 813, Training Loss: 0.12983, Tokens/sec: 72344.6041956377\n",
      "Step: 814, Training Loss: 0.12431, Tokens/sec: 72150.38352235303\n",
      "Step: 815, Training Loss: 0.12433, Tokens/sec: 71805.39196203745\n",
      "Step: 816, Training Loss: 0.11576, Tokens/sec: 71933.76835398606\n",
      "Step: 817, Training Loss: 0.11374, Tokens/sec: 73621.95596476563\n",
      "Step: 818, Training Loss: 0.10690, Tokens/sec: 72893.5323008667\n",
      "Step: 819, Training Loss: 0.11424, Tokens/sec: 72103.3591082373\n",
      "Step: 820, Training Loss: 0.10379, Tokens/sec: 73111.92002957265\n",
      "Step: 821, Training Loss: 0.10337, Tokens/sec: 72824.18695365789\n",
      "Step: 822, Training Loss: 0.10876, Tokens/sec: 74200.58818471218\n",
      "Step: 823, Training Loss: 0.10719, Tokens/sec: 73339.48160290229\n",
      "Step: 824, Training Loss: 0.10762, Tokens/sec: 72428.9511606632\n",
      "Step: 825, Training Loss: 0.09140, Tokens/sec: 74117.90364564065\n",
      "Step: 826, Training Loss: 0.08001, Tokens/sec: 73350.67762330835\n",
      "Step: 827, Training Loss: 0.08180, Tokens/sec: 72523.59949734823\n",
      "Step: 828, Training Loss: 0.08934, Tokens/sec: 73807.64823144085\n",
      "Step: 829, Training Loss: 0.08440, Tokens/sec: 73934.0515030512\n",
      "Step: 830, Training Loss: 0.07010, Tokens/sec: 74237.51078036924\n",
      "Step: 831, Training Loss: 0.07039, Tokens/sec: 72211.76031842988\n",
      "Step: 832, Training Loss: 0.07302, Tokens/sec: 73341.71042145211\n",
      "Step: 833, Training Loss: 0.07988, Tokens/sec: 73074.48193774318\n",
      "Step: 834, Training Loss: 0.06607, Tokens/sec: 74212.3552488566\n",
      "Step: 835, Training Loss: 0.05726, Tokens/sec: 73565.69634514375\n",
      "Step: 836, Training Loss: 0.04722, Tokens/sec: 59061.43421726644\n",
      "Step: 837, Training Loss: 0.05884, Tokens/sec: 65102.052891555315\n",
      "Step: 838, Training Loss: 0.06035, Tokens/sec: 73480.59396208437\n",
      "Step: 839, Training Loss: 0.05416, Tokens/sec: 72208.48228277291\n",
      "Step: 840, Training Loss: 0.05314, Tokens/sec: 72769.75103586042\n",
      "Step: 841, Training Loss: 0.05176, Tokens/sec: 72696.41547312861\n",
      "Step: 842, Training Loss: 0.05226, Tokens/sec: 73867.96487094017\n",
      "Step: 843, Training Loss: 0.04747, Tokens/sec: 73673.8644792298\n",
      "Step: 844, Training Loss: 0.05113, Tokens/sec: 73817.4048723685\n",
      "Step: 845, Training Loss: 0.04352, Tokens/sec: 73733.03312844818\n",
      "Step: 846, Training Loss: 0.04109, Tokens/sec: 72644.20268812115\n",
      "Step: 847, Training Loss: 0.03913, Tokens/sec: 73303.0731171005\n",
      "Step: 848, Training Loss: 0.04006, Tokens/sec: 73188.93088184239\n",
      "Step: 849, Training Loss: 0.03497, Tokens/sec: 73497.26522142597\n",
      "Step: 850, Training Loss: 0.03861, Tokens/sec: 72748.20268965467\n",
      "Step: 851, Training Loss: 0.03948, Tokens/sec: 72471.41305827013\n",
      "Step: 852, Training Loss: 0.03855, Tokens/sec: 71692.18278825082\n",
      "Step: 853, Training Loss: 0.03514, Tokens/sec: 72970.97076362841\n",
      "Step: 854, Training Loss: 0.03208, Tokens/sec: 73035.66296313037\n",
      "Step: 855, Training Loss: 0.03743, Tokens/sec: 73542.8026021315\n",
      "Step: 856, Training Loss: 0.03322, Tokens/sec: 73393.37297094798\n",
      "Step: 857, Training Loss: 0.02843, Tokens/sec: 72773.73026482442\n",
      "Step: 858, Training Loss: 0.02515, Tokens/sec: 71867.31717457855\n",
      "Step: 859, Training Loss: 0.02328, Tokens/sec: 71821.28773355481\n",
      "Step: 860, Training Loss: 0.02548, Tokens/sec: 72593.1674205231\n",
      "Step: 861, Training Loss: 0.02285, Tokens/sec: 73074.49171495682\n",
      "Step: 862, Training Loss: 0.02272, Tokens/sec: 73443.77102728326\n",
      "Step: 863, Training Loss: 0.02681, Tokens/sec: 73695.40754335775\n",
      "Step: 864, Training Loss: 0.02504, Tokens/sec: 73024.86851444232\n",
      "Step: 865, Training Loss: 0.02192, Tokens/sec: 73700.3466332557\n",
      "Step: 866, Training Loss: 0.02212, Tokens/sec: 72798.12293343521\n",
      "Step: 867, Training Loss: 0.02563, Tokens/sec: 72963.91249619884\n",
      "Step: 868, Training Loss: 0.01810, Tokens/sec: 72914.97473613144\n",
      "Step: 869, Training Loss: 0.01629, Tokens/sec: 72727.05481837915\n",
      "Step: 870, Training Loss: 0.01597, Tokens/sec: 71397.57560029245\n",
      "Step: 871, Training Loss: 0.01807, Tokens/sec: 72689.2554193267\n",
      "Step: 872, Training Loss: 0.01492, Tokens/sec: 69377.5603989699\n",
      "Step: 873, Training Loss: 0.01363, Tokens/sec: 72125.83807031739\n",
      "Step: 874, Training Loss: 0.01497, Tokens/sec: 73018.42136625243\n",
      "Step: 875, Training Loss: 0.01761, Tokens/sec: 64224.95241107033\n",
      "Step: 876, Training Loss: 0.01589, Tokens/sec: 74169.77610279621\n",
      "Step: 877, Training Loss: 0.01392, Tokens/sec: 73492.6173660789\n",
      "Step: 878, Training Loss: 0.01412, Tokens/sec: 73763.59923372966\n",
      "Step: 879, Training Loss: 0.01155, Tokens/sec: 74110.91006068309\n",
      "Step: 880, Training Loss: 0.01302, Tokens/sec: 71864.19074993936\n",
      "Step: 881, Training Loss: 0.01032, Tokens/sec: 72891.73826987825\n",
      "Step: 882, Training Loss: 0.01110, Tokens/sec: 67503.79475118972\n",
      "Step: 883, Training Loss: 0.00980, Tokens/sec: 68802.08073081165\n",
      "Step: 884, Training Loss: 0.00912, Tokens/sec: 73551.34027341045\n",
      "Step: 885, Training Loss: 0.00859, Tokens/sec: 73571.52689878747\n",
      "Step: 886, Training Loss: 0.01026, Tokens/sec: 73822.35035300787\n",
      "Step: 887, Training Loss: 0.00869, Tokens/sec: 71476.08244414572\n",
      "Step: 888, Training Loss: 0.00830, Tokens/sec: 70538.90891297089\n",
      "Step: 889, Training Loss: 0.00721, Tokens/sec: 72895.65917514947\n",
      "Step: 890, Training Loss: 0.00679, Tokens/sec: 72881.24600515646\n",
      "Step: 891, Training Loss: 0.00842, Tokens/sec: 69552.34938125942\n",
      "Step: 892, Training Loss: 0.00733, Tokens/sec: 69371.22126512333\n",
      "Step: 893, Training Loss: 0.00908, Tokens/sec: 71411.67242612322\n",
      "Step: 894, Training Loss: 0.00957, Tokens/sec: 69959.88028809552\n",
      "Step: 895, Training Loss: 0.00680, Tokens/sec: 69596.89916973263\n",
      "Step: 896, Training Loss: 0.00622, Tokens/sec: 72904.91989325246\n",
      "Step: 897, Training Loss: 0.00539, Tokens/sec: 74001.41862687422\n",
      "Step: 898, Training Loss: 0.00785, Tokens/sec: 73392.86995590547\n",
      "Step: 899, Training Loss: 0.00687, Tokens/sec: 74774.11419826584\n",
      "Step: 900, Training Loss: 0.00564, Tokens/sec: 67277.82421337905\n",
      "Step: 901, Training Loss: 0.00683, Tokens/sec: 73128.08200537055\n",
      "Step: 902, Training Loss: 0.00431, Tokens/sec: 73152.49171962212\n",
      "Step: 903, Training Loss: 0.00562, Tokens/sec: 70755.53689981373\n",
      "Step: 904, Training Loss: 0.00555, Tokens/sec: 73720.78100302652\n",
      "Step: 905, Training Loss: 0.00492, Tokens/sec: 73579.07264568214\n",
      "Step: 906, Training Loss: 0.00452, Tokens/sec: 73848.15083082268\n",
      "Step: 907, Training Loss: 0.00414, Tokens/sec: 71956.86821741867\n",
      "Step: 908, Training Loss: 0.00362, Tokens/sec: 73654.45904464432\n",
      "Step: 909, Training Loss: 0.00363, Tokens/sec: 74118.27883908605\n",
      "Step: 910, Training Loss: 0.00485, Tokens/sec: 73636.94427455252\n",
      "Step: 911, Training Loss: 0.00488, Tokens/sec: 73369.75146171032\n",
      "Step: 912, Training Loss: 0.00461, Tokens/sec: 73111.89001508677\n",
      "Step: 913, Training Loss: 0.00410, Tokens/sec: 72809.51664879246\n",
      "Step: 914, Training Loss: 0.00429, Tokens/sec: 72762.95428653974\n",
      "Step: 915, Training Loss: 0.00345, Tokens/sec: 73220.99440471889\n",
      "Step: 916, Training Loss: 0.00386, Tokens/sec: 71972.8874124735\n",
      "Step: 917, Training Loss: 0.00487, Tokens/sec: 74311.24589511496\n",
      "Step: 918, Training Loss: 0.00351, Tokens/sec: 73612.17716391616\n",
      "Step: 919, Training Loss: 0.00264, Tokens/sec: 65276.45559145926\n",
      "Step: 920, Training Loss: 0.00320, Tokens/sec: 69656.92011104918\n",
      "Step: 921, Training Loss: 0.00284, Tokens/sec: 70776.25341926776\n",
      "Step: 922, Training Loss: 0.00400, Tokens/sec: 73073.60782795046\n",
      "Step: 923, Training Loss: 0.00526, Tokens/sec: 73729.9632460804\n",
      "Step: 924, Training Loss: 0.00740, Tokens/sec: 71966.13185261404\n",
      "Step: 925, Training Loss: 0.00285, Tokens/sec: 68531.3504470769\n",
      "Step: 926, Training Loss: 0.00276, Tokens/sec: 71239.10496387507\n",
      "Step: 927, Training Loss: 0.00229, Tokens/sec: 73017.20756621112\n",
      "Step: 928, Training Loss: 0.00212, Tokens/sec: 72283.67052299922\n",
      "Step: 929, Training Loss: 0.00254, Tokens/sec: 72303.728792704\n",
      "Step: 930, Training Loss: 0.00227, Tokens/sec: 73090.07338416648\n",
      "Step: 931, Training Loss: 0.00276, Tokens/sec: 73202.9289690247\n",
      "Step: 932, Training Loss: 0.00340, Tokens/sec: 72615.6596732789\n",
      "Step: 933, Training Loss: 0.00391, Tokens/sec: 71977.06138916357\n",
      "Step: 934, Training Loss: 0.00214, Tokens/sec: 72217.80381878688\n",
      "Step: 935, Training Loss: 0.00394, Tokens/sec: 72856.40721613454\n",
      "Step: 936, Training Loss: 0.00486, Tokens/sec: 69218.81881924008\n",
      "Step: 937, Training Loss: 0.00439, Tokens/sec: 73394.17156433928\n",
      "Step: 938, Training Loss: 0.00319, Tokens/sec: 72737.03708121456\n",
      "Step: 939, Training Loss: 0.00279, Tokens/sec: 72697.37606151613\n",
      "Step: 940, Training Loss: 0.00221, Tokens/sec: 71684.72016925435\n",
      "Step: 941, Training Loss: 0.00196, Tokens/sec: 73811.6849201634\n",
      "Step: 942, Training Loss: 0.00217, Tokens/sec: 73010.9696701347\n",
      "Step: 943, Training Loss: 0.00284, Tokens/sec: 73010.49140334937\n",
      "Step: 944, Training Loss: 0.00255, Tokens/sec: 72566.3553527435\n",
      "Step: 945, Training Loss: 0.00237, Tokens/sec: 72879.82863504608\n",
      "Step: 946, Training Loss: 0.00296, Tokens/sec: 73429.86728683267\n",
      "Step: 947, Training Loss: 0.00218, Tokens/sec: 73038.40407408311\n",
      "Step: 948, Training Loss: 0.00315, Tokens/sec: 73444.11012811266\n",
      "Step: 949, Training Loss: 0.00262, Tokens/sec: 73337.59661963674\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T07:32:42.164826Z",
     "start_time": "2024-12-16T07:32:41.252597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_ids = tokenizer([\"HAMLET:\\nTo be or\"], return_tensors=\"pt\")['input_ids'].to(trainer.device)\n",
    "idx = model.generate(input_ids, temperature=0.25, top_k=50, max_new_tokens=64)\n",
    "print(tokenizer.batch_decode(idx)[0])"
   ],
   "id": "8b5596eda083de0a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAMLET:\n",
      "To be or tombs, there as that ere I was said\n",
      "That now, now, I cannot thou:\n",
      "Therefore I have disont to come back to joy;\n",
      "Which now I am all, for thy soul:\n",
      "What then is't that now but that far;\n",
      "But, he's fallen not honest against her\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T07:32:42.168132Z",
     "start_time": "2024-12-16T07:32:42.167003Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "bc3bcc343073d67a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
