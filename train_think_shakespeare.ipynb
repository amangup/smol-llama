{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "from think_model import ThinkModelConfig, ThinkTransformer\n",
    "from train import TrainerConfig, SimpleDataLoader, Trainer\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f28fa23c987e72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_id = \"HuggingFaceTB/SmolLM2-135M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bb4e51aa142abee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cde027092af8291e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = ThinkModelConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    #\n",
    "    # Generate model\n",
    "    d_model=576,\n",
    "    d_head=64,\n",
    "    d_mlp_proj=1536,\n",
    "    n_generate_layers=16,\n",
    "    n_kv_heads=3,\n",
    "    n_attn_heads=9,\n",
    "    n_cross_attn_heads=9,\n",
    "    generate_initializer_range=0.002,\n",
    "    #\n",
    "    # Think model\n",
    "    think_d_model=576,\n",
    "    think_d_head=64,\n",
    "    think_d_mlp_proj=1536,\n",
    "    n_think_kv_heads=3,\n",
    "    n_think_attn_heads=9,\n",
    "    n_think_layers=16,\n",
    "    think_initializer_range=0.02,\n",
    "    #\n",
    "    # Others\n",
    "    think_seq_prefix_ratio=0.33334,\n",
    "    thought_embedding_init_normal=False,\n",
    "    train_recurrence=1,\n",
    "    rms_norm_eps=1e-5,\n",
    "    rope_theta=100000.0,\n",
    "    padding_idx=tokenizer.pad_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "809773e662327a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = TrainerConfig(\n",
    "    per_device_train_batch_size=8,\n",
    "    max_seq_len=768,\n",
    "    num_epochs=16,\n",
    "    eval_interval_steps=25,\n",
    "    learning_rate=1e-3,\n",
    "    grad_clip_norm=1.0,\n",
    "    val_size=0.05,\n",
    "    log_dir=\"runs/shakespeare_think_test\",\n",
    "    warmup_ratio=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "374f398bb34f7ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/complete_shakespeare.txt\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a912a0ec92039d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens                   | 1,596,672\n",
      "Num Trainable Params           | 219,461,760\n",
      "Train device                   | cuda, NVIDIA H200, N=4\n",
      "Training precision             | torch.bfloat16\n",
      "Flash Attention                | True\n",
      "torch.compile()                | True\n",
      "DistributedDataParallel        | False\n",
      "Batch size                     | 6,144\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = ThinkTransformer(model_config)\n",
    "dataloader = SimpleDataLoader(train_config, tokenizer, text=text)\n",
    "trainer = Trainer(train_config, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee8c2059258a0195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training steps                 | 3,952 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:674.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Training Loss: 10.80273, LR: 0.0000500, Tokens/sec: 175.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:674.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1, Training Loss: 10.70053, LR: 0.0000524, Tokens/sec: 155.71\n",
      "Step: 2, Training Loss: 10.66124, LR: 0.0000548, Tokens/sec: 162045.48\n",
      "Step: 3, Training Loss: 10.62207, LR: 0.0000572, Tokens/sec: 178747.33\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 3, Eval Loss: 10.58776\n",
      "Step: 4, Training Loss: 10.58339, LR: 0.0000596, Tokens/sec: 144244.96\n",
      "Step: 5, Training Loss: 10.56650, LR: 0.0000620, Tokens/sec: 178085.35\n",
      "Step: 6, Training Loss: 10.51839, LR: 0.0000644, Tokens/sec: 180879.42\n",
      "Step: 7, Training Loss: 10.46982, LR: 0.0000668, Tokens/sec: 180871.45\n",
      "Step: 8, Training Loss: 10.41533, LR: 0.0000692, Tokens/sec: 181404.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:674.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 9, Training Loss: 10.37029, LR: 0.0000716, Tokens/sec: 179175.51\n",
      "Step: 10, Training Loss: 10.30681, LR: 0.0000741, Tokens/sec: 180032.85\n",
      "Step: 11, Training Loss: 10.25718, LR: 0.0000765, Tokens/sec: 181804.67\n",
      "Step: 12, Training Loss: 10.21304, LR: 0.0000789, Tokens/sec: 182371.93\n",
      "Step: 13, Training Loss: 10.14803, LR: 0.0000813, Tokens/sec: 182149.07\n",
      "Step: 14, Training Loss: 10.07475, LR: 0.0000837, Tokens/sec: 182411.10\n",
      "Step: 15, Training Loss: 10.01700, LR: 0.0000861, Tokens/sec: 180293.24\n",
      "Step: 16, Training Loss: 9.96263, LR: 0.0000885, Tokens/sec: 182176.59\n",
      "Step: 17, Training Loss: 9.86445, LR: 0.0000909, Tokens/sec: 181778.60\n",
      "Step: 18, Training Loss: 9.83467, LR: 0.0000933, Tokens/sec: 182220.07\n",
      "Step: 19, Training Loss: 9.75168, LR: 0.0000957, Tokens/sec: 4538.42\n",
      "Step: 20, Training Loss: 9.66495, LR: 0.0000981, Tokens/sec: 166149.45\n",
      "Step: 21, Training Loss: 9.56609, LR: 0.0001005, Tokens/sec: 177363.94\n",
      "Step: 22, Training Loss: 9.53662, LR: 0.0001029, Tokens/sec: 180095.74\n",
      "Step: 23, Training Loss: 9.45336, LR: 0.0001053, Tokens/sec: 179711.03\n",
      "Step: 24, Training Loss: 9.39195, LR: 0.0001077, Tokens/sec: 180243.92\n",
      "Step: 25, Training Loss: 9.27969, LR: 0.0001101, Tokens/sec: 178978.36\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 25, Eval Loss: 9.22528\n",
      "Step: 26, Training Loss: 9.22981, LR: 0.0001125, Tokens/sec: 180697.78\n",
      "Step: 27, Training Loss: 9.16605, LR: 0.0001149, Tokens/sec: 179039.82\n",
      "Step: 28, Training Loss: 9.05340, LR: 0.0001173, Tokens/sec: 180820.38\n",
      "Step: 29, Training Loss: 9.00481, LR: 0.0001197, Tokens/sec: 181059.85\n",
      "Step: 30, Training Loss: 8.88740, LR: 0.0001222, Tokens/sec: 180602.64\n",
      "Step: 31, Training Loss: 8.79667, LR: 0.0001246, Tokens/sec: 181452.42\n",
      "Step: 32, Training Loss: 8.75990, LR: 0.0001270, Tokens/sec: 181244.26\n",
      "Step: 33, Training Loss: 8.64588, LR: 0.0001294, Tokens/sec: 180292.63\n",
      "Step: 34, Training Loss: 8.55459, LR: 0.0001318, Tokens/sec: 181841.04\n",
      "Step: 35, Training Loss: 8.56922, LR: 0.0001342, Tokens/sec: 181156.32\n",
      "Step: 36, Training Loss: 8.44547, LR: 0.0001366, Tokens/sec: 181898.29\n",
      "Step: 37, Training Loss: 8.35331, LR: 0.0001390, Tokens/sec: 182167.93\n",
      "Step: 38, Training Loss: 8.24842, LR: 0.0001414, Tokens/sec: 182327.46\n",
      "Step: 39, Training Loss: 8.25519, LR: 0.0001438, Tokens/sec: 179730.42\n",
      "Step: 40, Training Loss: 8.14121, LR: 0.0001462, Tokens/sec: 181662.40\n",
      "Step: 41, Training Loss: 8.16736, LR: 0.0001486, Tokens/sec: 181356.55\n",
      "Step: 42, Training Loss: 7.98240, LR: 0.0001510, Tokens/sec: 181717.98\n",
      "Step: 43, Training Loss: 7.94748, LR: 0.0001534, Tokens/sec: 181160.36\n",
      "Step: 44, Training Loss: 7.87622, LR: 0.0001558, Tokens/sec: 182291.33\n",
      "Step: 45, Training Loss: 7.77497, LR: 0.0001582, Tokens/sec: 179810.93\n",
      "Step: 46, Training Loss: 7.79061, LR: 0.0001606, Tokens/sec: 181501.88\n",
      "Step: 47, Training Loss: 7.56994, LR: 0.0001630, Tokens/sec: 181730.08\n",
      "Step: 48, Training Loss: 7.76299, LR: 0.0001654, Tokens/sec: 182355.51\n",
      "Step: 49, Training Loss: 7.63452, LR: 0.0001678, Tokens/sec: 182108.89\n",
      "Step: 50, Training Loss: 7.53944, LR: 0.0001703, Tokens/sec: 182121.42\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 50, Eval Loss: 7.42720\n",
      "Step: 51, Training Loss: 7.45969, LR: 0.0001727, Tokens/sec: 179111.81\n",
      "Step: 52, Training Loss: 7.28630, LR: 0.0001751, Tokens/sec: 180692.64\n",
      "Step: 53, Training Loss: 7.34331, LR: 0.0001775, Tokens/sec: 180434.21\n",
      "Step: 54, Training Loss: 7.30497, LR: 0.0001799, Tokens/sec: 181892.76\n",
      "Step: 55, Training Loss: 7.18996, LR: 0.0001823, Tokens/sec: 180496.31\n",
      "Step: 56, Training Loss: 7.19749, LR: 0.0001847, Tokens/sec: 178535.92\n",
      "Step: 57, Training Loss: 7.27776, LR: 0.0001871, Tokens/sec: 180279.74\n",
      "Step: 58, Training Loss: 7.21053, LR: 0.0001895, Tokens/sec: 181018.30\n",
      "Step: 59, Training Loss: 7.07520, LR: 0.0001919, Tokens/sec: 181148.16\n",
      "Step: 60, Training Loss: 7.10007, LR: 0.0001943, Tokens/sec: 181713.56\n",
      "Step: 61, Training Loss: 6.99780, LR: 0.0001967, Tokens/sec: 181231.44\n",
      "Step: 62, Training Loss: 6.95776, LR: 0.0001991, Tokens/sec: 180422.21\n",
      "Step: 63, Training Loss: 6.86834, LR: 0.0002015, Tokens/sec: 181683.62\n",
      "Step: 64, Training Loss: 6.86236, LR: 0.0002039, Tokens/sec: 181590.93\n",
      "Step: 65, Training Loss: 6.70960, LR: 0.0002063, Tokens/sec: 181109.34\n",
      "Step: 66, Training Loss: 6.83063, LR: 0.0002087, Tokens/sec: 180869.29\n",
      "Step: 67, Training Loss: 6.96874, LR: 0.0002111, Tokens/sec: 181105.63\n",
      "Step: 68, Training Loss: 6.79921, LR: 0.0002135, Tokens/sec: 179750.10\n",
      "Step: 69, Training Loss: 6.86938, LR: 0.0002159, Tokens/sec: 181139.26\n",
      "Step: 70, Training Loss: 6.83261, LR: 0.0002184, Tokens/sec: 181361.74\n",
      "Step: 71, Training Loss: 6.84732, LR: 0.0002208, Tokens/sec: 181720.95\n",
      "Step: 72, Training Loss: 6.85105, LR: 0.0002232, Tokens/sec: 182176.47\n",
      "Step: 73, Training Loss: 6.97818, LR: 0.0002256, Tokens/sec: 181944.69\n",
      "Step: 74, Training Loss: 6.68403, LR: 0.0002280, Tokens/sec: 178792.83\n",
      "Step: 75, Training Loss: 6.69389, LR: 0.0002304, Tokens/sec: 180729.23\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 75, Eval Loss: 6.79026\n",
      "Step: 76, Training Loss: 6.74474, LR: 0.0002328, Tokens/sec: 179084.67\n",
      "Step: 77, Training Loss: 6.76630, LR: 0.0002352, Tokens/sec: 180514.85\n",
      "Step: 78, Training Loss: 6.84893, LR: 0.0002376, Tokens/sec: 180484.40\n",
      "Step: 79, Training Loss: 6.94025, LR: 0.0002400, Tokens/sec: 181027.76\n",
      "Step: 80, Training Loss: 6.91766, LR: 0.0002424, Tokens/sec: 180776.16\n",
      "Step: 81, Training Loss: 6.72366, LR: 0.0002448, Tokens/sec: 180545.67\n",
      "Step: 82, Training Loss: 6.79411, LR: 0.0002472, Tokens/sec: 174402.29\n",
      "Step: 83, Training Loss: 6.66369, LR: 0.0002496, Tokens/sec: 180097.73\n",
      "Step: 84, Training Loss: 6.70123, LR: 0.0002520, Tokens/sec: 181064.81\n",
      "Step: 85, Training Loss: 6.68087, LR: 0.0002544, Tokens/sec: 180911.74\n",
      "Step: 86, Training Loss: 6.75381, LR: 0.0002568, Tokens/sec: 181237.89\n",
      "Step: 87, Training Loss: 6.70177, LR: 0.0002592, Tokens/sec: 180770.32\n",
      "Step: 88, Training Loss: 9.33777, LR: 0.0002616, Tokens/sec: 180336.96\n",
      "Step: 89, Training Loss: 6.84065, LR: 0.0002641, Tokens/sec: 180785.31\n",
      "Step: 90, Training Loss: 6.57115, LR: 0.0002665, Tokens/sec: 180913.30\n",
      "Step: 91, Training Loss: 6.64100, LR: 0.0002689, Tokens/sec: 180843.27\n",
      "Step: 92, Training Loss: 6.64711, LR: 0.0002713, Tokens/sec: 181614.41\n",
      "Step: 93, Training Loss: 6.67988, LR: 0.0002737, Tokens/sec: 181020.03\n",
      "Step: 94, Training Loss: 6.73098, LR: 0.0002761, Tokens/sec: 180734.95\n",
      "Step: 95, Training Loss: 6.74396, LR: 0.0002785, Tokens/sec: 180628.81\n",
      "Step: 96, Training Loss: 6.59522, LR: 0.0002809, Tokens/sec: 181411.81\n",
      "Step: 97, Training Loss: 6.84001, LR: 0.0002833, Tokens/sec: 180881.72\n",
      "Step: 98, Training Loss: 6.71185, LR: 0.0002857, Tokens/sec: 181415.82\n",
      "Step: 99, Training Loss: 6.70073, LR: 0.0002881, Tokens/sec: 181253.55\n",
      "Step: 100, Training Loss: 6.65285, LR: 0.0002905, Tokens/sec: 180966.55\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 100, Eval Loss: 6.72169\n",
      "Step: 101, Training Loss: 6.68683, LR: 0.0002929, Tokens/sec: 179194.93\n",
      "Step: 102, Training Loss: 6.45357, LR: 0.0002953, Tokens/sec: 177827.57\n",
      "Step: 103, Training Loss: 6.58237, LR: 0.0002977, Tokens/sec: 179846.33\n",
      "Step: 104, Training Loss: 6.54326, LR: 0.0003001, Tokens/sec: 179700.94\n",
      "Step: 105, Training Loss: 6.60630, LR: 0.0003025, Tokens/sec: 179628.70\n",
      "Step: 106, Training Loss: 6.50663, LR: 0.0003049, Tokens/sec: 181331.27\n",
      "Step: 107, Training Loss: 6.54981, LR: 0.0003073, Tokens/sec: 180593.41\n",
      "Step: 108, Training Loss: 6.67328, LR: 0.0003097, Tokens/sec: 179865.67\n",
      "Step: 109, Training Loss: 6.59546, LR: 0.0003122, Tokens/sec: 181127.72\n",
      "Step: 110, Training Loss: 6.64239, LR: 0.0003146, Tokens/sec: 181144.33\n",
      "Step: 111, Training Loss: 6.68415, LR: 0.0003170, Tokens/sec: 181011.09\n",
      "Step: 112, Training Loss: 6.56363, LR: 0.0003194, Tokens/sec: 181184.98\n",
      "Step: 113, Training Loss: 6.55354, LR: 0.0003218, Tokens/sec: 181683.72\n",
      "Step: 114, Training Loss: 6.58945, LR: 0.0003242, Tokens/sec: 180199.75\n",
      "Step: 115, Training Loss: 6.47208, LR: 0.0003266, Tokens/sec: 181283.83\n",
      "Step: 116, Training Loss: 6.38996, LR: 0.0003290, Tokens/sec: 180670.96\n",
      "Step: 117, Training Loss: 6.49041, LR: 0.0003314, Tokens/sec: 180697.73\n",
      "Step: 118, Training Loss: 6.39923, LR: 0.0003338, Tokens/sec: 181198.64\n",
      "Step: 119, Training Loss: 6.49100, LR: 0.0003362, Tokens/sec: 180783.02\n",
      "Step: 120, Training Loss: 6.52417, LR: 0.0003386, Tokens/sec: 180170.99\n",
      "Step: 121, Training Loss: 6.51935, LR: 0.0003410, Tokens/sec: 180971.92\n",
      "Step: 122, Training Loss: 6.63742, LR: 0.0003434, Tokens/sec: 181122.36\n",
      "Step: 123, Training Loss: 6.36843, LR: 0.0003458, Tokens/sec: 181133.78\n",
      "Step: 124, Training Loss: 6.33173, LR: 0.0003482, Tokens/sec: 181220.46\n",
      "Step: 125, Training Loss: 6.34008, LR: 0.0003506, Tokens/sec: 180025.97\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 125, Eval Loss: 6.41842\n",
      "Step: 126, Training Loss: 6.24561, LR: 0.0003530, Tokens/sec: 181368.28\n",
      "Step: 127, Training Loss: 6.36600, LR: 0.0003554, Tokens/sec: 178922.58\n",
      "Step: 128, Training Loss: 6.35223, LR: 0.0003578, Tokens/sec: 172251.26\n",
      "Step: 129, Training Loss: 6.44971, LR: 0.0003603, Tokens/sec: 180251.33\n",
      "Step: 130, Training Loss: 6.48357, LR: 0.0003627, Tokens/sec: 180491.63\n",
      "Step: 131, Training Loss: 6.37902, LR: 0.0003651, Tokens/sec: 178992.21\n",
      "Step: 132, Training Loss: 6.39252, LR: 0.0003675, Tokens/sec: 181094.90\n",
      "Step: 133, Training Loss: 6.34548, LR: 0.0003699, Tokens/sec: 180806.96\n",
      "Step: 134, Training Loss: 6.27098, LR: 0.0003723, Tokens/sec: 181606.54\n",
      "Step: 135, Training Loss: 6.28886, LR: 0.0003747, Tokens/sec: 181605.99\n",
      "Step: 136, Training Loss: 6.15698, LR: 0.0003771, Tokens/sec: 181734.09\n",
      "Step: 137, Training Loss: 6.44745, LR: 0.0003795, Tokens/sec: 180204.58\n",
      "Step: 138, Training Loss: 6.44762, LR: 0.0003819, Tokens/sec: 181741.94\n",
      "Step: 139, Training Loss: 6.39185, LR: 0.0003843, Tokens/sec: 181346.34\n",
      "Step: 140, Training Loss: 6.51921, LR: 0.0003867, Tokens/sec: 180966.30\n",
      "Step: 141, Training Loss: 6.32333, LR: 0.0003891, Tokens/sec: 181479.24\n",
      "Step: 142, Training Loss: 6.45096, LR: 0.0003915, Tokens/sec: 181700.87\n",
      "Step: 143, Training Loss: 6.22579, LR: 0.0003939, Tokens/sec: 180234.37\n",
      "Step: 144, Training Loss: 6.28789, LR: 0.0003963, Tokens/sec: 181359.57\n",
      "Step: 145, Training Loss: 6.31452, LR: 0.0003987, Tokens/sec: 181426.06\n",
      "Step: 146, Training Loss: 6.21449, LR: 0.0004011, Tokens/sec: 181224.53\n",
      "Step: 147, Training Loss: 6.33023, LR: 0.0004035, Tokens/sec: 181530.08\n",
      "Step: 148, Training Loss: 6.53287, LR: 0.0004059, Tokens/sec: 181191.69\n",
      "Step: 149, Training Loss: 6.25490, LR: 0.0004084, Tokens/sec: 180326.73\n",
      "Step: 150, Training Loss: 6.22069, LR: 0.0004108, Tokens/sec: 181876.02\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 150, Eval Loss: 6.28990\n",
      "Step: 151, Training Loss: 6.27200, LR: 0.0004132, Tokens/sec: 180080.64\n",
      "Step: 152, Training Loss: 6.21978, LR: 0.0004156, Tokens/sec: 181231.41\n",
      "Step: 153, Training Loss: 6.52563, LR: 0.0004180, Tokens/sec: 181554.88\n",
      "Step: 154, Training Loss: 6.17045, LR: 0.0004204, Tokens/sec: 181385.42\n",
      "Step: 155, Training Loss: 6.57060, LR: 0.0004228, Tokens/sec: 181581.90\n",
      "Step: 156, Training Loss: 6.33862, LR: 0.0004252, Tokens/sec: 181381.72\n",
      "Step: 157, Training Loss: 6.41701, LR: 0.0004276, Tokens/sec: 180317.54\n",
      "Step: 158, Training Loss: 6.16060, LR: 0.0004300, Tokens/sec: 181317.65\n",
      "Step: 159, Training Loss: 6.22708, LR: 0.0004324, Tokens/sec: 181356.71\n",
      "Step: 160, Training Loss: 6.29137, LR: 0.0004348, Tokens/sec: 181427.43\n",
      "Step: 161, Training Loss: 6.10526, LR: 0.0004372, Tokens/sec: 180988.21\n",
      "Step: 162, Training Loss: 6.29204, LR: 0.0004396, Tokens/sec: 181350.63\n",
      "Step: 163, Training Loss: 6.23239, LR: 0.0004420, Tokens/sec: 180077.92\n",
      "Step: 164, Training Loss: 6.20048, LR: 0.0004444, Tokens/sec: 180539.44\n",
      "Step: 165, Training Loss: 6.11166, LR: 0.0004468, Tokens/sec: 180844.52\n",
      "Step: 166, Training Loss: 6.18220, LR: 0.0004492, Tokens/sec: 181328.66\n",
      "Step: 167, Training Loss: 6.21403, LR: 0.0004516, Tokens/sec: 181299.73\n",
      "Step: 168, Training Loss: 6.39736, LR: 0.0004541, Tokens/sec: 180989.85\n",
      "Step: 169, Training Loss: 6.50245, LR: 0.0004565, Tokens/sec: 180351.30\n",
      "Step: 170, Training Loss: 6.31052, LR: 0.0004589, Tokens/sec: 180985.87\n",
      "Step: 171, Training Loss: 6.36614, LR: 0.0004613, Tokens/sec: 180834.26\n",
      "Step: 172, Training Loss: 6.05987, LR: 0.0004637, Tokens/sec: 180719.10\n",
      "Step: 173, Training Loss: 6.20049, LR: 0.0004661, Tokens/sec: 181171.43\n",
      "Step: 174, Training Loss: 6.11076, LR: 0.0004685, Tokens/sec: 181539.09\n",
      "Step: 175, Training Loss: 6.20685, LR: 0.0004709, Tokens/sec: 180410.00\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 175, Eval Loss: 6.13182\n",
      "Step: 176, Training Loss: 6.37699, LR: 0.0004733, Tokens/sec: 181114.66\n",
      "Step: 177, Training Loss: 6.25901, LR: 0.0004757, Tokens/sec: 180464.73\n",
      "Step: 178, Training Loss: 6.22910, LR: 0.0004781, Tokens/sec: 181568.73\n",
      "Step: 179, Training Loss: 6.22774, LR: 0.0004805, Tokens/sec: 181299.94\n",
      "Step: 180, Training Loss: 6.06620, LR: 0.0004829, Tokens/sec: 181587.08\n",
      "Step: 181, Training Loss: 6.13551, LR: 0.0004853, Tokens/sec: 181667.49\n",
      "Step: 182, Training Loss: 6.33645, LR: 0.0004877, Tokens/sec: 181950.07\n",
      "Step: 183, Training Loss: 5.99099, LR: 0.0004901, Tokens/sec: 180150.81\n",
      "Step: 184, Training Loss: 6.02193, LR: 0.0004925, Tokens/sec: 181552.76\n",
      "Step: 185, Training Loss: 6.17854, LR: 0.0004949, Tokens/sec: 181050.94\n",
      "Step: 186, Training Loss: 6.30609, LR: 0.0004973, Tokens/sec: 181928.93\n",
      "Step: 187, Training Loss: 6.06504, LR: 0.0004997, Tokens/sec: 181061.26\n",
      "Step: 188, Training Loss: 5.93515, LR: 0.0005022, Tokens/sec: 180693.34\n",
      "Step: 189, Training Loss: 6.11830, LR: 0.0005046, Tokens/sec: 180230.65\n",
      "Step: 190, Training Loss: 6.04083, LR: 0.0005070, Tokens/sec: 181435.37\n",
      "Step: 191, Training Loss: 6.25889, LR: 0.0005094, Tokens/sec: 181554.49\n",
      "Step: 192, Training Loss: 6.02026, LR: 0.0005118, Tokens/sec: 181633.25\n",
      "Step: 193, Training Loss: 6.07419, LR: 0.0005142, Tokens/sec: 181897.48\n",
      "Step: 194, Training Loss: 6.11210, LR: 0.0005166, Tokens/sec: 181814.83\n",
      "Step: 195, Training Loss: 5.97556, LR: 0.0005190, Tokens/sec: 180548.78\n",
      "Step: 196, Training Loss: 6.13367, LR: 0.0005214, Tokens/sec: 181643.37\n",
      "Step: 197, Training Loss: 6.17362, LR: 0.0005238, Tokens/sec: 181389.42\n",
      "Step: 198, Training Loss: 6.30978, LR: 0.0005262, Tokens/sec: 181744.19\n",
      "Step: 199, Training Loss: 6.02072, LR: 0.0005286, Tokens/sec: 181347.47\n",
      "Step: 200, Training Loss: 5.87138, LR: 0.0005310, Tokens/sec: 181565.56\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 200, Eval Loss: 6.06060\n",
      "Step: 201, Training Loss: 6.29682, LR: 0.0005334, Tokens/sec: 180832.92\n",
      "Step: 202, Training Loss: 6.06510, LR: 0.0005358, Tokens/sec: 181414.73\n",
      "Step: 203, Training Loss: 6.01005, LR: 0.0005382, Tokens/sec: 181133.42\n",
      "Step: 204, Training Loss: 6.10342, LR: 0.0005406, Tokens/sec: 181997.78\n",
      "Step: 205, Training Loss: 5.83190, LR: 0.0005430, Tokens/sec: 181491.70\n",
      "Step: 206, Training Loss: 6.06744, LR: 0.0005454, Tokens/sec: 179937.46\n",
      "Step: 207, Training Loss: 5.87309, LR: 0.0005478, Tokens/sec: 181390.34\n",
      "Step: 208, Training Loss: 5.82663, LR: 0.0005503, Tokens/sec: 179400.39\n",
      "Step: 209, Training Loss: 5.79576, LR: 0.0005527, Tokens/sec: 180095.97\n",
      "Step: 210, Training Loss: 5.92354, LR: 0.0005551, Tokens/sec: 180872.44\n",
      "Step: 211, Training Loss: 6.06417, LR: 0.0005575, Tokens/sec: 180517.19\n",
      "Step: 212, Training Loss: 6.06661, LR: 0.0005599, Tokens/sec: 179392.24\n",
      "Step: 213, Training Loss: 6.08015, LR: 0.0005623, Tokens/sec: 181006.34\n",
      "Step: 214, Training Loss: 6.01917, LR: 0.0005647, Tokens/sec: 181662.91\n",
      "Step: 215, Training Loss: 5.90346, LR: 0.0005671, Tokens/sec: 180671.17\n",
      "Step: 216, Training Loss: 5.99282, LR: 0.0005695, Tokens/sec: 181578.24\n",
      "Step: 217, Training Loss: 6.04782, LR: 0.0005719, Tokens/sec: 180787.64\n",
      "Step: 218, Training Loss: 5.72471, LR: 0.0005743, Tokens/sec: 181525.35\n",
      "Step: 219, Training Loss: 6.38036, LR: 0.0005767, Tokens/sec: 180343.94\n",
      "Step: 220, Training Loss: 5.95716, LR: 0.0005791, Tokens/sec: 181109.82\n",
      "Step: 221, Training Loss: 5.98238, LR: 0.0005815, Tokens/sec: 180325.15\n",
      "Step: 222, Training Loss: 5.86013, LR: 0.0005839, Tokens/sec: 180604.68\n",
      "Step: 223, Training Loss: 5.87909, LR: 0.0005863, Tokens/sec: 180786.62\n",
      "Step: 224, Training Loss: 5.78402, LR: 0.0005887, Tokens/sec: 179318.39\n",
      "Step: 225, Training Loss: 6.01420, LR: 0.0005911, Tokens/sec: 181147.08\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 225, Eval Loss: 5.97311\n",
      "Step: 226, Training Loss: 5.80422, LR: 0.0005935, Tokens/sec: 179455.12\n",
      "Step: 227, Training Loss: 5.97061, LR: 0.0005959, Tokens/sec: 180575.30\n",
      "Step: 228, Training Loss: 5.97457, LR: 0.0005984, Tokens/sec: 181659.40\n",
      "Step: 229, Training Loss: 5.79105, LR: 0.0006008, Tokens/sec: 180610.08\n",
      "Step: 230, Training Loss: 5.88270, LR: 0.0006032, Tokens/sec: 181462.31\n",
      "Step: 231, Training Loss: 5.82582, LR: 0.0006056, Tokens/sec: 181185.19\n",
      "Step: 232, Training Loss: 5.90919, LR: 0.0006080, Tokens/sec: 180556.89\n",
      "Step: 233, Training Loss: 5.95272, LR: 0.0006104, Tokens/sec: 180852.31\n",
      "Step: 234, Training Loss: 5.85998, LR: 0.0006128, Tokens/sec: 180946.33\n",
      "Step: 235, Training Loss: 5.90337, LR: 0.0006152, Tokens/sec: 181148.36\n",
      "Step: 236, Training Loss: 5.98630, LR: 0.0006176, Tokens/sec: 172961.31\n",
      "Step: 237, Training Loss: 5.92886, LR: 0.0006200, Tokens/sec: 181016.47\n",
      "Step: 238, Training Loss: 5.63767, LR: 0.0006224, Tokens/sec: 178801.63\n",
      "Step: 239, Training Loss: 6.02380, LR: 0.0006248, Tokens/sec: 180728.68\n",
      "Step: 240, Training Loss: 5.74150, LR: 0.0006272, Tokens/sec: 181512.67\n",
      "Step: 241, Training Loss: 5.85452, LR: 0.0006296, Tokens/sec: 181301.59\n",
      "Step: 242, Training Loss: 5.59526, LR: 0.0006320, Tokens/sec: 181237.86\n",
      "Step: 243, Training Loss: 5.89405, LR: 0.0006344, Tokens/sec: 181195.99\n",
      "Step: 244, Training Loss: 6.13851, LR: 0.0006368, Tokens/sec: 181790.05\n",
      "Step: 245, Training Loss: 5.84987, LR: 0.0006392, Tokens/sec: 180804.26\n",
      "Step: 246, Training Loss: 5.72412, LR: 0.0006416, Tokens/sec: 175474.31\n",
      "Step: 247, Training Loss: 5.61593, LR: 0.0006441, Tokens/sec: 181359.52\n",
      "Step: 248, Training Loss: 5.69067, LR: 0.0006465, Tokens/sec: 181441.62\n",
      "Step: 249, Training Loss: 5.73673, LR: 0.0006489, Tokens/sec: 181663.50\n",
      "Step: 250, Training Loss: 5.68607, LR: 0.0006513, Tokens/sec: 180273.25\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 250, Eval Loss: 5.80919\n",
      "Step: 251, Training Loss: 5.86111, LR: 0.0006537, Tokens/sec: 180385.60\n",
      "Step: 252, Training Loss: 5.92378, LR: 0.0006561, Tokens/sec: 179962.26\n",
      "Step: 253, Training Loss: 5.65623, LR: 0.0006585, Tokens/sec: 181596.75\n",
      "Step: 254, Training Loss: 5.87156, LR: 0.0006609, Tokens/sec: 181164.88\n",
      "Step: 255, Training Loss: 5.72339, LR: 0.0006633, Tokens/sec: 181306.42\n",
      "Step: 256, Training Loss: 5.61611, LR: 0.0006657, Tokens/sec: 181126.14\n",
      "Step: 257, Training Loss: 5.80323, LR: 0.0006681, Tokens/sec: 181387.15\n",
      "Step: 258, Training Loss: 5.66290, LR: 0.0006705, Tokens/sec: 180723.09\n",
      "Step: 259, Training Loss: 5.83331, LR: 0.0006729, Tokens/sec: 181999.96\n",
      "Step: 260, Training Loss: 5.56352, LR: 0.0006753, Tokens/sec: 181479.50\n",
      "Step: 261, Training Loss: 5.64025, LR: 0.0006777, Tokens/sec: 181294.67\n",
      "Step: 262, Training Loss: 5.56830, LR: 0.0006801, Tokens/sec: 181754.74\n",
      "Step: 263, Training Loss: 5.59174, LR: 0.0006825, Tokens/sec: 181549.35\n",
      "Step: 264, Training Loss: 5.67230, LR: 0.0006849, Tokens/sec: 180720.65\n",
      "Step: 265, Training Loss: 5.66380, LR: 0.0006873, Tokens/sec: 181623.81\n",
      "Step: 266, Training Loss: 5.50772, LR: 0.0006897, Tokens/sec: 181884.43\n",
      "Step: 267, Training Loss: 5.54060, LR: 0.0006922, Tokens/sec: 181522.28\n",
      "Step: 268, Training Loss: 5.68976, LR: 0.0006946, Tokens/sec: 181617.03\n",
      "Step: 269, Training Loss: 5.62825, LR: 0.0006970, Tokens/sec: 182196.86\n",
      "Step: 270, Training Loss: 5.51074, LR: 0.0006994, Tokens/sec: 180549.93\n",
      "Step: 271, Training Loss: 5.90715, LR: 0.0007018, Tokens/sec: 181545.97\n",
      "Step: 272, Training Loss: 5.56081, LR: 0.0007042, Tokens/sec: 181665.54\n",
      "Step: 273, Training Loss: 5.71235, LR: 0.0007066, Tokens/sec: 182284.65\n",
      "Step: 274, Training Loss: 5.66389, LR: 0.0007090, Tokens/sec: 180793.40\n",
      "Step: 275, Training Loss: 5.88421, LR: 0.0007114, Tokens/sec: 181096.40\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 275, Eval Loss: 5.66508\n",
      "Step: 276, Training Loss: 5.67348, LR: 0.0007138, Tokens/sec: 180678.68\n",
      "Step: 277, Training Loss: 5.45787, LR: 0.0007162, Tokens/sec: 181499.15\n",
      "Step: 278, Training Loss: 5.57357, LR: 0.0007186, Tokens/sec: 181648.19\n",
      "Step: 279, Training Loss: 5.79698, LR: 0.0007210, Tokens/sec: 181612.62\n",
      "Step: 280, Training Loss: 5.57691, LR: 0.0007234, Tokens/sec: 181888.75\n",
      "Step: 281, Training Loss: 5.87718, LR: 0.0007258, Tokens/sec: 180332.13\n",
      "Step: 282, Training Loss: 5.65398, LR: 0.0007282, Tokens/sec: 181808.78\n",
      "Step: 283, Training Loss: 5.63002, LR: 0.0007306, Tokens/sec: 181873.22\n",
      "Step: 284, Training Loss: 5.60349, LR: 0.0007330, Tokens/sec: 181607.15\n",
      "Step: 285, Training Loss: 5.69311, LR: 0.0007354, Tokens/sec: 181722.84\n",
      "Step: 286, Training Loss: 5.76377, LR: 0.0007378, Tokens/sec: 181877.08\n",
      "Step: 287, Training Loss: 5.89727, LR: 0.0007403, Tokens/sec: 180357.15\n",
      "Step: 288, Training Loss: 5.53660, LR: 0.0007427, Tokens/sec: 182170.45\n",
      "Step: 289, Training Loss: 5.63481, LR: 0.0007451, Tokens/sec: 181595.03\n",
      "Step: 290, Training Loss: 5.76721, LR: 0.0007475, Tokens/sec: 181466.02\n",
      "Step: 291, Training Loss: 5.64464, LR: 0.0007499, Tokens/sec: 181088.53\n",
      "Step: 292, Training Loss: 5.59684, LR: 0.0007523, Tokens/sec: 181457.04\n",
      "Step: 293, Training Loss: 5.79863, LR: 0.0007547, Tokens/sec: 180440.87\n",
      "Step: 294, Training Loss: 5.86139, LR: 0.0007571, Tokens/sec: 181583.70\n",
      "Step: 295, Training Loss: 5.72380, LR: 0.0007595, Tokens/sec: 181369.07\n",
      "Step: 296, Training Loss: 5.60705, LR: 0.0007619, Tokens/sec: 181314.66\n",
      "Step: 297, Training Loss: 5.47259, LR: 0.0007643, Tokens/sec: 181218.03\n",
      "Step: 298, Training Loss: 5.53820, LR: 0.0007667, Tokens/sec: 182122.25\n",
      "Step: 299, Training Loss: 5.46921, LR: 0.0007691, Tokens/sec: 180236.92\n",
      "Step: 300, Training Loss: 5.80072, LR: 0.0007715, Tokens/sec: 179485.23\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 300, Eval Loss: 5.59501\n",
      "Step: 301, Training Loss: 5.52562, LR: 0.0007739, Tokens/sec: 176136.25\n",
      "Step: 302, Training Loss: 5.41604, LR: 0.0007763, Tokens/sec: 179627.06\n",
      "Step: 303, Training Loss: 5.57702, LR: 0.0007787, Tokens/sec: 180558.78\n",
      "Step: 304, Training Loss: 5.70392, LR: 0.0007811, Tokens/sec: 176658.75\n",
      "Step: 305, Training Loss: 5.75023, LR: 0.0007835, Tokens/sec: 180738.27\n",
      "Step: 306, Training Loss: 5.52892, LR: 0.0007859, Tokens/sec: 179449.36\n",
      "Step: 307, Training Loss: 5.38664, LR: 0.0007884, Tokens/sec: 179567.01\n",
      "Step: 308, Training Loss: 5.48216, LR: 0.0007908, Tokens/sec: 180523.48\n",
      "Step: 309, Training Loss: 5.39970, LR: 0.0007932, Tokens/sec: 181200.13\n",
      "Step: 310, Training Loss: 5.37360, LR: 0.0007956, Tokens/sec: 180670.04\n",
      "Step: 311, Training Loss: 5.38655, LR: 0.0007980, Tokens/sec: 180793.01\n",
      "Step: 312, Training Loss: 5.56329, LR: 0.0008004, Tokens/sec: 181700.37\n",
      "Step: 313, Training Loss: 5.42015, LR: 0.0008028, Tokens/sec: 174146.93\n",
      "Step: 314, Training Loss: 5.58848, LR: 0.0008052, Tokens/sec: 176032.29\n",
      "Step: 315, Training Loss: 5.64914, LR: 0.0008076, Tokens/sec: 180310.95\n",
      "Step: 316, Training Loss: 5.67965, LR: 0.0008100, Tokens/sec: 181173.71\n",
      "Step: 317, Training Loss: 5.49366, LR: 0.0008124, Tokens/sec: 181036.46\n",
      "Step: 318, Training Loss: 5.55129, LR: 0.0008148, Tokens/sec: 181007.91\n",
      "Step: 319, Training Loss: 5.36710, LR: 0.0008172, Tokens/sec: 179010.36\n",
      "Step: 320, Training Loss: 5.51829, LR: 0.0008196, Tokens/sec: 181301.12\n",
      "Step: 321, Training Loss: 5.65051, LR: 0.0008220, Tokens/sec: 179322.87\n",
      "Step: 322, Training Loss: 5.63665, LR: 0.0008244, Tokens/sec: 181942.20\n",
      "Step: 323, Training Loss: 5.36782, LR: 0.0008268, Tokens/sec: 181693.24\n",
      "Step: 324, Training Loss: 5.28662, LR: 0.0008292, Tokens/sec: 181342.77\n",
      "Step: 325, Training Loss: 5.54583, LR: 0.0008316, Tokens/sec: 180059.74\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 325, Eval Loss: 5.47471\n",
      "Step: 326, Training Loss: 5.46048, LR: 0.0008341, Tokens/sec: 181461.21\n",
      "Step: 327, Training Loss: 5.08647, LR: 0.0008365, Tokens/sec: 179712.58\n",
      "Step: 328, Training Loss: 5.46092, LR: 0.0008389, Tokens/sec: 181134.50\n",
      "Step: 329, Training Loss: 5.50943, LR: 0.0008413, Tokens/sec: 181209.61\n",
      "Step: 330, Training Loss: 5.53837, LR: 0.0008437, Tokens/sec: 181193.89\n",
      "Step: 331, Training Loss: 5.22661, LR: 0.0008461, Tokens/sec: 181737.58\n",
      "Step: 332, Training Loss: 5.41947, LR: 0.0008485, Tokens/sec: 181696.01\n",
      "Step: 333, Training Loss: 5.39876, LR: 0.0008509, Tokens/sec: 180258.78\n",
      "Step: 334, Training Loss: 5.66151, LR: 0.0008533, Tokens/sec: 181821.44\n",
      "Step: 335, Training Loss: 5.55936, LR: 0.0008557, Tokens/sec: 181255.28\n",
      "Step: 336, Training Loss: 5.49017, LR: 0.0008581, Tokens/sec: 181731.03\n",
      "Step: 337, Training Loss: 5.26417, LR: 0.0008605, Tokens/sec: 181329.44\n",
      "Step: 338, Training Loss: 5.43670, LR: 0.0008629, Tokens/sec: 181260.63\n",
      "Step: 339, Training Loss: 5.46010, LR: 0.0008653, Tokens/sec: 180156.66\n",
      "Step: 340, Training Loss: 5.09523, LR: 0.0008677, Tokens/sec: 178251.96\n",
      "Step: 341, Training Loss: 5.27021, LR: 0.0008701, Tokens/sec: 181026.48\n",
      "Step: 342, Training Loss: 5.27696, LR: 0.0008725, Tokens/sec: 181364.70\n",
      "Step: 343, Training Loss: 5.39643, LR: 0.0008749, Tokens/sec: 166818.37\n",
      "Step: 344, Training Loss: 5.21668, LR: 0.0008773, Tokens/sec: 179419.15\n",
      "Step: 345, Training Loss: 5.44250, LR: 0.0008797, Tokens/sec: 179586.31\n",
      "Step: 346, Training Loss: 5.46671, LR: 0.0008822, Tokens/sec: 181125.29\n",
      "Step: 347, Training Loss: 5.64076, LR: 0.0008846, Tokens/sec: 180991.87\n",
      "Step: 348, Training Loss: 5.17278, LR: 0.0008870, Tokens/sec: 181278.45\n",
      "Step: 349, Training Loss: 5.39043, LR: 0.0008894, Tokens/sec: 181511.09\n",
      "Step: 350, Training Loss: 5.50084, LR: 0.0008918, Tokens/sec: 181335.71\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 350, Eval Loss: 5.35253\n",
      "Step: 351, Training Loss: 5.23275, LR: 0.0008942, Tokens/sec: 180406.83\n",
      "Step: 352, Training Loss: 5.38863, LR: 0.0008966, Tokens/sec: 181539.80\n",
      "Step: 353, Training Loss: 5.45096, LR: 0.0008990, Tokens/sec: 181299.89\n",
      "Step: 354, Training Loss: 5.12669, LR: 0.0009014, Tokens/sec: 181621.06\n",
      "Step: 355, Training Loss: 5.43090, LR: 0.0009038, Tokens/sec: 181873.22\n",
      "Step: 356, Training Loss: 5.39353, LR: 0.0009062, Tokens/sec: 180846.70\n",
      "Step: 357, Training Loss: 5.34503, LR: 0.0009086, Tokens/sec: 180593.36\n",
      "Step: 358, Training Loss: 5.14975, LR: 0.0009110, Tokens/sec: 181464.26\n",
      "Step: 359, Training Loss: 5.25263, LR: 0.0009134, Tokens/sec: 181096.65\n",
      "Step: 360, Training Loss: 5.24932, LR: 0.0009158, Tokens/sec: 181296.76\n",
      "Step: 361, Training Loss: 5.15776, LR: 0.0009182, Tokens/sec: 181612.98\n",
      "Step: 362, Training Loss: 5.26773, LR: 0.0009206, Tokens/sec: 179772.16\n",
      "Step: 363, Training Loss: 5.39128, LR: 0.0009230, Tokens/sec: 181245.85\n",
      "Step: 364, Training Loss: 5.37993, LR: 0.0009254, Tokens/sec: 181413.09\n",
      "Step: 365, Training Loss: 5.43662, LR: 0.0009278, Tokens/sec: 181239.03\n",
      "Step: 366, Training Loss: 5.40926, LR: 0.0009303, Tokens/sec: 181512.68\n",
      "Step: 367, Training Loss: 5.49533, LR: 0.0009327, Tokens/sec: 181080.03\n",
      "Step: 368, Training Loss: 5.60479, LR: 0.0009351, Tokens/sec: 180576.03\n",
      "Step: 369, Training Loss: 6.97155, LR: 0.0009375, Tokens/sec: 181092.37\n",
      "Step: 370, Training Loss: 5.57881, LR: 0.0009399, Tokens/sec: 181088.04\n",
      "Step: 371, Training Loss: 5.45171, LR: 0.0009423, Tokens/sec: 182116.03\n",
      "Step: 372, Training Loss: 5.18900, LR: 0.0009447, Tokens/sec: 181910.07\n",
      "Step: 373, Training Loss: 5.48006, LR: 0.0009471, Tokens/sec: 181858.82\n",
      "Step: 374, Training Loss: 5.25969, LR: 0.0009495, Tokens/sec: 180145.38\n",
      "Step: 375, Training Loss: 5.33076, LR: 0.0009519, Tokens/sec: 181125.18\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 375, Eval Loss: 5.34346\n",
      "Step: 376, Training Loss: 5.28503, LR: 0.0009543, Tokens/sec: 180737.75\n",
      "Step: 377, Training Loss: 5.31216, LR: 0.0009567, Tokens/sec: 180887.24\n",
      "Step: 378, Training Loss: 5.23446, LR: 0.0009591, Tokens/sec: 181244.68\n",
      "Step: 379, Training Loss: 5.23642, LR: 0.0009615, Tokens/sec: 181035.32\n",
      "Step: 380, Training Loss: 5.21004, LR: 0.0009639, Tokens/sec: 181867.01\n",
      "Step: 381, Training Loss: 5.09386, LR: 0.0009663, Tokens/sec: 180967.13\n",
      "Step: 382, Training Loss: 5.27520, LR: 0.0009687, Tokens/sec: 180016.32\n",
      "Step: 383, Training Loss: 5.17025, LR: 0.0009711, Tokens/sec: 180989.20\n",
      "Step: 384, Training Loss: 5.09199, LR: 0.0009735, Tokens/sec: 181408.68\n",
      "Step: 385, Training Loss: 5.25833, LR: 0.0009759, Tokens/sec: 181598.41\n",
      "Step: 386, Training Loss: 5.15579, LR: 0.0009784, Tokens/sec: 180917.60\n",
      "Step: 387, Training Loss: 5.08989, LR: 0.0009808, Tokens/sec: 181551.95\n",
      "Step: 388, Training Loss: 5.07077, LR: 0.0009832, Tokens/sec: 180283.84\n",
      "Step: 389, Training Loss: 5.21045, LR: 0.0009856, Tokens/sec: 182313.79\n",
      "Step: 390, Training Loss: 5.07639, LR: 0.0009880, Tokens/sec: 182137.03\n",
      "Step: 391, Training Loss: 5.20521, LR: 0.0009904, Tokens/sec: 181848.85\n",
      "Step: 392, Training Loss: 5.11269, LR: 0.0009928, Tokens/sec: 181716.91\n",
      "Step: 393, Training Loss: 5.39910, LR: 0.0009952, Tokens/sec: 181725.64\n",
      "Step: 394, Training Loss: 5.32713, LR: 0.0009976, Tokens/sec: 180401.38\n",
      "Step: 395, Training Loss: 5.37333, LR: 0.0010000, Tokens/sec: 181023.82\n",
      "Step: 396, Training Loss: 5.42410, LR: 0.0010000, Tokens/sec: 181516.86\n",
      "Step: 397, Training Loss: 5.24652, LR: 0.0010000, Tokens/sec: 181259.44\n",
      "Step: 398, Training Loss: 5.23400, LR: 0.0010000, Tokens/sec: 181289.76\n",
      "Step: 399, Training Loss: 5.27646, LR: 0.0010000, Tokens/sec: 181518.71\n",
      "Step: 400, Training Loss: 5.23607, LR: 0.0010000, Tokens/sec: 180201.72\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 400, Eval Loss: 5.22987\n",
      "Step: 401, Training Loss: 5.24585, LR: 0.0010000, Tokens/sec: 180867.19\n",
      "Step: 402, Training Loss: 5.11012, LR: 0.0010000, Tokens/sec: 180086.85\n",
      "Step: 403, Training Loss: 5.23118, LR: 0.0010000, Tokens/sec: 180956.43\n",
      "Step: 404, Training Loss: 5.02906, LR: 0.0010000, Tokens/sec: 181306.96\n",
      "Step: 405, Training Loss: 5.28905, LR: 0.0010000, Tokens/sec: 181797.29\n",
      "Step: 406, Training Loss: 5.05994, LR: 0.0010000, Tokens/sec: 181548.13\n",
      "Step: 407, Training Loss: 5.34406, LR: 0.0010000, Tokens/sec: 181886.88\n",
      "Step: 408, Training Loss: 5.30510, LR: 0.0010000, Tokens/sec: 180169.04\n",
      "Step: 409, Training Loss: 4.94944, LR: 0.0010000, Tokens/sec: 181489.71\n",
      "Step: 410, Training Loss: 4.84634, LR: 0.0010000, Tokens/sec: 181225.95\n",
      "Step: 411, Training Loss: 5.22478, LR: 0.0010000, Tokens/sec: 181472.01\n",
      "Step: 412, Training Loss: 5.23087, LR: 0.0009999, Tokens/sec: 181585.94\n",
      "Step: 413, Training Loss: 5.12841, LR: 0.0009999, Tokens/sec: 181309.66\n",
      "Step: 414, Training Loss: 5.29032, LR: 0.0009999, Tokens/sec: 180511.14\n",
      "Step: 415, Training Loss: 5.19833, LR: 0.0009999, Tokens/sec: 181245.94\n",
      "Step: 416, Training Loss: 4.97209, LR: 0.0009999, Tokens/sec: 181810.89\n",
      "Step: 417, Training Loss: 5.31547, LR: 0.0009999, Tokens/sec: 181500.82\n",
      "Step: 418, Training Loss: 5.22069, LR: 0.0009999, Tokens/sec: 181922.37\n",
      "Step: 419, Training Loss: 5.09130, LR: 0.0009999, Tokens/sec: 181337.78\n",
      "Step: 420, Training Loss: 5.14286, LR: 0.0009999, Tokens/sec: 180356.55\n",
      "Step: 421, Training Loss: 5.13147, LR: 0.0009999, Tokens/sec: 181806.92\n",
      "Step: 422, Training Loss: 4.89008, LR: 0.0009999, Tokens/sec: 181735.49\n",
      "Step: 423, Training Loss: 5.04577, LR: 0.0009999, Tokens/sec: 181424.67\n",
      "Step: 424, Training Loss: 5.19337, LR: 0.0009999, Tokens/sec: 181408.63\n",
      "Step: 425, Training Loss: 5.19353, LR: 0.0009998, Tokens/sec: 181233.47\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 425, Eval Loss: 5.12788\n",
      "Step: 426, Training Loss: 4.86427, LR: 0.0009998, Tokens/sec: 179454.73\n",
      "Step: 427, Training Loss: 5.27299, LR: 0.0009998, Tokens/sec: 181135.56\n",
      "Step: 428, Training Loss: 5.08359, LR: 0.0009998, Tokens/sec: 181644.06\n",
      "Step: 429, Training Loss: 5.08989, LR: 0.0009998, Tokens/sec: 180670.76\n",
      "Step: 430, Training Loss: 4.89668, LR: 0.0009998, Tokens/sec: 180756.22\n",
      "Step: 431, Training Loss: 5.02516, LR: 0.0009998, Tokens/sec: 179660.94\n",
      "Step: 432, Training Loss: 5.06161, LR: 0.0009998, Tokens/sec: 181316.25\n",
      "Step: 433, Training Loss: 5.08216, LR: 0.0009997, Tokens/sec: 181416.27\n",
      "Step: 434, Training Loss: 5.05174, LR: 0.0009997, Tokens/sec: 181316.66\n",
      "Step: 435, Training Loss: 5.12765, LR: 0.0009997, Tokens/sec: 181502.07\n",
      "Step: 436, Training Loss: 5.06004, LR: 0.0009997, Tokens/sec: 181585.98\n",
      "Step: 437, Training Loss: 4.89692, LR: 0.0009997, Tokens/sec: 180569.92\n",
      "Step: 438, Training Loss: 5.06183, LR: 0.0009997, Tokens/sec: 181422.70\n",
      "Step: 439, Training Loss: 5.09370, LR: 0.0009997, Tokens/sec: 181453.60\n",
      "Step: 440, Training Loss: 4.92042, LR: 0.0009996, Tokens/sec: 181026.04\n",
      "Step: 441, Training Loss: 5.28731, LR: 0.0009996, Tokens/sec: 181116.75\n",
      "Step: 442, Training Loss: 5.02234, LR: 0.0009996, Tokens/sec: 181145.27\n",
      "Step: 443, Training Loss: 5.37771, LR: 0.0009996, Tokens/sec: 179963.40\n",
      "Step: 444, Training Loss: 5.02458, LR: 0.0009996, Tokens/sec: 181268.23\n",
      "Step: 445, Training Loss: 4.76608, LR: 0.0009996, Tokens/sec: 180980.39\n",
      "Step: 446, Training Loss: 5.12937, LR: 0.0009995, Tokens/sec: 181237.65\n",
      "Step: 447, Training Loss: 5.06674, LR: 0.0009995, Tokens/sec: 180837.16\n",
      "Step: 448, Training Loss: 5.01617, LR: 0.0009995, Tokens/sec: 181108.59\n",
      "Step: 449, Training Loss: 5.19059, LR: 0.0009995, Tokens/sec: 179468.63\n",
      "Step: 450, Training Loss: 5.00740, LR: 0.0009995, Tokens/sec: 181443.48\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 450, Eval Loss: 5.06181\n",
      "Step: 451, Training Loss: 5.31741, LR: 0.0009994, Tokens/sec: 180704.38\n",
      "Step: 452, Training Loss: 5.05809, LR: 0.0009994, Tokens/sec: 180552.80\n",
      "Step: 453, Training Loss: 4.96273, LR: 0.0009994, Tokens/sec: 178759.79\n",
      "Step: 454, Training Loss: 5.09682, LR: 0.0009994, Tokens/sec: 179786.20\n",
      "Step: 455, Training Loss: 5.13859, LR: 0.0009994, Tokens/sec: 180682.98\n",
      "Step: 456, Training Loss: 5.00224, LR: 0.0009993, Tokens/sec: 180685.28\n",
      "Step: 457, Training Loss: 5.03599, LR: 0.0009993, Tokens/sec: 179309.93\n",
      "Step: 458, Training Loss: 4.95632, LR: 0.0009993, Tokens/sec: 181447.53\n",
      "Step: 459, Training Loss: 5.09847, LR: 0.0009993, Tokens/sec: 181230.36\n",
      "Step: 460, Training Loss: 5.00110, LR: 0.0009993, Tokens/sec: 180634.57\n",
      "Step: 461, Training Loss: 5.08134, LR: 0.0009992, Tokens/sec: 181115.76\n",
      "Step: 462, Training Loss: 4.94302, LR: 0.0009992, Tokens/sec: 181188.46\n",
      "Step: 463, Training Loss: 5.05112, LR: 0.0009992, Tokens/sec: 179796.45\n",
      "Step: 464, Training Loss: 4.95975, LR: 0.0009992, Tokens/sec: 181210.54\n",
      "Step: 465, Training Loss: 5.08929, LR: 0.0009991, Tokens/sec: 181280.45\n",
      "Step: 466, Training Loss: 4.97633, LR: 0.0009991, Tokens/sec: 181196.17\n",
      "Step: 467, Training Loss: 5.08801, LR: 0.0009991, Tokens/sec: 181013.20\n",
      "Step: 468, Training Loss: 5.22425, LR: 0.0009991, Tokens/sec: 181465.71\n",
      "Step: 469, Training Loss: 4.92723, LR: 0.0009990, Tokens/sec: 180394.23\n",
      "Step: 470, Training Loss: 5.20432, LR: 0.0009990, Tokens/sec: 181237.25\n",
      "Step: 471, Training Loss: 5.02670, LR: 0.0009990, Tokens/sec: 180829.89\n",
      "Step: 472, Training Loss: 5.05617, LR: 0.0009990, Tokens/sec: 181369.27\n",
      "Step: 473, Training Loss: 5.05437, LR: 0.0009989, Tokens/sec: 181506.66\n",
      "Step: 474, Training Loss: 4.90395, LR: 0.0009989, Tokens/sec: 181468.55\n",
      "Step: 475, Training Loss: 4.81096, LR: 0.0009989, Tokens/sec: 180304.52\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 475, Eval Loss: 4.95085\n",
      "Step: 476, Training Loss: 5.11483, LR: 0.0009988, Tokens/sec: 181565.70\n",
      "Step: 477, Training Loss: 5.13635, LR: 0.0009988, Tokens/sec: 180202.50\n",
      "Step: 478, Training Loss: 5.08049, LR: 0.0009988, Tokens/sec: 180650.98\n",
      "Step: 479, Training Loss: 4.97848, LR: 0.0009988, Tokens/sec: 171731.54\n",
      "Step: 480, Training Loss: 5.10922, LR: 0.0009987, Tokens/sec: 181294.37\n",
      "Step: 481, Training Loss: 5.07217, LR: 0.0009987, Tokens/sec: 181068.49\n",
      "Step: 482, Training Loss: 5.23880, LR: 0.0009987, Tokens/sec: 181188.23\n",
      "Step: 483, Training Loss: 5.26375, LR: 0.0009986, Tokens/sec: 180350.16\n",
      "Step: 484, Training Loss: 5.08746, LR: 0.0009986, Tokens/sec: 180825.85\n",
      "Step: 485, Training Loss: 5.06380, LR: 0.0009986, Tokens/sec: 181319.04\n",
      "Step: 486, Training Loss: 5.01769, LR: 0.0009985, Tokens/sec: 181663.48\n",
      "Step: 487, Training Loss: 4.91249, LR: 0.0009985, Tokens/sec: 181307.20\n",
      "Step: 488, Training Loss: 5.04960, LR: 0.0009985, Tokens/sec: 181484.47\n",
      "Step: 489, Training Loss: 4.92278, LR: 0.0009985, Tokens/sec: 180188.04\n",
      "Step: 490, Training Loss: 4.97784, LR: 0.0009984, Tokens/sec: 181353.77\n",
      "Step: 491, Training Loss: 5.25162, LR: 0.0009984, Tokens/sec: 181786.50\n",
      "Step: 492, Training Loss: 4.73274, LR: 0.0009983, Tokens/sec: 181723.06\n",
      "Step: 493, Training Loss: 5.01674, LR: 0.0009983, Tokens/sec: 176152.51\n",
      "Step: 494, Training Loss: 4.98614, LR: 0.0009983, Tokens/sec: 181805.91\n",
      "Step: 495, Training Loss: 5.05043, LR: 0.0009982, Tokens/sec: 180020.69\n",
      "Step: 496, Training Loss: 5.25863, LR: 0.0009982, Tokens/sec: 181450.96\n",
      "Step: 497, Training Loss: 5.13685, LR: 0.0009982, Tokens/sec: 180993.64\n",
      "Step: 498, Training Loss: 4.88033, LR: 0.0009981, Tokens/sec: 181318.70\n",
      "Step: 499, Training Loss: 5.15002, LR: 0.0009981, Tokens/sec: 181950.53\n",
      "Step: 500, Training Loss: 4.85697, LR: 0.0009981, Tokens/sec: 181950.47\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 500, Eval Loss: 4.92847\n",
      "Step: 501, Training Loss: 4.73238, LR: 0.0009980, Tokens/sec: 180304.54\n",
      "Step: 502, Training Loss: 4.74055, LR: 0.0009980, Tokens/sec: 181098.68\n",
      "Step: 503, Training Loss: 4.93417, LR: 0.0009980, Tokens/sec: 181006.69\n",
      "Step: 504, Training Loss: 4.79235, LR: 0.0009979, Tokens/sec: 181570.92\n",
      "Step: 505, Training Loss: 5.02857, LR: 0.0009979, Tokens/sec: 181164.95\n",
      "Step: 506, Training Loss: 4.74100, LR: 0.0009978, Tokens/sec: 180081.10\n",
      "Step: 507, Training Loss: 5.16641, LR: 0.0009978, Tokens/sec: 181919.28\n",
      "Step: 508, Training Loss: 5.12233, LR: 0.0009978, Tokens/sec: 181107.39\n",
      "Step: 509, Training Loss: 4.84231, LR: 0.0009977, Tokens/sec: 181325.94\n",
      "Step: 510, Training Loss: 4.86995, LR: 0.0009977, Tokens/sec: 181060.85\n",
      "Step: 511, Training Loss: 5.02645, LR: 0.0009976, Tokens/sec: 180961.58\n",
      "Step: 512, Training Loss: 4.80591, LR: 0.0009976, Tokens/sec: 180467.97\n",
      "Step: 513, Training Loss: 5.20345, LR: 0.0009976, Tokens/sec: 181037.76\n",
      "Step: 514, Training Loss: 4.99400, LR: 0.0009975, Tokens/sec: 181955.82\n",
      "Step: 515, Training Loss: 5.10679, LR: 0.0009975, Tokens/sec: 181146.00\n",
      "Step: 516, Training Loss: 4.93293, LR: 0.0009974, Tokens/sec: 181891.12\n",
      "Step: 517, Training Loss: 4.99339, LR: 0.0009974, Tokens/sec: 181564.21\n",
      "Step: 518, Training Loss: 4.95396, LR: 0.0009973, Tokens/sec: 180104.87\n",
      "Step: 519, Training Loss: 4.96541, LR: 0.0009973, Tokens/sec: 181204.17\n",
      "Step: 520, Training Loss: 4.86769, LR: 0.0009973, Tokens/sec: 181571.27\n",
      "Step: 521, Training Loss: 4.75094, LR: 0.0009972, Tokens/sec: 181992.63\n",
      "Step: 522, Training Loss: 5.08535, LR: 0.0009972, Tokens/sec: 181086.22\n",
      "Step: 523, Training Loss: 4.98339, LR: 0.0009971, Tokens/sec: 181101.26\n",
      "Step: 524, Training Loss: 4.87543, LR: 0.0009971, Tokens/sec: 180119.00\n",
      "Step: 525, Training Loss: 4.72089, LR: 0.0009970, Tokens/sec: 180923.79\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 525, Eval Loss: 4.87901\n",
      "Step: 526, Training Loss: 4.76364, LR: 0.0009970, Tokens/sec: 180252.61\n",
      "Step: 527, Training Loss: 4.96230, LR: 0.0009969, Tokens/sec: 180775.88\n",
      "Step: 528, Training Loss: 4.75689, LR: 0.0009969, Tokens/sec: 181361.89\n",
      "Step: 529, Training Loss: 4.86438, LR: 0.0009969, Tokens/sec: 181303.63\n",
      "Step: 530, Training Loss: 4.90040, LR: 0.0009968, Tokens/sec: 181111.55\n",
      "Step: 531, Training Loss: 4.78373, LR: 0.0009968, Tokens/sec: 181614.55\n",
      "Step: 532, Training Loss: 4.92032, LR: 0.0009967, Tokens/sec: 180221.42\n",
      "Step: 533, Training Loss: 4.87905, LR: 0.0009967, Tokens/sec: 180970.35\n",
      "Step: 534, Training Loss: 4.72832, LR: 0.0009966, Tokens/sec: 181307.51\n",
      "Step: 535, Training Loss: 4.90417, LR: 0.0009966, Tokens/sec: 180943.68\n",
      "Step: 536, Training Loss: 4.88980, LR: 0.0009965, Tokens/sec: 181389.48\n",
      "Step: 537, Training Loss: 5.14874, LR: 0.0009965, Tokens/sec: 181492.23\n",
      "Step: 538, Training Loss: 4.56614, LR: 0.0009964, Tokens/sec: 180205.69\n",
      "Step: 539, Training Loss: 4.66179, LR: 0.0009964, Tokens/sec: 181225.41\n",
      "Step: 540, Training Loss: 4.60576, LR: 0.0009963, Tokens/sec: 181096.66\n",
      "Step: 541, Training Loss: 4.90679, LR: 0.0009963, Tokens/sec: 181359.69\n",
      "Step: 542, Training Loss: 4.88894, LR: 0.0009962, Tokens/sec: 181661.05\n",
      "Step: 543, Training Loss: 4.75869, LR: 0.0009962, Tokens/sec: 181941.08\n",
      "Step: 544, Training Loss: 4.76945, LR: 0.0009961, Tokens/sec: 180784.06\n",
      "Step: 545, Training Loss: 4.77794, LR: 0.0009961, Tokens/sec: 181372.80\n",
      "Step: 546, Training Loss: 4.76283, LR: 0.0009960, Tokens/sec: 181267.75\n",
      "Step: 547, Training Loss: 4.89698, LR: 0.0009960, Tokens/sec: 181858.24\n",
      "Step: 548, Training Loss: 4.63604, LR: 0.0009959, Tokens/sec: 181792.69\n",
      "Step: 549, Training Loss: 4.96727, LR: 0.0009958, Tokens/sec: 181756.68\n",
      "Step: 550, Training Loss: 4.71489, LR: 0.0009958, Tokens/sec: 180234.48\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 550, Eval Loss: 4.89263\n",
      "Step: 551, Training Loss: 4.75351, LR: 0.0009957, Tokens/sec: 173581.58\n",
      "Step: 552, Training Loss: 4.68866, LR: 0.0009957, Tokens/sec: 177170.76\n",
      "Step: 553, Training Loss: 4.56540, LR: 0.0009956, Tokens/sec: 180265.27\n",
      "Step: 554, Training Loss: 4.68047, LR: 0.0009956, Tokens/sec: 180274.95\n",
      "Step: 555, Training Loss: 4.77481, LR: 0.0009955, Tokens/sec: 174373.34\n",
      "Step: 556, Training Loss: 5.00995, LR: 0.0009955, Tokens/sec: 180729.15\n",
      "Step: 557, Training Loss: 4.61932, LR: 0.0009954, Tokens/sec: 180816.84\n",
      "Step: 558, Training Loss: 5.09719, LR: 0.0009953, Tokens/sec: 178774.77\n",
      "Step: 559, Training Loss: 4.60468, LR: 0.0009953, Tokens/sec: 179838.80\n",
      "Step: 560, Training Loss: 4.87358, LR: 0.0009952, Tokens/sec: 179972.47\n",
      "Step: 561, Training Loss: 4.82520, LR: 0.0009952, Tokens/sec: 180102.12\n",
      "Step: 562, Training Loss: 4.77269, LR: 0.0009951, Tokens/sec: 181226.68\n",
      "Step: 563, Training Loss: 4.69504, LR: 0.0009951, Tokens/sec: 179467.19\n",
      "Step: 564, Training Loss: 4.92651, LR: 0.0009950, Tokens/sec: 178590.21\n",
      "Step: 565, Training Loss: 4.95529, LR: 0.0009949, Tokens/sec: 180933.52\n",
      "Step: 566, Training Loss: 4.79508, LR: 0.0009949, Tokens/sec: 181621.79\n",
      "Step: 567, Training Loss: 4.59076, LR: 0.0009948, Tokens/sec: 181162.73\n",
      "Step: 568, Training Loss: 4.97164, LR: 0.0009948, Tokens/sec: 181966.04\n",
      "Step: 569, Training Loss: 4.30713, LR: 0.0009947, Tokens/sec: 181382.19\n",
      "Step: 570, Training Loss: 4.76226, LR: 0.0009946, Tokens/sec: 180228.67\n",
      "Step: 571, Training Loss: 4.83048, LR: 0.0009946, Tokens/sec: 181345.91\n",
      "Step: 572, Training Loss: 4.68280, LR: 0.0009945, Tokens/sec: 181712.00\n",
      "Step: 573, Training Loss: 4.78691, LR: 0.0009945, Tokens/sec: 181513.44\n",
      "Step: 574, Training Loss: 4.73906, LR: 0.0009944, Tokens/sec: 182059.18\n",
      "Step: 575, Training Loss: 5.09600, LR: 0.0009943, Tokens/sec: 181137.57\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 575, Eval Loss: 4.82273\n",
      "Step: 576, Training Loss: 4.87640, LR: 0.0009943, Tokens/sec: 180378.82\n",
      "Step: 577, Training Loss: 4.63279, LR: 0.0009942, Tokens/sec: 181410.46\n",
      "Step: 578, Training Loss: 4.38727, LR: 0.0009941, Tokens/sec: 181233.93\n",
      "Step: 579, Training Loss: 4.96684, LR: 0.0009941, Tokens/sec: 181472.76\n",
      "Step: 580, Training Loss: 4.78843, LR: 0.0009940, Tokens/sec: 181894.93\n",
      "Step: 581, Training Loss: 4.73104, LR: 0.0009939, Tokens/sec: 180246.97\n",
      "Step: 582, Training Loss: 4.88787, LR: 0.0009939, Tokens/sec: 181597.33\n",
      "Step: 583, Training Loss: 4.86742, LR: 0.0009938, Tokens/sec: 181751.20\n",
      "Step: 584, Training Loss: 4.60968, LR: 0.0009937, Tokens/sec: 181170.95\n",
      "Step: 585, Training Loss: 4.79466, LR: 0.0009937, Tokens/sec: 182140.98\n",
      "Step: 586, Training Loss: 4.76150, LR: 0.0009936, Tokens/sec: 181819.38\n",
      "Step: 587, Training Loss: 4.80656, LR: 0.0009935, Tokens/sec: 179620.64\n",
      "Step: 588, Training Loss: 5.06323, LR: 0.0009935, Tokens/sec: 181400.56\n",
      "Step: 589, Training Loss: 4.68558, LR: 0.0009934, Tokens/sec: 181244.21\n",
      "Step: 590, Training Loss: 4.81120, LR: 0.0009933, Tokens/sec: 180090.61\n",
      "Step: 591, Training Loss: 4.91232, LR: 0.0009933, Tokens/sec: 181946.89\n",
      "Step: 592, Training Loss: 4.67946, LR: 0.0009932, Tokens/sec: 180881.76\n",
      "Step: 593, Training Loss: 4.62970, LR: 0.0009931, Tokens/sec: 180007.10\n",
      "Step: 594, Training Loss: 4.66509, LR: 0.0009931, Tokens/sec: 181542.10\n",
      "Step: 595, Training Loss: 4.71704, LR: 0.0009930, Tokens/sec: 181483.05\n",
      "Step: 596, Training Loss: 4.78579, LR: 0.0009929, Tokens/sec: 182200.45\n",
      "Step: 597, Training Loss: 4.95731, LR: 0.0009929, Tokens/sec: 181850.17\n",
      "Step: 598, Training Loss: 4.86185, LR: 0.0009928, Tokens/sec: 181690.37\n",
      "Step: 599, Training Loss: 4.91852, LR: 0.0009927, Tokens/sec: 180291.40\n",
      "Step: 600, Training Loss: 4.72318, LR: 0.0009926, Tokens/sec: 181725.92\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 600, Eval Loss: 4.80782\n",
      "Step: 601, Training Loss: 4.73533, LR: 0.0009926, Tokens/sec: 181170.69\n",
      "Step: 602, Training Loss: 4.74794, LR: 0.0009925, Tokens/sec: 181308.79\n",
      "Step: 603, Training Loss: 4.59948, LR: 0.0009924, Tokens/sec: 181346.83\n",
      "Step: 604, Training Loss: 4.53039, LR: 0.0009924, Tokens/sec: 181887.59\n",
      "Step: 605, Training Loss: 4.70333, LR: 0.0009923, Tokens/sec: 181673.81\n",
      "Step: 606, Training Loss: 4.79733, LR: 0.0009922, Tokens/sec: 181474.87\n",
      "Step: 607, Training Loss: 4.84038, LR: 0.0009921, Tokens/sec: 180781.54\n",
      "Step: 608, Training Loss: 4.80505, LR: 0.0009921, Tokens/sec: 181411.01\n",
      "Step: 609, Training Loss: 4.54882, LR: 0.0009920, Tokens/sec: 181785.01\n",
      "Step: 610, Training Loss: 4.65823, LR: 0.0009919, Tokens/sec: 181368.39\n",
      "Step: 611, Training Loss: 4.83547, LR: 0.0009918, Tokens/sec: 181829.56\n",
      "Step: 612, Training Loss: 4.55194, LR: 0.0009918, Tokens/sec: 181342.00\n",
      "Step: 613, Training Loss: 4.62884, LR: 0.0009917, Tokens/sec: 179912.07\n",
      "Step: 614, Training Loss: 4.71800, LR: 0.0009916, Tokens/sec: 181131.79\n",
      "Step: 615, Training Loss: 4.63710, LR: 0.0009915, Tokens/sec: 181416.01\n",
      "Step: 616, Training Loss: 4.66480, LR: 0.0009915, Tokens/sec: 181436.87\n",
      "Step: 617, Training Loss: 4.99520, LR: 0.0009914, Tokens/sec: 181129.79\n",
      "Step: 618, Training Loss: 4.63292, LR: 0.0009913, Tokens/sec: 181362.43\n",
      "Step: 619, Training Loss: 4.82828, LR: 0.0009912, Tokens/sec: 180051.02\n",
      "Step: 620, Training Loss: 4.65910, LR: 0.0009911, Tokens/sec: 181818.91\n",
      "Step: 621, Training Loss: 4.65219, LR: 0.0009911, Tokens/sec: 182063.48\n",
      "Step: 622, Training Loss: 4.38649, LR: 0.0009910, Tokens/sec: 181657.77\n",
      "Step: 623, Training Loss: 4.75532, LR: 0.0009909, Tokens/sec: 181991.93\n",
      "Step: 624, Training Loss: 4.70686, LR: 0.0009908, Tokens/sec: 181887.32\n",
      "Step: 625, Training Loss: 4.90294, LR: 0.0009907, Tokens/sec: 180863.91\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 625, Eval Loss: 4.77429\n",
      "Step: 626, Training Loss: 4.53925, LR: 0.0009907, Tokens/sec: 181394.45\n",
      "Step: 627, Training Loss: 4.99247, LR: 0.0009906, Tokens/sec: 181452.02\n",
      "Step: 628, Training Loss: 4.99348, LR: 0.0009905, Tokens/sec: 181495.23\n",
      "Step: 629, Training Loss: 4.42448, LR: 0.0009904, Tokens/sec: 181447.41\n",
      "Step: 630, Training Loss: 4.78017, LR: 0.0009903, Tokens/sec: 181988.81\n",
      "Step: 631, Training Loss: 4.82224, LR: 0.0009903, Tokens/sec: 181481.78\n",
      "Step: 632, Training Loss: 4.92194, LR: 0.0009902, Tokens/sec: 181296.07\n",
      "Step: 633, Training Loss: 4.44073, LR: 0.0009901, Tokens/sec: 180162.40\n",
      "Step: 634, Training Loss: 4.64182, LR: 0.0009900, Tokens/sec: 181859.81\n",
      "Step: 635, Training Loss: 4.66663, LR: 0.0009899, Tokens/sec: 181361.11\n",
      "Step: 636, Training Loss: 4.52749, LR: 0.0009898, Tokens/sec: 181054.74\n",
      "Step: 637, Training Loss: 4.91433, LR: 0.0009898, Tokens/sec: 182051.61\n",
      "Step: 638, Training Loss: 4.73290, LR: 0.0009897, Tokens/sec: 181705.31\n",
      "Step: 639, Training Loss: 4.67867, LR: 0.0009896, Tokens/sec: 180790.35\n",
      "Step: 640, Training Loss: 4.68852, LR: 0.0009895, Tokens/sec: 181419.76\n",
      "Step: 641, Training Loss: 4.49109, LR: 0.0009894, Tokens/sec: 180946.71\n",
      "Step: 642, Training Loss: 4.57304, LR: 0.0009893, Tokens/sec: 181803.38\n",
      "Step: 643, Training Loss: 4.62335, LR: 0.0009892, Tokens/sec: 180982.60\n",
      "Step: 644, Training Loss: 4.71038, LR: 0.0009892, Tokens/sec: 181189.26\n",
      "Step: 645, Training Loss: 4.53810, LR: 0.0009891, Tokens/sec: 180363.04\n",
      "Step: 646, Training Loss: 4.65777, LR: 0.0009890, Tokens/sec: 181328.72\n",
      "Step: 647, Training Loss: 4.69874, LR: 0.0009889, Tokens/sec: 180999.02\n",
      "Step: 648, Training Loss: 4.50738, LR: 0.0009888, Tokens/sec: 181088.81\n",
      "Step: 649, Training Loss: 4.52665, LR: 0.0009887, Tokens/sec: 181430.14\n",
      "Step: 650, Training Loss: 6.11139, LR: 0.0009886, Tokens/sec: 181082.02\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 650, Eval Loss: 4.68710\n",
      "Step: 651, Training Loss: 4.75624, LR: 0.0009885, Tokens/sec: 180210.21\n",
      "Step: 652, Training Loss: 4.44365, LR: 0.0009885, Tokens/sec: 181226.97\n",
      "Step: 653, Training Loss: 4.85903, LR: 0.0009884, Tokens/sec: 181746.97\n",
      "Step: 654, Training Loss: 4.79172, LR: 0.0009883, Tokens/sec: 181452.48\n",
      "Step: 655, Training Loss: 4.43654, LR: 0.0009882, Tokens/sec: 181327.51\n",
      "Step: 656, Training Loss: 4.42904, LR: 0.0009881, Tokens/sec: 180605.18\n",
      "Step: 657, Training Loss: 4.58643, LR: 0.0009880, Tokens/sec: 181169.51\n",
      "Step: 658, Training Loss: 4.73487, LR: 0.0009879, Tokens/sec: 181151.75\n",
      "Step: 659, Training Loss: 4.74673, LR: 0.0009878, Tokens/sec: 180793.64\n",
      "Step: 660, Training Loss: 4.38284, LR: 0.0009877, Tokens/sec: 181423.15\n",
      "Step: 661, Training Loss: 4.57641, LR: 0.0009876, Tokens/sec: 181394.82\n",
      "Step: 662, Training Loss: 4.48769, LR: 0.0009875, Tokens/sec: 179882.14\n",
      "Step: 663, Training Loss: 4.61197, LR: 0.0009875, Tokens/sec: 180925.86\n",
      "Step: 664, Training Loss: 4.67129, LR: 0.0009874, Tokens/sec: 181415.85\n",
      "Step: 665, Training Loss: 4.49943, LR: 0.0009873, Tokens/sec: 181183.32\n",
      "Step: 666, Training Loss: 4.74805, LR: 0.0009872, Tokens/sec: 181019.44\n",
      "Step: 667, Training Loss: 4.92912, LR: 0.0009871, Tokens/sec: 181594.67\n",
      "Step: 668, Training Loss: 4.70034, LR: 0.0009870, Tokens/sec: 180171.25\n",
      "Step: 669, Training Loss: 4.75515, LR: 0.0009869, Tokens/sec: 180925.00\n",
      "Step: 670, Training Loss: 4.86317, LR: 0.0009868, Tokens/sec: 181313.45\n",
      "Step: 671, Training Loss: 4.73047, LR: 0.0009867, Tokens/sec: 180454.41\n",
      "Step: 672, Training Loss: 4.82990, LR: 0.0009866, Tokens/sec: 181538.80\n",
      "Step: 673, Training Loss: 4.58697, LR: 0.0009865, Tokens/sec: 181456.26\n",
      "Step: 674, Training Loss: 4.63627, LR: 0.0009864, Tokens/sec: 180052.19\n",
      "Step: 675, Training Loss: 4.71884, LR: 0.0009863, Tokens/sec: 181496.55\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 675, Eval Loss: 4.69527\n",
      "Step: 676, Training Loss: 4.44246, LR: 0.0009862, Tokens/sec: 180120.00\n",
      "Step: 677, Training Loss: 4.36894, LR: 0.0009861, Tokens/sec: 181318.44\n",
      "Step: 678, Training Loss: 4.45605, LR: 0.0009860, Tokens/sec: 181875.44\n",
      "Step: 679, Training Loss: 4.46600, LR: 0.0009859, Tokens/sec: 181683.50\n",
      "Step: 680, Training Loss: 4.94622, LR: 0.0009858, Tokens/sec: 181638.63\n",
      "Step: 681, Training Loss: 4.60368, LR: 0.0009857, Tokens/sec: 181653.24\n",
      "Step: 682, Training Loss: 4.47725, LR: 0.0009856, Tokens/sec: 180188.74\n",
      "Step: 683, Training Loss: 4.46558, LR: 0.0009855, Tokens/sec: 182282.60\n",
      "Step: 684, Training Loss: 4.79899, LR: 0.0009854, Tokens/sec: 181513.50\n",
      "Step: 685, Training Loss: 5.03895, LR: 0.0009853, Tokens/sec: 181272.34\n",
      "Step: 686, Training Loss: 4.66711, LR: 0.0009852, Tokens/sec: 181131.84\n",
      "Step: 687, Training Loss: 4.39732, LR: 0.0009851, Tokens/sec: 181517.01\n",
      "Step: 688, Training Loss: 4.69033, LR: 0.0009850, Tokens/sec: 180388.77\n",
      "Step: 689, Training Loss: 4.59100, LR: 0.0009849, Tokens/sec: 181865.07\n",
      "Step: 690, Training Loss: 4.32710, LR: 0.0009848, Tokens/sec: 181275.31\n",
      "Step: 691, Training Loss: 4.39726, LR: 0.0009847, Tokens/sec: 181417.17\n",
      "Step: 692, Training Loss: 4.65704, LR: 0.0009846, Tokens/sec: 181739.89\n",
      "Step: 693, Training Loss: 5.01065, LR: 0.0009845, Tokens/sec: 181741.00\n",
      "Step: 694, Training Loss: 4.91974, LR: 0.0009844, Tokens/sec: 180958.41\n",
      "Step: 695, Training Loss: 4.76420, LR: 0.0009843, Tokens/sec: 180314.65\n",
      "Step: 696, Training Loss: 4.50264, LR: 0.0009842, Tokens/sec: 178116.45\n",
      "Step: 697, Training Loss: 4.53642, LR: 0.0009841, Tokens/sec: 181207.46\n",
      "Step: 698, Training Loss: 4.52140, LR: 0.0009840, Tokens/sec: 182125.18\n",
      "Step: 699, Training Loss: 4.39737, LR: 0.0009839, Tokens/sec: 182212.01\n",
      "Step: 700, Training Loss: 4.71319, LR: 0.0009838, Tokens/sec: 180204.51\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 700, Eval Loss: 4.58263\n",
      "Step: 701, Training Loss: 4.08650, LR: 0.0009837, Tokens/sec: 177077.31\n",
      "Step: 702, Training Loss: 4.54554, LR: 0.0009836, Tokens/sec: 179792.92\n",
      "Step: 703, Training Loss: 4.44783, LR: 0.0009835, Tokens/sec: 182517.74\n",
      "Step: 704, Training Loss: 4.40732, LR: 0.0009833, Tokens/sec: 182501.54\n",
      "Step: 705, Training Loss: 4.32639, LR: 0.0009832, Tokens/sec: 182805.68\n",
      "Step: 706, Training Loss: 4.54321, LR: 0.0009831, Tokens/sec: 182716.40\n",
      "Step: 707, Training Loss: 4.70318, LR: 0.0009830, Tokens/sec: 182548.35\n",
      "Step: 708, Training Loss: 4.88466, LR: 0.0009829, Tokens/sec: 181924.49\n",
      "Step: 709, Training Loss: 4.51097, LR: 0.0009828, Tokens/sec: 182625.49\n",
      "Step: 710, Training Loss: 4.68919, LR: 0.0009827, Tokens/sec: 183172.13\n",
      "Step: 711, Training Loss: 4.62255, LR: 0.0009826, Tokens/sec: 182467.59\n",
      "Step: 712, Training Loss: 4.64156, LR: 0.0009825, Tokens/sec: 182725.13\n",
      "Step: 713, Training Loss: 4.42609, LR: 0.0009824, Tokens/sec: 182271.92\n",
      "Step: 714, Training Loss: 4.52178, LR: 0.0009823, Tokens/sec: 181731.50\n",
      "Step: 715, Training Loss: 4.66890, LR: 0.0009821, Tokens/sec: 182975.56\n",
      "Step: 716, Training Loss: 4.66865, LR: 0.0009820, Tokens/sec: 183362.58\n",
      "Step: 717, Training Loss: 4.54516, LR: 0.0009819, Tokens/sec: 182165.78\n",
      "Step: 718, Training Loss: 4.82870, LR: 0.0009818, Tokens/sec: 182512.00\n",
      "Step: 719, Training Loss: 4.44458, LR: 0.0009817, Tokens/sec: 183009.52\n",
      "Step: 720, Training Loss: 4.43932, LR: 0.0009816, Tokens/sec: 165808.41\n",
      "Step: 721, Training Loss: 4.67378, LR: 0.0009815, Tokens/sec: 176864.13\n",
      "Step: 722, Training Loss: 4.57280, LR: 0.0009814, Tokens/sec: 181875.06\n",
      "Step: 723, Training Loss: 4.45417, LR: 0.0009812, Tokens/sec: 181501.47\n",
      "Step: 724, Training Loss: 4.63215, LR: 0.0009811, Tokens/sec: 182218.20\n",
      "Step: 725, Training Loss: 4.64750, LR: 0.0009810, Tokens/sec: 181741.34\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 725, Eval Loss: 4.63955\n",
      "Step: 726, Training Loss: 4.38515, LR: 0.0009809, Tokens/sec: 181500.69\n",
      "Step: 727, Training Loss: 4.44462, LR: 0.0009808, Tokens/sec: 182299.84\n",
      "Step: 728, Training Loss: 4.73352, LR: 0.0009807, Tokens/sec: 182622.28\n",
      "Step: 729, Training Loss: 4.82780, LR: 0.0009806, Tokens/sec: 182671.72\n",
      "Step: 730, Training Loss: 4.50943, LR: 0.0009804, Tokens/sec: 182862.01\n",
      "Step: 731, Training Loss: 4.67256, LR: 0.0009803, Tokens/sec: 182010.85\n",
      "Step: 732, Training Loss: 4.54028, LR: 0.0009802, Tokens/sec: 183133.06\n",
      "Step: 733, Training Loss: 4.36101, LR: 0.0009801, Tokens/sec: 182675.49\n",
      "Step: 734, Training Loss: 4.80373, LR: 0.0009800, Tokens/sec: 182501.05\n",
      "Step: 735, Training Loss: 4.28688, LR: 0.0009799, Tokens/sec: 182676.68\n",
      "Step: 736, Training Loss: 4.54099, LR: 0.0009797, Tokens/sec: 182896.90\n",
      "Step: 737, Training Loss: 5.08576, LR: 0.0009796, Tokens/sec: 181814.02\n",
      "Step: 738, Training Loss: 4.70359, LR: 0.0009795, Tokens/sec: 182423.72\n",
      "Step: 739, Training Loss: 4.54520, LR: 0.0009794, Tokens/sec: 182300.59\n",
      "Step: 740, Training Loss: 4.23434, LR: 0.0009793, Tokens/sec: 177144.78\n",
      "Step: 741, Training Loss: 4.48983, LR: 0.0009792, Tokens/sec: 182479.92\n",
      "Step: 742, Training Loss: 4.14971, LR: 0.0009790, Tokens/sec: 182077.70\n",
      "Step: 743, Training Loss: 4.63489, LR: 0.0009789, Tokens/sec: 181783.51\n",
      "Step: 744, Training Loss: 4.40939, LR: 0.0009788, Tokens/sec: 182537.38\n",
      "Step: 745, Training Loss: 4.45473, LR: 0.0009787, Tokens/sec: 182811.65\n",
      "Step: 746, Training Loss: 4.23704, LR: 0.0009785, Tokens/sec: 182792.62\n",
      "Step: 747, Training Loss: 4.55146, LR: 0.0009784, Tokens/sec: 182534.95\n",
      "Step: 748, Training Loss: 4.24446, LR: 0.0009783, Tokens/sec: 183036.43\n",
      "Step: 749, Training Loss: 4.45070, LR: 0.0009782, Tokens/sec: 182054.14\n",
      "Step: 750, Training Loss: 4.39457, LR: 0.0009781, Tokens/sec: 182741.41\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 750, Eval Loss: 4.48045\n",
      "Step: 751, Training Loss: 4.44169, LR: 0.0009779, Tokens/sec: 181124.33\n",
      "Step: 752, Training Loss: 4.81413, LR: 0.0009778, Tokens/sec: 182675.33\n",
      "Step: 753, Training Loss: 4.45503, LR: 0.0009777, Tokens/sec: 183603.80\n",
      "Step: 754, Training Loss: 4.37725, LR: 0.0009776, Tokens/sec: 182819.68\n",
      "Step: 755, Training Loss: 4.48766, LR: 0.0009774, Tokens/sec: 183256.08\n",
      "Step: 756, Training Loss: 4.52958, LR: 0.0009773, Tokens/sec: 182835.69\n",
      "Step: 757, Training Loss: 4.63264, LR: 0.0009772, Tokens/sec: 181882.21\n",
      "Step: 758, Training Loss: 4.38687, LR: 0.0009771, Tokens/sec: 182695.57\n",
      "Step: 759, Training Loss: 4.73110, LR: 0.0009769, Tokens/sec: 182630.28\n",
      "Step: 760, Training Loss: 4.67255, LR: 0.0009768, Tokens/sec: 182588.62\n",
      "Step: 761, Training Loss: 4.13057, LR: 0.0009767, Tokens/sec: 182966.48\n",
      "Step: 762, Training Loss: 4.52652, LR: 0.0009766, Tokens/sec: 182684.98\n",
      "Step: 763, Training Loss: 4.29478, LR: 0.0009764, Tokens/sec: 181548.90\n",
      "Step: 764, Training Loss: 4.57710, LR: 0.0009763, Tokens/sec: 182861.96\n",
      "Step: 765, Training Loss: 4.65955, LR: 0.0009762, Tokens/sec: 183133.38\n",
      "Step: 766, Training Loss: 4.76872, LR: 0.0009761, Tokens/sec: 182815.29\n",
      "Step: 767, Training Loss: 4.56968, LR: 0.0009759, Tokens/sec: 182829.26\n",
      "Step: 768, Training Loss: 4.42963, LR: 0.0009758, Tokens/sec: 183171.74\n",
      "Step: 769, Training Loss: 4.52547, LR: 0.0009757, Tokens/sec: 181799.41\n",
      "Step: 770, Training Loss: 4.33082, LR: 0.0009755, Tokens/sec: 183064.89\n",
      "Step: 771, Training Loss: 4.59841, LR: 0.0009754, Tokens/sec: 183073.02\n",
      "Step: 772, Training Loss: 4.48502, LR: 0.0009753, Tokens/sec: 183022.18\n",
      "Step: 773, Training Loss: 4.66267, LR: 0.0009752, Tokens/sec: 183165.17\n",
      "Step: 774, Training Loss: 4.69602, LR: 0.0009750, Tokens/sec: 183090.23\n",
      "Step: 775, Training Loss: 4.70153, LR: 0.0009749, Tokens/sec: 182227.78\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 775, Eval Loss: 4.59579\n",
      "Step: 776, Training Loss: 4.40252, LR: 0.0009748, Tokens/sec: 182063.87\n",
      "Step: 777, Training Loss: 4.27766, LR: 0.0009746, Tokens/sec: 182304.94\n",
      "Step: 778, Training Loss: 4.47874, LR: 0.0009745, Tokens/sec: 182342.35\n",
      "Step: 779, Training Loss: 4.37047, LR: 0.0009744, Tokens/sec: 182580.38\n",
      "Step: 780, Training Loss: 4.43707, LR: 0.0009742, Tokens/sec: 183433.71\n",
      "Step: 781, Training Loss: 4.33035, LR: 0.0009741, Tokens/sec: 183200.40\n",
      "Step: 782, Training Loss: 4.73725, LR: 0.0009740, Tokens/sec: 183360.16\n",
      "Step: 783, Training Loss: 4.68850, LR: 0.0009738, Tokens/sec: 182269.04\n",
      "Step: 784, Training Loss: 4.23161, LR: 0.0009737, Tokens/sec: 182111.69\n",
      "Step: 785, Training Loss: 4.47146, LR: 0.0009736, Tokens/sec: 182360.52\n",
      "Step: 786, Training Loss: 4.26870, LR: 0.0009734, Tokens/sec: 182203.25\n",
      "Step: 787, Training Loss: 4.48264, LR: 0.0009733, Tokens/sec: 183593.66\n",
      "Step: 788, Training Loss: 4.55876, LR: 0.0009732, Tokens/sec: 182670.64\n",
      "Step: 789, Training Loss: 4.46474, LR: 0.0009730, Tokens/sec: 182128.35\n",
      "Step: 790, Training Loss: 4.47421, LR: 0.0009729, Tokens/sec: 182984.99\n",
      "Step: 791, Training Loss: 4.54658, LR: 0.0009728, Tokens/sec: 182198.39\n",
      "Step: 792, Training Loss: 4.49860, LR: 0.0009726, Tokens/sec: 182610.25\n",
      "Step: 793, Training Loss: 4.37064, LR: 0.0009725, Tokens/sec: 182631.65\n",
      "Step: 794, Training Loss: 4.50742, LR: 0.0009723, Tokens/sec: 182877.91\n",
      "Step: 795, Training Loss: 4.48170, LR: 0.0009722, Tokens/sec: 177673.25\n",
      "Step: 796, Training Loss: 4.43836, LR: 0.0009721, Tokens/sec: 170460.41\n",
      "Step: 797, Training Loss: 4.37231, LR: 0.0009719, Tokens/sec: 181039.18\n",
      "Step: 798, Training Loss: 4.36293, LR: 0.0009718, Tokens/sec: 181499.61\n",
      "Step: 799, Training Loss: 4.24488, LR: 0.0009717, Tokens/sec: 180818.16\n",
      "Step: 800, Training Loss: 4.65751, LR: 0.0009715, Tokens/sec: 181746.47\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 800, Eval Loss: 4.56729\n",
      "Step: 801, Training Loss: 4.35176, LR: 0.0009714, Tokens/sec: 178976.74\n",
      "Step: 802, Training Loss: 4.76140, LR: 0.0009712, Tokens/sec: 181713.38\n",
      "Step: 803, Training Loss: 4.40544, LR: 0.0009711, Tokens/sec: 177931.39\n",
      "Step: 804, Training Loss: 4.80898, LR: 0.0009710, Tokens/sec: 179081.84\n",
      "Step: 805, Training Loss: 4.75573, LR: 0.0009708, Tokens/sec: 181317.76\n",
      "Step: 806, Training Loss: 4.56824, LR: 0.0009707, Tokens/sec: 178599.01\n",
      "Step: 807, Training Loss: 4.38476, LR: 0.0009705, Tokens/sec: 180191.85\n",
      "Step: 808, Training Loss: 4.57393, LR: 0.0009704, Tokens/sec: 181439.23\n",
      "Step: 809, Training Loss: 4.35936, LR: 0.0009703, Tokens/sec: 180503.84\n",
      "Step: 810, Training Loss: 4.57028, LR: 0.0009701, Tokens/sec: 180800.00\n",
      "Step: 811, Training Loss: 4.31339, LR: 0.0009700, Tokens/sec: 180637.29\n",
      "Step: 812, Training Loss: 4.23814, LR: 0.0009698, Tokens/sec: 179311.48\n",
      "Step: 813, Training Loss: 4.32563, LR: 0.0009697, Tokens/sec: 181373.24\n",
      "Step: 814, Training Loss: 4.48876, LR: 0.0009695, Tokens/sec: 181254.47\n",
      "Step: 815, Training Loss: 4.24486, LR: 0.0009694, Tokens/sec: 181286.90\n",
      "Step: 816, Training Loss: 4.74619, LR: 0.0009692, Tokens/sec: 181674.31\n",
      "Step: 817, Training Loss: 4.55129, LR: 0.0009691, Tokens/sec: 181757.48\n",
      "Step: 818, Training Loss: 4.51508, LR: 0.0009690, Tokens/sec: 180272.89\n",
      "Step: 819, Training Loss: 4.58694, LR: 0.0009688, Tokens/sec: 181203.48\n",
      "Step: 820, Training Loss: 4.24117, LR: 0.0009687, Tokens/sec: 181604.37\n",
      "Step: 821, Training Loss: 4.28284, LR: 0.0009685, Tokens/sec: 181730.14\n",
      "Step: 822, Training Loss: 4.51137, LR: 0.0009684, Tokens/sec: 181502.28\n",
      "Step: 823, Training Loss: 4.75032, LR: 0.0009682, Tokens/sec: 181599.34\n",
      "Step: 824, Training Loss: 4.31477, LR: 0.0009681, Tokens/sec: 180718.04\n",
      "Step: 825, Training Loss: 4.39806, LR: 0.0009679, Tokens/sec: 181198.22\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 825, Eval Loss: 4.43651\n",
      "Step: 826, Training Loss: 4.38223, LR: 0.0009678, Tokens/sec: 179750.00\n",
      "Step: 827, Training Loss: 4.48397, LR: 0.0009676, Tokens/sec: 181545.67\n",
      "Step: 828, Training Loss: 4.37832, LR: 0.0009675, Tokens/sec: 181291.76\n",
      "Step: 829, Training Loss: 4.49670, LR: 0.0009673, Tokens/sec: 181419.61\n",
      "Step: 830, Training Loss: 4.18099, LR: 0.0009672, Tokens/sec: 181096.91\n",
      "Step: 831, Training Loss: 4.33540, LR: 0.0009670, Tokens/sec: 181009.73\n",
      "Step: 832, Training Loss: 4.33839, LR: 0.0009669, Tokens/sec: 180362.16\n",
      "Step: 833, Training Loss: 4.25946, LR: 0.0009667, Tokens/sec: 181743.23\n",
      "Step: 834, Training Loss: 4.24693, LR: 0.0009666, Tokens/sec: 181232.85\n",
      "Step: 835, Training Loss: 4.13012, LR: 0.0009664, Tokens/sec: 181042.46\n",
      "Step: 836, Training Loss: 4.69396, LR: 0.0009663, Tokens/sec: 181522.28\n",
      "Step: 837, Training Loss: 4.32600, LR: 0.0009661, Tokens/sec: 181124.70\n",
      "Step: 838, Training Loss: 4.25045, LR: 0.0009660, Tokens/sec: 180492.52\n",
      "Step: 839, Training Loss: 4.53410, LR: 0.0009658, Tokens/sec: 180961.97\n",
      "Step: 840, Training Loss: 4.52177, LR: 0.0009657, Tokens/sec: 181859.34\n",
      "Step: 841, Training Loss: 4.62651, LR: 0.0009655, Tokens/sec: 181306.57\n",
      "Step: 842, Training Loss: 4.53703, LR: 0.0009654, Tokens/sec: 181493.87\n",
      "Step: 843, Training Loss: 4.14336, LR: 0.0009652, Tokens/sec: 180773.74\n",
      "Step: 844, Training Loss: 4.36860, LR: 0.0009651, Tokens/sec: 180215.78\n",
      "Step: 845, Training Loss: 4.39691, LR: 0.0009649, Tokens/sec: 181424.51\n",
      "Step: 846, Training Loss: 4.04268, LR: 0.0009648, Tokens/sec: 181236.63\n",
      "Step: 847, Training Loss: 4.40412, LR: 0.0009646, Tokens/sec: 181527.68\n",
      "Step: 848, Training Loss: 4.35094, LR: 0.0009645, Tokens/sec: 181618.03\n",
      "Step: 849, Training Loss: 4.44591, LR: 0.0009643, Tokens/sec: 181618.29\n",
      "Step: 850, Training Loss: 4.29887, LR: 0.0009642, Tokens/sec: 180396.11\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 850, Eval Loss: 4.42273\n",
      "Step: 851, Training Loss: 4.19595, LR: 0.0009640, Tokens/sec: 180606.98\n",
      "Step: 852, Training Loss: 4.79150, LR: 0.0009638, Tokens/sec: 179894.00\n",
      "Step: 853, Training Loss: 4.45712, LR: 0.0009637, Tokens/sec: 180980.43\n",
      "Step: 854, Training Loss: 4.08778, LR: 0.0009635, Tokens/sec: 180758.34\n",
      "Step: 855, Training Loss: 4.17208, LR: 0.0009634, Tokens/sec: 181371.90\n",
      "Step: 856, Training Loss: 4.42692, LR: 0.0009632, Tokens/sec: 182133.09\n",
      "Step: 857, Training Loss: 4.18800, LR: 0.0009631, Tokens/sec: 181588.00\n",
      "Step: 858, Training Loss: 4.55349, LR: 0.0009629, Tokens/sec: 180508.10\n",
      "Step: 859, Training Loss: 4.53397, LR: 0.0009627, Tokens/sec: 181205.99\n",
      "Step: 860, Training Loss: 4.53215, LR: 0.0009626, Tokens/sec: 181298.00\n",
      "Step: 861, Training Loss: 4.08798, LR: 0.0009624, Tokens/sec: 181245.64\n",
      "Step: 862, Training Loss: 4.69285, LR: 0.0009623, Tokens/sec: 181445.63\n",
      "Step: 863, Training Loss: 4.19591, LR: 0.0009621, Tokens/sec: 181488.45\n",
      "Step: 864, Training Loss: 4.64369, LR: 0.0009619, Tokens/sec: 180378.70\n",
      "Step: 865, Training Loss: 4.63332, LR: 0.0009618, Tokens/sec: 181163.30\n",
      "Step: 866, Training Loss: 4.49363, LR: 0.0009616, Tokens/sec: 181633.08\n",
      "Step: 867, Training Loss: 4.53414, LR: 0.0009615, Tokens/sec: 180651.45\n",
      "Step: 868, Training Loss: 4.62403, LR: 0.0009613, Tokens/sec: 181424.73\n",
      "Step: 869, Training Loss: 4.19021, LR: 0.0009611, Tokens/sec: 180820.67\n",
      "Step: 870, Training Loss: 4.39687, LR: 0.0009610, Tokens/sec: 180541.97\n",
      "Step: 871, Training Loss: 4.36691, LR: 0.0009608, Tokens/sec: 180814.66\n",
      "Step: 872, Training Loss: 4.63030, LR: 0.0009607, Tokens/sec: 181624.26\n",
      "Step: 873, Training Loss: 4.34864, LR: 0.0009605, Tokens/sec: 181411.78\n",
      "Step: 874, Training Loss: 4.15991, LR: 0.0009603, Tokens/sec: 181635.01\n",
      "Step: 875, Training Loss: 4.44736, LR: 0.0009602, Tokens/sec: 181692.57\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 875, Eval Loss: 4.40608\n",
      "Step: 876, Training Loss: 4.09627, LR: 0.0009600, Tokens/sec: 180662.25\n",
      "Step: 877, Training Loss: 4.21168, LR: 0.0009598, Tokens/sec: 153717.08\n",
      "Step: 878, Training Loss: 4.35872, LR: 0.0009597, Tokens/sec: 180981.77\n",
      "Step: 879, Training Loss: 4.26363, LR: 0.0009595, Tokens/sec: 180608.58\n",
      "Step: 880, Training Loss: 4.36956, LR: 0.0009593, Tokens/sec: 180431.60\n",
      "Step: 881, Training Loss: 4.51991, LR: 0.0009592, Tokens/sec: 179463.13\n",
      "Step: 882, Training Loss: 4.34993, LR: 0.0009590, Tokens/sec: 180835.21\n",
      "Step: 883, Training Loss: 4.08560, LR: 0.0009588, Tokens/sec: 181529.61\n",
      "Step: 884, Training Loss: 4.25812, LR: 0.0009587, Tokens/sec: 181281.60\n",
      "Step: 885, Training Loss: 4.32367, LR: 0.0009585, Tokens/sec: 181171.77\n",
      "Step: 886, Training Loss: 4.12025, LR: 0.0009583, Tokens/sec: 180936.64\n",
      "Step: 887, Training Loss: 4.40483, LR: 0.0009582, Tokens/sec: 180738.91\n",
      "Step: 888, Training Loss: 4.26493, LR: 0.0009580, Tokens/sec: 181530.22\n",
      "Step: 889, Training Loss: 4.12445, LR: 0.0009578, Tokens/sec: 181681.40\n",
      "Step: 890, Training Loss: 4.35157, LR: 0.0009577, Tokens/sec: 181886.85\n",
      "Step: 891, Training Loss: 4.42952, LR: 0.0009575, Tokens/sec: 181566.45\n",
      "Step: 892, Training Loss: 4.40055, LR: 0.0009573, Tokens/sec: 181931.04\n",
      "Step: 893, Training Loss: 4.58506, LR: 0.0009572, Tokens/sec: 177615.62\n",
      "Step: 894, Training Loss: 4.40851, LR: 0.0009570, Tokens/sec: 181740.77\n",
      "Step: 895, Training Loss: 4.31283, LR: 0.0009568, Tokens/sec: 181742.23\n",
      "Step: 896, Training Loss: 4.38081, LR: 0.0009567, Tokens/sec: 181433.18\n",
      "Step: 897, Training Loss: 4.40443, LR: 0.0009565, Tokens/sec: 181463.71\n",
      "Step: 898, Training Loss: 4.60133, LR: 0.0009563, Tokens/sec: 181308.43\n",
      "Step: 899, Training Loss: 4.33858, LR: 0.0009561, Tokens/sec: 180590.09\n",
      "Step: 900, Training Loss: 4.31424, LR: 0.0009560, Tokens/sec: 180874.90\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 900, Eval Loss: 4.37304\n",
      "Step: 901, Training Loss: 4.29014, LR: 0.0009558, Tokens/sec: 178696.85\n",
      "Step: 902, Training Loss: 4.48129, LR: 0.0009556, Tokens/sec: 181830.89\n",
      "Step: 903, Training Loss: 4.51395, LR: 0.0009555, Tokens/sec: 181351.23\n",
      "Step: 904, Training Loss: 4.29380, LR: 0.0009553, Tokens/sec: 181788.80\n",
      "Step: 905, Training Loss: 4.48650, LR: 0.0009551, Tokens/sec: 181852.72\n",
      "Step: 906, Training Loss: 4.28456, LR: 0.0009549, Tokens/sec: 181721.53\n",
      "Step: 907, Training Loss: 3.90776, LR: 0.0009548, Tokens/sec: 180330.78\n",
      "Step: 908, Training Loss: 4.27816, LR: 0.0009546, Tokens/sec: 181677.56\n",
      "Step: 909, Training Loss: 4.47521, LR: 0.0009544, Tokens/sec: 182113.11\n",
      "Step: 910, Training Loss: 4.27661, LR: 0.0009542, Tokens/sec: 182001.87\n",
      "Step: 911, Training Loss: 4.48713, LR: 0.0009541, Tokens/sec: 181418.16\n",
      "Step: 912, Training Loss: 4.51677, LR: 0.0009539, Tokens/sec: 181470.40\n",
      "Step: 913, Training Loss: 4.27981, LR: 0.0009537, Tokens/sec: 179839.98\n",
      "Step: 914, Training Loss: 4.47599, LR: 0.0009535, Tokens/sec: 181509.12\n",
      "Step: 915, Training Loss: 4.46556, LR: 0.0009534, Tokens/sec: 180839.83\n",
      "Step: 916, Training Loss: 4.37300, LR: 0.0009532, Tokens/sec: 181623.88\n",
      "Step: 917, Training Loss: 4.10277, LR: 0.0009530, Tokens/sec: 181311.87\n",
      "Step: 918, Training Loss: 4.44172, LR: 0.0009528, Tokens/sec: 181586.31\n",
      "Step: 919, Training Loss: 4.21333, LR: 0.0009527, Tokens/sec: 180407.06\n",
      "Step: 920, Training Loss: 4.29932, LR: 0.0009525, Tokens/sec: 181845.21\n",
      "Step: 921, Training Loss: 4.62514, LR: 0.0009523, Tokens/sec: 181729.63\n",
      "Step: 922, Training Loss: 4.34737, LR: 0.0009521, Tokens/sec: 180570.47\n",
      "Step: 923, Training Loss: 4.52505, LR: 0.0009519, Tokens/sec: 181445.88\n",
      "Step: 924, Training Loss: 4.22559, LR: 0.0009518, Tokens/sec: 181414.66\n",
      "Step: 925, Training Loss: 4.18541, LR: 0.0009516, Tokens/sec: 180485.52\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 925, Eval Loss: 4.40336\n",
      "Step: 926, Training Loss: 4.41410, LR: 0.0009514, Tokens/sec: 181197.04\n",
      "Step: 927, Training Loss: 4.52944, LR: 0.0009512, Tokens/sec: 179959.85\n",
      "Step: 928, Training Loss: 4.51950, LR: 0.0009511, Tokens/sec: 180166.91\n",
      "Step: 929, Training Loss: 4.37546, LR: 0.0009509, Tokens/sec: 180790.05\n",
      "Step: 930, Training Loss: 4.29724, LR: 0.0009507, Tokens/sec: 181182.53\n",
      "Step: 931, Training Loss: 4.34273, LR: 0.0009505, Tokens/sec: 180521.33\n",
      "Step: 932, Training Loss: 4.35007, LR: 0.0009503, Tokens/sec: 181106.42\n",
      "Step: 933, Training Loss: 4.06901, LR: 0.0009501, Tokens/sec: 179020.02\n",
      "Step: 934, Training Loss: 4.22974, LR: 0.0009500, Tokens/sec: 182009.78\n",
      "Step: 935, Training Loss: 4.24039, LR: 0.0009498, Tokens/sec: 181462.94\n",
      "Step: 936, Training Loss: 4.39453, LR: 0.0009496, Tokens/sec: 181764.03\n",
      "Step: 937, Training Loss: 4.16135, LR: 0.0009494, Tokens/sec: 181561.29\n",
      "Step: 938, Training Loss: 4.23576, LR: 0.0009492, Tokens/sec: 181303.53\n",
      "Step: 939, Training Loss: 4.43842, LR: 0.0009491, Tokens/sec: 180518.76\n",
      "Step: 940, Training Loss: 4.26152, LR: 0.0009489, Tokens/sec: 181284.32\n",
      "Step: 941, Training Loss: 4.54967, LR: 0.0009487, Tokens/sec: 181157.73\n",
      "Step: 942, Training Loss: 4.09932, LR: 0.0009485, Tokens/sec: 180525.29\n",
      "Step: 943, Training Loss: 4.32860, LR: 0.0009483, Tokens/sec: 180929.75\n",
      "Step: 944, Training Loss: 4.27090, LR: 0.0009481, Tokens/sec: 181104.40\n",
      "Step: 945, Training Loss: 4.13925, LR: 0.0009479, Tokens/sec: 180465.94\n",
      "Step: 946, Training Loss: 4.20166, LR: 0.0009478, Tokens/sec: 180809.42\n",
      "Step: 947, Training Loss: 3.81515, LR: 0.0009476, Tokens/sec: 181439.74\n",
      "Step: 948, Training Loss: 4.39764, LR: 0.0009474, Tokens/sec: 181241.56\n",
      "Step: 949, Training Loss: 4.34339, LR: 0.0009472, Tokens/sec: 181854.49\n",
      "Step: 950, Training Loss: 4.11655, LR: 0.0009470, Tokens/sec: 180880.75\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 950, Eval Loss: 4.27598\n",
      "Step: 951, Training Loss: 4.31422, LR: 0.0009468, Tokens/sec: 181089.04\n",
      "Step: 952, Training Loss: 4.28476, LR: 0.0009466, Tokens/sec: 181343.29\n",
      "Step: 953, Training Loss: 3.89781, LR: 0.0009464, Tokens/sec: 181501.10\n",
      "Step: 954, Training Loss: 4.71900, LR: 0.0009463, Tokens/sec: 181576.25\n",
      "Step: 955, Training Loss: 4.41966, LR: 0.0009461, Tokens/sec: 181274.40\n",
      "Step: 956, Training Loss: 4.22177, LR: 0.0009459, Tokens/sec: 179238.06\n",
      "Step: 957, Training Loss: 4.30508, LR: 0.0009457, Tokens/sec: 181360.06\n",
      "Step: 958, Training Loss: 4.21621, LR: 0.0009455, Tokens/sec: 181078.30\n",
      "Step: 959, Training Loss: 4.79392, LR: 0.0009453, Tokens/sec: 181799.33\n",
      "Step: 960, Training Loss: 4.11018, LR: 0.0009451, Tokens/sec: 181372.58\n",
      "Step: 961, Training Loss: 4.06257, LR: 0.0009449, Tokens/sec: 181337.07\n",
      "Step: 962, Training Loss: 4.48671, LR: 0.0009447, Tokens/sec: 181130.95\n",
      "Step: 963, Training Loss: 4.31793, LR: 0.0009446, Tokens/sec: 181013.92\n",
      "Step: 964, Training Loss: 4.23044, LR: 0.0009444, Tokens/sec: 181177.26\n",
      "Step: 965, Training Loss: 4.19954, LR: 0.0009442, Tokens/sec: 181170.20\n",
      "Step: 966, Training Loss: 4.52419, LR: 0.0009440, Tokens/sec: 181076.74\n",
      "Step: 967, Training Loss: 4.22769, LR: 0.0009438, Tokens/sec: 181198.89\n",
      "Step: 968, Training Loss: 4.24561, LR: 0.0009436, Tokens/sec: 180596.11\n",
      "Step: 969, Training Loss: 4.21381, LR: 0.0009434, Tokens/sec: 181191.61\n",
      "Step: 970, Training Loss: 4.26100, LR: 0.0009432, Tokens/sec: 181853.96\n",
      "Step: 971, Training Loss: 4.21950, LR: 0.0009430, Tokens/sec: 181748.00\n",
      "Step: 972, Training Loss: 4.32092, LR: 0.0009428, Tokens/sec: 182346.28\n",
      "Step: 973, Training Loss: 4.49016, LR: 0.0009426, Tokens/sec: 181722.95\n",
      "Step: 974, Training Loss: 4.29035, LR: 0.0009424, Tokens/sec: 180222.44\n",
      "Step: 975, Training Loss: 4.48841, LR: 0.0009422, Tokens/sec: 181725.84\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 975, Eval Loss: 4.34254\n",
      "Step: 976, Training Loss: 4.55973, LR: 0.0009420, Tokens/sec: 180181.42\n",
      "Step: 977, Training Loss: 4.49151, LR: 0.0009418, Tokens/sec: 181548.54\n",
      "Step: 978, Training Loss: 4.28463, LR: 0.0009417, Tokens/sec: 181354.28\n",
      "Step: 979, Training Loss: 4.33403, LR: 0.0009415, Tokens/sec: 181478.03\n",
      "Step: 980, Training Loss: 4.33973, LR: 0.0009413, Tokens/sec: 181345.89\n",
      "Step: 981, Training Loss: 4.46818, LR: 0.0009411, Tokens/sec: 181312.55\n",
      "Step: 982, Training Loss: 3.60946, LR: 0.0009409, Tokens/sec: 180066.02\n",
      "Step: 983, Training Loss: 4.40916, LR: 0.0009407, Tokens/sec: 180984.88\n",
      "Step: 984, Training Loss: 4.46974, LR: 0.0009405, Tokens/sec: 181562.02\n",
      "Step: 985, Training Loss: 3.97717, LR: 0.0009403, Tokens/sec: 181856.84\n",
      "Step: 986, Training Loss: 4.00901, LR: 0.0009401, Tokens/sec: 182149.12\n",
      "Step: 987, Training Loss: 4.55325, LR: 0.0009399, Tokens/sec: 176280.91\n",
      "Step: 988, Training Loss: 4.12972, LR: 0.0009397, Tokens/sec: 180579.78\n",
      "Step: 989, Training Loss: 4.18476, LR: 0.0009395, Tokens/sec: 181334.96\n",
      "Step: 990, Training Loss: 3.99729, LR: 0.0009393, Tokens/sec: 181625.67\n",
      "Step: 991, Training Loss: 4.41232, LR: 0.0009391, Tokens/sec: 181849.67\n",
      "Step: 992, Training Loss: 4.43432, LR: 0.0009389, Tokens/sec: 181743.65\n",
      "Step: 993, Training Loss: 4.28457, LR: 0.0009387, Tokens/sec: 181928.42\n",
      "Step: 994, Training Loss: 4.31454, LR: 0.0009385, Tokens/sec: 180859.09\n",
      "Step: 995, Training Loss: 4.20852, LR: 0.0009383, Tokens/sec: 181401.37\n",
      "Step: 996, Training Loss: 4.29029, LR: 0.0009381, Tokens/sec: 181929.86\n",
      "Step: 997, Training Loss: 4.13802, LR: 0.0009379, Tokens/sec: 181503.19\n",
      "Step: 998, Training Loss: 4.35931, LR: 0.0009377, Tokens/sec: 181924.89\n",
      "Step: 999, Training Loss: 4.24774, LR: 0.0009375, Tokens/sec: 181833.54\n",
      "Step: 1000, Training Loss: 4.33633, LR: 0.0009373, Tokens/sec: 181040.09\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 1000, Eval Loss: 4.29345\n",
      "Step: 1001, Training Loss: 4.13887, LR: 0.0009371, Tokens/sec: 180659.45\n",
      "Step: 1002, Training Loss: 4.20923, LR: 0.0009369, Tokens/sec: 179660.02\n",
      "Step: 1003, Training Loss: 4.35559, LR: 0.0009367, Tokens/sec: 181086.34\n",
      "Step: 1004, Training Loss: 4.13298, LR: 0.0009365, Tokens/sec: 181423.83\n",
      "Step: 1005, Training Loss: 4.16980, LR: 0.0009363, Tokens/sec: 181490.02\n",
      "Step: 1006, Training Loss: 4.22872, LR: 0.0009361, Tokens/sec: 181133.46\n",
      "Step: 1007, Training Loss: 4.22946, LR: 0.0009358, Tokens/sec: 181768.71\n",
      "Step: 1008, Training Loss: 4.47721, LR: 0.0009356, Tokens/sec: 180528.95\n",
      "Step: 1009, Training Loss: 4.37567, LR: 0.0009354, Tokens/sec: 181312.90\n",
      "Step: 1010, Training Loss: 4.23088, LR: 0.0009352, Tokens/sec: 182129.38\n",
      "Step: 1011, Training Loss: 4.36674, LR: 0.0009350, Tokens/sec: 181080.69\n",
      "Step: 1012, Training Loss: 4.32230, LR: 0.0009348, Tokens/sec: 181582.22\n",
      "Step: 1013, Training Loss: 4.25554, LR: 0.0009346, Tokens/sec: 181207.41\n",
      "Step: 1014, Training Loss: 4.12843, LR: 0.0009344, Tokens/sec: 180500.71\n",
      "Step: 1015, Training Loss: 4.02482, LR: 0.0009342, Tokens/sec: 181184.44\n",
      "Step: 1016, Training Loss: 4.18808, LR: 0.0009340, Tokens/sec: 181304.46\n",
      "Step: 1017, Training Loss: 4.45081, LR: 0.0009338, Tokens/sec: 181251.92\n",
      "Step: 1018, Training Loss: 4.30939, LR: 0.0009336, Tokens/sec: 181025.25\n",
      "Step: 1019, Training Loss: 4.39445, LR: 0.0009334, Tokens/sec: 181960.46\n",
      "Step: 1020, Training Loss: 3.88802, LR: 0.0009332, Tokens/sec: 180356.57\n",
      "Step: 1021, Training Loss: 4.09919, LR: 0.0009330, Tokens/sec: 181407.67\n",
      "Step: 1022, Training Loss: 4.18460, LR: 0.0009327, Tokens/sec: 181354.51\n",
      "Step: 1023, Training Loss: 4.36593, LR: 0.0009325, Tokens/sec: 181186.09\n",
      "Step: 1024, Training Loss: 4.06058, LR: 0.0009323, Tokens/sec: 181187.98\n",
      "Step: 1025, Training Loss: 4.40815, LR: 0.0009321, Tokens/sec: 181408.96\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 1025, Eval Loss: 4.25687\n",
      "Step: 1026, Training Loss: 4.06860, LR: 0.0009319, Tokens/sec: 180439.11\n",
      "Step: 1027, Training Loss: 4.27948, LR: 0.0009317, Tokens/sec: 180847.10\n",
      "Step: 1028, Training Loss: 4.18048, LR: 0.0009315, Tokens/sec: 181329.56\n",
      "Step: 1029, Training Loss: 4.17399, LR: 0.0009313, Tokens/sec: 180529.06\n",
      "Step: 1030, Training Loss: 4.19761, LR: 0.0009311, Tokens/sec: 181467.91\n",
      "Step: 1031, Training Loss: 4.25794, LR: 0.0009309, Tokens/sec: 179935.33\n",
      "Step: 1032, Training Loss: 4.14935, LR: 0.0009306, Tokens/sec: 180901.28\n",
      "Step: 1033, Training Loss: 4.36721, LR: 0.0009304, Tokens/sec: 181572.19\n",
      "Step: 1034, Training Loss: 3.61006, LR: 0.0009302, Tokens/sec: 179938.87\n",
      "Step: 1035, Training Loss: 4.11481, LR: 0.0009300, Tokens/sec: 181516.17\n",
      "Step: 1036, Training Loss: 4.02481, LR: 0.0009298, Tokens/sec: 181378.82\n",
      "Step: 1037, Training Loss: 3.95737, LR: 0.0009296, Tokens/sec: 179867.66\n",
      "Step: 1038, Training Loss: 4.39407, LR: 0.0009294, Tokens/sec: 180916.89\n",
      "Step: 1039, Training Loss: 4.22438, LR: 0.0009291, Tokens/sec: 181103.57\n",
      "Step: 1040, Training Loss: 4.10508, LR: 0.0009289, Tokens/sec: 172479.26\n",
      "Step: 1041, Training Loss: 4.09060, LR: 0.0009287, Tokens/sec: 180488.16\n",
      "Step: 1042, Training Loss: 4.20785, LR: 0.0009285, Tokens/sec: 180473.32\n",
      "Step: 1043, Training Loss: 4.23284, LR: 0.0009283, Tokens/sec: 179202.37\n",
      "Step: 1044, Training Loss: 4.27564, LR: 0.0009281, Tokens/sec: 180588.67\n",
      "Step: 1045, Training Loss: 4.31455, LR: 0.0009279, Tokens/sec: 180319.51\n",
      "Step: 1046, Training Loss: 4.21285, LR: 0.0009276, Tokens/sec: 180452.79\n",
      "Step: 1047, Training Loss: 4.17451, LR: 0.0009274, Tokens/sec: 180735.55\n",
      "Step: 1048, Training Loss: 4.28989, LR: 0.0009272, Tokens/sec: 181001.12\n",
      "Step: 1049, Training Loss: 4.30244, LR: 0.0009270, Tokens/sec: 179856.44\n",
      "Step: 1050, Training Loss: 4.10654, LR: 0.0009268, Tokens/sec: 180236.83\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 1050, Eval Loss: 4.12564\n",
      "Step: 1051, Training Loss: 4.17683, LR: 0.0009266, Tokens/sec: 178605.19\n",
      "Step: 1052, Training Loss: 4.13981, LR: 0.0009263, Tokens/sec: 181054.64\n",
      "Step: 1053, Training Loss: 4.27057, LR: 0.0009261, Tokens/sec: 181321.57\n",
      "Step: 1054, Training Loss: 4.25289, LR: 0.0009259, Tokens/sec: 180531.95\n",
      "Step: 1055, Training Loss: 4.02092, LR: 0.0009257, Tokens/sec: 180937.82\n",
      "Step: 1056, Training Loss: 4.05622, LR: 0.0009255, Tokens/sec: 180833.35\n",
      "Step: 1057, Training Loss: 4.12429, LR: 0.0009252, Tokens/sec: 180312.00\n",
      "Step: 1058, Training Loss: 4.31353, LR: 0.0009250, Tokens/sec: 180477.39\n",
      "Step: 1059, Training Loss: 3.91930, LR: 0.0009248, Tokens/sec: 181135.72\n",
      "Step: 1060, Training Loss: 5.63230, LR: 0.0009246, Tokens/sec: 181392.92\n",
      "Step: 1061, Training Loss: 4.43133, LR: 0.0009244, Tokens/sec: 181320.62\n",
      "Step: 1062, Training Loss: 4.13940, LR: 0.0009241, Tokens/sec: 181738.98\n",
      "Step: 1063, Training Loss: 3.93274, LR: 0.0009239, Tokens/sec: 179941.29\n",
      "Step: 1064, Training Loss: 4.07571, LR: 0.0009237, Tokens/sec: 181012.53\n",
      "Step: 1065, Training Loss: 4.11595, LR: 0.0009235, Tokens/sec: 181173.48\n",
      "Step: 1066, Training Loss: 4.19954, LR: 0.0009233, Tokens/sec: 181231.59\n",
      "Step: 1067, Training Loss: 4.10324, LR: 0.0009230, Tokens/sec: 180883.62\n",
      "Step: 1068, Training Loss: 4.29863, LR: 0.0009228, Tokens/sec: 180969.13\n",
      "Step: 1069, Training Loss: 4.29417, LR: 0.0009226, Tokens/sec: 180216.77\n",
      "Step: 1070, Training Loss: 3.87310, LR: 0.0009224, Tokens/sec: 181180.43\n",
      "Step: 1071, Training Loss: 4.07903, LR: 0.0009221, Tokens/sec: 181464.97\n",
      "Step: 1072, Training Loss: 3.92458, LR: 0.0009219, Tokens/sec: 181054.97\n",
      "Step: 1073, Training Loss: 4.09312, LR: 0.0009217, Tokens/sec: 180386.02\n",
      "Step: 1074, Training Loss: 4.29261, LR: 0.0009215, Tokens/sec: 180949.08\n",
      "Step: 1075, Training Loss: 4.09630, LR: 0.0009213, Tokens/sec: 180724.81\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 1075, Eval Loss: 4.20038\n",
      "Step: 1076, Training Loss: 4.01759, LR: 0.0009210, Tokens/sec: 180883.05\n",
      "Step: 1077, Training Loss: 4.05728, LR: 0.0009208, Tokens/sec: 179568.00\n",
      "Step: 1078, Training Loss: 4.36249, LR: 0.0009206, Tokens/sec: 181007.39\n",
      "Step: 1079, Training Loss: 4.26164, LR: 0.0009204, Tokens/sec: 180634.36\n",
      "Step: 1080, Training Loss: 4.22437, LR: 0.0009201, Tokens/sec: 181155.31\n",
      "Step: 1081, Training Loss: 4.07824, LR: 0.0009199, Tokens/sec: 181198.28\n",
      "Step: 1082, Training Loss: 4.26640, LR: 0.0009197, Tokens/sec: 180690.93\n",
      "Step: 1083, Training Loss: 4.06000, LR: 0.0009194, Tokens/sec: 179572.95\n",
      "Step: 1084, Training Loss: 4.28240, LR: 0.0009192, Tokens/sec: 181078.33\n",
      "Step: 1085, Training Loss: 4.08064, LR: 0.0009190, Tokens/sec: 181488.63\n",
      "Step: 1086, Training Loss: 4.01469, LR: 0.0009188, Tokens/sec: 181188.89\n",
      "Step: 1087, Training Loss: 3.92637, LR: 0.0009185, Tokens/sec: 181358.84\n",
      "Step: 1088, Training Loss: 3.82404, LR: 0.0009183, Tokens/sec: 181167.59\n",
      "Step: 1089, Training Loss: 3.98039, LR: 0.0009181, Tokens/sec: 179818.38\n",
      "Step: 1090, Training Loss: 3.86954, LR: 0.0009179, Tokens/sec: 180833.00\n",
      "Step: 1091, Training Loss: 4.02159, LR: 0.0009176, Tokens/sec: 181236.18\n",
      "Step: 1092, Training Loss: 3.97775, LR: 0.0009174, Tokens/sec: 181152.85\n",
      "Step: 1093, Training Loss: 4.14172, LR: 0.0009172, Tokens/sec: 181254.81\n",
      "Step: 1094, Training Loss: 4.28716, LR: 0.0009169, Tokens/sec: 181055.57\n",
      "Step: 1095, Training Loss: 3.78364, LR: 0.0009167, Tokens/sec: 179963.41\n",
      "Step: 1096, Training Loss: 4.73803, LR: 0.0009165, Tokens/sec: 181533.37\n",
      "Step: 1097, Training Loss: 4.12390, LR: 0.0009162, Tokens/sec: 181682.99\n",
      "Step: 1098, Training Loss: 3.97614, LR: 0.0009160, Tokens/sec: 181646.49\n",
      "Step: 1099, Training Loss: 4.40740, LR: 0.0009158, Tokens/sec: 181690.45\n",
      "Step: 1100, Training Loss: 4.05537, LR: 0.0009155, Tokens/sec: 181411.54\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 1100, Eval Loss: 4.14513\n",
      "Step: 1101, Training Loss: 4.16966, LR: 0.0009153, Tokens/sec: 173094.86\n",
      "Step: 1102, Training Loss: 4.25092, LR: 0.0009151, Tokens/sec: 180548.15\n",
      "Step: 1103, Training Loss: 4.35992, LR: 0.0009149, Tokens/sec: 180884.85\n",
      "Step: 1104, Training Loss: 4.31812, LR: 0.0009146, Tokens/sec: 181616.29\n",
      "Step: 1105, Training Loss: 3.99092, LR: 0.0009144, Tokens/sec: 181080.41\n",
      "Step: 1106, Training Loss: 4.08605, LR: 0.0009142, Tokens/sec: 179968.60\n",
      "Step: 1107, Training Loss: 4.14846, LR: 0.0009139, Tokens/sec: 181031.99\n",
      "Step: 1108, Training Loss: 4.21108, LR: 0.0009137, Tokens/sec: 180413.95\n",
      "Step: 1109, Training Loss: 3.94652, LR: 0.0009134, Tokens/sec: 180761.04\n",
      "Step: 1110, Training Loss: 4.07072, LR: 0.0009132, Tokens/sec: 181067.35\n",
      "Step: 1111, Training Loss: 4.08948, LR: 0.0009130, Tokens/sec: 181622.81\n",
      "Step: 1112, Training Loss: 4.35981, LR: 0.0009127, Tokens/sec: 179825.20\n",
      "Step: 1113, Training Loss: 4.19473, LR: 0.0009125, Tokens/sec: 181124.00\n",
      "Step: 1114, Training Loss: 3.95895, LR: 0.0009123, Tokens/sec: 181485.54\n",
      "Step: 1115, Training Loss: 4.07014, LR: 0.0009120, Tokens/sec: 181466.25\n",
      "Step: 1116, Training Loss: 3.94632, LR: 0.0009118, Tokens/sec: 181223.07\n",
      "Step: 1117, Training Loss: 4.18074, LR: 0.0009116, Tokens/sec: 181484.88\n",
      "Step: 1118, Training Loss: 4.16093, LR: 0.0009113, Tokens/sec: 180076.09\n",
      "Step: 1119, Training Loss: 3.87096, LR: 0.0009111, Tokens/sec: 181019.81\n",
      "Step: 1120, Training Loss: 4.04628, LR: 0.0009109, Tokens/sec: 181448.34\n",
      "Step: 1121, Training Loss: 4.10884, LR: 0.0009106, Tokens/sec: 180870.81\n",
      "Step: 1122, Training Loss: 4.14132, LR: 0.0009104, Tokens/sec: 181041.91\n",
      "Step: 1123, Training Loss: 4.09998, LR: 0.0009101, Tokens/sec: 181308.41\n",
      "Step: 1124, Training Loss: 4.17350, LR: 0.0009099, Tokens/sec: 179879.13\n",
      "Step: 1125, Training Loss: 3.71095, LR: 0.0009097, Tokens/sec: 180880.44\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 1125, Eval Loss: 4.12036\n",
      "Step: 1126, Training Loss: 4.08381, LR: 0.0009094, Tokens/sec: 179565.38\n",
      "Step: 1127, Training Loss: 3.83292, LR: 0.0009092, Tokens/sec: 181105.55\n",
      "Step: 1128, Training Loss: 4.38190, LR: 0.0009089, Tokens/sec: 180971.21\n",
      "Step: 1129, Training Loss: 4.13321, LR: 0.0009087, Tokens/sec: 181688.88\n",
      "Step: 1130, Training Loss: 4.08504, LR: 0.0009085, Tokens/sec: 181099.34\n",
      "Step: 1131, Training Loss: 4.03580, LR: 0.0009082, Tokens/sec: 181636.98\n",
      "Step: 1132, Training Loss: 3.98675, LR: 0.0009080, Tokens/sec: 180372.41\n",
      "Step: 1133, Training Loss: 4.59929, LR: 0.0009077, Tokens/sec: 180467.30\n",
      "Step: 1134, Training Loss: 3.93794, LR: 0.0009075, Tokens/sec: 181085.58\n",
      "Step: 1135, Training Loss: 4.26694, LR: 0.0009073, Tokens/sec: 180948.83\n",
      "Step: 1136, Training Loss: 4.05099, LR: 0.0009070, Tokens/sec: 181314.78\n",
      "Step: 1137, Training Loss: 4.27877, LR: 0.0009068, Tokens/sec: 181231.02\n",
      "Step: 1138, Training Loss: 4.11487, LR: 0.0009065, Tokens/sec: 180714.56\n",
      "Step: 1139, Training Loss: 4.21662, LR: 0.0009063, Tokens/sec: 180824.13\n",
      "Step: 1140, Training Loss: 3.95462, LR: 0.0009060, Tokens/sec: 181598.04\n",
      "Step: 1141, Training Loss: 4.42867, LR: 0.0009058, Tokens/sec: 181141.22\n",
      "Step: 1142, Training Loss: 4.16352, LR: 0.0009056, Tokens/sec: 181533.62\n",
      "Step: 1143, Training Loss: 4.13059, LR: 0.0009053, Tokens/sec: 181030.91\n",
      "Step: 1144, Training Loss: 4.28421, LR: 0.0009051, Tokens/sec: 179987.27\n",
      "Step: 1145, Training Loss: 4.46442, LR: 0.0009048, Tokens/sec: 181391.58\n",
      "Step: 1146, Training Loss: 3.80644, LR: 0.0009046, Tokens/sec: 181172.79\n",
      "Step: 1147, Training Loss: 4.04138, LR: 0.0009043, Tokens/sec: 181424.34\n",
      "Step: 1148, Training Loss: 4.05064, LR: 0.0009041, Tokens/sec: 180913.50\n",
      "Step: 1149, Training Loss: 4.16781, LR: 0.0009039, Tokens/sec: 181473.10\n",
      "Step: 1150, Training Loss: 4.03020, LR: 0.0009036, Tokens/sec: 180041.99\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 1150, Eval Loss: 4.06796\n",
      "Step: 1151, Training Loss: 4.25032, LR: 0.0009034, Tokens/sec: 180974.68\n",
      "Step: 1152, Training Loss: 4.10715, LR: 0.0009031, Tokens/sec: 180966.30\n",
      "Step: 1153, Training Loss: 4.14704, LR: 0.0009029, Tokens/sec: 181321.93\n",
      "Step: 1154, Training Loss: 4.00475, LR: 0.0009026, Tokens/sec: 181314.18\n",
      "Step: 1155, Training Loss: 4.33467, LR: 0.0009024, Tokens/sec: 181525.98\n",
      "Step: 1156, Training Loss: 4.32219, LR: 0.0009021, Tokens/sec: 180919.27\n",
      "Step: 1157, Training Loss: 3.77121, LR: 0.0009019, Tokens/sec: 181033.65\n",
      "Step: 1158, Training Loss: 3.85442, LR: 0.0009016, Tokens/sec: 179886.92\n",
      "Step: 1159, Training Loss: 4.44007, LR: 0.0009014, Tokens/sec: 181611.52\n",
      "Step: 1160, Training Loss: 4.06976, LR: 0.0009011, Tokens/sec: 181979.55\n",
      "Step: 1161, Training Loss: 4.09115, LR: 0.0009009, Tokens/sec: 181334.44\n",
      "Step: 1162, Training Loss: 4.03729, LR: 0.0009006, Tokens/sec: 181295.74\n",
      "Step: 1163, Training Loss: 4.19392, LR: 0.0009004, Tokens/sec: 181272.07\n",
      "Step: 1164, Training Loss: 4.36015, LR: 0.0009001, Tokens/sec: 180670.13\n",
      "Step: 1165, Training Loss: 4.26251, LR: 0.0008999, Tokens/sec: 180871.99\n",
      "Step: 1166, Training Loss: 3.98526, LR: 0.0008996, Tokens/sec: 181498.11\n",
      "Step: 1167, Training Loss: 4.19685, LR: 0.0008994, Tokens/sec: 181396.29\n",
      "Step: 1168, Training Loss: 3.96325, LR: 0.0008991, Tokens/sec: 181590.96\n",
      "Step: 1169, Training Loss: 3.93088, LR: 0.0008989, Tokens/sec: 181725.09\n",
      "Step: 1170, Training Loss: 3.86134, LR: 0.0008986, Tokens/sec: 180781.81\n",
      "Step: 1171, Training Loss: 4.10627, LR: 0.0008984, Tokens/sec: 181556.45\n",
      "Step: 1172, Training Loss: 4.03378, LR: 0.0008981, Tokens/sec: 181329.52\n",
      "Step: 1173, Training Loss: 3.73789, LR: 0.0008979, Tokens/sec: 181356.83\n",
      "Step: 1174, Training Loss: 3.92283, LR: 0.0008976, Tokens/sec: 180875.89\n",
      "Step: 1175, Training Loss: 4.33052, LR: 0.0008974, Tokens/sec: 181187.70\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 1175, Eval Loss: 4.03782\n",
      "Step: 1176, Training Loss: 3.93079, LR: 0.0008971, Tokens/sec: 180369.34\n",
      "Step: 1177, Training Loss: 3.92790, LR: 0.0008969, Tokens/sec: 180707.31\n",
      "Step: 1178, Training Loss: 4.08185, LR: 0.0008966, Tokens/sec: 181108.43\n",
      "Step: 1179, Training Loss: 3.99000, LR: 0.0008964, Tokens/sec: 180990.67\n",
      "Step: 1180, Training Loss: 4.13670, LR: 0.0008961, Tokens/sec: 181441.35\n",
      "Step: 1181, Training Loss: 4.25836, LR: 0.0008959, Tokens/sec: 179811.70\n",
      "Step: 1182, Training Loss: 4.00609, LR: 0.0008956, Tokens/sec: 181518.94\n",
      "Step: 1183, Training Loss: 4.15964, LR: 0.0008953, Tokens/sec: 180728.17\n",
      "Step: 1184, Training Loss: 4.15682, LR: 0.0008951, Tokens/sec: 181654.34\n",
      "Step: 1185, Training Loss: 4.03338, LR: 0.0008948, Tokens/sec: 181364.16\n",
      "Step: 1186, Training Loss: 3.62933, LR: 0.0008946, Tokens/sec: 181141.16\n",
      "Step: 1187, Training Loss: 4.16536, LR: 0.0008943, Tokens/sec: 180410.75\n",
      "Step: 1188, Training Loss: 4.16250, LR: 0.0008941, Tokens/sec: 181399.21\n",
      "Step: 1189, Training Loss: 3.76958, LR: 0.0008938, Tokens/sec: 181368.23\n",
      "Step: 1190, Training Loss: 3.97002, LR: 0.0008936, Tokens/sec: 180986.67\n",
      "Step: 1191, Training Loss: 4.06104, LR: 0.0008933, Tokens/sec: 177870.76\n",
      "Step: 1192, Training Loss: 4.01574, LR: 0.0008930, Tokens/sec: 180366.12\n",
      "Step: 1193, Training Loss: 4.05737, LR: 0.0008928, Tokens/sec: 178458.87\n",
      "Step: 1194, Training Loss: 3.84618, LR: 0.0008925, Tokens/sec: 181486.97\n",
      "Step: 1195, Training Loss: 4.05348, LR: 0.0008923, Tokens/sec: 181052.76\n",
      "Step: 1196, Training Loss: 4.06745, LR: 0.0008920, Tokens/sec: 181431.72\n",
      "Step: 1197, Training Loss: 4.04429, LR: 0.0008917, Tokens/sec: 181514.56\n",
      "Step: 1198, Training Loss: 4.13678, LR: 0.0008915, Tokens/sec: 181259.03\n",
      "Step: 1199, Training Loss: 4.10478, LR: 0.0008912, Tokens/sec: 179760.62\n",
      "Step: 1200, Training Loss: 4.34469, LR: 0.0008910, Tokens/sec: 181125.17\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 1200, Eval Loss: 4.14853\n",
      "Step: 1201, Training Loss: 4.20449, LR: 0.0008907, Tokens/sec: 175855.81\n",
      "Step: 1202, Training Loss: 4.26775, LR: 0.0008905, Tokens/sec: 181107.36\n",
      "Step: 1203, Training Loss: 3.97639, LR: 0.0008902, Tokens/sec: 181376.18\n",
      "Step: 1204, Training Loss: 3.76845, LR: 0.0008899, Tokens/sec: 181333.41\n",
      "Step: 1205, Training Loss: 4.09343, LR: 0.0008897, Tokens/sec: 181537.30\n",
      "Step: 1206, Training Loss: 4.13856, LR: 0.0008894, Tokens/sec: 181197.01\n",
      "Step: 1207, Training Loss: 4.11658, LR: 0.0008892, Tokens/sec: 180167.37\n",
      "Step: 1208, Training Loss: 4.15975, LR: 0.0008889, Tokens/sec: 181107.35\n",
      "Step: 1209, Training Loss: 4.11481, LR: 0.0008886, Tokens/sec: 181274.76\n",
      "Step: 1210, Training Loss: 4.36792, LR: 0.0008884, Tokens/sec: 180807.56\n",
      "Step: 1211, Training Loss: 3.79886, LR: 0.0008881, Tokens/sec: 181067.82\n",
      "Step: 1212, Training Loss: 4.15282, LR: 0.0008878, Tokens/sec: 181264.51\n",
      "Step: 1213, Training Loss: 3.96721, LR: 0.0008876, Tokens/sec: 180779.20\n",
      "Step: 1214, Training Loss: 4.23763, LR: 0.0008873, Tokens/sec: 181341.60\n",
      "Step: 1215, Training Loss: 4.28023, LR: 0.0008871, Tokens/sec: 180652.30\n",
      "Step: 1216, Training Loss: 4.10672, LR: 0.0008868, Tokens/sec: 181165.02\n",
      "Step: 1217, Training Loss: 4.32670, LR: 0.0008865, Tokens/sec: 181507.37\n",
      "Step: 1218, Training Loss: 4.17382, LR: 0.0008863, Tokens/sec: 181564.28\n",
      "Step: 1219, Training Loss: 4.34385, LR: 0.0008860, Tokens/sec: 180556.13\n",
      "Step: 1220, Training Loss: 3.93445, LR: 0.0008857, Tokens/sec: 181443.43\n",
      "Step: 1221, Training Loss: 4.03175, LR: 0.0008855, Tokens/sec: 181121.02\n",
      "Step: 1222, Training Loss: 4.03723, LR: 0.0008852, Tokens/sec: 180859.29\n",
      "Step: 1223, Training Loss: 3.88238, LR: 0.0008849, Tokens/sec: 181189.08\n",
      "Step: 1224, Training Loss: 3.88053, LR: 0.0008847, Tokens/sec: 181539.41\n",
      "Step: 1225, Training Loss: 4.19713, LR: 0.0008844, Tokens/sec: 179802.89\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 1225, Eval Loss: 4.05337\n",
      "Step: 1226, Training Loss: 4.32511, LR: 0.0008841, Tokens/sec: 180661.87\n",
      "Step: 1227, Training Loss: 4.27423, LR: 0.0008839, Tokens/sec: 179502.03\n",
      "Step: 1228, Training Loss: 4.12964, LR: 0.0008836, Tokens/sec: 181257.61\n",
      "Step: 1229, Training Loss: 3.71389, LR: 0.0008833, Tokens/sec: 180849.38\n",
      "Step: 1230, Training Loss: 3.95102, LR: 0.0008831, Tokens/sec: 181035.38\n",
      "Step: 1231, Training Loss: 3.89705, LR: 0.0008828, Tokens/sec: 181531.00\n",
      "Step: 1232, Training Loss: 4.28198, LR: 0.0008825, Tokens/sec: 181152.55\n",
      "Step: 1233, Training Loss: 4.05980, LR: 0.0008823, Tokens/sec: 179647.20\n",
      "Step: 1234, Training Loss: 3.93956, LR: 0.0008820, Tokens/sec: 175875.29\n",
      "Step: 1235, Training Loss: 3.67053, LR: 0.0008817, Tokens/sec: 180954.50\n",
      "Step: 1236, Training Loss: 3.74080, LR: 0.0008815, Tokens/sec: 181813.91\n",
      "Step: 1237, Training Loss: 4.14955, LR: 0.0008812, Tokens/sec: 181149.43\n",
      "Step: 1238, Training Loss: 4.10354, LR: 0.0008809, Tokens/sec: 180924.38\n",
      "Step: 1239, Training Loss: 4.25070, LR: 0.0008807, Tokens/sec: 179458.12\n",
      "Step: 1240, Training Loss: 4.32399, LR: 0.0008804, Tokens/sec: 181329.57\n",
      "Step: 1241, Training Loss: 3.96655, LR: 0.0008801, Tokens/sec: 181565.01\n",
      "Step: 1242, Training Loss: 3.93571, LR: 0.0008798, Tokens/sec: 181631.16\n",
      "Step: 1243, Training Loss: 3.86002, LR: 0.0008796, Tokens/sec: 181399.45\n",
      "Step: 1244, Training Loss: 3.64226, LR: 0.0008793, Tokens/sec: 181199.86\n",
      "Step: 1245, Training Loss: 4.16571, LR: 0.0008790, Tokens/sec: 180576.72\n",
      "Step: 1246, Training Loss: 4.12944, LR: 0.0008788, Tokens/sec: 181097.68\n",
      "Step: 1247, Training Loss: 4.30293, LR: 0.0008785, Tokens/sec: 181529.95\n",
      "Step: 1248, Training Loss: 4.04675, LR: 0.0008782, Tokens/sec: 181773.73\n",
      "Step: 1249, Training Loss: 3.77899, LR: 0.0008779, Tokens/sec: 181391.44\n",
      "Step: 1250, Training Loss: 4.11505, LR: 0.0008777, Tokens/sec: 180932.06\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 1250, Eval Loss: 3.95328\n",
      "Step: 1251, Training Loss: 3.74646, LR: 0.0008774, Tokens/sec: 180379.40\n",
      "Step: 1252, Training Loss: 3.98402, LR: 0.0008771, Tokens/sec: 181663.73\n",
      "Step: 1253, Training Loss: 3.92577, LR: 0.0008769, Tokens/sec: 181008.15\n",
      "Step: 1254, Training Loss: 3.75262, LR: 0.0008766, Tokens/sec: 181262.95\n",
      "Step: 1255, Training Loss: 4.06800, LR: 0.0008763, Tokens/sec: 181433.98\n",
      "Step: 1256, Training Loss: 4.16515, LR: 0.0008760, Tokens/sec: 180029.31\n",
      "Step: 1257, Training Loss: 3.92834, LR: 0.0008758, Tokens/sec: 181106.23\n",
      "Step: 1258, Training Loss: 3.76116, LR: 0.0008755, Tokens/sec: 181493.20\n",
      "Step: 1259, Training Loss: 3.98916, LR: 0.0008752, Tokens/sec: 182192.16\n",
      "Step: 1260, Training Loss: 4.04712, LR: 0.0008749, Tokens/sec: 181536.45\n",
      "Step: 1261, Training Loss: 4.02483, LR: 0.0008747, Tokens/sec: 181561.81\n",
      "Step: 1262, Training Loss: 3.80687, LR: 0.0008744, Tokens/sec: 180162.82\n",
      "Step: 1263, Training Loss: 4.04770, LR: 0.0008741, Tokens/sec: 181914.69\n",
      "Step: 1264, Training Loss: 4.25187, LR: 0.0008738, Tokens/sec: 181465.11\n",
      "Step: 1265, Training Loss: 3.76469, LR: 0.0008736, Tokens/sec: 181136.68\n",
      "Step: 1266, Training Loss: 4.19594, LR: 0.0008733, Tokens/sec: 181354.36\n",
      "Step: 1267, Training Loss: 4.25244, LR: 0.0008730, Tokens/sec: 182258.20\n",
      "Step: 1268, Training Loss: 4.06803, LR: 0.0008727, Tokens/sec: 180544.04\n",
      "Step: 1269, Training Loss: 3.99728, LR: 0.0008725, Tokens/sec: 181381.26\n",
      "Step: 1270, Training Loss: 3.95764, LR: 0.0008722, Tokens/sec: 181406.68\n",
      "Step: 1271, Training Loss: 4.20091, LR: 0.0008719, Tokens/sec: 181498.04\n",
      "Step: 1272, Training Loss: 4.17445, LR: 0.0008716, Tokens/sec: 181845.67\n",
      "Step: 1273, Training Loss: 3.93548, LR: 0.0008713, Tokens/sec: 181409.74\n",
      "Step: 1274, Training Loss: 3.95051, LR: 0.0008711, Tokens/sec: 180577.00\n",
      "Step: 1275, Training Loss: 4.13407, LR: 0.0008708, Tokens/sec: 181224.97\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 1275, Eval Loss: 3.92616\n",
      "Step: 1276, Training Loss: 3.87685, LR: 0.0008705, Tokens/sec: 180944.87\n",
      "Step: 1277, Training Loss: 3.68538, LR: 0.0008702, Tokens/sec: 181387.80\n",
      "Step: 1278, Training Loss: 3.92711, LR: 0.0008699, Tokens/sec: 182132.77\n",
      "Step: 1279, Training Loss: 3.77323, LR: 0.0008697, Tokens/sec: 181419.02\n",
      "Step: 1280, Training Loss: 3.92635, LR: 0.0008694, Tokens/sec: 181399.98\n",
      "Step: 1281, Training Loss: 3.75638, LR: 0.0008691, Tokens/sec: 181339.22\n",
      "Step: 1282, Training Loss: 4.24654, LR: 0.0008688, Tokens/sec: 179273.07\n",
      "Step: 1283, Training Loss: 3.97052, LR: 0.0008685, Tokens/sec: 180185.09\n",
      "Step: 1284, Training Loss: 4.35249, LR: 0.0008683, Tokens/sec: 181094.69\n",
      "Step: 1285, Training Loss: 3.92866, LR: 0.0008680, Tokens/sec: 180980.89\n",
      "Step: 1286, Training Loss: 4.20302, LR: 0.0008677, Tokens/sec: 181176.40\n",
      "Step: 1287, Training Loss: 4.22433, LR: 0.0008674, Tokens/sec: 181299.37\n",
      "Step: 1288, Training Loss: 4.33185, LR: 0.0008671, Tokens/sec: 179892.82\n",
      "Step: 1289, Training Loss: 3.88157, LR: 0.0008669, Tokens/sec: 181323.51\n",
      "Step: 1290, Training Loss: 3.89093, LR: 0.0008666, Tokens/sec: 180988.40\n",
      "Step: 1291, Training Loss: 3.92723, LR: 0.0008663, Tokens/sec: 181741.09\n",
      "Step: 1292, Training Loss: 3.81522, LR: 0.0008660, Tokens/sec: 181455.71\n",
      "Step: 1293, Training Loss: 4.02220, LR: 0.0008657, Tokens/sec: 181619.15\n",
      "Step: 1294, Training Loss: 3.99141, LR: 0.0008654, Tokens/sec: 179621.50\n",
      "Step: 1295, Training Loss: 3.80809, LR: 0.0008652, Tokens/sec: 180905.53\n",
      "Step: 1296, Training Loss: 3.87190, LR: 0.0008649, Tokens/sec: 180875.94\n",
      "Step: 1297, Training Loss: 3.85220, LR: 0.0008646, Tokens/sec: 181138.69\n",
      "Step: 1298, Training Loss: 3.84449, LR: 0.0008643, Tokens/sec: 180811.68\n",
      "Step: 1299, Training Loss: 3.84785, LR: 0.0008640, Tokens/sec: 181068.14\n",
      "Step: 1300, Training Loss: 4.05075, LR: 0.0008637, Tokens/sec: 179402.36\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 1300, Eval Loss: 4.04684\n",
      "Step: 1301, Training Loss: 3.63256, LR: 0.0008635, Tokens/sec: 181163.42\n",
      "Step: 1302, Training Loss: 4.01143, LR: 0.0008632, Tokens/sec: 179959.99\n",
      "Step: 1303, Training Loss: 3.82918, LR: 0.0008629, Tokens/sec: 180982.89\n",
      "Step: 1304, Training Loss: 3.86244, LR: 0.0008626, Tokens/sec: 181427.60\n",
      "Step: 1305, Training Loss: 4.03374, LR: 0.0008623, Tokens/sec: 180871.97\n",
      "Step: 1306, Training Loss: 3.97574, LR: 0.0008620, Tokens/sec: 181318.57\n",
      "Step: 1307, Training Loss: 3.91661, LR: 0.0008617, Tokens/sec: 181629.10\n",
      "Step: 1308, Training Loss: 4.05284, LR: 0.0008615, Tokens/sec: 180133.60\n",
      "Step: 1309, Training Loss: 3.84968, LR: 0.0008612, Tokens/sec: 180975.61\n",
      "Step: 1310, Training Loss: 4.04775, LR: 0.0008609, Tokens/sec: 181355.30\n",
      "Step: 1311, Training Loss: 3.99173, LR: 0.0008606, Tokens/sec: 181932.00\n",
      "Step: 1312, Training Loss: 4.08884, LR: 0.0008603, Tokens/sec: 181559.85\n",
      "Step: 1313, Training Loss: 4.17815, LR: 0.0008600, Tokens/sec: 181657.41\n",
      "Step: 1314, Training Loss: 4.19877, LR: 0.0008597, Tokens/sec: 180216.57\n",
      "Step: 1315, Training Loss: 4.09616, LR: 0.0008594, Tokens/sec: 180792.80\n",
      "Step: 1316, Training Loss: 4.24479, LR: 0.0008592, Tokens/sec: 181373.96\n",
      "Step: 1317, Training Loss: 3.77315, LR: 0.0008589, Tokens/sec: 181189.15\n",
      "Step: 1318, Training Loss: 3.65752, LR: 0.0008586, Tokens/sec: 181191.17\n",
      "Step: 1319, Training Loss: 3.80657, LR: 0.0008583, Tokens/sec: 180980.08\n",
      "Step: 1320, Training Loss: 3.89966, LR: 0.0008580, Tokens/sec: 179032.64\n",
      "Step: 1321, Training Loss: 4.17893, LR: 0.0008577, Tokens/sec: 180434.73\n",
      "Step: 1322, Training Loss: 3.88477, LR: 0.0008574, Tokens/sec: 180365.09\n",
      "Step: 1323, Training Loss: 3.80428, LR: 0.0008571, Tokens/sec: 181257.93\n",
      "Step: 1324, Training Loss: 3.68070, LR: 0.0008568, Tokens/sec: 181026.92\n",
      "Step: 1325, Training Loss: 3.99918, LR: 0.0008565, Tokens/sec: 180904.89\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 1325, Eval Loss: 3.95059\n",
      "Step: 1326, Training Loss: 3.73170, LR: 0.0008563, Tokens/sec: 179909.20\n",
      "Step: 1327, Training Loss: 3.76135, LR: 0.0008560, Tokens/sec: 181325.68\n",
      "Step: 1328, Training Loss: 4.01083, LR: 0.0008557, Tokens/sec: 181322.23\n",
      "Step: 1329, Training Loss: 3.96722, LR: 0.0008554, Tokens/sec: 181355.75\n",
      "Step: 1330, Training Loss: 3.81103, LR: 0.0008551, Tokens/sec: 181695.13\n",
      "Step: 1331, Training Loss: 3.84122, LR: 0.0008548, Tokens/sec: 180460.13\n",
      "Step: 1332, Training Loss: 4.00450, LR: 0.0008545, Tokens/sec: 181052.90\n",
      "Step: 1333, Training Loss: 3.92276, LR: 0.0008542, Tokens/sec: 181212.22\n",
      "Step: 1334, Training Loss: 4.10291, LR: 0.0008539, Tokens/sec: 181611.48\n",
      "Step: 1335, Training Loss: 3.87171, LR: 0.0008536, Tokens/sec: 181914.25\n",
      "Step: 1336, Training Loss: 4.06113, LR: 0.0008533, Tokens/sec: 181344.65\n",
      "Step: 1337, Training Loss: 3.92900, LR: 0.0008530, Tokens/sec: 180612.67\n",
      "Step: 1338, Training Loss: 3.99308, LR: 0.0008527, Tokens/sec: 181236.35\n",
      "Step: 1339, Training Loss: 3.63566, LR: 0.0008524, Tokens/sec: 181921.95\n",
      "Step: 1340, Training Loss: 3.94679, LR: 0.0008522, Tokens/sec: 181690.76\n",
      "Step: 1341, Training Loss: 3.70025, LR: 0.0008519, Tokens/sec: 181684.14\n",
      "Step: 1342, Training Loss: 4.05723, LR: 0.0008516, Tokens/sec: 181513.42\n",
      "Step: 1343, Training Loss: 4.13055, LR: 0.0008513, Tokens/sec: 180797.73\n",
      "Step: 1344, Training Loss: 3.85204, LR: 0.0008510, Tokens/sec: 181528.35\n",
      "Step: 1345, Training Loss: 3.76866, LR: 0.0008507, Tokens/sec: 181523.71\n",
      "Step: 1346, Training Loss: 3.77624, LR: 0.0008504, Tokens/sec: 181428.12\n",
      "Step: 1347, Training Loss: 3.97616, LR: 0.0008501, Tokens/sec: 181591.38\n",
      "Step: 1348, Training Loss: 3.91169, LR: 0.0008498, Tokens/sec: 181937.69\n",
      "Step: 1349, Training Loss: 3.83889, LR: 0.0008495, Tokens/sec: 180597.42\n",
      "Step: 1350, Training Loss: 3.79911, LR: 0.0008492, Tokens/sec: 181644.39\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 1350, Eval Loss: 3.93339\n",
      "Step: 1351, Training Loss: 3.91543, LR: 0.0008489, Tokens/sec: 180195.38\n",
      "Step: 1352, Training Loss: 3.60742, LR: 0.0008486, Tokens/sec: 181357.49\n",
      "Step: 1353, Training Loss: 4.22131, LR: 0.0008483, Tokens/sec: 181749.61\n",
      "Step: 1354, Training Loss: 3.94666, LR: 0.0008480, Tokens/sec: 181650.51\n",
      "Step: 1355, Training Loss: 4.00249, LR: 0.0008477, Tokens/sec: 181274.75\n",
      "Step: 1356, Training Loss: 3.72671, LR: 0.0008474, Tokens/sec: 181441.74\n",
      "Step: 1357, Training Loss: 3.51946, LR: 0.0008471, Tokens/sec: 180676.06\n",
      "Step: 1358, Training Loss: 3.79238, LR: 0.0008468, Tokens/sec: 181416.25\n",
      "Step: 1359, Training Loss: 3.76409, LR: 0.0008465, Tokens/sec: 181096.37\n",
      "Step: 1360, Training Loss: 3.64428, LR: 0.0008462, Tokens/sec: 181485.67\n",
      "Step: 1361, Training Loss: 3.93169, LR: 0.0008459, Tokens/sec: 181437.61\n",
      "Step: 1362, Training Loss: 3.94030, LR: 0.0008456, Tokens/sec: 181502.70\n",
      "Step: 1363, Training Loss: 3.80976, LR: 0.0008453, Tokens/sec: 180434.16\n",
      "Step: 1364, Training Loss: 3.92718, LR: 0.0008450, Tokens/sec: 180947.49\n",
      "Step: 1365, Training Loss: 3.85877, LR: 0.0008447, Tokens/sec: 181212.57\n",
      "Step: 1366, Training Loss: 4.13569, LR: 0.0008444, Tokens/sec: 181407.48\n",
      "Step: 1367, Training Loss: 3.89414, LR: 0.0008441, Tokens/sec: 181341.57\n",
      "Step: 1368, Training Loss: 3.99154, LR: 0.0008438, Tokens/sec: 181771.85\n",
      "Step: 1369, Training Loss: 3.86495, LR: 0.0008435, Tokens/sec: 180606.15\n",
      "Step: 1370, Training Loss: 3.71890, LR: 0.0008432, Tokens/sec: 181456.45\n",
      "Step: 1371, Training Loss: 3.75933, LR: 0.0008429, Tokens/sec: 181363.22\n",
      "Step: 1372, Training Loss: 3.98128, LR: 0.0008426, Tokens/sec: 181452.42\n",
      "Step: 1373, Training Loss: 3.71067, LR: 0.0008423, Tokens/sec: 181654.21\n",
      "Step: 1374, Training Loss: 4.24910, LR: 0.0008420, Tokens/sec: 181619.37\n",
      "Step: 1375, Training Loss: 4.02917, LR: 0.0008417, Tokens/sec: 180027.08\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 1375, Eval Loss: 3.97877\n",
      "Step: 1376, Training Loss: 3.86277, LR: 0.0008414, Tokens/sec: 181014.92\n",
      "Step: 1377, Training Loss: 3.97965, LR: 0.0008411, Tokens/sec: 180757.88\n",
      "Step: 1378, Training Loss: 3.91080, LR: 0.0008408, Tokens/sec: 181238.80\n",
      "Step: 1379, Training Loss: 3.81476, LR: 0.0008405, Tokens/sec: 181590.51\n",
      "Step: 1380, Training Loss: 3.84422, LR: 0.0008402, Tokens/sec: 181326.86\n",
      "Step: 1381, Training Loss: 3.45960, LR: 0.0008399, Tokens/sec: 181257.69\n",
      "Step: 1382, Training Loss: 4.15268, LR: 0.0008396, Tokens/sec: 181572.45\n",
      "Step: 1383, Training Loss: 3.64796, LR: 0.0008393, Tokens/sec: 180547.29\n",
      "Step: 1384, Training Loss: 3.72409, LR: 0.0008390, Tokens/sec: 181224.19\n",
      "Step: 1385, Training Loss: 4.16741, LR: 0.0008387, Tokens/sec: 181983.24\n",
      "Step: 1386, Training Loss: 3.91934, LR: 0.0008384, Tokens/sec: 181513.40\n",
      "Step: 1387, Training Loss: 3.77748, LR: 0.0008381, Tokens/sec: 181367.88\n",
      "Step: 1388, Training Loss: 3.77789, LR: 0.0008377, Tokens/sec: 181877.67\n",
      "Step: 1389, Training Loss: 3.84943, LR: 0.0008374, Tokens/sec: 180572.39\n",
      "Step: 1390, Training Loss: 4.06385, LR: 0.0008371, Tokens/sec: 181417.08\n",
      "Step: 1391, Training Loss: 4.25513, LR: 0.0008368, Tokens/sec: 181696.45\n",
      "Step: 1392, Training Loss: 3.67396, LR: 0.0008365, Tokens/sec: 181778.75\n",
      "Step: 1393, Training Loss: 3.70696, LR: 0.0008362, Tokens/sec: 181698.19\n",
      "Step: 1394, Training Loss: 3.91828, LR: 0.0008359, Tokens/sec: 173132.17\n",
      "Step: 1395, Training Loss: 4.05213, LR: 0.0008356, Tokens/sec: 178883.23\n",
      "Step: 1396, Training Loss: 3.84433, LR: 0.0008353, Tokens/sec: 181555.67\n",
      "Step: 1397, Training Loss: 4.28701, LR: 0.0008350, Tokens/sec: 181994.99\n",
      "Step: 1398, Training Loss: 3.57790, LR: 0.0008347, Tokens/sec: 181100.90\n",
      "Step: 1399, Training Loss: 3.69449, LR: 0.0008344, Tokens/sec: 181581.46\n",
      "Step: 1400, Training Loss: 3.80030, LR: 0.0008341, Tokens/sec: 181744.26\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 1400, Eval Loss: 3.87307\n",
      "Step: 1401, Training Loss: 3.82148, LR: 0.0008338, Tokens/sec: 180766.20\n",
      "Step: 1402, Training Loss: 3.94572, LR: 0.0008334, Tokens/sec: 180820.44\n",
      "Step: 1403, Training Loss: 3.43263, LR: 0.0008331, Tokens/sec: 181146.22\n",
      "Step: 1404, Training Loss: 3.76125, LR: 0.0008328, Tokens/sec: 180535.92\n",
      "Step: 1405, Training Loss: 3.57212, LR: 0.0008325, Tokens/sec: 181215.91\n",
      "Step: 1406, Training Loss: 3.83860, LR: 0.0008322, Tokens/sec: 179870.98\n",
      "Step: 1407, Training Loss: 3.88546, LR: 0.0008319, Tokens/sec: 181254.81\n",
      "Step: 1408, Training Loss: 3.60521, LR: 0.0008316, Tokens/sec: 180900.87\n",
      "Step: 1409, Training Loss: 3.86711, LR: 0.0008313, Tokens/sec: 180771.73\n",
      "Step: 1410, Training Loss: 3.69030, LR: 0.0008310, Tokens/sec: 180768.09\n",
      "Step: 1411, Training Loss: 3.83926, LR: 0.0008307, Tokens/sec: 181181.02\n",
      "Step: 1412, Training Loss: 3.96765, LR: 0.0008303, Tokens/sec: 179696.67\n",
      "Step: 1413, Training Loss: 4.08700, LR: 0.0008300, Tokens/sec: 181249.20\n",
      "Step: 1414, Training Loss: 3.90790, LR: 0.0008297, Tokens/sec: 181284.90\n",
      "Step: 1415, Training Loss: 3.88535, LR: 0.0008294, Tokens/sec: 181214.52\n",
      "Step: 1416, Training Loss: 4.11056, LR: 0.0008291, Tokens/sec: 180926.91\n",
      "Step: 1417, Training Loss: 4.01919, LR: 0.0008288, Tokens/sec: 181584.91\n",
      "Step: 1418, Training Loss: 3.97853, LR: 0.0008285, Tokens/sec: 180849.48\n",
      "Step: 1419, Training Loss: 3.93331, LR: 0.0008282, Tokens/sec: 181271.52\n",
      "Step: 1420, Training Loss: 3.66080, LR: 0.0008279, Tokens/sec: 180820.89\n",
      "Step: 1421, Training Loss: 3.66911, LR: 0.0008275, Tokens/sec: 181318.25\n",
      "Step: 1422, Training Loss: 3.84104, LR: 0.0008272, Tokens/sec: 180769.75\n",
      "Step: 1423, Training Loss: 4.20371, LR: 0.0008269, Tokens/sec: 180024.30\n",
      "Step: 1424, Training Loss: 3.86976, LR: 0.0008266, Tokens/sec: 178956.63\n",
      "Step: 1425, Training Loss: 3.98131, LR: 0.0008263, Tokens/sec: 180656.72\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 1425, Eval Loss: 3.98073\n",
      "Step: 1426, Training Loss: 3.50550, LR: 0.0008260, Tokens/sec: 179856.39\n",
      "Step: 1427, Training Loss: 3.77086, LR: 0.0008257, Tokens/sec: 180610.82\n",
      "Step: 1428, Training Loss: 3.94207, LR: 0.0008253, Tokens/sec: 181114.80\n",
      "Step: 1429, Training Loss: 4.04036, LR: 0.0008250, Tokens/sec: 181408.35\n",
      "Step: 1430, Training Loss: 3.75732, LR: 0.0008247, Tokens/sec: 180282.17\n",
      "Step: 1431, Training Loss: 3.79222, LR: 0.0008244, Tokens/sec: 180946.16\n",
      "Step: 1432, Training Loss: 3.82942, LR: 0.0008241, Tokens/sec: 179790.51\n",
      "Step: 1433, Training Loss: 4.06661, LR: 0.0008238, Tokens/sec: 181434.67\n",
      "Step: 1434, Training Loss: 3.98032, LR: 0.0008235, Tokens/sec: 181179.00\n",
      "Step: 1435, Training Loss: 3.96878, LR: 0.0008231, Tokens/sec: 171599.50\n",
      "Step: 1436, Training Loss: 3.64272, LR: 0.0008228, Tokens/sec: 178540.30\n",
      "Step: 1437, Training Loss: 3.82460, LR: 0.0008225, Tokens/sec: 179585.21\n",
      "Step: 1438, Training Loss: 3.77749, LR: 0.0008222, Tokens/sec: 178587.60\n",
      "Step: 1439, Training Loss: 3.64056, LR: 0.0008219, Tokens/sec: 180364.23\n",
      "Step: 1440, Training Loss: 4.08366, LR: 0.0008216, Tokens/sec: 181212.71\n",
      "Step: 1441, Training Loss: 3.73167, LR: 0.0008212, Tokens/sec: 181193.54\n",
      "Step: 1442, Training Loss: 3.56590, LR: 0.0008209, Tokens/sec: 181600.13\n",
      "Step: 1443, Training Loss: 3.91213, LR: 0.0008206, Tokens/sec: 181572.94\n",
      "Step: 1444, Training Loss: 4.08216, LR: 0.0008203, Tokens/sec: 179914.19\n",
      "Step: 1445, Training Loss: 3.73798, LR: 0.0008200, Tokens/sec: 180639.86\n",
      "Step: 1446, Training Loss: 4.12116, LR: 0.0008197, Tokens/sec: 181440.65\n",
      "Step: 1447, Training Loss: 3.74619, LR: 0.0008193, Tokens/sec: 181218.20\n",
      "Step: 1448, Training Loss: 3.59999, LR: 0.0008190, Tokens/sec: 181474.34\n",
      "Step: 1449, Training Loss: 3.79786, LR: 0.0008187, Tokens/sec: 181483.30\n",
      "Step: 1450, Training Loss: 3.87398, LR: 0.0008184, Tokens/sec: 180148.99\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 1450, Eval Loss: 3.85555\n",
      "Step: 1451, Training Loss: 3.94304, LR: 0.0008181, Tokens/sec: 181536.09\n",
      "Step: 1452, Training Loss: 4.03247, LR: 0.0008177, Tokens/sec: 179652.62\n",
      "Step: 1453, Training Loss: 3.82858, LR: 0.0008174, Tokens/sec: 181046.49\n",
      "Step: 1454, Training Loss: 3.96217, LR: 0.0008171, Tokens/sec: 181053.67\n",
      "Step: 1455, Training Loss: 3.62909, LR: 0.0008168, Tokens/sec: 181105.71\n",
      "Step: 1456, Training Loss: 3.57003, LR: 0.0008165, Tokens/sec: 181384.98\n",
      "Step: 1457, Training Loss: 3.93568, LR: 0.0008161, Tokens/sec: 181351.02\n",
      "Step: 1458, Training Loss: 3.97654, LR: 0.0008158, Tokens/sec: 179935.91\n",
      "Step: 1459, Training Loss: 3.73948, LR: 0.0008155, Tokens/sec: 181268.21\n",
      "Step: 1460, Training Loss: 3.87202, LR: 0.0008152, Tokens/sec: 181275.26\n",
      "Step: 1461, Training Loss: 3.89327, LR: 0.0008149, Tokens/sec: 181629.44\n",
      "Step: 1462, Training Loss: 3.69852, LR: 0.0008145, Tokens/sec: 181625.46\n",
      "Step: 1463, Training Loss: 3.69859, LR: 0.0008142, Tokens/sec: 181529.75\n",
      "Step: 1464, Training Loss: 3.66489, LR: 0.0008139, Tokens/sec: 180688.28\n",
      "Step: 1465, Training Loss: 3.74084, LR: 0.0008136, Tokens/sec: 181144.28\n",
      "Step: 1466, Training Loss: 3.83175, LR: 0.0008132, Tokens/sec: 181190.24\n",
      "Step: 1467, Training Loss: 3.78737, LR: 0.0008129, Tokens/sec: 181941.91\n",
      "Step: 1468, Training Loss: 3.83643, LR: 0.0008126, Tokens/sec: 181616.45\n",
      "Step: 1469, Training Loss: 3.68424, LR: 0.0008123, Tokens/sec: 181574.79\n",
      "Step: 1470, Training Loss: 4.12577, LR: 0.0008120, Tokens/sec: 179933.64\n",
      "Step: 1471, Training Loss: 3.71489, LR: 0.0008116, Tokens/sec: 181146.58\n",
      "Step: 1472, Training Loss: 3.58843, LR: 0.0008113, Tokens/sec: 181514.67\n",
      "Step: 1473, Training Loss: 3.99773, LR: 0.0008110, Tokens/sec: 181506.89\n",
      "Step: 1474, Training Loss: 3.59029, LR: 0.0008107, Tokens/sec: 181514.66\n",
      "Step: 1475, Training Loss: 3.74913, LR: 0.0008103, Tokens/sec: 181815.08\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 1475, Eval Loss: 3.73459\n",
      "Step: 1476, Training Loss: 3.85594, LR: 0.0008100, Tokens/sec: 180513.08\n",
      "Step: 1477, Training Loss: 3.67282, LR: 0.0008097, Tokens/sec: 180871.89\n",
      "Step: 1478, Training Loss: 3.76013, LR: 0.0008094, Tokens/sec: 181100.30\n",
      "Step: 1479, Training Loss: 3.82411, LR: 0.0008090, Tokens/sec: 181327.38\n",
      "Step: 1480, Training Loss: 3.93372, LR: 0.0008087, Tokens/sec: 181316.43\n",
      "Step: 1481, Training Loss: 3.80138, LR: 0.0008084, Tokens/sec: 174851.30\n",
      "Step: 1482, Training Loss: 3.91248, LR: 0.0008081, Tokens/sec: 181294.87\n",
      "Step: 1483, Training Loss: 3.67463, LR: 0.0008077, Tokens/sec: 181566.62\n",
      "Step: 1484, Training Loss: 3.88641, LR: 0.0008074, Tokens/sec: 180778.45\n",
      "Step: 1485, Training Loss: 3.84838, LR: 0.0008071, Tokens/sec: 181492.22\n",
      "Step: 1486, Training Loss: 3.67239, LR: 0.0008068, Tokens/sec: 181502.61\n",
      "Step: 1487, Training Loss: 3.74253, LR: 0.0008064, Tokens/sec: 180219.29\n",
      "Step: 1488, Training Loss: 3.80933, LR: 0.0008061, Tokens/sec: 181076.59\n",
      "Step: 1489, Training Loss: 4.10289, LR: 0.0008058, Tokens/sec: 181730.33\n",
      "Step: 1490, Training Loss: 4.00910, LR: 0.0008055, Tokens/sec: 180900.24\n",
      "Step: 1491, Training Loss: 3.80146, LR: 0.0008051, Tokens/sec: 182217.81\n",
      "Step: 1492, Training Loss: 3.76890, LR: 0.0008048, Tokens/sec: 181332.05\n",
      "Step: 1493, Training Loss: 3.81107, LR: 0.0008045, Tokens/sec: 180337.21\n",
      "Step: 1494, Training Loss: 3.83195, LR: 0.0008041, Tokens/sec: 180893.52\n",
      "Step: 1495, Training Loss: 3.44087, LR: 0.0008038, Tokens/sec: 180508.54\n",
      "Step: 1496, Training Loss: 4.13144, LR: 0.0008035, Tokens/sec: 180754.16\n",
      "Step: 1497, Training Loss: 3.91001, LR: 0.0008032, Tokens/sec: 181116.19\n",
      "Step: 1498, Training Loss: 3.85525, LR: 0.0008028, Tokens/sec: 181243.54\n",
      "Step: 1499, Training Loss: 3.80729, LR: 0.0008025, Tokens/sec: 180534.23\n",
      "Step: 1500, Training Loss: 3.43190, LR: 0.0008022, Tokens/sec: 181546.93\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 1500, Eval Loss: 3.87458\n",
      "Step: 1501, Training Loss: 3.63102, LR: 0.0008018, Tokens/sec: 180562.54\n",
      "Step: 1502, Training Loss: 3.51426, LR: 0.0008015, Tokens/sec: 181670.45\n",
      "Step: 1503, Training Loss: 3.23482, LR: 0.0008012, Tokens/sec: 181866.13\n",
      "Step: 1504, Training Loss: 3.77706, LR: 0.0008009, Tokens/sec: 181214.05\n",
      "Step: 1505, Training Loss: 3.62884, LR: 0.0008005, Tokens/sec: 181357.09\n",
      "Step: 1506, Training Loss: 3.99575, LR: 0.0008002, Tokens/sec: 181246.94\n",
      "Step: 1507, Training Loss: 3.81941, LR: 0.0007999, Tokens/sec: 181090.21\n",
      "Step: 1508, Training Loss: 3.79550, LR: 0.0007995, Tokens/sec: 180692.70\n",
      "Step: 1509, Training Loss: 3.62041, LR: 0.0007992, Tokens/sec: 181204.46\n",
      "Step: 1510, Training Loss: 3.61377, LR: 0.0007989, Tokens/sec: 181708.95\n",
      "Step: 1511, Training Loss: 3.86856, LR: 0.0007985, Tokens/sec: 181139.56\n",
      "Step: 1512, Training Loss: 3.49199, LR: 0.0007982, Tokens/sec: 181405.53\n",
      "Step: 1513, Training Loss: 3.88204, LR: 0.0007979, Tokens/sec: 180263.31\n",
      "Step: 1514, Training Loss: 3.15168, LR: 0.0007975, Tokens/sec: 181053.37\n",
      "Step: 1515, Training Loss: 3.86492, LR: 0.0007972, Tokens/sec: 181079.40\n",
      "Step: 1516, Training Loss: 3.87335, LR: 0.0007969, Tokens/sec: 181166.61\n",
      "Step: 1517, Training Loss: 4.89609, LR: 0.0007965, Tokens/sec: 181181.67\n",
      "Step: 1518, Training Loss: 3.67583, LR: 0.0007962, Tokens/sec: 181026.55\n",
      "Step: 1519, Training Loss: 3.77997, LR: 0.0007959, Tokens/sec: 179211.85\n",
      "Step: 1520, Training Loss: 4.00810, LR: 0.0007955, Tokens/sec: 180824.27\n",
      "Step: 1521, Training Loss: 3.97347, LR: 0.0007952, Tokens/sec: 180734.24\n",
      "Step: 1522, Training Loss: 3.92724, LR: 0.0007949, Tokens/sec: 181267.48\n",
      "Step: 1523, Training Loss: 3.90773, LR: 0.0007945, Tokens/sec: 181487.58\n",
      "Step: 1524, Training Loss: 3.53646, LR: 0.0007942, Tokens/sec: 181166.50\n",
      "Step: 1525, Training Loss: 3.42055, LR: 0.0007939, Tokens/sec: 180366.50\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 1525, Eval Loss: 3.78433\n",
      "Step: 1526, Training Loss: 3.48886, LR: 0.0007935, Tokens/sec: 171795.78\n",
      "Step: 1527, Training Loss: 3.70043, LR: 0.0007932, Tokens/sec: 175065.49\n",
      "Step: 1528, Training Loss: 3.65871, LR: 0.0007929, Tokens/sec: 179183.71\n",
      "Step: 1529, Training Loss: 3.60227, LR: 0.0007925, Tokens/sec: 180512.02\n",
      "Step: 1530, Training Loss: 3.70583, LR: 0.0007922, Tokens/sec: 180728.41\n",
      "Step: 1531, Training Loss: 3.84893, LR: 0.0007919, Tokens/sec: 180300.33\n",
      "Step: 1532, Training Loss: 3.87858, LR: 0.0007915, Tokens/sec: 181645.68\n",
      "Step: 1533, Training Loss: 3.78056, LR: 0.0007912, Tokens/sec: 177773.43\n",
      "Step: 1534, Training Loss: 3.83874, LR: 0.0007909, Tokens/sec: 180665.20\n",
      "Step: 1535, Training Loss: 3.74428, LR: 0.0007905, Tokens/sec: 180696.12\n",
      "Step: 1536, Training Loss: 3.63383, LR: 0.0007902, Tokens/sec: 181445.37\n",
      "Step: 1537, Training Loss: 4.00928, LR: 0.0007899, Tokens/sec: 180643.33\n",
      "Step: 1538, Training Loss: 4.09188, LR: 0.0007895, Tokens/sec: 181459.01\n",
      "Step: 1539, Training Loss: 3.56601, LR: 0.0007892, Tokens/sec: 180140.29\n",
      "Step: 1540, Training Loss: 3.71701, LR: 0.0007888, Tokens/sec: 180994.74\n",
      "Step: 1541, Training Loss: 3.90101, LR: 0.0007885, Tokens/sec: 178446.90\n",
      "Step: 1542, Training Loss: 3.86847, LR: 0.0007882, Tokens/sec: 178635.80\n",
      "Step: 1543, Training Loss: 3.69515, LR: 0.0007878, Tokens/sec: 181296.73\n",
      "Step: 1544, Training Loss: 3.67290, LR: 0.0007875, Tokens/sec: 181403.04\n",
      "Step: 1545, Training Loss: 3.72488, LR: 0.0007872, Tokens/sec: 179952.76\n",
      "Step: 1546, Training Loss: 3.87327, LR: 0.0007868, Tokens/sec: 180995.04\n",
      "Step: 1547, Training Loss: 3.71215, LR: 0.0007865, Tokens/sec: 181313.90\n",
      "Step: 1548, Training Loss: 3.73924, LR: 0.0007861, Tokens/sec: 180796.86\n",
      "Step: 1549, Training Loss: 3.63750, LR: 0.0007858, Tokens/sec: 181185.69\n",
      "Step: 1550, Training Loss: 3.73210, LR: 0.0007855, Tokens/sec: 181322.77\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 1550, Eval Loss: 3.77603\n",
      "Step: 1551, Training Loss: 3.87162, LR: 0.0007851, Tokens/sec: 181356.57\n",
      "Step: 1552, Training Loss: 3.53152, LR: 0.0007848, Tokens/sec: 181415.69\n",
      "Step: 1553, Training Loss: 3.68119, LR: 0.0007845, Tokens/sec: 181354.32\n",
      "Step: 1554, Training Loss: 4.07875, LR: 0.0007841, Tokens/sec: 182125.33\n",
      "Step: 1555, Training Loss: 3.80171, LR: 0.0007838, Tokens/sec: 181623.95\n",
      "Step: 1556, Training Loss: 3.67060, LR: 0.0007834, Tokens/sec: 180220.89\n",
      "Step: 1557, Training Loss: 3.65043, LR: 0.0007831, Tokens/sec: 181181.35\n",
      "Step: 1558, Training Loss: 3.81248, LR: 0.0007828, Tokens/sec: 181138.64\n",
      "Step: 1559, Training Loss: 3.99391, LR: 0.0007824, Tokens/sec: 181233.48\n",
      "Step: 1560, Training Loss: 3.71950, LR: 0.0007821, Tokens/sec: 181321.08\n",
      "Step: 1561, Training Loss: 3.60629, LR: 0.0007817, Tokens/sec: 181799.28\n",
      "Step: 1562, Training Loss: 3.86033, LR: 0.0007814, Tokens/sec: 180369.34\n",
      "Step: 1563, Training Loss: 3.75914, LR: 0.0007811, Tokens/sec: 181061.24\n",
      "Step: 1564, Training Loss: 3.59778, LR: 0.0007807, Tokens/sec: 181871.11\n",
      "Step: 1565, Training Loss: 3.73065, LR: 0.0007804, Tokens/sec: 181205.01\n",
      "Step: 1566, Training Loss: 3.82106, LR: 0.0007800, Tokens/sec: 181783.86\n",
      "Step: 1567, Training Loss: 3.44279, LR: 0.0007797, Tokens/sec: 181046.01\n",
      "Step: 1568, Training Loss: 3.66734, LR: 0.0007793, Tokens/sec: 180798.65\n",
      "Step: 1569, Training Loss: 3.65901, LR: 0.0007790, Tokens/sec: 182052.56\n",
      "Step: 1570, Training Loss: 3.73302, LR: 0.0007787, Tokens/sec: 182150.98\n",
      "Step: 1571, Training Loss: 3.64933, LR: 0.0007783, Tokens/sec: 181360.44\n",
      "Step: 1572, Training Loss: 3.74281, LR: 0.0007780, Tokens/sec: 181506.65\n",
      "Step: 1573, Training Loss: 3.70314, LR: 0.0007776, Tokens/sec: 181282.17\n",
      "Step: 1574, Training Loss: 3.51411, LR: 0.0007773, Tokens/sec: 180182.70\n",
      "Step: 1575, Training Loss: 3.48789, LR: 0.0007769, Tokens/sec: 181210.16\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 1575, Eval Loss: 3.78399\n",
      "Step: 1576, Training Loss: 3.53204, LR: 0.0007766, Tokens/sec: 180792.28\n",
      "Step: 1577, Training Loss: 3.86858, LR: 0.0007763, Tokens/sec: 181454.95\n",
      "Step: 1578, Training Loss: 3.86628, LR: 0.0007759, Tokens/sec: 181528.80\n",
      "Step: 1579, Training Loss: 3.84595, LR: 0.0007756, Tokens/sec: 181250.70\n",
      "Step: 1580, Training Loss: 3.55200, LR: 0.0007752, Tokens/sec: 181609.79\n",
      "Step: 1581, Training Loss: 3.70670, LR: 0.0007749, Tokens/sec: 181497.62\n",
      "Step: 1582, Training Loss: 3.62429, LR: 0.0007745, Tokens/sec: 179637.76\n",
      "Step: 1583, Training Loss: 3.57101, LR: 0.0007742, Tokens/sec: 181752.15\n",
      "Step: 1584, Training Loss: 3.65410, LR: 0.0007739, Tokens/sec: 178209.65\n",
      "Step: 1585, Training Loss: 3.70202, LR: 0.0007735, Tokens/sec: 181566.21\n",
      "Step: 1586, Training Loss: 3.82673, LR: 0.0007732, Tokens/sec: 181322.30\n",
      "Step: 1587, Training Loss: 3.79155, LR: 0.0007728, Tokens/sec: 181648.54\n",
      "Step: 1588, Training Loss: 3.60244, LR: 0.0007725, Tokens/sec: 179888.81\n",
      "Step: 1589, Training Loss: 3.43663, LR: 0.0007721, Tokens/sec: 181318.01\n",
      "Step: 1590, Training Loss: 3.80017, LR: 0.0007718, Tokens/sec: 181139.57\n",
      "Step: 1591, Training Loss: 3.58724, LR: 0.0007714, Tokens/sec: 181445.87\n",
      "Step: 1592, Training Loss: 3.77953, LR: 0.0007711, Tokens/sec: 181294.17\n",
      "Step: 1593, Training Loss: 3.41013, LR: 0.0007707, Tokens/sec: 181173.24\n",
      "Step: 1594, Training Loss: 3.93366, LR: 0.0007704, Tokens/sec: 179934.04\n",
      "Step: 1595, Training Loss: 3.66391, LR: 0.0007700, Tokens/sec: 176614.96\n",
      "Step: 1596, Training Loss: 3.74547, LR: 0.0007697, Tokens/sec: 170311.68\n",
      "Step: 1597, Training Loss: 3.47557, LR: 0.0007694, Tokens/sec: 180987.02\n",
      "Step: 1598, Training Loss: 3.75790, LR: 0.0007690, Tokens/sec: 180927.70\n",
      "Step: 1599, Training Loss: 3.73312, LR: 0.0007687, Tokens/sec: 181219.33\n",
      "Step: 1600, Training Loss: 3.68130, LR: 0.0007683, Tokens/sec: 179524.95\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 1600, Eval Loss: 3.79790\n",
      "Step: 1601, Training Loss: 4.02228, LR: 0.0007680, Tokens/sec: 180365.13\n",
      "Step: 1602, Training Loss: 3.34143, LR: 0.0007676, Tokens/sec: 179545.84\n",
      "Step: 1603, Training Loss: 3.58781, LR: 0.0007673, Tokens/sec: 181499.49\n",
      "Step: 1604, Training Loss: 3.71238, LR: 0.0007669, Tokens/sec: 181099.65\n",
      "Step: 1605, Training Loss: 3.52602, LR: 0.0007666, Tokens/sec: 180196.69\n",
      "Step: 1606, Training Loss: 3.85057, LR: 0.0007662, Tokens/sec: 181014.03\n",
      "Step: 1607, Training Loss: 3.49481, LR: 0.0007659, Tokens/sec: 181476.60\n",
      "Step: 1608, Training Loss: 3.89856, LR: 0.0007655, Tokens/sec: 179940.38\n",
      "Step: 1609, Training Loss: 3.74214, LR: 0.0007652, Tokens/sec: 181543.81\n",
      "Step: 1610, Training Loss: 3.85454, LR: 0.0007648, Tokens/sec: 181625.36\n",
      "Step: 1611, Training Loss: 4.13468, LR: 0.0007645, Tokens/sec: 181086.71\n",
      "Step: 1612, Training Loss: 3.43491, LR: 0.0007641, Tokens/sec: 181351.02\n",
      "Step: 1613, Training Loss: 3.83711, LR: 0.0007638, Tokens/sec: 181065.43\n",
      "Step: 1614, Training Loss: 3.68472, LR: 0.0007634, Tokens/sec: 180238.20\n",
      "Step: 1615, Training Loss: 3.83928, LR: 0.0007631, Tokens/sec: 181229.03\n",
      "Step: 1616, Training Loss: 3.65412, LR: 0.0007627, Tokens/sec: 180766.62\n",
      "Step: 1617, Training Loss: 3.71083, LR: 0.0007624, Tokens/sec: 181448.69\n",
      "Step: 1618, Training Loss: 3.48634, LR: 0.0007620, Tokens/sec: 181015.19\n",
      "Step: 1619, Training Loss: 3.48488, LR: 0.0007617, Tokens/sec: 181341.41\n",
      "Step: 1620, Training Loss: 3.54771, LR: 0.0007613, Tokens/sec: 180819.23\n",
      "Step: 1621, Training Loss: 3.62052, LR: 0.0007610, Tokens/sec: 178833.49\n",
      "Step: 1622, Training Loss: 3.65897, LR: 0.0007606, Tokens/sec: 181618.25\n",
      "Step: 1623, Training Loss: 3.78858, LR: 0.0007603, Tokens/sec: 181261.65\n",
      "Step: 1624, Training Loss: 3.63403, LR: 0.0007599, Tokens/sec: 181120.71\n",
      "Step: 1625, Training Loss: 3.79117, LR: 0.0007596, Tokens/sec: 181612.81\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 1625, Eval Loss: 3.67630\n",
      "Step: 1626, Training Loss: 3.78106, LR: 0.0007592, Tokens/sec: 180915.25\n",
      "Step: 1627, Training Loss: 3.81126, LR: 0.0007589, Tokens/sec: 181527.39\n",
      "Step: 1628, Training Loss: 3.27884, LR: 0.0007585, Tokens/sec: 180740.10\n",
      "Step: 1629, Training Loss: 3.37849, LR: 0.0007582, Tokens/sec: 181453.87\n",
      "Step: 1630, Training Loss: 3.34668, LR: 0.0007578, Tokens/sec: 181777.38\n",
      "Step: 1631, Training Loss: 3.62429, LR: 0.0007575, Tokens/sec: 180352.69\n",
      "Step: 1632, Training Loss: 3.95467, LR: 0.0007571, Tokens/sec: 181241.16\n",
      "Step: 1633, Training Loss: 4.04091, LR: 0.0007568, Tokens/sec: 181648.97\n",
      "Step: 1634, Training Loss: 3.42480, LR: 0.0007564, Tokens/sec: 181228.37\n",
      "Step: 1635, Training Loss: 3.75336, LR: 0.0007560, Tokens/sec: 181312.28\n",
      "Step: 1636, Training Loss: 3.90761, LR: 0.0007557, Tokens/sec: 181580.85\n",
      "Step: 1637, Training Loss: 3.50318, LR: 0.0007553, Tokens/sec: 180238.88\n",
      "Step: 1638, Training Loss: 3.87310, LR: 0.0007550, Tokens/sec: 180982.13\n",
      "Step: 1639, Training Loss: 3.47696, LR: 0.0007546, Tokens/sec: 181391.95\n",
      "Step: 1640, Training Loss: 3.66997, LR: 0.0007543, Tokens/sec: 181503.04\n",
      "Step: 1641, Training Loss: 3.79708, LR: 0.0007539, Tokens/sec: 181237.42\n",
      "Step: 1642, Training Loss: 3.73487, LR: 0.0007536, Tokens/sec: 181445.84\n",
      "Step: 1643, Training Loss: 3.86998, LR: 0.0007532, Tokens/sec: 180244.00\n",
      "Step: 1644, Training Loss: 3.71979, LR: 0.0007529, Tokens/sec: 181459.17\n",
      "Step: 1645, Training Loss: 3.72264, LR: 0.0007525, Tokens/sec: 181454.31\n",
      "Step: 1646, Training Loss: 3.82877, LR: 0.0007522, Tokens/sec: 181638.96\n",
      "Step: 1647, Training Loss: 3.58535, LR: 0.0007518, Tokens/sec: 181394.41\n",
      "Step: 1648, Training Loss: 3.90570, LR: 0.0007514, Tokens/sec: 181601.45\n",
      "Step: 1649, Training Loss: 3.97454, LR: 0.0007511, Tokens/sec: 180858.30\n",
      "Step: 1650, Training Loss: 3.53640, LR: 0.0007507, Tokens/sec: 181503.03\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 1650, Eval Loss: 3.62676\n",
      "Step: 1651, Training Loss: 3.71145, LR: 0.0007504, Tokens/sec: 179865.93\n",
      "Step: 1652, Training Loss: 3.81977, LR: 0.0007500, Tokens/sec: 180992.21\n",
      "Step: 1653, Training Loss: 3.44022, LR: 0.0007497, Tokens/sec: 181953.21\n",
      "Step: 1654, Training Loss: 3.92896, LR: 0.0007493, Tokens/sec: 181060.50\n",
      "Step: 1655, Training Loss: 3.73857, LR: 0.0007489, Tokens/sec: 181835.07\n",
      "Step: 1656, Training Loss: 3.43314, LR: 0.0007486, Tokens/sec: 181623.29\n",
      "Step: 1657, Training Loss: 3.55224, LR: 0.0007482, Tokens/sec: 180340.02\n",
      "Step: 1658, Training Loss: 3.95498, LR: 0.0007479, Tokens/sec: 180988.90\n",
      "Step: 1659, Training Loss: 3.82503, LR: 0.0007475, Tokens/sec: 181344.48\n",
      "Step: 1660, Training Loss: 3.73510, LR: 0.0007472, Tokens/sec: 181333.53\n",
      "Step: 1661, Training Loss: 3.87525, LR: 0.0007468, Tokens/sec: 181544.35\n",
      "Step: 1662, Training Loss: 3.83618, LR: 0.0007464, Tokens/sec: 181845.56\n",
      "Step: 1663, Training Loss: 3.50914, LR: 0.0007461, Tokens/sec: 179969.37\n",
      "Step: 1664, Training Loss: 3.54205, LR: 0.0007457, Tokens/sec: 180809.49\n",
      "Step: 1665, Training Loss: 3.59194, LR: 0.0007454, Tokens/sec: 182177.82\n",
      "Step: 1666, Training Loss: 3.37715, LR: 0.0007450, Tokens/sec: 181462.90\n",
      "Step: 1667, Training Loss: 3.28318, LR: 0.0007447, Tokens/sec: 181325.71\n",
      "Step: 1668, Training Loss: 3.49047, LR: 0.0007443, Tokens/sec: 181456.28\n",
      "Step: 1669, Training Loss: 3.82175, LR: 0.0007439, Tokens/sec: 180077.59\n",
      "Step: 1670, Training Loss: 3.93265, LR: 0.0007436, Tokens/sec: 181843.25\n",
      "Step: 1671, Training Loss: 3.79290, LR: 0.0007432, Tokens/sec: 181991.36\n",
      "Step: 1672, Training Loss: 3.54921, LR: 0.0007429, Tokens/sec: 181103.34\n",
      "Step: 1673, Training Loss: 3.33987, LR: 0.0007425, Tokens/sec: 181608.96\n",
      "Step: 1674, Training Loss: 3.47310, LR: 0.0007421, Tokens/sec: 181490.87\n",
      "Step: 1675, Training Loss: 3.89180, LR: 0.0007418, Tokens/sec: 180705.27\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 1675, Eval Loss: 3.58242\n",
      "Step: 1676, Training Loss: 3.27613, LR: 0.0007414, Tokens/sec: 181154.40\n",
      "Step: 1677, Training Loss: 3.52632, LR: 0.0007411, Tokens/sec: 180135.12\n",
      "Step: 1678, Training Loss: 3.28222, LR: 0.0007407, Tokens/sec: 181042.91\n",
      "Step: 1679, Training Loss: 3.33060, LR: 0.0007403, Tokens/sec: 181790.40\n",
      "Step: 1680, Training Loss: 3.65167, LR: 0.0007400, Tokens/sec: 181155.75\n",
      "Step: 1681, Training Loss: 4.01632, LR: 0.0007396, Tokens/sec: 181047.20\n",
      "Step: 1682, Training Loss: 3.49124, LR: 0.0007393, Tokens/sec: 181791.36\n",
      "Step: 1683, Training Loss: 3.78948, LR: 0.0007389, Tokens/sec: 179911.13\n",
      "Step: 1684, Training Loss: 3.34693, LR: 0.0007385, Tokens/sec: 181108.99\n",
      "Step: 1685, Training Loss: 3.40241, LR: 0.0007382, Tokens/sec: 181351.26\n",
      "Step: 1686, Training Loss: 3.26674, LR: 0.0007378, Tokens/sec: 181787.29\n",
      "Step: 1687, Training Loss: 3.65215, LR: 0.0007375, Tokens/sec: 181557.30\n",
      "Step: 1688, Training Loss: 3.66710, LR: 0.0007371, Tokens/sec: 181673.74\n",
      "Step: 1689, Training Loss: 3.03633, LR: 0.0007367, Tokens/sec: 180420.86\n",
      "Step: 1690, Training Loss: 3.65124, LR: 0.0007364, Tokens/sec: 181890.78\n",
      "Step: 1691, Training Loss: 3.84381, LR: 0.0007360, Tokens/sec: 181406.85\n",
      "Step: 1692, Training Loss: 3.76852, LR: 0.0007357, Tokens/sec: 181618.96\n",
      "Step: 1693, Training Loss: 3.68119, LR: 0.0007353, Tokens/sec: 181332.87\n",
      "Step: 1694, Training Loss: 3.53468, LR: 0.0007349, Tokens/sec: 168760.47\n",
      "Step: 1695, Training Loss: 3.46995, LR: 0.0007346, Tokens/sec: 173422.34\n",
      "Step: 1696, Training Loss: 3.64872, LR: 0.0007342, Tokens/sec: 180702.38\n",
      "Step: 1697, Training Loss: 3.65962, LR: 0.0007338, Tokens/sec: 181356.60\n",
      "Step: 1698, Training Loss: 3.39570, LR: 0.0007335, Tokens/sec: 181160.83\n",
      "Step: 1699, Training Loss: 3.61048, LR: 0.0007331, Tokens/sec: 181361.54\n",
      "Step: 1700, Training Loss: 3.47186, LR: 0.0007328, Tokens/sec: 181529.75\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 1700, Eval Loss: 3.52743\n",
      "Step: 1701, Training Loss: 3.44354, LR: 0.0007324, Tokens/sec: 179894.35\n",
      "Step: 1702, Training Loss: 3.43031, LR: 0.0007320, Tokens/sec: 180762.28\n",
      "Step: 1703, Training Loss: 3.73973, LR: 0.0007317, Tokens/sec: 180685.61\n",
      "Step: 1704, Training Loss: 3.77930, LR: 0.0007313, Tokens/sec: 181850.62\n",
      "Step: 1705, Training Loss: 3.48811, LR: 0.0007309, Tokens/sec: 180864.39\n",
      "Step: 1706, Training Loss: 3.63744, LR: 0.0007306, Tokens/sec: 180295.81\n",
      "Step: 1707, Training Loss: 3.77745, LR: 0.0007302, Tokens/sec: 180788.46\n",
      "Step: 1708, Training Loss: 3.70420, LR: 0.0007298, Tokens/sec: 181891.30\n",
      "Step: 1709, Training Loss: 3.58600, LR: 0.0007295, Tokens/sec: 180944.37\n",
      "Step: 1710, Training Loss: 3.62618, LR: 0.0007291, Tokens/sec: 181803.73\n",
      "Step: 1711, Training Loss: 3.31036, LR: 0.0007287, Tokens/sec: 181217.53\n",
      "Step: 1712, Training Loss: 3.84039, LR: 0.0007284, Tokens/sec: 180410.00\n",
      "Step: 1713, Training Loss: 3.24992, LR: 0.0007280, Tokens/sec: 181107.01\n",
      "Step: 1714, Training Loss: 3.55044, LR: 0.0007277, Tokens/sec: 181250.67\n",
      "Step: 1715, Training Loss: 3.57630, LR: 0.0007273, Tokens/sec: 180834.40\n",
      "Step: 1716, Training Loss: 3.54428, LR: 0.0007269, Tokens/sec: 181345.47\n",
      "Step: 1717, Training Loss: 3.96762, LR: 0.0007266, Tokens/sec: 181231.58\n",
      "Step: 1718, Training Loss: 3.94914, LR: 0.0007262, Tokens/sec: 180737.48\n",
      "Step: 1719, Training Loss: 3.57715, LR: 0.0007258, Tokens/sec: 181270.14\n",
      "Step: 1720, Training Loss: 3.63645, LR: 0.0007255, Tokens/sec: 181401.72\n",
      "Step: 1721, Training Loss: 3.43041, LR: 0.0007251, Tokens/sec: 181378.49\n",
      "Step: 1722, Training Loss: 3.52682, LR: 0.0007247, Tokens/sec: 181310.02\n",
      "Step: 1723, Training Loss: 3.91763, LR: 0.0007244, Tokens/sec: 180821.08\n",
      "Step: 1724, Training Loss: 3.82894, LR: 0.0007240, Tokens/sec: 180456.29\n",
      "Step: 1725, Training Loss: 3.72886, LR: 0.0007236, Tokens/sec: 182167.48\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 1725, Eval Loss: 3.52694\n",
      "Step: 1726, Training Loss: 3.68490, LR: 0.0007233, Tokens/sec: 180474.50\n",
      "Step: 1727, Training Loss: 3.16384, LR: 0.0007229, Tokens/sec: 180981.90\n",
      "Step: 1728, Training Loss: 3.56765, LR: 0.0007225, Tokens/sec: 176891.57\n",
      "Step: 1729, Training Loss: 3.96485, LR: 0.0007222, Tokens/sec: 181467.02\n",
      "Step: 1730, Training Loss: 3.39260, LR: 0.0007218, Tokens/sec: 181590.68\n",
      "Step: 1731, Training Loss: 3.52246, LR: 0.0007214, Tokens/sec: 181793.59\n",
      "Step: 1732, Training Loss: 3.86011, LR: 0.0007211, Tokens/sec: 180237.38\n",
      "Step: 1733, Training Loss: 3.41143, LR: 0.0007207, Tokens/sec: 181193.12\n",
      "Step: 1734, Training Loss: 3.50904, LR: 0.0007203, Tokens/sec: 181448.47\n",
      "Step: 1735, Training Loss: 3.66533, LR: 0.0007200, Tokens/sec: 181428.22\n",
      "Step: 1736, Training Loss: 3.45775, LR: 0.0007196, Tokens/sec: 181799.65\n",
      "Step: 1737, Training Loss: 3.78403, LR: 0.0007192, Tokens/sec: 180897.11\n",
      "Step: 1738, Training Loss: 3.38923, LR: 0.0007189, Tokens/sec: 180595.53\n",
      "Step: 1739, Training Loss: 3.65115, LR: 0.0007185, Tokens/sec: 180752.19\n",
      "Step: 1740, Training Loss: 3.89117, LR: 0.0007181, Tokens/sec: 181417.93\n",
      "Step: 1741, Training Loss: 3.72766, LR: 0.0007177, Tokens/sec: 181088.92\n",
      "Step: 1742, Training Loss: 4.09274, LR: 0.0007174, Tokens/sec: 181472.09\n",
      "Step: 1743, Training Loss: 3.23894, LR: 0.0007170, Tokens/sec: 181523.12\n",
      "Step: 1744, Training Loss: 3.57043, LR: 0.0007166, Tokens/sec: 180670.28\n",
      "Step: 1745, Training Loss: 3.64802, LR: 0.0007163, Tokens/sec: 181155.89\n",
      "Step: 1746, Training Loss: 3.80272, LR: 0.0007159, Tokens/sec: 181506.42\n",
      "Step: 1747, Training Loss: 3.41406, LR: 0.0007155, Tokens/sec: 180386.35\n",
      "Step: 1748, Training Loss: 3.68309, LR: 0.0007152, Tokens/sec: 181189.01\n",
      "Step: 1749, Training Loss: 3.16999, LR: 0.0007148, Tokens/sec: 181127.73\n",
      "Step: 1750, Training Loss: 3.63351, LR: 0.0007144, Tokens/sec: 180855.53\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 1750, Eval Loss: 3.54659\n",
      "Step: 1751, Training Loss: 3.39332, LR: 0.0007141, Tokens/sec: 180863.21\n",
      "Step: 1752, Training Loss: 3.27965, LR: 0.0007137, Tokens/sec: 180461.23\n",
      "Step: 1753, Training Loss: 3.83396, LR: 0.0007133, Tokens/sec: 180963.53\n",
      "Step: 1754, Training Loss: 3.64826, LR: 0.0007129, Tokens/sec: 181336.68\n",
      "Step: 1755, Training Loss: 3.52304, LR: 0.0007126, Tokens/sec: 181031.56\n",
      "Step: 1756, Training Loss: 3.61353, LR: 0.0007122, Tokens/sec: 181207.36\n",
      "Step: 1757, Training Loss: 3.44938, LR: 0.0007118, Tokens/sec: 181199.30\n",
      "Step: 1758, Training Loss: 3.63318, LR: 0.0007115, Tokens/sec: 180135.76\n",
      "Step: 1759, Training Loss: 3.34680, LR: 0.0007111, Tokens/sec: 181199.92\n",
      "Step: 1760, Training Loss: 3.35260, LR: 0.0007107, Tokens/sec: 181231.89\n",
      "Step: 1761, Training Loss: 3.57302, LR: 0.0007103, Tokens/sec: 181672.99\n",
      "Step: 1762, Training Loss: 3.52263, LR: 0.0007100, Tokens/sec: 181316.98\n",
      "Step: 1763, Training Loss: 3.62780, LR: 0.0007096, Tokens/sec: 181153.93\n",
      "Step: 1764, Training Loss: 3.37660, LR: 0.0007092, Tokens/sec: 180053.28\n",
      "Step: 1765, Training Loss: 3.49307, LR: 0.0007089, Tokens/sec: 180805.98\n",
      "Step: 1766, Training Loss: 3.81846, LR: 0.0007085, Tokens/sec: 181549.53\n",
      "Step: 1767, Training Loss: 3.75144, LR: 0.0007081, Tokens/sec: 180405.70\n",
      "Step: 1768, Training Loss: 3.51310, LR: 0.0007077, Tokens/sec: 180882.49\n",
      "Step: 1769, Training Loss: 3.32650, LR: 0.0007074, Tokens/sec: 181649.98\n",
      "Step: 1770, Training Loss: 3.47112, LR: 0.0007070, Tokens/sec: 179883.14\n",
      "Step: 1771, Training Loss: 3.86408, LR: 0.0007066, Tokens/sec: 180639.98\n",
      "Step: 1772, Training Loss: 3.32118, LR: 0.0007063, Tokens/sec: 181843.54\n",
      "Step: 1773, Training Loss: 3.46746, LR: 0.0007059, Tokens/sec: 181460.58\n",
      "Step: 1774, Training Loss: 3.94780, LR: 0.0007055, Tokens/sec: 181332.67\n",
      "Step: 1775, Training Loss: 3.49785, LR: 0.0007051, Tokens/sec: 178623.96\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 1775, Eval Loss: 3.55437\n",
      "Step: 1776, Training Loss: 3.65560, LR: 0.0007048, Tokens/sec: 178650.52\n",
      "Step: 1777, Training Loss: 2.92968, LR: 0.0007044, Tokens/sec: 180024.70\n",
      "Step: 1778, Training Loss: 3.87118, LR: 0.0007040, Tokens/sec: 180742.90\n",
      "Step: 1779, Training Loss: 3.59626, LR: 0.0007036, Tokens/sec: 179446.17\n",
      "Step: 1780, Training Loss: 3.15017, LR: 0.0007033, Tokens/sec: 180358.16\n",
      "Step: 1781, Training Loss: 3.51644, LR: 0.0007029, Tokens/sec: 178510.51\n",
      "Step: 1782, Training Loss: 3.71759, LR: 0.0007025, Tokens/sec: 181289.26\n",
      "Step: 1783, Training Loss: 3.66620, LR: 0.0007021, Tokens/sec: 181207.17\n",
      "Step: 1784, Training Loss: 3.49031, LR: 0.0007018, Tokens/sec: 179556.01\n",
      "Step: 1785, Training Loss: 3.58956, LR: 0.0007014, Tokens/sec: 172394.82\n",
      "Step: 1786, Training Loss: 3.76734, LR: 0.0007010, Tokens/sec: 158013.90\n",
      "Step: 1787, Training Loss: 3.35026, LR: 0.0007006, Tokens/sec: 179593.23\n",
      "Step: 1788, Training Loss: 3.26907, LR: 0.0007003, Tokens/sec: 181176.45\n",
      "Step: 1789, Training Loss: 3.87370, LR: 0.0006999, Tokens/sec: 181539.95\n",
      "Step: 1790, Training Loss: 3.91637, LR: 0.0006995, Tokens/sec: 181875.53\n",
      "Step: 1791, Training Loss: 3.47152, LR: 0.0006991, Tokens/sec: 181567.08\n",
      "Step: 1792, Training Loss: 3.43579, LR: 0.0006988, Tokens/sec: 181174.39\n",
      "Step: 1793, Training Loss: 3.74646, LR: 0.0006984, Tokens/sec: 179751.87\n",
      "Step: 1794, Training Loss: 3.48284, LR: 0.0006980, Tokens/sec: 181816.98\n",
      "Step: 1795, Training Loss: 3.67669, LR: 0.0006976, Tokens/sec: 181180.12\n",
      "Step: 1796, Training Loss: 3.41158, LR: 0.0006973, Tokens/sec: 181316.49\n",
      "Step: 1797, Training Loss: 3.62034, LR: 0.0006969, Tokens/sec: 182016.36\n",
      "Step: 1798, Training Loss: 3.69772, LR: 0.0006965, Tokens/sec: 182123.52\n",
      "Step: 1799, Training Loss: 3.16970, LR: 0.0006961, Tokens/sec: 180181.17\n",
      "Step: 1800, Training Loss: 3.43945, LR: 0.0006958, Tokens/sec: 181499.41\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 1800, Eval Loss: 3.61613\n",
      "Step: 1801, Training Loss: 3.38649, LR: 0.0006954, Tokens/sec: 179786.98\n",
      "Step: 1802, Training Loss: 3.48254, LR: 0.0006950, Tokens/sec: 181002.21\n",
      "Step: 1803, Training Loss: 3.83249, LR: 0.0006946, Tokens/sec: 181129.98\n",
      "Step: 1804, Training Loss: 3.64878, LR: 0.0006943, Tokens/sec: 181401.20\n",
      "Step: 1805, Training Loss: 3.45611, LR: 0.0006939, Tokens/sec: 181162.25\n",
      "Step: 1806, Training Loss: 3.26108, LR: 0.0006935, Tokens/sec: 181358.89\n",
      "Step: 1807, Training Loss: 3.70769, LR: 0.0006931, Tokens/sec: 179521.48\n",
      "Step: 1808, Training Loss: 3.58932, LR: 0.0006928, Tokens/sec: 181391.34\n",
      "Step: 1809, Training Loss: 3.67594, LR: 0.0006924, Tokens/sec: 181573.96\n",
      "Step: 1810, Training Loss: 3.17368, LR: 0.0006920, Tokens/sec: 181366.88\n",
      "Step: 1811, Training Loss: 3.58576, LR: 0.0006916, Tokens/sec: 181309.22\n",
      "Step: 1812, Training Loss: 3.35816, LR: 0.0006912, Tokens/sec: 181352.53\n",
      "Step: 1813, Training Loss: 3.50584, LR: 0.0006909, Tokens/sec: 180922.27\n",
      "Step: 1814, Training Loss: 3.80388, LR: 0.0006905, Tokens/sec: 182015.88\n",
      "Step: 1815, Training Loss: 3.53416, LR: 0.0006901, Tokens/sec: 182264.69\n",
      "Step: 1816, Training Loss: 3.17887, LR: 0.0006897, Tokens/sec: 181477.21\n",
      "Step: 1817, Training Loss: 3.71256, LR: 0.0006894, Tokens/sec: 181295.98\n",
      "Step: 1818, Training Loss: 3.58555, LR: 0.0006890, Tokens/sec: 182143.18\n",
      "Step: 1819, Training Loss: 3.91187, LR: 0.0006886, Tokens/sec: 180859.53\n",
      "Step: 1820, Training Loss: 3.54772, LR: 0.0006882, Tokens/sec: 181800.04\n",
      "Step: 1821, Training Loss: 3.83378, LR: 0.0006878, Tokens/sec: 182088.35\n",
      "Step: 1822, Training Loss: 3.55246, LR: 0.0006875, Tokens/sec: 181640.50\n",
      "Step: 1823, Training Loss: 3.40819, LR: 0.0006871, Tokens/sec: 182062.72\n",
      "Step: 1824, Training Loss: 3.67905, LR: 0.0006867, Tokens/sec: 182145.96\n",
      "Step: 1825, Training Loss: 3.49328, LR: 0.0006863, Tokens/sec: 180664.73\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 1825, Eval Loss: 3.61126\n",
      "Step: 1826, Training Loss: 3.25520, LR: 0.0006860, Tokens/sec: 181251.82\n",
      "Step: 1827, Training Loss: 3.21602, LR: 0.0006856, Tokens/sec: 180559.51\n",
      "Step: 1828, Training Loss: 3.24946, LR: 0.0006852, Tokens/sec: 181794.86\n",
      "Step: 1829, Training Loss: 3.42821, LR: 0.0006848, Tokens/sec: 182149.42\n",
      "Step: 1830, Training Loss: 3.21863, LR: 0.0006844, Tokens/sec: 181987.68\n",
      "Step: 1831, Training Loss: 3.59575, LR: 0.0006841, Tokens/sec: 181437.79\n",
      "Step: 1832, Training Loss: 3.57631, LR: 0.0006837, Tokens/sec: 182206.64\n",
      "Step: 1833, Training Loss: 3.57692, LR: 0.0006833, Tokens/sec: 180908.81\n",
      "Step: 1834, Training Loss: 3.33836, LR: 0.0006829, Tokens/sec: 181860.38\n",
      "Step: 1835, Training Loss: 3.69568, LR: 0.0006825, Tokens/sec: 182559.20\n",
      "Step: 1836, Training Loss: 3.34564, LR: 0.0006822, Tokens/sec: 181917.36\n",
      "Step: 1837, Training Loss: 3.20622, LR: 0.0006818, Tokens/sec: 181979.18\n",
      "Step: 1838, Training Loss: 3.78715, LR: 0.0006814, Tokens/sec: 182379.60\n",
      "Step: 1839, Training Loss: 3.56930, LR: 0.0006810, Tokens/sec: 181048.45\n",
      "Step: 1840, Training Loss: 3.37499, LR: 0.0006806, Tokens/sec: 182097.24\n",
      "Step: 1841, Training Loss: 3.42760, LR: 0.0006803, Tokens/sec: 182099.48\n",
      "Step: 1842, Training Loss: 3.33844, LR: 0.0006799, Tokens/sec: 181940.28\n",
      "Step: 1843, Training Loss: 3.24343, LR: 0.0006795, Tokens/sec: 182301.58\n",
      "Step: 1844, Training Loss: 3.28001, LR: 0.0006791, Tokens/sec: 182322.95\n",
      "Step: 1845, Training Loss: 3.60584, LR: 0.0006787, Tokens/sec: 180700.66\n",
      "Step: 1846, Training Loss: 3.75251, LR: 0.0006784, Tokens/sec: 181930.52\n",
      "Step: 1847, Training Loss: 3.41292, LR: 0.0006780, Tokens/sec: 182572.96\n",
      "Step: 1848, Training Loss: 3.45396, LR: 0.0006776, Tokens/sec: 182032.14\n",
      "Step: 1849, Training Loss: 3.39551, LR: 0.0006772, Tokens/sec: 182222.87\n",
      "Step: 1850, Training Loss: 3.50893, LR: 0.0006768, Tokens/sec: 182050.43\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 1850, Eval Loss: 3.40457\n",
      "Step: 1851, Training Loss: 3.31250, LR: 0.0006765, Tokens/sec: 181224.46\n",
      "Step: 1852, Training Loss: 3.53433, LR: 0.0006761, Tokens/sec: 181972.24\n",
      "Step: 1853, Training Loss: 3.28633, LR: 0.0006757, Tokens/sec: 181701.59\n",
      "Step: 1854, Training Loss: 2.99976, LR: 0.0006753, Tokens/sec: 181459.71\n",
      "Step: 1855, Training Loss: 3.93982, LR: 0.0006749, Tokens/sec: 182252.71\n",
      "Step: 1856, Training Loss: 3.52777, LR: 0.0006745, Tokens/sec: 180943.41\n",
      "Step: 1857, Training Loss: 3.54235, LR: 0.0006742, Tokens/sec: 180769.16\n",
      "Step: 1858, Training Loss: 3.27612, LR: 0.0006738, Tokens/sec: 181870.71\n",
      "Step: 1859, Training Loss: 3.49805, LR: 0.0006734, Tokens/sec: 181470.95\n",
      "Step: 1860, Training Loss: 3.53514, LR: 0.0006730, Tokens/sec: 181393.34\n",
      "Step: 1861, Training Loss: 3.11232, LR: 0.0006726, Tokens/sec: 181187.45\n",
      "Step: 1862, Training Loss: 3.25719, LR: 0.0006722, Tokens/sec: 180785.23\n",
      "Step: 1863, Training Loss: 3.46865, LR: 0.0006719, Tokens/sec: 182399.73\n",
      "Step: 1864, Training Loss: 3.55279, LR: 0.0006715, Tokens/sec: 181193.90\n",
      "Step: 1865, Training Loss: 3.53775, LR: 0.0006711, Tokens/sec: 181710.26\n",
      "Step: 1866, Training Loss: 3.38610, LR: 0.0006707, Tokens/sec: 181685.27\n",
      "Step: 1867, Training Loss: 3.47226, LR: 0.0006703, Tokens/sec: 181684.60\n",
      "Step: 1868, Training Loss: 3.68177, LR: 0.0006700, Tokens/sec: 180486.33\n",
      "Step: 1869, Training Loss: 3.33255, LR: 0.0006696, Tokens/sec: 181661.82\n",
      "Step: 1870, Training Loss: 3.51785, LR: 0.0006692, Tokens/sec: 181726.51\n",
      "Step: 1871, Training Loss: 3.64950, LR: 0.0006688, Tokens/sec: 181703.88\n",
      "Step: 1872, Training Loss: 3.72215, LR: 0.0006684, Tokens/sec: 181769.47\n",
      "Step: 1873, Training Loss: 3.84465, LR: 0.0006680, Tokens/sec: 181381.79\n",
      "Step: 1874, Training Loss: 3.50795, LR: 0.0006677, Tokens/sec: 181377.80\n",
      "Step: 1875, Training Loss: 3.31033, LR: 0.0006673, Tokens/sec: 181108.21\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 1875, Eval Loss: 3.55005\n",
      "Step: 1876, Training Loss: 3.24435, LR: 0.0006669, Tokens/sec: 181601.58\n",
      "Step: 1877, Training Loss: 3.58476, LR: 0.0006665, Tokens/sec: 182362.61\n",
      "Step: 1878, Training Loss: 3.93101, LR: 0.0006661, Tokens/sec: 182091.66\n",
      "Step: 1879, Training Loss: 3.45642, LR: 0.0006657, Tokens/sec: 181476.29\n",
      "Step: 1880, Training Loss: 3.45846, LR: 0.0006653, Tokens/sec: 181934.33\n",
      "Step: 1881, Training Loss: 3.47148, LR: 0.0006650, Tokens/sec: 181984.51\n",
      "Step: 1882, Training Loss: 3.76549, LR: 0.0006646, Tokens/sec: 180541.23\n",
      "Step: 1883, Training Loss: 3.30688, LR: 0.0006642, Tokens/sec: 181397.26\n",
      "Step: 1884, Training Loss: 3.50651, LR: 0.0006638, Tokens/sec: 181848.68\n",
      "Step: 1885, Training Loss: 3.47497, LR: 0.0006634, Tokens/sec: 181946.35\n",
      "Step: 1886, Training Loss: 3.41667, LR: 0.0006630, Tokens/sec: 181328.74\n",
      "Step: 1887, Training Loss: 3.76807, LR: 0.0006627, Tokens/sec: 181301.20\n",
      "Step: 1888, Training Loss: 3.44225, LR: 0.0006623, Tokens/sec: 181160.77\n",
      "Step: 1889, Training Loss: 3.67191, LR: 0.0006619, Tokens/sec: 181800.99\n",
      "Step: 1890, Training Loss: 3.58574, LR: 0.0006615, Tokens/sec: 181607.76\n",
      "Step: 1891, Training Loss: 3.89553, LR: 0.0006611, Tokens/sec: 181768.28\n",
      "Step: 1892, Training Loss: 3.60431, LR: 0.0006607, Tokens/sec: 181762.02\n",
      "Step: 1893, Training Loss: 3.37588, LR: 0.0006603, Tokens/sec: 181639.85\n",
      "Step: 1894, Training Loss: 3.46504, LR: 0.0006600, Tokens/sec: 180806.75\n",
      "Step: 1895, Training Loss: 3.40378, LR: 0.0006596, Tokens/sec: 181621.45\n",
      "Step: 1896, Training Loss: 3.08840, LR: 0.0006592, Tokens/sec: 181896.40\n",
      "Step: 1897, Training Loss: 3.56784, LR: 0.0006588, Tokens/sec: 181783.05\n",
      "Step: 1898, Training Loss: 3.24877, LR: 0.0006584, Tokens/sec: 181815.72\n",
      "Step: 1899, Training Loss: 3.67748, LR: 0.0006580, Tokens/sec: 180934.95\n",
      "Step: 1900, Training Loss: 3.47166, LR: 0.0006576, Tokens/sec: 180450.37\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 1900, Eval Loss: 3.40749\n",
      "Step: 1901, Training Loss: 3.49768, LR: 0.0006573, Tokens/sec: 180942.96\n",
      "Step: 1902, Training Loss: 3.50992, LR: 0.0006569, Tokens/sec: 179984.18\n",
      "Step: 1903, Training Loss: 3.10305, LR: 0.0006565, Tokens/sec: 181474.02\n",
      "Step: 1904, Training Loss: 3.30703, LR: 0.0006561, Tokens/sec: 181732.73\n",
      "Step: 1905, Training Loss: 3.35148, LR: 0.0006557, Tokens/sec: 181858.61\n",
      "Step: 1906, Training Loss: 3.12236, LR: 0.0006553, Tokens/sec: 181361.03\n",
      "Step: 1907, Training Loss: 3.28281, LR: 0.0006549, Tokens/sec: 181710.55\n",
      "Step: 1908, Training Loss: 3.45073, LR: 0.0006546, Tokens/sec: 180237.18\n",
      "Step: 1909, Training Loss: 3.91382, LR: 0.0006542, Tokens/sec: 181254.81\n",
      "Step: 1910, Training Loss: 3.39422, LR: 0.0006538, Tokens/sec: 181656.15\n",
      "Step: 1911, Training Loss: 3.37423, LR: 0.0006534, Tokens/sec: 181309.40\n",
      "Step: 1912, Training Loss: 3.40888, LR: 0.0006530, Tokens/sec: 181363.31\n",
      "Step: 1913, Training Loss: 3.37977, LR: 0.0006526, Tokens/sec: 181739.62\n",
      "Step: 1914, Training Loss: 3.74015, LR: 0.0006522, Tokens/sec: 180575.47\n",
      "Step: 1915, Training Loss: 3.45262, LR: 0.0006518, Tokens/sec: 181435.44\n",
      "Step: 1916, Training Loss: 3.63617, LR: 0.0006515, Tokens/sec: 182355.51\n",
      "Step: 1917, Training Loss: 3.26166, LR: 0.0006511, Tokens/sec: 181751.39\n",
      "Step: 1918, Training Loss: 3.42787, LR: 0.0006507, Tokens/sec: 181913.90\n",
      "Step: 1919, Training Loss: 3.17202, LR: 0.0006503, Tokens/sec: 181332.26\n",
      "Step: 1920, Training Loss: 3.09645, LR: 0.0006499, Tokens/sec: 181038.84\n",
      "Step: 1921, Training Loss: 3.53337, LR: 0.0006495, Tokens/sec: 181534.77\n",
      "Step: 1922, Training Loss: 3.17805, LR: 0.0006491, Tokens/sec: 182233.75\n",
      "Step: 1923, Training Loss: 3.10880, LR: 0.0006488, Tokens/sec: 181369.91\n",
      "Step: 1924, Training Loss: 3.42168, LR: 0.0006484, Tokens/sec: 181712.65\n",
      "Step: 1925, Training Loss: 3.34273, LR: 0.0006480, Tokens/sec: 180867.56\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 1925, Eval Loss: 3.43559\n",
      "Step: 1926, Training Loss: 3.28513, LR: 0.0006476, Tokens/sec: 172382.89\n",
      "Step: 1927, Training Loss: 3.45382, LR: 0.0006472, Tokens/sec: 179462.40\n",
      "Step: 1928, Training Loss: 3.78521, LR: 0.0006468, Tokens/sec: 180591.68\n",
      "Step: 1929, Training Loss: 3.56333, LR: 0.0006464, Tokens/sec: 180734.16\n",
      "Step: 1930, Training Loss: 3.45917, LR: 0.0006460, Tokens/sec: 180905.73\n",
      "Step: 1931, Training Loss: 3.18509, LR: 0.0006456, Tokens/sec: 179621.96\n",
      "Step: 1932, Training Loss: 3.36309, LR: 0.0006453, Tokens/sec: 181781.48\n",
      "Step: 1933, Training Loss: 3.70035, LR: 0.0006449, Tokens/sec: 181475.55\n",
      "Step: 1934, Training Loss: 3.43692, LR: 0.0006445, Tokens/sec: 181583.12\n",
      "Step: 1935, Training Loss: 3.07579, LR: 0.0006441, Tokens/sec: 181652.56\n",
      "Step: 1936, Training Loss: 3.49760, LR: 0.0006437, Tokens/sec: 181581.19\n",
      "Step: 1937, Training Loss: 3.23015, LR: 0.0006433, Tokens/sec: 180542.40\n",
      "Step: 1938, Training Loss: 3.29314, LR: 0.0006429, Tokens/sec: 182108.54\n",
      "Step: 1939, Training Loss: 3.63627, LR: 0.0006425, Tokens/sec: 181916.54\n",
      "Step: 1940, Training Loss: 3.46772, LR: 0.0006421, Tokens/sec: 181840.89\n",
      "Step: 1941, Training Loss: 3.04685, LR: 0.0006418, Tokens/sec: 181846.27\n",
      "Step: 1942, Training Loss: 3.50779, LR: 0.0006414, Tokens/sec: 182085.67\n",
      "Step: 1943, Training Loss: 3.40069, LR: 0.0006410, Tokens/sec: 180505.78\n",
      "Step: 1944, Training Loss: 3.33192, LR: 0.0006406, Tokens/sec: 181380.57\n",
      "Step: 1945, Training Loss: 3.30436, LR: 0.0006402, Tokens/sec: 182156.83\n",
      "Step: 1946, Training Loss: 3.24454, LR: 0.0006398, Tokens/sec: 181696.15\n",
      "Step: 1947, Training Loss: 3.03460, LR: 0.0006394, Tokens/sec: 182067.63\n",
      "Step: 1948, Training Loss: 3.44753, LR: 0.0006390, Tokens/sec: 181430.32\n",
      "Step: 1949, Training Loss: 3.35481, LR: 0.0006386, Tokens/sec: 180317.31\n",
      "Step: 1950, Training Loss: 3.51733, LR: 0.0006383, Tokens/sec: 182145.70\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 1950, Eval Loss: 3.33253\n",
      "Step: 1951, Training Loss: 3.44417, LR: 0.0006379, Tokens/sec: 180546.71\n",
      "Step: 1952, Training Loss: 3.36286, LR: 0.0006375, Tokens/sec: 181513.66\n",
      "Step: 1953, Training Loss: 3.41551, LR: 0.0006371, Tokens/sec: 182083.08\n",
      "Step: 1954, Training Loss: 3.50461, LR: 0.0006367, Tokens/sec: 181741.19\n",
      "Step: 1955, Training Loss: 3.37473, LR: 0.0006363, Tokens/sec: 181788.12\n",
      "Step: 1956, Training Loss: 3.38938, LR: 0.0006359, Tokens/sec: 182282.87\n",
      "Step: 1957, Training Loss: 3.24187, LR: 0.0006355, Tokens/sec: 180462.46\n",
      "Step: 1958, Training Loss: 3.46682, LR: 0.0006351, Tokens/sec: 181860.26\n",
      "Step: 1959, Training Loss: 3.58538, LR: 0.0006347, Tokens/sec: 182114.53\n",
      "Step: 1960, Training Loss: 3.82224, LR: 0.0006344, Tokens/sec: 181756.88\n",
      "Step: 1961, Training Loss: 3.39685, LR: 0.0006340, Tokens/sec: 182237.15\n",
      "Step: 1962, Training Loss: 3.66317, LR: 0.0006336, Tokens/sec: 181573.52\n",
      "Step: 1963, Training Loss: 3.55494, LR: 0.0006332, Tokens/sec: 180985.00\n",
      "Step: 1964, Training Loss: 3.49756, LR: 0.0006328, Tokens/sec: 182439.16\n",
      "Step: 1965, Training Loss: 3.45482, LR: 0.0006324, Tokens/sec: 181207.15\n",
      "Step: 1966, Training Loss: 3.34388, LR: 0.0006320, Tokens/sec: 181188.14\n",
      "Step: 1967, Training Loss: 3.60922, LR: 0.0006316, Tokens/sec: 181355.52\n",
      "Step: 1968, Training Loss: 3.42070, LR: 0.0006312, Tokens/sec: 181248.59\n",
      "Step: 1969, Training Loss: 3.78810, LR: 0.0006308, Tokens/sec: 179752.80\n",
      "Step: 1970, Training Loss: 3.18474, LR: 0.0006304, Tokens/sec: 181585.92\n",
      "Step: 1971, Training Loss: 3.19957, LR: 0.0006301, Tokens/sec: 182370.28\n",
      "Step: 1972, Training Loss: 3.46246, LR: 0.0006297, Tokens/sec: 158965.77\n",
      "Step: 1973, Training Loss: 3.18058, LR: 0.0006293, Tokens/sec: 169117.08\n",
      "Step: 1974, Training Loss: 3.41620, LR: 0.0006289, Tokens/sec: 180542.10\n",
      "Step: 1975, Training Loss: 3.10353, LR: 0.0006285, Tokens/sec: 174145.70\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 1975, Eval Loss: 3.28353\n",
      "Step: 1976, Training Loss: 3.35325, LR: 0.0006281, Tokens/sec: 180226.96\n",
      "Step: 1977, Training Loss: 3.13618, LR: 0.0006277, Tokens/sec: 180712.57\n",
      "Step: 1978, Training Loss: 3.36208, LR: 0.0006273, Tokens/sec: 181413.77\n",
      "Step: 1979, Training Loss: 3.45028, LR: 0.0006269, Tokens/sec: 181164.89\n",
      "Step: 1980, Training Loss: 3.39718, LR: 0.0006265, Tokens/sec: 182069.17\n",
      "Step: 1981, Training Loss: 3.29122, LR: 0.0006261, Tokens/sec: 181302.94\n",
      "Step: 1982, Training Loss: 3.08311, LR: 0.0006257, Tokens/sec: 181138.95\n",
      "Step: 1983, Training Loss: 3.42013, LR: 0.0006254, Tokens/sec: 180271.78\n",
      "Step: 1984, Training Loss: 3.42997, LR: 0.0006250, Tokens/sec: 181179.38\n",
      "Step: 1985, Training Loss: 3.29056, LR: 0.0006246, Tokens/sec: 181860.98\n",
      "Step: 1986, Training Loss: 3.67842, LR: 0.0006242, Tokens/sec: 181489.79\n",
      "Step: 1987, Training Loss: 3.34710, LR: 0.0006238, Tokens/sec: 181240.40\n",
      "Step: 1988, Training Loss: 3.45443, LR: 0.0006234, Tokens/sec: 181175.24\n",
      "Step: 1989, Training Loss: 3.41597, LR: 0.0006230, Tokens/sec: 180407.28\n",
      "Step: 1990, Training Loss: 2.87709, LR: 0.0006226, Tokens/sec: 181086.57\n",
      "Step: 1991, Training Loss: 3.37722, LR: 0.0006222, Tokens/sec: 181146.51\n",
      "Step: 1992, Training Loss: 3.56554, LR: 0.0006218, Tokens/sec: 180940.46\n",
      "Step: 1993, Training Loss: 3.28231, LR: 0.0006214, Tokens/sec: 181096.22\n",
      "Step: 1994, Training Loss: 3.47527, LR: 0.0006210, Tokens/sec: 181504.16\n",
      "Step: 1995, Training Loss: 3.02770, LR: 0.0006207, Tokens/sec: 180947.13\n",
      "Step: 1996, Training Loss: 3.23308, LR: 0.0006203, Tokens/sec: 181289.32\n",
      "Step: 1997, Training Loss: 3.17740, LR: 0.0006199, Tokens/sec: 181170.73\n",
      "Step: 1998, Training Loss: 3.63939, LR: 0.0006195, Tokens/sec: 181300.03\n",
      "Step: 1999, Training Loss: 3.40502, LR: 0.0006191, Tokens/sec: 181473.92\n",
      "Step: 2000, Training Loss: 3.35341, LR: 0.0006187, Tokens/sec: 181202.09\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 2000, Eval Loss: 3.33241\n",
      "Step: 2001, Training Loss: 3.20592, LR: 0.0006183, Tokens/sec: 180817.76\n",
      "Step: 2002, Training Loss: 3.65288, LR: 0.0006179, Tokens/sec: 181808.19\n",
      "Step: 2003, Training Loss: 3.26134, LR: 0.0006175, Tokens/sec: 181001.99\n",
      "Step: 2004, Training Loss: 3.34154, LR: 0.0006171, Tokens/sec: 181571.80\n",
      "Step: 2005, Training Loss: 3.16903, LR: 0.0006167, Tokens/sec: 181324.56\n",
      "Step: 2006, Training Loss: 3.24907, LR: 0.0006163, Tokens/sec: 180049.96\n",
      "Step: 2007, Training Loss: 3.29629, LR: 0.0006159, Tokens/sec: 180949.30\n",
      "Step: 2008, Training Loss: 3.35196, LR: 0.0006155, Tokens/sec: 182124.98\n",
      "Step: 2009, Training Loss: 3.55369, LR: 0.0006152, Tokens/sec: 181475.39\n",
      "Step: 2010, Training Loss: 3.32482, LR: 0.0006148, Tokens/sec: 181659.16\n",
      "Step: 2011, Training Loss: 3.22250, LR: 0.0006144, Tokens/sec: 181066.18\n",
      "Step: 2012, Training Loss: 3.29932, LR: 0.0006140, Tokens/sec: 180475.78\n",
      "Step: 2013, Training Loss: 2.96273, LR: 0.0006136, Tokens/sec: 182038.77\n",
      "Step: 2014, Training Loss: 3.16575, LR: 0.0006132, Tokens/sec: 181576.16\n",
      "Step: 2015, Training Loss: 3.40903, LR: 0.0006128, Tokens/sec: 180716.08\n",
      "Step: 2016, Training Loss: 3.36361, LR: 0.0006124, Tokens/sec: 178206.92\n",
      "Step: 2017, Training Loss: 3.30553, LR: 0.0006120, Tokens/sec: 172275.22\n",
      "Step: 2018, Training Loss: 3.10750, LR: 0.0006116, Tokens/sec: 178445.42\n",
      "Step: 2019, Training Loss: 3.21072, LR: 0.0006112, Tokens/sec: 181696.97\n",
      "Step: 2020, Training Loss: 3.73065, LR: 0.0006108, Tokens/sec: 178790.66\n",
      "Step: 2021, Training Loss: 3.60188, LR: 0.0006104, Tokens/sec: 180729.47\n",
      "Step: 2022, Training Loss: 3.45757, LR: 0.0006100, Tokens/sec: 181285.36\n",
      "Step: 2023, Training Loss: 3.34892, LR: 0.0006096, Tokens/sec: 180681.18\n",
      "Step: 2024, Training Loss: 3.55222, LR: 0.0006092, Tokens/sec: 179934.43\n",
      "Step: 2025, Training Loss: 3.63487, LR: 0.0006089, Tokens/sec: 180857.08\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 2025, Eval Loss: 3.43066\n",
      "Step: 2026, Training Loss: 3.34231, LR: 0.0006085, Tokens/sec: 169691.58\n",
      "Step: 2027, Training Loss: 3.54077, LR: 0.0006081, Tokens/sec: 156718.08\n",
      "Step: 2028, Training Loss: 3.45452, LR: 0.0006077, Tokens/sec: 181162.74\n",
      "Step: 2029, Training Loss: 3.29115, LR: 0.0006073, Tokens/sec: 181109.15\n",
      "Step: 2030, Training Loss: 3.34852, LR: 0.0006069, Tokens/sec: 180846.75\n",
      "Step: 2031, Training Loss: 3.01524, LR: 0.0006065, Tokens/sec: 181392.48\n",
      "Step: 2032, Training Loss: 2.75520, LR: 0.0006061, Tokens/sec: 179923.42\n",
      "Step: 2033, Training Loss: 3.32864, LR: 0.0006057, Tokens/sec: 181021.82\n",
      "Step: 2034, Training Loss: 3.37998, LR: 0.0006053, Tokens/sec: 181248.72\n",
      "Step: 2035, Training Loss: 2.99569, LR: 0.0006049, Tokens/sec: 181654.19\n",
      "Step: 2036, Training Loss: 3.54258, LR: 0.0006045, Tokens/sec: 182083.48\n",
      "Step: 2037, Training Loss: 3.00959, LR: 0.0006041, Tokens/sec: 181218.72\n",
      "Step: 2038, Training Loss: 3.05608, LR: 0.0006037, Tokens/sec: 181102.07\n",
      "Step: 2039, Training Loss: 3.16364, LR: 0.0006033, Tokens/sec: 181192.37\n",
      "Step: 2040, Training Loss: 3.27548, LR: 0.0006029, Tokens/sec: 182395.13\n",
      "Step: 2041, Training Loss: 2.96859, LR: 0.0006025, Tokens/sec: 181744.19\n",
      "Step: 2042, Training Loss: 3.30982, LR: 0.0006021, Tokens/sec: 181908.76\n",
      "Step: 2043, Training Loss: 3.22111, LR: 0.0006018, Tokens/sec: 181924.23\n",
      "Step: 2044, Training Loss: 3.30409, LR: 0.0006014, Tokens/sec: 180434.06\n",
      "Step: 2045, Training Loss: 3.55908, LR: 0.0006010, Tokens/sec: 181202.38\n",
      "Step: 2046, Training Loss: 2.99836, LR: 0.0006006, Tokens/sec: 181568.49\n",
      "Step: 2047, Training Loss: 3.35462, LR: 0.0006002, Tokens/sec: 181542.14\n",
      "Step: 2048, Training Loss: 3.28007, LR: 0.0005998, Tokens/sec: 181886.50\n",
      "Step: 2049, Training Loss: 3.07622, LR: 0.0005994, Tokens/sec: 182073.46\n",
      "Step: 2050, Training Loss: 3.78424, LR: 0.0005990, Tokens/sec: 180494.42\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 2050, Eval Loss: 3.28883\n",
      "Step: 2051, Training Loss: 3.17203, LR: 0.0005986, Tokens/sec: 181471.46\n",
      "Step: 2052, Training Loss: 3.21226, LR: 0.0005982, Tokens/sec: 180394.80\n",
      "Step: 2053, Training Loss: 2.93261, LR: 0.0005978, Tokens/sec: 180485.21\n",
      "Step: 2054, Training Loss: 3.21342, LR: 0.0005974, Tokens/sec: 181272.13\n",
      "Step: 2055, Training Loss: 3.04780, LR: 0.0005970, Tokens/sec: 182034.91\n",
      "Step: 2056, Training Loss: 3.25050, LR: 0.0005966, Tokens/sec: 181654.07\n",
      "Step: 2057, Training Loss: 3.26757, LR: 0.0005962, Tokens/sec: 182283.19\n",
      "Step: 2058, Training Loss: 2.90794, LR: 0.0005958, Tokens/sec: 180594.04\n",
      "Step: 2059, Training Loss: 3.34256, LR: 0.0005954, Tokens/sec: 182280.91\n",
      "Step: 2060, Training Loss: 3.43602, LR: 0.0005950, Tokens/sec: 182314.39\n",
      "Step: 2061, Training Loss: 3.28794, LR: 0.0005946, Tokens/sec: 182224.18\n",
      "Step: 2062, Training Loss: 3.49078, LR: 0.0005942, Tokens/sec: 182141.78\n",
      "Step: 2063, Training Loss: 3.33044, LR: 0.0005938, Tokens/sec: 181485.46\n",
      "Step: 2064, Training Loss: 3.30690, LR: 0.0005935, Tokens/sec: 180843.86\n",
      "Step: 2065, Training Loss: 3.23479, LR: 0.0005931, Tokens/sec: 181589.90\n",
      "Step: 2066, Training Loss: 3.19401, LR: 0.0005927, Tokens/sec: 181525.61\n",
      "Step: 2067, Training Loss: 3.25824, LR: 0.0005923, Tokens/sec: 181603.04\n",
      "Step: 2068, Training Loss: 3.22917, LR: 0.0005919, Tokens/sec: 181466.16\n",
      "Step: 2069, Training Loss: 3.39086, LR: 0.0005915, Tokens/sec: 182287.67\n",
      "Step: 2070, Training Loss: 3.35377, LR: 0.0005911, Tokens/sec: 180821.77\n",
      "Step: 2071, Training Loss: 3.52466, LR: 0.0005907, Tokens/sec: 181605.06\n",
      "Step: 2072, Training Loss: 3.01139, LR: 0.0005903, Tokens/sec: 181738.31\n",
      "Step: 2073, Training Loss: 3.20218, LR: 0.0005899, Tokens/sec: 182349.77\n",
      "Step: 2074, Training Loss: 3.55126, LR: 0.0005895, Tokens/sec: 181930.16\n",
      "Step: 2075, Training Loss: 2.99274, LR: 0.0005891, Tokens/sec: 181446.30\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 2075, Eval Loss: 3.17541\n",
      "Step: 2076, Training Loss: 3.58547, LR: 0.0005887, Tokens/sec: 180040.18\n",
      "Step: 2077, Training Loss: 3.41356, LR: 0.0005883, Tokens/sec: 181525.27\n",
      "Step: 2078, Training Loss: 3.50559, LR: 0.0005879, Tokens/sec: 182261.68\n",
      "Step: 2079, Training Loss: 3.32254, LR: 0.0005875, Tokens/sec: 181339.68\n",
      "Step: 2080, Training Loss: 3.04719, LR: 0.0005871, Tokens/sec: 182047.27\n",
      "Step: 2081, Training Loss: 3.06708, LR: 0.0005867, Tokens/sec: 180210.94\n",
      "Step: 2082, Training Loss: 3.26421, LR: 0.0005863, Tokens/sec: 182190.74\n",
      "Step: 2083, Training Loss: 3.00279, LR: 0.0005859, Tokens/sec: 181714.27\n",
      "Step: 2084, Training Loss: 3.23625, LR: 0.0005855, Tokens/sec: 181582.61\n",
      "Step: 2085, Training Loss: 3.13824, LR: 0.0005851, Tokens/sec: 181608.72\n",
      "Step: 2086, Training Loss: 3.02010, LR: 0.0005847, Tokens/sec: 182061.51\n",
      "Step: 2087, Training Loss: 2.98807, LR: 0.0005843, Tokens/sec: 180651.72\n",
      "Step: 2088, Training Loss: 3.54130, LR: 0.0005839, Tokens/sec: 181388.81\n",
      "Step: 2089, Training Loss: 3.43444, LR: 0.0005836, Tokens/sec: 182208.87\n",
      "Step: 2090, Training Loss: 3.28783, LR: 0.0005832, Tokens/sec: 181642.64\n",
      "Step: 2091, Training Loss: 3.34906, LR: 0.0005828, Tokens/sec: 181718.10\n",
      "Step: 2092, Training Loss: 3.26027, LR: 0.0005824, Tokens/sec: 182562.09\n",
      "Step: 2093, Training Loss: 3.37964, LR: 0.0005820, Tokens/sec: 180513.01\n",
      "Step: 2094, Training Loss: 3.05217, LR: 0.0005816, Tokens/sec: 181119.90\n",
      "Step: 2095, Training Loss: 3.18740, LR: 0.0005812, Tokens/sec: 181779.71\n",
      "Step: 2096, Training Loss: 2.95913, LR: 0.0005808, Tokens/sec: 181592.19\n",
      "Step: 2097, Training Loss: 3.44317, LR: 0.0005804, Tokens/sec: 182031.64\n",
      "Step: 2098, Training Loss: 3.00185, LR: 0.0005800, Tokens/sec: 181366.55\n",
      "Step: 2099, Training Loss: 3.15488, LR: 0.0005796, Tokens/sec: 180632.24\n",
      "Step: 2100, Training Loss: 3.32838, LR: 0.0005792, Tokens/sec: 182170.62\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 2100, Eval Loss: 3.25173\n",
      "Step: 2101, Training Loss: 3.37254, LR: 0.0005788, Tokens/sec: 181104.71\n",
      "Step: 2102, Training Loss: 3.08392, LR: 0.0005784, Tokens/sec: 181606.07\n",
      "Step: 2103, Training Loss: 3.21081, LR: 0.0005780, Tokens/sec: 181281.04\n",
      "Step: 2104, Training Loss: 3.15325, LR: 0.0005776, Tokens/sec: 181700.01\n",
      "Step: 2105, Training Loss: 3.15378, LR: 0.0005772, Tokens/sec: 181538.60\n",
      "Step: 2106, Training Loss: 3.44506, LR: 0.0005768, Tokens/sec: 181808.44\n",
      "Step: 2107, Training Loss: 3.00492, LR: 0.0005764, Tokens/sec: 180574.69\n",
      "Step: 2108, Training Loss: 3.47903, LR: 0.0005760, Tokens/sec: 181789.37\n",
      "Step: 2109, Training Loss: 3.39521, LR: 0.0005756, Tokens/sec: 182471.08\n",
      "Step: 2110, Training Loss: 2.92700, LR: 0.0005752, Tokens/sec: 182382.10\n",
      "Step: 2111, Training Loss: 3.10863, LR: 0.0005748, Tokens/sec: 182070.87\n",
      "Step: 2112, Training Loss: 3.18067, LR: 0.0005744, Tokens/sec: 181706.82\n",
      "Step: 2113, Training Loss: 3.04601, LR: 0.0005740, Tokens/sec: 180642.46\n",
      "Step: 2114, Training Loss: 2.89554, LR: 0.0005736, Tokens/sec: 182379.22\n",
      "Step: 2115, Training Loss: 3.16566, LR: 0.0005732, Tokens/sec: 181967.61\n",
      "Step: 2116, Training Loss: 3.23998, LR: 0.0005728, Tokens/sec: 181385.63\n",
      "Step: 2117, Training Loss: 3.02936, LR: 0.0005724, Tokens/sec: 181842.28\n",
      "Step: 2118, Training Loss: 2.94490, LR: 0.0005720, Tokens/sec: 181611.20\n",
      "Step: 2119, Training Loss: 3.45960, LR: 0.0005717, Tokens/sec: 180649.87\n",
      "Step: 2120, Training Loss: 3.45649, LR: 0.0005713, Tokens/sec: 181677.70\n",
      "Step: 2121, Training Loss: 3.13945, LR: 0.0005709, Tokens/sec: 181820.80\n",
      "Step: 2122, Training Loss: 3.23438, LR: 0.0005705, Tokens/sec: 181640.91\n",
      "Step: 2123, Training Loss: 2.93648, LR: 0.0005701, Tokens/sec: 181864.10\n",
      "Step: 2124, Training Loss: 3.26133, LR: 0.0005697, Tokens/sec: 182052.92\n",
      "Step: 2125, Training Loss: 3.14882, LR: 0.0005693, Tokens/sec: 181199.36\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 2125, Eval Loss: 3.20325\n",
      "Step: 2126, Training Loss: 3.01157, LR: 0.0005689, Tokens/sec: 180838.30\n",
      "Step: 2127, Training Loss: 3.19799, LR: 0.0005685, Tokens/sec: 180400.11\n",
      "Step: 2128, Training Loss: 3.09509, LR: 0.0005681, Tokens/sec: 181765.56\n",
      "Step: 2129, Training Loss: 2.96834, LR: 0.0005677, Tokens/sec: 181426.00\n",
      "Step: 2130, Training Loss: 3.17932, LR: 0.0005673, Tokens/sec: 182269.02\n",
      "Step: 2131, Training Loss: 3.17617, LR: 0.0005669, Tokens/sec: 181328.32\n",
      "Step: 2132, Training Loss: 2.97153, LR: 0.0005665, Tokens/sec: 182013.88\n",
      "Step: 2133, Training Loss: 3.29035, LR: 0.0005661, Tokens/sec: 181346.87\n",
      "Step: 2134, Training Loss: 3.40480, LR: 0.0005657, Tokens/sec: 182221.38\n",
      "Step: 2135, Training Loss: 3.33590, LR: 0.0005653, Tokens/sec: 181810.20\n",
      "Step: 2136, Training Loss: 3.28801, LR: 0.0005649, Tokens/sec: 181609.72\n",
      "Step: 2137, Training Loss: 3.14932, LR: 0.0005645, Tokens/sec: 181782.56\n",
      "Step: 2138, Training Loss: 2.87409, LR: 0.0005641, Tokens/sec: 181902.87\n",
      "Step: 2139, Training Loss: 3.60540, LR: 0.0005637, Tokens/sec: 180647.84\n",
      "Step: 2140, Training Loss: 3.30978, LR: 0.0005633, Tokens/sec: 181993.52\n",
      "Step: 2141, Training Loss: 3.41626, LR: 0.0005629, Tokens/sec: 181942.31\n",
      "Step: 2142, Training Loss: 3.00322, LR: 0.0005625, Tokens/sec: 182279.53\n",
      "Step: 2143, Training Loss: 3.50985, LR: 0.0005621, Tokens/sec: 181082.61\n",
      "Step: 2144, Training Loss: 3.14044, LR: 0.0005617, Tokens/sec: 182010.01\n",
      "Step: 2145, Training Loss: 3.29169, LR: 0.0005613, Tokens/sec: 181138.17\n",
      "Step: 2146, Training Loss: 3.06504, LR: 0.0005609, Tokens/sec: 181788.49\n",
      "Step: 2147, Training Loss: 2.98519, LR: 0.0005605, Tokens/sec: 182013.53\n",
      "Step: 2148, Training Loss: 3.26781, LR: 0.0005601, Tokens/sec: 181618.68\n",
      "Step: 2149, Training Loss: 2.89091, LR: 0.0005597, Tokens/sec: 181953.85\n",
      "Step: 2150, Training Loss: 3.46160, LR: 0.0005593, Tokens/sec: 182086.46\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 2150, Eval Loss: 3.17211\n",
      "Step: 2151, Training Loss: 2.97749, LR: 0.0005589, Tokens/sec: 181082.27\n",
      "Step: 2152, Training Loss: 3.24579, LR: 0.0005585, Tokens/sec: 181939.61\n",
      "Step: 2153, Training Loss: 3.18774, LR: 0.0005581, Tokens/sec: 181856.01\n",
      "Step: 2154, Training Loss: 2.88987, LR: 0.0005577, Tokens/sec: 180980.84\n",
      "Step: 2155, Training Loss: 3.30437, LR: 0.0005574, Tokens/sec: 182379.53\n",
      "Step: 2156, Training Loss: 3.37899, LR: 0.0005570, Tokens/sec: 179816.78\n",
      "Step: 2157, Training Loss: 3.08743, LR: 0.0005566, Tokens/sec: 181859.21\n",
      "Step: 2158, Training Loss: 3.19126, LR: 0.0005562, Tokens/sec: 182085.13\n",
      "Step: 2159, Training Loss: 3.51694, LR: 0.0005558, Tokens/sec: 181060.04\n",
      "Step: 2160, Training Loss: 3.04327, LR: 0.0005554, Tokens/sec: 179547.19\n",
      "Step: 2161, Training Loss: 3.08643, LR: 0.0005550, Tokens/sec: 179374.23\n",
      "Step: 2162, Training Loss: 3.14725, LR: 0.0005546, Tokens/sec: 173391.71\n",
      "Step: 2163, Training Loss: 3.26813, LR: 0.0005542, Tokens/sec: 182419.68\n",
      "Step: 2164, Training Loss: 3.49968, LR: 0.0005538, Tokens/sec: 181628.77\n",
      "Step: 2165, Training Loss: 3.21655, LR: 0.0005534, Tokens/sec: 182084.36\n",
      "Step: 2166, Training Loss: 3.33212, LR: 0.0005530, Tokens/sec: 182211.18\n",
      "Step: 2167, Training Loss: 3.05963, LR: 0.0005526, Tokens/sec: 182142.49\n",
      "Step: 2168, Training Loss: 3.40826, LR: 0.0005522, Tokens/sec: 180869.83\n",
      "Step: 2169, Training Loss: 3.00487, LR: 0.0005518, Tokens/sec: 182647.76\n",
      "Step: 2170, Training Loss: 2.84505, LR: 0.0005514, Tokens/sec: 182818.94\n",
      "Step: 2171, Training Loss: 3.24267, LR: 0.0005510, Tokens/sec: 182168.12\n",
      "Step: 2172, Training Loss: 3.19676, LR: 0.0005506, Tokens/sec: 182234.49\n",
      "Step: 2173, Training Loss: 3.11324, LR: 0.0005502, Tokens/sec: 181085.16\n",
      "Step: 2174, Training Loss: 3.05727, LR: 0.0005498, Tokens/sec: 179080.78\n",
      "Step: 2175, Training Loss: 2.95189, LR: 0.0005494, Tokens/sec: 182019.23\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 2175, Eval Loss: 3.22716\n",
      "Step: 2176, Training Loss: 3.00800, LR: 0.0005490, Tokens/sec: 179975.23\n",
      "Step: 2177, Training Loss: 3.36933, LR: 0.0005486, Tokens/sec: 181903.68\n",
      "Step: 2178, Training Loss: 2.81848, LR: 0.0005482, Tokens/sec: 182530.56\n",
      "Step: 2179, Training Loss: 2.90865, LR: 0.0005478, Tokens/sec: 182212.76\n",
      "Step: 2180, Training Loss: 3.38266, LR: 0.0005474, Tokens/sec: 182535.75\n",
      "Step: 2181, Training Loss: 3.10387, LR: 0.0005470, Tokens/sec: 182399.65\n",
      "Step: 2182, Training Loss: 3.12407, LR: 0.0005466, Tokens/sec: 180956.61\n",
      "Step: 2183, Training Loss: 2.93989, LR: 0.0005462, Tokens/sec: 182429.70\n",
      "Step: 2184, Training Loss: 2.99167, LR: 0.0005458, Tokens/sec: 183142.30\n",
      "Step: 2185, Training Loss: 2.52103, LR: 0.0005454, Tokens/sec: 182585.33\n",
      "Step: 2186, Training Loss: 2.88888, LR: 0.0005450, Tokens/sec: 182633.36\n",
      "Step: 2187, Training Loss: 3.06675, LR: 0.0005446, Tokens/sec: 182184.86\n",
      "Step: 2188, Training Loss: 3.20029, LR: 0.0005442, Tokens/sec: 182662.51\n",
      "Step: 2189, Training Loss: 3.17029, LR: 0.0005438, Tokens/sec: 182647.59\n",
      "Step: 2190, Training Loss: 3.50501, LR: 0.0005434, Tokens/sec: 182870.33\n",
      "Step: 2191, Training Loss: 3.29485, LR: 0.0005430, Tokens/sec: 182568.27\n",
      "Step: 2192, Training Loss: 3.14648, LR: 0.0005426, Tokens/sec: 182371.00\n",
      "Step: 2193, Training Loss: 2.99123, LR: 0.0005423, Tokens/sec: 183160.24\n",
      "Step: 2194, Training Loss: 3.16080, LR: 0.0005419, Tokens/sec: 181049.45\n",
      "Step: 2195, Training Loss: 2.88378, LR: 0.0005415, Tokens/sec: 182626.46\n",
      "Step: 2196, Training Loss: 3.20525, LR: 0.0005411, Tokens/sec: 182664.30\n",
      "Step: 2197, Training Loss: 3.07721, LR: 0.0005407, Tokens/sec: 182770.55\n",
      "Step: 2198, Training Loss: 3.31597, LR: 0.0005403, Tokens/sec: 182685.08\n",
      "Step: 2199, Training Loss: 2.98025, LR: 0.0005399, Tokens/sec: 182707.98\n",
      "Step: 2200, Training Loss: 2.93200, LR: 0.0005395, Tokens/sec: 180976.34\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 2200, Eval Loss: 3.07211\n",
      "Step: 2201, Training Loss: 2.81372, LR: 0.0005391, Tokens/sec: 182058.45\n",
      "Step: 2202, Training Loss: 3.36782, LR: 0.0005387, Tokens/sec: 182397.54\n",
      "Step: 2203, Training Loss: 3.06270, LR: 0.0005383, Tokens/sec: 182012.32\n",
      "Step: 2204, Training Loss: 2.96965, LR: 0.0005379, Tokens/sec: 182818.23\n",
      "Step: 2205, Training Loss: 2.80582, LR: 0.0005375, Tokens/sec: 182780.34\n",
      "Step: 2206, Training Loss: 3.18321, LR: 0.0005371, Tokens/sec: 182923.82\n",
      "Step: 2207, Training Loss: 3.09337, LR: 0.0005367, Tokens/sec: 182880.45\n",
      "Step: 2208, Training Loss: 2.82225, LR: 0.0005363, Tokens/sec: 182216.24\n",
      "Step: 2209, Training Loss: 2.79410, LR: 0.0005359, Tokens/sec: 183126.79\n",
      "Step: 2210, Training Loss: 3.26573, LR: 0.0005355, Tokens/sec: 183030.79\n",
      "Step: 2211, Training Loss: 3.10575, LR: 0.0005351, Tokens/sec: 182263.05\n",
      "Step: 2212, Training Loss: 2.94234, LR: 0.0005347, Tokens/sec: 182718.07\n",
      "Step: 2213, Training Loss: 2.81401, LR: 0.0005343, Tokens/sec: 182480.94\n",
      "Step: 2214, Training Loss: 3.42283, LR: 0.0005339, Tokens/sec: 181647.55\n",
      "Step: 2215, Training Loss: 3.06587, LR: 0.0005335, Tokens/sec: 182081.67\n",
      "Step: 2216, Training Loss: 3.03151, LR: 0.0005331, Tokens/sec: 182748.78\n",
      "Step: 2217, Training Loss: 3.10905, LR: 0.0005327, Tokens/sec: 182874.38\n",
      "Step: 2218, Training Loss: 2.99528, LR: 0.0005323, Tokens/sec: 182618.30\n",
      "Step: 2219, Training Loss: 3.29287, LR: 0.0005319, Tokens/sec: 182954.60\n",
      "Step: 2220, Training Loss: 3.03679, LR: 0.0005315, Tokens/sec: 181926.69\n",
      "Step: 2221, Training Loss: 2.97635, LR: 0.0005311, Tokens/sec: 182473.80\n",
      "Step: 2222, Training Loss: 3.32389, LR: 0.0005307, Tokens/sec: 177624.76\n",
      "Step: 2223, Training Loss: 2.91798, LR: 0.0005303, Tokens/sec: 182866.34\n",
      "Step: 2224, Training Loss: 2.91006, LR: 0.0005299, Tokens/sec: 182384.81\n",
      "Step: 2225, Training Loss: 3.34572, LR: 0.0005295, Tokens/sec: 182695.23\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 2225, Eval Loss: 3.06224\n",
      "Step: 2226, Training Loss: 2.81412, LR: 0.0005291, Tokens/sec: 181431.82\n",
      "Step: 2227, Training Loss: 2.80461, LR: 0.0005287, Tokens/sec: 181997.14\n",
      "Step: 2228, Training Loss: 2.99972, LR: 0.0005283, Tokens/sec: 182279.25\n",
      "Step: 2229, Training Loss: 3.30357, LR: 0.0005280, Tokens/sec: 182655.16\n",
      "Step: 2230, Training Loss: 3.60089, LR: 0.0005276, Tokens/sec: 182619.61\n",
      "Step: 2231, Training Loss: 2.98915, LR: 0.0005272, Tokens/sec: 181249.95\n",
      "Step: 2232, Training Loss: 2.95303, LR: 0.0005268, Tokens/sec: 182271.41\n",
      "Step: 2233, Training Loss: 3.43158, LR: 0.0005264, Tokens/sec: 182768.97\n",
      "Step: 2234, Training Loss: 2.87355, LR: 0.0005260, Tokens/sec: 183146.95\n",
      "Step: 2235, Training Loss: 3.44348, LR: 0.0005256, Tokens/sec: 182583.60\n",
      "Step: 2236, Training Loss: 3.26291, LR: 0.0005252, Tokens/sec: 183401.96\n",
      "Step: 2237, Training Loss: 3.30000, LR: 0.0005248, Tokens/sec: 180937.27\n",
      "Step: 2238, Training Loss: 3.20784, LR: 0.0005244, Tokens/sec: 182147.62\n",
      "Step: 2239, Training Loss: 2.99753, LR: 0.0005240, Tokens/sec: 182878.35\n",
      "Step: 2240, Training Loss: 2.95614, LR: 0.0005236, Tokens/sec: 182445.03\n",
      "Step: 2241, Training Loss: 3.40605, LR: 0.0005232, Tokens/sec: 182828.04\n",
      "Step: 2242, Training Loss: 3.23463, LR: 0.0005228, Tokens/sec: 182919.55\n",
      "Step: 2243, Training Loss: 3.04695, LR: 0.0005224, Tokens/sec: 181694.94\n",
      "Step: 2244, Training Loss: 3.07015, LR: 0.0005220, Tokens/sec: 182678.96\n",
      "Step: 2245, Training Loss: 3.27788, LR: 0.0005216, Tokens/sec: 182444.16\n",
      "Step: 2246, Training Loss: 3.19710, LR: 0.0005212, Tokens/sec: 183463.09\n",
      "Step: 2247, Training Loss: 2.73319, LR: 0.0005208, Tokens/sec: 182490.19\n",
      "Step: 2248, Training Loss: 2.92183, LR: 0.0005204, Tokens/sec: 183281.26\n",
      "Step: 2249, Training Loss: 3.28600, LR: 0.0005200, Tokens/sec: 181872.86\n",
      "Step: 2250, Training Loss: 2.80597, LR: 0.0005196, Tokens/sec: 183122.94\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 2250, Eval Loss: 3.04280\n",
      "Step: 2251, Training Loss: 2.92087, LR: 0.0005192, Tokens/sec: 181661.09\n",
      "Step: 2252, Training Loss: 3.23960, LR: 0.0005188, Tokens/sec: 182508.97\n",
      "Step: 2253, Training Loss: 2.81017, LR: 0.0005184, Tokens/sec: 182763.30\n",
      "Step: 2254, Training Loss: 2.62158, LR: 0.0005180, Tokens/sec: 183113.37\n",
      "Step: 2255, Training Loss: 3.23642, LR: 0.0005176, Tokens/sec: 182909.04\n",
      "Step: 2256, Training Loss: 2.60159, LR: 0.0005172, Tokens/sec: 182493.24\n",
      "Step: 2257, Training Loss: 3.10225, LR: 0.0005168, Tokens/sec: 181440.55\n",
      "Step: 2258, Training Loss: 3.02515, LR: 0.0005164, Tokens/sec: 183031.62\n",
      "Step: 2259, Training Loss: 2.85599, LR: 0.0005161, Tokens/sec: 182793.01\n",
      "Step: 2260, Training Loss: 2.97758, LR: 0.0005157, Tokens/sec: 182844.43\n",
      "Step: 2261, Training Loss: 3.04876, LR: 0.0005153, Tokens/sec: 182486.52\n",
      "Step: 2262, Training Loss: 3.06485, LR: 0.0005149, Tokens/sec: 182129.71\n",
      "Step: 2263, Training Loss: 2.94020, LR: 0.0005145, Tokens/sec: 181150.98\n",
      "Step: 2264, Training Loss: 3.08982, LR: 0.0005141, Tokens/sec: 182655.39\n",
      "Step: 2265, Training Loss: 3.00433, LR: 0.0005137, Tokens/sec: 182434.55\n",
      "Step: 2266, Training Loss: 3.05537, LR: 0.0005133, Tokens/sec: 181639.12\n",
      "Step: 2267, Training Loss: 2.72159, LR: 0.0005129, Tokens/sec: 181296.30\n",
      "Step: 2268, Training Loss: 3.09706, LR: 0.0005125, Tokens/sec: 182513.02\n",
      "Step: 2269, Training Loss: 2.62183, LR: 0.0005121, Tokens/sec: 181001.04\n",
      "Step: 2270, Training Loss: 2.76455, LR: 0.0005117, Tokens/sec: 181696.73\n",
      "Step: 2271, Training Loss: 2.93276, LR: 0.0005113, Tokens/sec: 182666.04\n",
      "Step: 2272, Training Loss: 2.99599, LR: 0.0005109, Tokens/sec: 181090.35\n",
      "Step: 2273, Training Loss: 3.07294, LR: 0.0005105, Tokens/sec: 181498.17\n",
      "Step: 2274, Training Loss: 3.30685, LR: 0.0005101, Tokens/sec: 183050.06\n",
      "Step: 2275, Training Loss: 2.96554, LR: 0.0005097, Tokens/sec: 180632.95\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 2275, Eval Loss: 3.16374\n",
      "Step: 2276, Training Loss: 3.09185, LR: 0.0005093, Tokens/sec: 180882.11\n",
      "Step: 2277, Training Loss: 3.38235, LR: 0.0005089, Tokens/sec: 179693.65\n",
      "Step: 2278, Training Loss: 3.02586, LR: 0.0005085, Tokens/sec: 181640.28\n",
      "Step: 2279, Training Loss: 2.80213, LR: 0.0005081, Tokens/sec: 182422.62\n",
      "Step: 2280, Training Loss: 3.06756, LR: 0.0005077, Tokens/sec: 181424.47\n",
      "Step: 2281, Training Loss: 3.05389, LR: 0.0005073, Tokens/sec: 182626.82\n",
      "Step: 2282, Training Loss: 3.07247, LR: 0.0005069, Tokens/sec: 182824.33\n",
      "Step: 2283, Training Loss: 3.33933, LR: 0.0005065, Tokens/sec: 181605.12\n",
      "Step: 2284, Training Loss: 2.84316, LR: 0.0005062, Tokens/sec: 180298.48\n",
      "Step: 2285, Training Loss: 2.80973, LR: 0.0005058, Tokens/sec: 182438.39\n",
      "Step: 2286, Training Loss: 2.73550, LR: 0.0005054, Tokens/sec: 182535.16\n",
      "Step: 2287, Training Loss: 3.18719, LR: 0.0005050, Tokens/sec: 182796.77\n",
      "Step: 2288, Training Loss: 3.05744, LR: 0.0005046, Tokens/sec: 182463.45\n",
      "Step: 2289, Training Loss: 3.07935, LR: 0.0005042, Tokens/sec: 180839.76\n",
      "Step: 2290, Training Loss: 3.01875, LR: 0.0005038, Tokens/sec: 182856.64\n",
      "Step: 2291, Training Loss: 2.96122, LR: 0.0005034, Tokens/sec: 182532.97\n",
      "Step: 2292, Training Loss: 2.58433, LR: 0.0005030, Tokens/sec: 182613.36\n",
      "Step: 2293, Training Loss: 3.31510, LR: 0.0005026, Tokens/sec: 182660.66\n",
      "Step: 2294, Training Loss: 3.15422, LR: 0.0005022, Tokens/sec: 182996.47\n",
      "Step: 2295, Training Loss: 3.00540, LR: 0.0005018, Tokens/sec: 181445.71\n",
      "Step: 2296, Training Loss: 3.10174, LR: 0.0005014, Tokens/sec: 182807.69\n",
      "Step: 2297, Training Loss: 2.67202, LR: 0.0005010, Tokens/sec: 183119.87\n",
      "Step: 2298, Training Loss: 2.73327, LR: 0.0005006, Tokens/sec: 182641.38\n",
      "Step: 2299, Training Loss: 2.98070, LR: 0.0005002, Tokens/sec: 182856.61\n",
      "Step: 2300, Training Loss: 2.94229, LR: 0.0004998, Tokens/sec: 182943.89\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 2300, Eval Loss: 3.02777\n",
      "Step: 2301, Training Loss: 2.75140, LR: 0.0004994, Tokens/sec: 181774.26\n",
      "Step: 2302, Training Loss: 3.15700, LR: 0.0004990, Tokens/sec: 182336.10\n",
      "Step: 2303, Training Loss: 3.28544, LR: 0.0004986, Tokens/sec: 182610.54\n",
      "Step: 2304, Training Loss: 3.00027, LR: 0.0004982, Tokens/sec: 182711.82\n",
      "Step: 2305, Training Loss: 3.25259, LR: 0.0004979, Tokens/sec: 183081.16\n",
      "Step: 2306, Training Loss: 3.40291, LR: 0.0004975, Tokens/sec: 181696.77\n",
      "Step: 2307, Training Loss: 2.84789, LR: 0.0004971, Tokens/sec: 182219.39\n",
      "Step: 2308, Training Loss: 3.12617, LR: 0.0004967, Tokens/sec: 183290.66\n",
      "Step: 2309, Training Loss: 2.90545, LR: 0.0004963, Tokens/sec: 182782.69\n",
      "Step: 2310, Training Loss: 2.60329, LR: 0.0004959, Tokens/sec: 183079.69\n",
      "Step: 2311, Training Loss: 3.01090, LR: 0.0004955, Tokens/sec: 182654.32\n",
      "Step: 2312, Training Loss: 3.00009, LR: 0.0004951, Tokens/sec: 183524.89\n",
      "Step: 2313, Training Loss: 3.07573, LR: 0.0004947, Tokens/sec: 182710.34\n",
      "Step: 2314, Training Loss: 2.72687, LR: 0.0004943, Tokens/sec: 182709.29\n",
      "Step: 2315, Training Loss: 2.86918, LR: 0.0004939, Tokens/sec: 182446.88\n",
      "Step: 2316, Training Loss: 3.18828, LR: 0.0004935, Tokens/sec: 182656.91\n",
      "Step: 2317, Training Loss: 2.55025, LR: 0.0004931, Tokens/sec: 183053.19\n",
      "Step: 2318, Training Loss: 2.81057, LR: 0.0004927, Tokens/sec: 181409.04\n",
      "Step: 2319, Training Loss: 2.62962, LR: 0.0004923, Tokens/sec: 182287.33\n",
      "Step: 2320, Training Loss: 2.86827, LR: 0.0004919, Tokens/sec: 182866.98\n",
      "Step: 2321, Training Loss: 3.01273, LR: 0.0004915, Tokens/sec: 182292.39\n",
      "Step: 2322, Training Loss: 3.46295, LR: 0.0004911, Tokens/sec: 182895.13\n",
      "Step: 2323, Training Loss: 2.66863, LR: 0.0004908, Tokens/sec: 182805.16\n",
      "Step: 2324, Training Loss: 3.25064, LR: 0.0004904, Tokens/sec: 181740.05\n",
      "Step: 2325, Training Loss: 2.86414, LR: 0.0004900, Tokens/sec: 182812.09\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 2325, Eval Loss: 3.05237\n",
      "Step: 2326, Training Loss: 2.87443, LR: 0.0004896, Tokens/sec: 181890.22\n",
      "Step: 2327, Training Loss: 2.98625, LR: 0.0004892, Tokens/sec: 182525.81\n",
      "Step: 2328, Training Loss: 2.86559, LR: 0.0004888, Tokens/sec: 183312.02\n",
      "Step: 2329, Training Loss: 2.92150, LR: 0.0004884, Tokens/sec: 182761.15\n",
      "Step: 2330, Training Loss: 3.09332, LR: 0.0004880, Tokens/sec: 182997.19\n",
      "Step: 2331, Training Loss: 3.21086, LR: 0.0004876, Tokens/sec: 183286.41\n",
      "Step: 2332, Training Loss: 2.78458, LR: 0.0004872, Tokens/sec: 182028.51\n",
      "Step: 2333, Training Loss: 2.86554, LR: 0.0004868, Tokens/sec: 183302.49\n",
      "Step: 2334, Training Loss: 3.21794, LR: 0.0004864, Tokens/sec: 182725.73\n",
      "Step: 2335, Training Loss: 2.81401, LR: 0.0004860, Tokens/sec: 182240.07\n",
      "Step: 2336, Training Loss: 2.47457, LR: 0.0004856, Tokens/sec: 183234.41\n",
      "Step: 2337, Training Loss: 2.75124, LR: 0.0004852, Tokens/sec: 182775.85\n",
      "Step: 2338, Training Loss: 2.72402, LR: 0.0004848, Tokens/sec: 181520.63\n",
      "Step: 2339, Training Loss: 3.08156, LR: 0.0004845, Tokens/sec: 183330.88\n",
      "Step: 2340, Training Loss: 2.91528, LR: 0.0004841, Tokens/sec: 183295.88\n",
      "Step: 2341, Training Loss: 2.75114, LR: 0.0004837, Tokens/sec: 183079.61\n",
      "Step: 2342, Training Loss: 2.89160, LR: 0.0004833, Tokens/sec: 183782.95\n",
      "Step: 2343, Training Loss: 3.01430, LR: 0.0004829, Tokens/sec: 182881.29\n",
      "Step: 2344, Training Loss: 3.23108, LR: 0.0004825, Tokens/sec: 182289.19\n",
      "Step: 2345, Training Loss: 3.05860, LR: 0.0004821, Tokens/sec: 182699.83\n",
      "Step: 2346, Training Loss: 2.98206, LR: 0.0004817, Tokens/sec: 183163.22\n",
      "Step: 2347, Training Loss: 3.05872, LR: 0.0004813, Tokens/sec: 183135.77\n",
      "Step: 2348, Training Loss: 2.98429, LR: 0.0004809, Tokens/sec: 183157.24\n",
      "Step: 2349, Training Loss: 2.67623, LR: 0.0004805, Tokens/sec: 183180.01\n",
      "Step: 2350, Training Loss: 3.09799, LR: 0.0004801, Tokens/sec: 181919.92\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 2350, Eval Loss: 3.09814\n",
      "Step: 2351, Training Loss: 2.87440, LR: 0.0004797, Tokens/sec: 182170.63\n",
      "Step: 2352, Training Loss: 3.25254, LR: 0.0004793, Tokens/sec: 182245.13\n",
      "Step: 2353, Training Loss: 2.78895, LR: 0.0004790, Tokens/sec: 182482.28\n",
      "Step: 2354, Training Loss: 2.91802, LR: 0.0004786, Tokens/sec: 182362.90\n",
      "Step: 2355, Training Loss: 2.75339, LR: 0.0004782, Tokens/sec: 174933.63\n",
      "Step: 2356, Training Loss: 2.87872, LR: 0.0004778, Tokens/sec: 182416.02\n",
      "Step: 2357, Training Loss: 3.31488, LR: 0.0004774, Tokens/sec: 182495.38\n",
      "Step: 2358, Training Loss: 2.98056, LR: 0.0004770, Tokens/sec: 179763.86\n",
      "Step: 2359, Training Loss: 2.82805, LR: 0.0004766, Tokens/sec: 182414.11\n",
      "Step: 2360, Training Loss: 3.00925, LR: 0.0004762, Tokens/sec: 183402.71\n",
      "Step: 2361, Training Loss: 2.95658, LR: 0.0004758, Tokens/sec: 183051.61\n",
      "Step: 2362, Training Loss: 2.91652, LR: 0.0004754, Tokens/sec: 183009.69\n",
      "Step: 2363, Training Loss: 2.93094, LR: 0.0004750, Tokens/sec: 182814.06\n",
      "Step: 2364, Training Loss: 2.60509, LR: 0.0004746, Tokens/sec: 181862.32\n",
      "Step: 2365, Training Loss: 3.33155, LR: 0.0004743, Tokens/sec: 182398.41\n",
      "Step: 2366, Training Loss: 3.03525, LR: 0.0004739, Tokens/sec: 182305.67\n",
      "Step: 2367, Training Loss: 2.91680, LR: 0.0004735, Tokens/sec: 182789.38\n",
      "Step: 2368, Training Loss: 2.83242, LR: 0.0004731, Tokens/sec: 182971.06\n",
      "Step: 2369, Training Loss: 3.20403, LR: 0.0004727, Tokens/sec: 182951.65\n",
      "Step: 2370, Training Loss: 3.05513, LR: 0.0004723, Tokens/sec: 181636.95\n",
      "Step: 2371, Training Loss: 3.15804, LR: 0.0004719, Tokens/sec: 182499.64\n",
      "Step: 2372, Training Loss: 2.81008, LR: 0.0004715, Tokens/sec: 182990.88\n",
      "Step: 2373, Training Loss: 2.78705, LR: 0.0004711, Tokens/sec: 182000.11\n",
      "Step: 2374, Training Loss: 2.72777, LR: 0.0004707, Tokens/sec: 182527.95\n",
      "Step: 2375, Training Loss: 3.10346, LR: 0.0004703, Tokens/sec: 182596.65\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 2375, Eval Loss: 2.97162\n",
      "Step: 2376, Training Loss: 2.79122, LR: 0.0004699, Tokens/sec: 181353.74\n",
      "Step: 2377, Training Loss: 2.69482, LR: 0.0004696, Tokens/sec: 183041.97\n",
      "Step: 2378, Training Loss: 3.11409, LR: 0.0004692, Tokens/sec: 182612.97\n",
      "Step: 2379, Training Loss: 2.65877, LR: 0.0004688, Tokens/sec: 182302.80\n",
      "Step: 2380, Training Loss: 2.70426, LR: 0.0004684, Tokens/sec: 182557.03\n",
      "Step: 2381, Training Loss: 2.67147, LR: 0.0004680, Tokens/sec: 180757.80\n",
      "Step: 2382, Training Loss: 2.86421, LR: 0.0004676, Tokens/sec: 182288.52\n",
      "Step: 2383, Training Loss: 2.70453, LR: 0.0004672, Tokens/sec: 182744.73\n",
      "Step: 2384, Training Loss: 2.90160, LR: 0.0004668, Tokens/sec: 182266.78\n",
      "Step: 2385, Training Loss: 3.11574, LR: 0.0004664, Tokens/sec: 182449.08\n",
      "Step: 2386, Training Loss: 2.89996, LR: 0.0004660, Tokens/sec: 182352.01\n",
      "Step: 2387, Training Loss: 2.67020, LR: 0.0004656, Tokens/sec: 181233.75\n",
      "Step: 2388, Training Loss: 3.06031, LR: 0.0004653, Tokens/sec: 181730.30\n",
      "Step: 2389, Training Loss: 3.14049, LR: 0.0004649, Tokens/sec: 183479.24\n",
      "Step: 2390, Training Loss: 2.98999, LR: 0.0004645, Tokens/sec: 183064.72\n",
      "Step: 2391, Training Loss: 3.09092, LR: 0.0004641, Tokens/sec: 182864.42\n",
      "Step: 2392, Training Loss: 3.22031, LR: 0.0004637, Tokens/sec: 182722.07\n",
      "Step: 2393, Training Loss: 3.26886, LR: 0.0004633, Tokens/sec: 181153.35\n",
      "Step: 2394, Training Loss: 2.98836, LR: 0.0004629, Tokens/sec: 182570.93\n",
      "Step: 2395, Training Loss: 2.99577, LR: 0.0004625, Tokens/sec: 182974.11\n",
      "Step: 2396, Training Loss: 2.67963, LR: 0.0004621, Tokens/sec: 182363.78\n",
      "Step: 2397, Training Loss: 2.93549, LR: 0.0004617, Tokens/sec: 182340.53\n",
      "Step: 2398, Training Loss: 2.93834, LR: 0.0004614, Tokens/sec: 182661.93\n",
      "Step: 2399, Training Loss: 2.88168, LR: 0.0004610, Tokens/sec: 180953.64\n",
      "Step: 2400, Training Loss: 2.79354, LR: 0.0004606, Tokens/sec: 182949.44\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 2400, Eval Loss: 2.93334\n",
      "Step: 2401, Training Loss: 2.92283, LR: 0.0004602, Tokens/sec: 180457.76\n",
      "Step: 2402, Training Loss: 2.90241, LR: 0.0004598, Tokens/sec: 182102.90\n",
      "Step: 2403, Training Loss: 3.07825, LR: 0.0004594, Tokens/sec: 182846.06\n",
      "Step: 2404, Training Loss: 2.74709, LR: 0.0004590, Tokens/sec: 182305.31\n",
      "Step: 2405, Training Loss: 3.04545, LR: 0.0004586, Tokens/sec: 182833.82\n",
      "Step: 2406, Training Loss: 2.85315, LR: 0.0004582, Tokens/sec: 182902.39\n",
      "Step: 2407, Training Loss: 2.76307, LR: 0.0004579, Tokens/sec: 181049.06\n",
      "Step: 2408, Training Loss: 3.15300, LR: 0.0004575, Tokens/sec: 182836.32\n",
      "Step: 2409, Training Loss: 3.00945, LR: 0.0004571, Tokens/sec: 182914.58\n",
      "Step: 2410, Training Loss: 2.93411, LR: 0.0004567, Tokens/sec: 182796.34\n",
      "Step: 2411, Training Loss: 2.67183, LR: 0.0004563, Tokens/sec: 182082.62\n",
      "Step: 2412, Training Loss: 2.60426, LR: 0.0004559, Tokens/sec: 182098.91\n",
      "Step: 2413, Training Loss: 2.93183, LR: 0.0004555, Tokens/sec: 181907.11\n",
      "Step: 2414, Training Loss: 2.57503, LR: 0.0004551, Tokens/sec: 183059.78\n",
      "Step: 2415, Training Loss: 2.93568, LR: 0.0004547, Tokens/sec: 183049.48\n",
      "Step: 2416, Training Loss: 2.67907, LR: 0.0004544, Tokens/sec: 182714.29\n",
      "Step: 2417, Training Loss: 2.49462, LR: 0.0004540, Tokens/sec: 183005.26\n",
      "Step: 2418, Training Loss: 2.65361, LR: 0.0004536, Tokens/sec: 182544.89\n",
      "Step: 2419, Training Loss: 2.58413, LR: 0.0004532, Tokens/sec: 181445.83\n",
      "Step: 2420, Training Loss: 3.47146, LR: 0.0004528, Tokens/sec: 181571.51\n",
      "Step: 2421, Training Loss: 2.87738, LR: 0.0004524, Tokens/sec: 183023.95\n",
      "Step: 2422, Training Loss: 2.82441, LR: 0.0004520, Tokens/sec: 182640.65\n",
      "Step: 2423, Training Loss: 2.69893, LR: 0.0004516, Tokens/sec: 182468.54\n",
      "Step: 2424, Training Loss: 3.05016, LR: 0.0004512, Tokens/sec: 182988.47\n",
      "Step: 2425, Training Loss: 3.41077, LR: 0.0004509, Tokens/sec: 181200.48\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 2425, Eval Loss: 2.89170\n",
      "Step: 2426, Training Loss: 2.62101, LR: 0.0004505, Tokens/sec: 182468.54\n",
      "Step: 2427, Training Loss: 3.22889, LR: 0.0004501, Tokens/sec: 181482.54\n",
      "Step: 2428, Training Loss: 2.53367, LR: 0.0004497, Tokens/sec: 182456.33\n",
      "Step: 2429, Training Loss: 2.79444, LR: 0.0004493, Tokens/sec: 182735.24\n",
      "Step: 2430, Training Loss: 2.57687, LR: 0.0004489, Tokens/sec: 182788.20\n",
      "Step: 2431, Training Loss: 2.77811, LR: 0.0004485, Tokens/sec: 182689.35\n",
      "Step: 2432, Training Loss: 2.66338, LR: 0.0004482, Tokens/sec: 182731.39\n",
      "Step: 2433, Training Loss: 3.14380, LR: 0.0004478, Tokens/sec: 181034.34\n",
      "Step: 2434, Training Loss: 3.18853, LR: 0.0004474, Tokens/sec: 183078.25\n",
      "Step: 2435, Training Loss: 2.92201, LR: 0.0004470, Tokens/sec: 182669.93\n",
      "Step: 2436, Training Loss: 2.76227, LR: 0.0004466, Tokens/sec: 182191.22\n",
      "Step: 2437, Training Loss: 2.90992, LR: 0.0004462, Tokens/sec: 182327.01\n",
      "Step: 2438, Training Loss: 2.70124, LR: 0.0004458, Tokens/sec: 182636.23\n",
      "Step: 2439, Training Loss: 2.97477, LR: 0.0004454, Tokens/sec: 181634.31\n",
      "Step: 2440, Training Loss: 2.84926, LR: 0.0004451, Tokens/sec: 182162.78\n",
      "Step: 2441, Training Loss: 3.02797, LR: 0.0004447, Tokens/sec: 182651.37\n",
      "Step: 2442, Training Loss: 3.16416, LR: 0.0004443, Tokens/sec: 182920.77\n",
      "Step: 2443, Training Loss: 2.80772, LR: 0.0004439, Tokens/sec: 183093.56\n",
      "Step: 2444, Training Loss: 3.12111, LR: 0.0004435, Tokens/sec: 182928.63\n",
      "Step: 2445, Training Loss: 2.76893, LR: 0.0004431, Tokens/sec: 182374.89\n",
      "Step: 2446, Training Loss: 2.93571, LR: 0.0004427, Tokens/sec: 182468.59\n",
      "Step: 2447, Training Loss: 3.00533, LR: 0.0004424, Tokens/sec: 182494.72\n",
      "Step: 2448, Training Loss: 2.76024, LR: 0.0004420, Tokens/sec: 182209.82\n",
      "Step: 2449, Training Loss: 2.79110, LR: 0.0004416, Tokens/sec: 182113.17\n",
      "Step: 2450, Training Loss: 2.78690, LR: 0.0004412, Tokens/sec: 182555.62\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 2450, Eval Loss: 2.84631\n",
      "Step: 2451, Training Loss: 2.83620, LR: 0.0004408, Tokens/sec: 181286.13\n",
      "Step: 2452, Training Loss: 2.93243, LR: 0.0004404, Tokens/sec: 182443.24\n",
      "Step: 2453, Training Loss: 2.79783, LR: 0.0004400, Tokens/sec: 182602.98\n",
      "Step: 2454, Training Loss: 2.65307, LR: 0.0004397, Tokens/sec: 182903.44\n",
      "Step: 2455, Training Loss: 2.94348, LR: 0.0004393, Tokens/sec: 182445.77\n",
      "Step: 2456, Training Loss: 2.64854, LR: 0.0004389, Tokens/sec: 181611.21\n",
      "Step: 2457, Training Loss: 2.74741, LR: 0.0004385, Tokens/sec: 183015.87\n",
      "Step: 2458, Training Loss: 2.68757, LR: 0.0004381, Tokens/sec: 182802.03\n",
      "Step: 2459, Training Loss: 2.73947, LR: 0.0004377, Tokens/sec: 182485.20\n",
      "Step: 2460, Training Loss: 2.86799, LR: 0.0004373, Tokens/sec: 182736.67\n",
      "Step: 2461, Training Loss: 2.79727, LR: 0.0004370, Tokens/sec: 182207.36\n",
      "Step: 2462, Training Loss: 2.71972, LR: 0.0004366, Tokens/sec: 181536.65\n",
      "Step: 2463, Training Loss: 2.79269, LR: 0.0004362, Tokens/sec: 183172.79\n",
      "Step: 2464, Training Loss: 3.05410, LR: 0.0004358, Tokens/sec: 183166.52\n",
      "Step: 2465, Training Loss: 2.64761, LR: 0.0004354, Tokens/sec: 182651.26\n",
      "Step: 2466, Training Loss: 2.64288, LR: 0.0004350, Tokens/sec: 182516.23\n",
      "Step: 2467, Training Loss: 2.55404, LR: 0.0004347, Tokens/sec: 182565.38\n",
      "Step: 2468, Training Loss: 2.98589, LR: 0.0004343, Tokens/sec: 181904.67\n",
      "Step: 2469, Training Loss: 2.73553, LR: 0.0004339, Tokens/sec: 177856.52\n",
      "Step: 2470, Training Loss: 2.19673, LR: 0.0004335, Tokens/sec: 183205.21\n",
      "Step: 2471, Training Loss: 2.39730, LR: 0.0004331, Tokens/sec: 182601.75\n",
      "Step: 2472, Training Loss: 2.69303, LR: 0.0004327, Tokens/sec: 182679.93\n",
      "Step: 2473, Training Loss: 2.36387, LR: 0.0004323, Tokens/sec: 182390.13\n",
      "Step: 2474, Training Loss: 3.10343, LR: 0.0004320, Tokens/sec: 181654.25\n",
      "Step: 2475, Training Loss: 2.80325, LR: 0.0004316, Tokens/sec: 182820.03\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 2475, Eval Loss: 2.93432\n",
      "Step: 2476, Training Loss: 2.93821, LR: 0.0004312, Tokens/sec: 182384.46\n",
      "Step: 2477, Training Loss: 2.63689, LR: 0.0004308, Tokens/sec: 182522.21\n",
      "Step: 2478, Training Loss: 2.51292, LR: 0.0004304, Tokens/sec: 182687.57\n",
      "Step: 2479, Training Loss: 2.65267, LR: 0.0004300, Tokens/sec: 182701.04\n",
      "Step: 2480, Training Loss: 2.71491, LR: 0.0004297, Tokens/sec: 182931.49\n",
      "Step: 2481, Training Loss: 2.79919, LR: 0.0004293, Tokens/sec: 182866.51\n",
      "Step: 2482, Training Loss: 2.68875, LR: 0.0004289, Tokens/sec: 181698.62\n",
      "Step: 2483, Training Loss: 2.56523, LR: 0.0004285, Tokens/sec: 182290.83\n",
      "Step: 2484, Training Loss: 2.96211, LR: 0.0004281, Tokens/sec: 183445.16\n",
      "Step: 2485, Training Loss: 2.93863, LR: 0.0004278, Tokens/sec: 182662.37\n",
      "Step: 2486, Training Loss: 3.18845, LR: 0.0004274, Tokens/sec: 182861.98\n",
      "Step: 2487, Training Loss: 2.86762, LR: 0.0004270, Tokens/sec: 182764.00\n",
      "Step: 2488, Training Loss: 2.39295, LR: 0.0004266, Tokens/sec: 181884.73\n",
      "Step: 2489, Training Loss: 2.66846, LR: 0.0004262, Tokens/sec: 183117.19\n",
      "Step: 2490, Training Loss: 3.32050, LR: 0.0004258, Tokens/sec: 183090.04\n",
      "Step: 2491, Training Loss: 2.78272, LR: 0.0004255, Tokens/sec: 183231.60\n",
      "Step: 2492, Training Loss: 3.08611, LR: 0.0004251, Tokens/sec: 183484.42\n",
      "Step: 2493, Training Loss: 2.96477, LR: 0.0004247, Tokens/sec: 183103.92\n",
      "Step: 2494, Training Loss: 3.10321, LR: 0.0004243, Tokens/sec: 182120.34\n",
      "Step: 2495, Training Loss: 3.04415, LR: 0.0004239, Tokens/sec: 182765.00\n",
      "Step: 2496, Training Loss: 2.95930, LR: 0.0004235, Tokens/sec: 182996.16\n",
      "Step: 2497, Training Loss: 2.63668, LR: 0.0004232, Tokens/sec: 182467.78\n",
      "Step: 2498, Training Loss: 2.53003, LR: 0.0004228, Tokens/sec: 184282.45\n",
      "Step: 2499, Training Loss: 2.84605, LR: 0.0004224, Tokens/sec: 182491.13\n",
      "Step: 2500, Training Loss: 2.55240, LR: 0.0004220, Tokens/sec: 182021.23\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 2500, Eval Loss: 2.68894\n",
      "Step: 2501, Training Loss: 3.16435, LR: 0.0004216, Tokens/sec: 180812.40\n",
      "Step: 2502, Training Loss: 2.29687, LR: 0.0004213, Tokens/sec: 182378.43\n",
      "Step: 2503, Training Loss: 2.52464, LR: 0.0004209, Tokens/sec: 182296.18\n",
      "Step: 2504, Training Loss: 2.72613, LR: 0.0004205, Tokens/sec: 182826.32\n",
      "Step: 2505, Training Loss: 3.05106, LR: 0.0004201, Tokens/sec: 182561.27\n",
      "Step: 2506, Training Loss: 2.67652, LR: 0.0004197, Tokens/sec: 182691.28\n",
      "Step: 2507, Training Loss: 2.57933, LR: 0.0004194, Tokens/sec: 182557.85\n",
      "Step: 2508, Training Loss: 2.52223, LR: 0.0004190, Tokens/sec: 181558.84\n",
      "Step: 2509, Training Loss: 2.74150, LR: 0.0004186, Tokens/sec: 182334.17\n",
      "Step: 2510, Training Loss: 2.24354, LR: 0.0004182, Tokens/sec: 182247.71\n",
      "Step: 2511, Training Loss: 3.00220, LR: 0.0004178, Tokens/sec: 182785.46\n",
      "Step: 2512, Training Loss: 3.03022, LR: 0.0004175, Tokens/sec: 182337.67\n",
      "Step: 2513, Training Loss: 3.00590, LR: 0.0004171, Tokens/sec: 182492.12\n",
      "Step: 2514, Training Loss: 2.38360, LR: 0.0004167, Tokens/sec: 181075.75\n",
      "Step: 2515, Training Loss: 3.26780, LR: 0.0004163, Tokens/sec: 182419.07\n",
      "Step: 2516, Training Loss: 2.71534, LR: 0.0004159, Tokens/sec: 183119.01\n",
      "Step: 2517, Training Loss: 2.53387, LR: 0.0004156, Tokens/sec: 183327.94\n",
      "Step: 2518, Training Loss: 2.65599, LR: 0.0004152, Tokens/sec: 182674.25\n",
      "Step: 2519, Training Loss: 2.63539, LR: 0.0004148, Tokens/sec: 182876.89\n",
      "Step: 2520, Training Loss: 2.69491, LR: 0.0004144, Tokens/sec: 181475.34\n",
      "Step: 2521, Training Loss: 2.82255, LR: 0.0004140, Tokens/sec: 182361.83\n",
      "Step: 2522, Training Loss: 2.70836, LR: 0.0004137, Tokens/sec: 182861.99\n",
      "Step: 2523, Training Loss: 2.61919, LR: 0.0004133, Tokens/sec: 182837.75\n",
      "Step: 2524, Training Loss: 2.88976, LR: 0.0004129, Tokens/sec: 182634.66\n",
      "Step: 2525, Training Loss: 2.79571, LR: 0.0004125, Tokens/sec: 183409.62\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 2525, Eval Loss: 2.71058\n",
      "Step: 2526, Training Loss: 2.63458, LR: 0.0004122, Tokens/sec: 182251.16\n",
      "Step: 2527, Training Loss: 2.56071, LR: 0.0004118, Tokens/sec: 182696.17\n",
      "Step: 2528, Training Loss: 2.75580, LR: 0.0004114, Tokens/sec: 183153.88\n",
      "Step: 2529, Training Loss: 2.64235, LR: 0.0004110, Tokens/sec: 182856.85\n",
      "Step: 2530, Training Loss: 2.64619, LR: 0.0004106, Tokens/sec: 182730.94\n",
      "Step: 2531, Training Loss: 3.00217, LR: 0.0004103, Tokens/sec: 181484.28\n",
      "Step: 2532, Training Loss: 2.32548, LR: 0.0004099, Tokens/sec: 182698.43\n",
      "Step: 2533, Training Loss: 2.29228, LR: 0.0004095, Tokens/sec: 183219.46\n",
      "Step: 2534, Training Loss: 2.84910, LR: 0.0004091, Tokens/sec: 183800.29\n",
      "Step: 2535, Training Loss: 2.14752, LR: 0.0004088, Tokens/sec: 182958.09\n",
      "Step: 2536, Training Loss: 2.60633, LR: 0.0004084, Tokens/sec: 182980.25\n",
      "Step: 2537, Training Loss: 2.68423, LR: 0.0004080, Tokens/sec: 181915.02\n",
      "Step: 2538, Training Loss: 2.87548, LR: 0.0004076, Tokens/sec: 182835.20\n",
      "Step: 2539, Training Loss: 2.84668, LR: 0.0004072, Tokens/sec: 182867.44\n",
      "Step: 2540, Training Loss: 2.95286, LR: 0.0004069, Tokens/sec: 182547.19\n",
      "Step: 2541, Training Loss: 2.76374, LR: 0.0004065, Tokens/sec: 182660.57\n",
      "Step: 2542, Training Loss: 2.72747, LR: 0.0004061, Tokens/sec: 182630.80\n",
      "Step: 2543, Training Loss: 2.30754, LR: 0.0004057, Tokens/sec: 182238.34\n",
      "Step: 2544, Training Loss: 2.66234, LR: 0.0004054, Tokens/sec: 183194.96\n",
      "Step: 2545, Training Loss: 2.98016, LR: 0.0004050, Tokens/sec: 183330.40\n",
      "Step: 2546, Training Loss: 2.53265, LR: 0.0004046, Tokens/sec: 182734.74\n",
      "Step: 2547, Training Loss: 2.80699, LR: 0.0004042, Tokens/sec: 183210.94\n",
      "Step: 2548, Training Loss: 2.98081, LR: 0.0004039, Tokens/sec: 182290.57\n",
      "Step: 2549, Training Loss: 2.98219, LR: 0.0004035, Tokens/sec: 182126.42\n",
      "Step: 2550, Training Loss: 2.63364, LR: 0.0004031, Tokens/sec: 182556.46\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 2550, Eval Loss: 2.78115\n",
      "Step: 2551, Training Loss: 2.43958, LR: 0.0004027, Tokens/sec: 181437.17\n",
      "Step: 2552, Training Loss: 2.80924, LR: 0.0004024, Tokens/sec: 182652.36\n",
      "Step: 2553, Training Loss: 2.86963, LR: 0.0004020, Tokens/sec: 183263.27\n",
      "Step: 2554, Training Loss: 2.83787, LR: 0.0004016, Tokens/sec: 182401.40\n",
      "Step: 2555, Training Loss: 3.06349, LR: 0.0004012, Tokens/sec: 182564.23\n",
      "Step: 2556, Training Loss: 2.61037, LR: 0.0004009, Tokens/sec: 182992.07\n",
      "Step: 2557, Training Loss: 2.47475, LR: 0.0004005, Tokens/sec: 181702.26\n",
      "Step: 2558, Training Loss: 2.59250, LR: 0.0004001, Tokens/sec: 182998.79\n",
      "Step: 2559, Training Loss: 2.82808, LR: 0.0003997, Tokens/sec: 183395.09\n",
      "Step: 2560, Training Loss: 2.74359, LR: 0.0003994, Tokens/sec: 182880.09\n",
      "Step: 2561, Training Loss: 2.51679, LR: 0.0003990, Tokens/sec: 183036.37\n",
      "Step: 2562, Training Loss: 2.63903, LR: 0.0003986, Tokens/sec: 183053.80\n",
      "Step: 2563, Training Loss: 2.69466, LR: 0.0003982, Tokens/sec: 181796.74\n",
      "Step: 2564, Training Loss: 2.66003, LR: 0.0003979, Tokens/sec: 182986.60\n",
      "Step: 2565, Training Loss: 2.58252, LR: 0.0003975, Tokens/sec: 183589.33\n",
      "Step: 2566, Training Loss: 2.91880, LR: 0.0003971, Tokens/sec: 183364.92\n",
      "Step: 2567, Training Loss: 2.63970, LR: 0.0003967, Tokens/sec: 182713.94\n",
      "Step: 2568, Training Loss: 2.71647, LR: 0.0003964, Tokens/sec: 182952.71\n",
      "Step: 2569, Training Loss: 2.67156, LR: 0.0003960, Tokens/sec: 182004.89\n",
      "Step: 2570, Training Loss: 3.05045, LR: 0.0003956, Tokens/sec: 182456.55\n",
      "Step: 2571, Training Loss: 2.80187, LR: 0.0003952, Tokens/sec: 183258.62\n",
      "Step: 2572, Training Loss: 3.23668, LR: 0.0003949, Tokens/sec: 182832.84\n",
      "Step: 2573, Training Loss: 2.36152, LR: 0.0003945, Tokens/sec: 183252.20\n",
      "Step: 2574, Training Loss: 2.21675, LR: 0.0003941, Tokens/sec: 183546.12\n",
      "Step: 2575, Training Loss: 2.28931, LR: 0.0003937, Tokens/sec: 181691.58\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 2575, Eval Loss: 2.82459\n",
      "Step: 2576, Training Loss: 3.03549, LR: 0.0003934, Tokens/sec: 182584.56\n",
      "Step: 2577, Training Loss: 2.76843, LR: 0.0003930, Tokens/sec: 181950.37\n",
      "Step: 2578, Training Loss: 2.56115, LR: 0.0003926, Tokens/sec: 183099.18\n",
      "Step: 2579, Training Loss: 2.71911, LR: 0.0003923, Tokens/sec: 183253.18\n",
      "Step: 2580, Training Loss: 2.53058, LR: 0.0003919, Tokens/sec: 183086.96\n",
      "Step: 2581, Training Loss: 2.95126, LR: 0.0003915, Tokens/sec: 182443.30\n",
      "Step: 2582, Training Loss: 2.37297, LR: 0.0003911, Tokens/sec: 182427.97\n",
      "Step: 2583, Training Loss: 2.36050, LR: 0.0003908, Tokens/sec: 181231.09\n",
      "Step: 2584, Training Loss: 3.04209, LR: 0.0003904, Tokens/sec: 183297.35\n",
      "Step: 2585, Training Loss: 2.85087, LR: 0.0003900, Tokens/sec: 182883.87\n",
      "Step: 2586, Training Loss: 2.73394, LR: 0.0003897, Tokens/sec: 183330.59\n",
      "Step: 2587, Training Loss: 2.25391, LR: 0.0003893, Tokens/sec: 182922.55\n",
      "Step: 2588, Training Loss: 2.57573, LR: 0.0003889, Tokens/sec: 182972.00\n",
      "Step: 2589, Training Loss: 2.72555, LR: 0.0003885, Tokens/sec: 181945.47\n",
      "Step: 2590, Training Loss: 2.64928, LR: 0.0003882, Tokens/sec: 182655.18\n",
      "Step: 2591, Training Loss: 2.45231, LR: 0.0003878, Tokens/sec: 183495.84\n",
      "Step: 2592, Training Loss: 2.43513, LR: 0.0003874, Tokens/sec: 183268.65\n",
      "Step: 2593, Training Loss: 2.71756, LR: 0.0003871, Tokens/sec: 182087.12\n",
      "Step: 2594, Training Loss: 2.80256, LR: 0.0003867, Tokens/sec: 174352.83\n",
      "Step: 2595, Training Loss: 2.29322, LR: 0.0003863, Tokens/sec: 180084.99\n",
      "Step: 2596, Training Loss: 2.53137, LR: 0.0003859, Tokens/sec: 181262.75\n",
      "Step: 2597, Training Loss: 2.79119, LR: 0.0003856, Tokens/sec: 182708.78\n",
      "Step: 2598, Training Loss: 2.33608, LR: 0.0003852, Tokens/sec: 181792.87\n",
      "Step: 2599, Training Loss: 2.86526, LR: 0.0003848, Tokens/sec: 182988.06\n",
      "Step: 2600, Training Loss: 2.74843, LR: 0.0003845, Tokens/sec: 182641.08\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 2600, Eval Loss: 2.65565\n",
      "Step: 2601, Training Loss: 2.34277, LR: 0.0003841, Tokens/sec: 181749.09\n",
      "Step: 2602, Training Loss: 2.55092, LR: 0.0003837, Tokens/sec: 179608.50\n",
      "Step: 2603, Training Loss: 2.66787, LR: 0.0003834, Tokens/sec: 181959.98\n",
      "Step: 2604, Training Loss: 2.45108, LR: 0.0003830, Tokens/sec: 182402.53\n",
      "Step: 2605, Training Loss: 2.56845, LR: 0.0003826, Tokens/sec: 181592.48\n",
      "Step: 2606, Training Loss: 2.43260, LR: 0.0003823, Tokens/sec: 181207.76\n",
      "Step: 2607, Training Loss: 2.39243, LR: 0.0003819, Tokens/sec: 181364.90\n",
      "Step: 2608, Training Loss: 2.04146, LR: 0.0003815, Tokens/sec: 182186.76\n",
      "Step: 2609, Training Loss: 2.57292, LR: 0.0003811, Tokens/sec: 182296.92\n",
      "Step: 2610, Training Loss: 2.34135, LR: 0.0003808, Tokens/sec: 182617.34\n",
      "Step: 2611, Training Loss: 2.51786, LR: 0.0003804, Tokens/sec: 182206.29\n",
      "Step: 2612, Training Loss: 2.57813, LR: 0.0003800, Tokens/sec: 181587.25\n",
      "Step: 2613, Training Loss: 2.97707, LR: 0.0003797, Tokens/sec: 182102.92\n",
      "Step: 2614, Training Loss: 2.41615, LR: 0.0003793, Tokens/sec: 182789.96\n",
      "Step: 2615, Training Loss: 2.68119, LR: 0.0003789, Tokens/sec: 181730.11\n",
      "Step: 2616, Training Loss: 3.01171, LR: 0.0003786, Tokens/sec: 182576.65\n",
      "Step: 2617, Training Loss: 2.66274, LR: 0.0003782, Tokens/sec: 183237.28\n",
      "Step: 2618, Training Loss: 2.54837, LR: 0.0003778, Tokens/sec: 181387.83\n",
      "Step: 2619, Training Loss: 2.72371, LR: 0.0003775, Tokens/sec: 182365.58\n",
      "Step: 2620, Training Loss: 2.47287, LR: 0.0003771, Tokens/sec: 182731.63\n",
      "Step: 2621, Training Loss: 2.66145, LR: 0.0003767, Tokens/sec: 182936.57\n",
      "Step: 2622, Training Loss: 2.47659, LR: 0.0003764, Tokens/sec: 182157.70\n",
      "Step: 2623, Training Loss: 2.93365, LR: 0.0003760, Tokens/sec: 182571.08\n",
      "Step: 2624, Training Loss: 2.65598, LR: 0.0003756, Tokens/sec: 181662.21\n",
      "Step: 2625, Training Loss: 2.54139, LR: 0.0003753, Tokens/sec: 182509.12\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 2625, Eval Loss: 2.58072\n",
      "Step: 2626, Training Loss: 2.93299, LR: 0.0003749, Tokens/sec: 181929.30\n",
      "Step: 2627, Training Loss: 2.38120, LR: 0.0003745, Tokens/sec: 182249.88\n",
      "Step: 2628, Training Loss: 2.80486, LR: 0.0003742, Tokens/sec: 182554.54\n",
      "Step: 2629, Training Loss: 2.50344, LR: 0.0003738, Tokens/sec: 182311.71\n",
      "Step: 2630, Training Loss: 2.57363, LR: 0.0003734, Tokens/sec: 182966.46\n",
      "Step: 2631, Training Loss: 2.49057, LR: 0.0003731, Tokens/sec: 182856.97\n",
      "Step: 2632, Training Loss: 2.35251, LR: 0.0003727, Tokens/sec: 181348.26\n",
      "Step: 2633, Training Loss: 2.46051, LR: 0.0003723, Tokens/sec: 182418.45\n",
      "Step: 2634, Training Loss: 2.48012, LR: 0.0003720, Tokens/sec: 183118.76\n",
      "Step: 2635, Training Loss: 2.63802, LR: 0.0003716, Tokens/sec: 182724.94\n",
      "Step: 2636, Training Loss: 2.78747, LR: 0.0003713, Tokens/sec: 183048.29\n",
      "Step: 2637, Training Loss: 3.00754, LR: 0.0003709, Tokens/sec: 182771.36\n",
      "Step: 2638, Training Loss: 2.22912, LR: 0.0003705, Tokens/sec: 181108.09\n",
      "Step: 2639, Training Loss: 2.67950, LR: 0.0003702, Tokens/sec: 181849.94\n",
      "Step: 2640, Training Loss: 2.63521, LR: 0.0003698, Tokens/sec: 182373.75\n",
      "Step: 2641, Training Loss: 2.23525, LR: 0.0003694, Tokens/sec: 180764.25\n",
      "Step: 2642, Training Loss: 2.54581, LR: 0.0003691, Tokens/sec: 182157.46\n",
      "Step: 2643, Training Loss: 2.32385, LR: 0.0003687, Tokens/sec: 181931.86\n",
      "Step: 2644, Training Loss: 2.78619, LR: 0.0003683, Tokens/sec: 181357.43\n",
      "Step: 2645, Training Loss: 2.50775, LR: 0.0003680, Tokens/sec: 182132.58\n",
      "Step: 2646, Training Loss: 2.71163, LR: 0.0003676, Tokens/sec: 182864.57\n",
      "Step: 2647, Training Loss: 2.62935, LR: 0.0003672, Tokens/sec: 182111.98\n",
      "Step: 2648, Training Loss: 2.35685, LR: 0.0003669, Tokens/sec: 183147.07\n",
      "Step: 2649, Training Loss: 3.07698, LR: 0.0003665, Tokens/sec: 182164.68\n",
      "Step: 2650, Training Loss: 2.43821, LR: 0.0003662, Tokens/sec: 180838.08\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 2650, Eval Loss: 2.64885\n",
      "Step: 2651, Training Loss: 3.14947, LR: 0.0003658, Tokens/sec: 182300.52\n",
      "Step: 2652, Training Loss: 3.02137, LR: 0.0003654, Tokens/sec: 181728.52\n",
      "Step: 2653, Training Loss: 2.84016, LR: 0.0003651, Tokens/sec: 182208.11\n",
      "Step: 2654, Training Loss: 2.50850, LR: 0.0003647, Tokens/sec: 183107.31\n",
      "Step: 2655, Training Loss: 2.61649, LR: 0.0003643, Tokens/sec: 182795.44\n",
      "Step: 2656, Training Loss: 3.01048, LR: 0.0003640, Tokens/sec: 182692.07\n",
      "Step: 2657, Training Loss: 2.47488, LR: 0.0003636, Tokens/sec: 182496.04\n",
      "Step: 2658, Training Loss: 2.37604, LR: 0.0003633, Tokens/sec: 181258.45\n",
      "Step: 2659, Training Loss: 2.45061, LR: 0.0003629, Tokens/sec: 182481.33\n",
      "Step: 2660, Training Loss: 2.79952, LR: 0.0003625, Tokens/sec: 180669.37\n",
      "Step: 2661, Training Loss: 2.27072, LR: 0.0003622, Tokens/sec: 183614.28\n",
      "Step: 2662, Training Loss: 2.65002, LR: 0.0003618, Tokens/sec: 182772.87\n",
      "Step: 2663, Training Loss: 2.46216, LR: 0.0003615, Tokens/sec: 182990.21\n",
      "Step: 2664, Training Loss: 2.72700, LR: 0.0003611, Tokens/sec: 180826.82\n",
      "Step: 2665, Training Loss: 2.75321, LR: 0.0003607, Tokens/sec: 183009.04\n",
      "Step: 2666, Training Loss: 2.38098, LR: 0.0003604, Tokens/sec: 180180.23\n",
      "Step: 2667, Training Loss: 2.67337, LR: 0.0003600, Tokens/sec: 180698.63\n",
      "Step: 2668, Training Loss: 2.90784, LR: 0.0003597, Tokens/sec: 181949.76\n",
      "Step: 2669, Training Loss: 2.33873, LR: 0.0003593, Tokens/sec: 182580.64\n",
      "Step: 2670, Training Loss: 2.48612, LR: 0.0003589, Tokens/sec: 179859.50\n",
      "Step: 2671, Training Loss: 2.63988, LR: 0.0003586, Tokens/sec: 182039.91\n",
      "Step: 2672, Training Loss: 2.78606, LR: 0.0003582, Tokens/sec: 182894.03\n",
      "Step: 2673, Training Loss: 2.15387, LR: 0.0003579, Tokens/sec: 182472.53\n",
      "Step: 2674, Training Loss: 2.60138, LR: 0.0003575, Tokens/sec: 182263.92\n",
      "Step: 2675, Training Loss: 2.01848, LR: 0.0003571, Tokens/sec: 182613.76\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 2675, Eval Loss: 2.63550\n",
      "Step: 2676, Training Loss: 2.48832, LR: 0.0003568, Tokens/sec: 181230.64\n",
      "Step: 2677, Training Loss: 2.61839, LR: 0.0003564, Tokens/sec: 182502.90\n",
      "Step: 2678, Training Loss: 2.54429, LR: 0.0003561, Tokens/sec: 181894.99\n",
      "Step: 2679, Training Loss: 2.82871, LR: 0.0003557, Tokens/sec: 182103.73\n",
      "Step: 2680, Training Loss: 2.85567, LR: 0.0003553, Tokens/sec: 182941.56\n",
      "Step: 2681, Training Loss: 2.79815, LR: 0.0003550, Tokens/sec: 180954.38\n",
      "Step: 2682, Training Loss: 2.38774, LR: 0.0003546, Tokens/sec: 182385.68\n",
      "Step: 2683, Training Loss: 2.57915, LR: 0.0003543, Tokens/sec: 182811.03\n",
      "Step: 2684, Training Loss: 2.75900, LR: 0.0003539, Tokens/sec: 182785.83\n",
      "Step: 2685, Training Loss: 2.54266, LR: 0.0003536, Tokens/sec: 182451.53\n",
      "Step: 2686, Training Loss: 2.50671, LR: 0.0003532, Tokens/sec: 182836.05\n",
      "Step: 2687, Training Loss: 2.93315, LR: 0.0003528, Tokens/sec: 181468.18\n",
      "Step: 2688, Training Loss: 2.72495, LR: 0.0003525, Tokens/sec: 183269.44\n",
      "Step: 2689, Training Loss: 2.86445, LR: 0.0003521, Tokens/sec: 183219.95\n",
      "Step: 2690, Training Loss: 2.20793, LR: 0.0003518, Tokens/sec: 182629.33\n",
      "Step: 2691, Training Loss: 2.34548, LR: 0.0003514, Tokens/sec: 182947.44\n",
      "Step: 2692, Training Loss: 2.46790, LR: 0.0003511, Tokens/sec: 182948.50\n",
      "Step: 2693, Training Loss: 3.18401, LR: 0.0003507, Tokens/sec: 181542.76\n",
      "Step: 2694, Training Loss: 2.37873, LR: 0.0003503, Tokens/sec: 182731.36\n",
      "Step: 2695, Training Loss: 2.00967, LR: 0.0003500, Tokens/sec: 182636.68\n",
      "Step: 2696, Training Loss: 2.33588, LR: 0.0003496, Tokens/sec: 182829.58\n",
      "Step: 2697, Training Loss: 2.10532, LR: 0.0003493, Tokens/sec: 182640.88\n",
      "Step: 2698, Training Loss: 2.60770, LR: 0.0003489, Tokens/sec: 183452.21\n",
      "Step: 2699, Training Loss: 2.32591, LR: 0.0003486, Tokens/sec: 181933.53\n",
      "Step: 2700, Training Loss: 2.42336, LR: 0.0003482, Tokens/sec: 182520.98\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 2700, Eval Loss: 2.40241\n",
      "Step: 2701, Training Loss: 2.23947, LR: 0.0003478, Tokens/sec: 182204.53\n",
      "Step: 2702, Training Loss: 2.49371, LR: 0.0003475, Tokens/sec: 182426.71\n",
      "Step: 2703, Training Loss: 2.83491, LR: 0.0003471, Tokens/sec: 182552.03\n",
      "Step: 2704, Training Loss: 2.53059, LR: 0.0003468, Tokens/sec: 182894.42\n",
      "Step: 2705, Training Loss: 2.33309, LR: 0.0003464, Tokens/sec: 182361.85\n",
      "Step: 2706, Training Loss: 2.50461, LR: 0.0003461, Tokens/sec: 182764.07\n",
      "Step: 2707, Training Loss: 2.20798, LR: 0.0003457, Tokens/sec: 180735.94\n",
      "Step: 2708, Training Loss: 2.71572, LR: 0.0003454, Tokens/sec: 182679.99\n",
      "Step: 2709, Training Loss: 2.48304, LR: 0.0003450, Tokens/sec: 182863.84\n",
      "Step: 2710, Training Loss: 2.35709, LR: 0.0003447, Tokens/sec: 182936.92\n",
      "Step: 2711, Training Loss: 2.18207, LR: 0.0003443, Tokens/sec: 183193.01\n",
      "Step: 2712, Training Loss: 2.73074, LR: 0.0003440, Tokens/sec: 182707.04\n",
      "Step: 2713, Training Loss: 2.03088, LR: 0.0003436, Tokens/sec: 181340.07\n",
      "Step: 2714, Training Loss: 2.11440, LR: 0.0003432, Tokens/sec: 182096.48\n",
      "Step: 2715, Training Loss: 2.52696, LR: 0.0003429, Tokens/sec: 182766.30\n",
      "Step: 2716, Training Loss: 2.54705, LR: 0.0003425, Tokens/sec: 176960.82\n",
      "Step: 2717, Training Loss: 2.58947, LR: 0.0003422, Tokens/sec: 182711.22\n",
      "Step: 2718, Training Loss: 2.72082, LR: 0.0003418, Tokens/sec: 182612.25\n",
      "Step: 2719, Training Loss: 2.42925, LR: 0.0003415, Tokens/sec: 181031.19\n",
      "Step: 2720, Training Loss: 2.26765, LR: 0.0003411, Tokens/sec: 182630.53\n",
      "Step: 2721, Training Loss: 2.01352, LR: 0.0003408, Tokens/sec: 182482.22\n",
      "Step: 2722, Training Loss: 2.25340, LR: 0.0003404, Tokens/sec: 182339.61\n",
      "Step: 2723, Training Loss: 1.98497, LR: 0.0003401, Tokens/sec: 182565.19\n",
      "Step: 2724, Training Loss: 2.45778, LR: 0.0003397, Tokens/sec: 182972.10\n",
      "Step: 2725, Training Loss: 2.04880, LR: 0.0003394, Tokens/sec: 181340.21\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 2725, Eval Loss: 2.52618\n",
      "Step: 2726, Training Loss: 2.40354, LR: 0.0003390, Tokens/sec: 181311.40\n",
      "Step: 2727, Training Loss: 2.51222, LR: 0.0003387, Tokens/sec: 181140.21\n",
      "Step: 2728, Training Loss: 2.30335, LR: 0.0003383, Tokens/sec: 182388.27\n",
      "Step: 2729, Training Loss: 2.50748, LR: 0.0003380, Tokens/sec: 182725.52\n",
      "Step: 2730, Training Loss: 2.17885, LR: 0.0003376, Tokens/sec: 182664.76\n",
      "Step: 2731, Training Loss: 2.37862, LR: 0.0003373, Tokens/sec: 182628.28\n",
      "Step: 2732, Training Loss: 2.67245, LR: 0.0003369, Tokens/sec: 182829.50\n",
      "Step: 2733, Training Loss: 2.63062, LR: 0.0003366, Tokens/sec: 181091.25\n",
      "Step: 2734, Training Loss: 2.99316, LR: 0.0003362, Tokens/sec: 182553.93\n",
      "Step: 2735, Training Loss: 2.69029, LR: 0.0003359, Tokens/sec: 182773.40\n",
      "Step: 2736, Training Loss: 2.28479, LR: 0.0003355, Tokens/sec: 182328.99\n",
      "Step: 2737, Training Loss: 1.95615, LR: 0.0003352, Tokens/sec: 182078.44\n",
      "Step: 2738, Training Loss: 2.44332, LR: 0.0003348, Tokens/sec: 182811.13\n",
      "Step: 2739, Training Loss: 2.58646, LR: 0.0003345, Tokens/sec: 181077.80\n",
      "Step: 2740, Training Loss: 2.79897, LR: 0.0003341, Tokens/sec: 182935.24\n",
      "Step: 2741, Training Loss: 2.36296, LR: 0.0003338, Tokens/sec: 182156.58\n",
      "Step: 2742, Training Loss: 2.75261, LR: 0.0003334, Tokens/sec: 182763.71\n",
      "Step: 2743, Training Loss: 2.34191, LR: 0.0003331, Tokens/sec: 181998.60\n",
      "Step: 2744, Training Loss: 2.51676, LR: 0.0003327, Tokens/sec: 182721.16\n",
      "Step: 2745, Training Loss: 2.44147, LR: 0.0003324, Tokens/sec: 181184.50\n",
      "Step: 2746, Training Loss: 3.21470, LR: 0.0003320, Tokens/sec: 182781.88\n",
      "Step: 2747, Training Loss: 2.72774, LR: 0.0003317, Tokens/sec: 182898.88\n",
      "Step: 2748, Training Loss: 2.63442, LR: 0.0003313, Tokens/sec: 182241.79\n",
      "Step: 2749, Training Loss: 2.66838, LR: 0.0003310, Tokens/sec: 183082.33\n",
      "Step: 2750, Training Loss: 2.06892, LR: 0.0003306, Tokens/sec: 182896.59\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 2750, Eval Loss: 2.52295\n",
      "Step: 2751, Training Loss: 2.59902, LR: 0.0003303, Tokens/sec: 178620.85\n",
      "Step: 2752, Training Loss: 2.45589, LR: 0.0003300, Tokens/sec: 181921.24\n",
      "Step: 2753, Training Loss: 2.24035, LR: 0.0003296, Tokens/sec: 181741.30\n",
      "Step: 2754, Training Loss: 2.12485, LR: 0.0003293, Tokens/sec: 181820.37\n",
      "Step: 2755, Training Loss: 2.23662, LR: 0.0003289, Tokens/sec: 182061.70\n",
      "Step: 2756, Training Loss: 2.29604, LR: 0.0003286, Tokens/sec: 180747.99\n",
      "Step: 2757, Training Loss: 2.41633, LR: 0.0003282, Tokens/sec: 182014.80\n",
      "Step: 2758, Training Loss: 2.32637, LR: 0.0003279, Tokens/sec: 182356.49\n",
      "Step: 2759, Training Loss: 2.92465, LR: 0.0003275, Tokens/sec: 182641.14\n",
      "Step: 2760, Training Loss: 2.26517, LR: 0.0003272, Tokens/sec: 182318.29\n",
      "Step: 2761, Training Loss: 2.39630, LR: 0.0003268, Tokens/sec: 182160.64\n",
      "Step: 2762, Training Loss: 2.23939, LR: 0.0003265, Tokens/sec: 180947.24\n",
      "Step: 2763, Training Loss: 2.37902, LR: 0.0003261, Tokens/sec: 182394.41\n",
      "Step: 2764, Training Loss: 2.65295, LR: 0.0003258, Tokens/sec: 182838.60\n",
      "Step: 2765, Training Loss: 2.55230, LR: 0.0003255, Tokens/sec: 182286.07\n",
      "Step: 2766, Training Loss: 2.43964, LR: 0.0003251, Tokens/sec: 182247.10\n",
      "Step: 2767, Training Loss: 1.93969, LR: 0.0003248, Tokens/sec: 182121.39\n",
      "Step: 2768, Training Loss: 2.48335, LR: 0.0003244, Tokens/sec: 181545.78\n",
      "Step: 2769, Training Loss: 2.34798, LR: 0.0003241, Tokens/sec: 182318.39\n",
      "Step: 2770, Training Loss: 2.26949, LR: 0.0003237, Tokens/sec: 182994.19\n",
      "Step: 2771, Training Loss: 2.52787, LR: 0.0003234, Tokens/sec: 182239.74\n",
      "Step: 2772, Training Loss: 2.30516, LR: 0.0003231, Tokens/sec: 182290.14\n",
      "Step: 2773, Training Loss: 2.56881, LR: 0.0003227, Tokens/sec: 183157.53\n",
      "Step: 2774, Training Loss: 2.51159, LR: 0.0003224, Tokens/sec: 181824.99\n",
      "Step: 2775, Training Loss: 1.98726, LR: 0.0003220, Tokens/sec: 182398.09\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 2775, Eval Loss: 2.54343\n",
      "Step: 2776, Training Loss: 2.14153, LR: 0.0003217, Tokens/sec: 181071.36\n",
      "Step: 2777, Training Loss: 2.15392, LR: 0.0003213, Tokens/sec: 181331.45\n",
      "Step: 2778, Training Loss: 1.94092, LR: 0.0003210, Tokens/sec: 182305.37\n",
      "Step: 2779, Training Loss: 2.13888, LR: 0.0003207, Tokens/sec: 182332.10\n",
      "Step: 2780, Training Loss: 2.91355, LR: 0.0003203, Tokens/sec: 182872.98\n",
      "Step: 2781, Training Loss: 2.37450, LR: 0.0003200, Tokens/sec: 181675.12\n",
      "Step: 2782, Training Loss: 2.27619, LR: 0.0003196, Tokens/sec: 181023.07\n",
      "Step: 2783, Training Loss: 2.91247, LR: 0.0003193, Tokens/sec: 182333.38\n",
      "Step: 2784, Training Loss: 2.25613, LR: 0.0003189, Tokens/sec: 182513.49\n",
      "Step: 2785, Training Loss: 2.49122, LR: 0.0003186, Tokens/sec: 181843.77\n",
      "Step: 2786, Training Loss: 2.05653, LR: 0.0003183, Tokens/sec: 182155.33\n",
      "Step: 2787, Training Loss: 2.43172, LR: 0.0003179, Tokens/sec: 182002.58\n",
      "Step: 2788, Training Loss: 2.15978, LR: 0.0003176, Tokens/sec: 181462.29\n",
      "Step: 2789, Training Loss: 2.92908, LR: 0.0003172, Tokens/sec: 181925.85\n",
      "Step: 2790, Training Loss: 2.42796, LR: 0.0003169, Tokens/sec: 181256.73\n",
      "Step: 2791, Training Loss: 2.18754, LR: 0.0003166, Tokens/sec: 181600.03\n",
      "Step: 2792, Training Loss: 2.52723, LR: 0.0003162, Tokens/sec: 182753.72\n",
      "Step: 2793, Training Loss: 2.38184, LR: 0.0003159, Tokens/sec: 181978.31\n",
      "Step: 2794, Training Loss: 2.05023, LR: 0.0003155, Tokens/sec: 181369.83\n",
      "Step: 2795, Training Loss: 2.10121, LR: 0.0003152, Tokens/sec: 182483.19\n",
      "Step: 2796, Training Loss: 2.22970, LR: 0.0003149, Tokens/sec: 182754.77\n",
      "Step: 2797, Training Loss: 2.60853, LR: 0.0003145, Tokens/sec: 182598.39\n",
      "Step: 2798, Training Loss: 2.38902, LR: 0.0003142, Tokens/sec: 182378.12\n",
      "Step: 2799, Training Loss: 2.24215, LR: 0.0003139, Tokens/sec: 182228.77\n",
      "Step: 2800, Training Loss: 2.43615, LR: 0.0003135, Tokens/sec: 180982.49\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 2800, Eval Loss: 2.41437\n",
      "Step: 2801, Training Loss: 2.79819, LR: 0.0003132, Tokens/sec: 181947.49\n",
      "Step: 2802, Training Loss: 2.21476, LR: 0.0003128, Tokens/sec: 182278.96\n",
      "Step: 2803, Training Loss: 2.46164, LR: 0.0003125, Tokens/sec: 181971.96\n",
      "Step: 2804, Training Loss: 2.24902, LR: 0.0003122, Tokens/sec: 182902.84\n",
      "Step: 2805, Training Loss: 2.34955, LR: 0.0003118, Tokens/sec: 182106.14\n",
      "Step: 2806, Training Loss: 2.46141, LR: 0.0003115, Tokens/sec: 182685.28\n",
      "Step: 2807, Training Loss: 2.68268, LR: 0.0003112, Tokens/sec: 182113.72\n",
      "Step: 2808, Training Loss: 2.30049, LR: 0.0003108, Tokens/sec: 181502.18\n",
      "Step: 2809, Training Loss: 2.44065, LR: 0.0003105, Tokens/sec: 182419.99\n",
      "Step: 2810, Training Loss: 2.18055, LR: 0.0003101, Tokens/sec: 182375.57\n",
      "Step: 2811, Training Loss: 2.40213, LR: 0.0003098, Tokens/sec: 182380.12\n",
      "Step: 2812, Training Loss: 2.09941, LR: 0.0003095, Tokens/sec: 182678.63\n",
      "Step: 2813, Training Loss: 2.62719, LR: 0.0003091, Tokens/sec: 182314.57\n",
      "Step: 2814, Training Loss: 2.45347, LR: 0.0003088, Tokens/sec: 181710.99\n",
      "Step: 2815, Training Loss: 2.38286, LR: 0.0003085, Tokens/sec: 182445.18\n",
      "Step: 2816, Training Loss: 1.81185, LR: 0.0003081, Tokens/sec: 182534.75\n",
      "Step: 2817, Training Loss: 2.74123, LR: 0.0003078, Tokens/sec: 182614.28\n",
      "Step: 2818, Training Loss: 2.44749, LR: 0.0003075, Tokens/sec: 182433.99\n",
      "Step: 2819, Training Loss: 2.39600, LR: 0.0003071, Tokens/sec: 182145.91\n",
      "Step: 2820, Training Loss: 2.26654, LR: 0.0003068, Tokens/sec: 180965.49\n",
      "Step: 2821, Training Loss: 2.49313, LR: 0.0003065, Tokens/sec: 182003.00\n",
      "Step: 2822, Training Loss: 2.57466, LR: 0.0003061, Tokens/sec: 182420.33\n",
      "Step: 2823, Training Loss: 2.92510, LR: 0.0003058, Tokens/sec: 182532.42\n",
      "Step: 2824, Training Loss: 2.12723, LR: 0.0003055, Tokens/sec: 182879.49\n",
      "Step: 2825, Training Loss: 2.48590, LR: 0.0003051, Tokens/sec: 182170.47\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 2825, Eval Loss: 2.44109\n",
      "Step: 2826, Training Loss: 2.25930, LR: 0.0003048, Tokens/sec: 181694.75\n",
      "Step: 2827, Training Loss: 2.56756, LR: 0.0003045, Tokens/sec: 182372.48\n",
      "Step: 2828, Training Loss: 2.32824, LR: 0.0003041, Tokens/sec: 182357.27\n",
      "Step: 2829, Training Loss: 2.54865, LR: 0.0003038, Tokens/sec: 182205.67\n",
      "Step: 2830, Training Loss: 2.45037, LR: 0.0003035, Tokens/sec: 182829.99\n",
      "Step: 2831, Training Loss: 2.71029, LR: 0.0003031, Tokens/sec: 180771.76\n",
      "Step: 2832, Training Loss: 2.38097, LR: 0.0003028, Tokens/sec: 182481.82\n",
      "Step: 2833, Training Loss: 2.34882, LR: 0.0003025, Tokens/sec: 182859.90\n",
      "Step: 2834, Training Loss: 2.12621, LR: 0.0003021, Tokens/sec: 182939.86\n",
      "Step: 2835, Training Loss: 2.53475, LR: 0.0003018, Tokens/sec: 182486.38\n",
      "Step: 2836, Training Loss: 2.39091, LR: 0.0003015, Tokens/sec: 182475.35\n",
      "Step: 2837, Training Loss: 2.22176, LR: 0.0003011, Tokens/sec: 181716.54\n",
      "Step: 2838, Training Loss: 2.59385, LR: 0.0003008, Tokens/sec: 182591.25\n",
      "Step: 2839, Training Loss: 2.29301, LR: 0.0003005, Tokens/sec: 182778.55\n",
      "Step: 2840, Training Loss: 2.61636, LR: 0.0003001, Tokens/sec: 182638.53\n",
      "Step: 2841, Training Loss: 2.55365, LR: 0.0002998, Tokens/sec: 183125.97\n",
      "Step: 2842, Training Loss: 2.30771, LR: 0.0002995, Tokens/sec: 182776.15\n",
      "Step: 2843, Training Loss: 2.43993, LR: 0.0002991, Tokens/sec: 181043.09\n",
      "Step: 2844, Training Loss: 2.19382, LR: 0.0002988, Tokens/sec: 182347.20\n",
      "Step: 2845, Training Loss: 2.07883, LR: 0.0002985, Tokens/sec: 182860.98\n",
      "Step: 2846, Training Loss: 2.03872, LR: 0.0002982, Tokens/sec: 183301.81\n",
      "Step: 2847, Training Loss: 2.53457, LR: 0.0002978, Tokens/sec: 182900.02\n",
      "Step: 2848, Training Loss: 2.50447, LR: 0.0002975, Tokens/sec: 182768.75\n",
      "Step: 2849, Training Loss: 2.60103, LR: 0.0002972, Tokens/sec: 181435.34\n",
      "Step: 2850, Training Loss: 2.95477, LR: 0.0002968, Tokens/sec: 182859.01\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 2850, Eval Loss: 2.34677\n",
      "Step: 2851, Training Loss: 2.22120, LR: 0.0002965, Tokens/sec: 182443.89\n",
      "Step: 2852, Training Loss: 2.64948, LR: 0.0002962, Tokens/sec: 182326.16\n",
      "Step: 2853, Training Loss: 2.28384, LR: 0.0002959, Tokens/sec: 182291.55\n",
      "Step: 2854, Training Loss: 2.35702, LR: 0.0002955, Tokens/sec: 182469.60\n",
      "Step: 2855, Training Loss: 2.14296, LR: 0.0002952, Tokens/sec: 182748.50\n",
      "Step: 2856, Training Loss: 1.67718, LR: 0.0002949, Tokens/sec: 182516.82\n",
      "Step: 2857, Training Loss: 2.40966, LR: 0.0002945, Tokens/sec: 181586.32\n",
      "Step: 2858, Training Loss: 2.48564, LR: 0.0002942, Tokens/sec: 183047.34\n",
      "Step: 2859, Training Loss: 2.11131, LR: 0.0002939, Tokens/sec: 182189.11\n",
      "Step: 2860, Training Loss: 2.16433, LR: 0.0002936, Tokens/sec: 182790.71\n",
      "Step: 2861, Training Loss: 2.54043, LR: 0.0002932, Tokens/sec: 182760.01\n",
      "Step: 2862, Training Loss: 2.28981, LR: 0.0002929, Tokens/sec: 182780.94\n",
      "Step: 2863, Training Loss: 2.23336, LR: 0.0002926, Tokens/sec: 181310.20\n",
      "Step: 2864, Training Loss: 2.39388, LR: 0.0002923, Tokens/sec: 182631.47\n",
      "Step: 2865, Training Loss: 2.57706, LR: 0.0002919, Tokens/sec: 182638.65\n",
      "Step: 2866, Training Loss: 2.42963, LR: 0.0002916, Tokens/sec: 182787.58\n",
      "Step: 2867, Training Loss: 2.27598, LR: 0.0002913, Tokens/sec: 182714.49\n",
      "Step: 2868, Training Loss: 2.26447, LR: 0.0002910, Tokens/sec: 182852.66\n",
      "Step: 2869, Training Loss: 2.35265, LR: 0.0002906, Tokens/sec: 181033.78\n",
      "Step: 2870, Training Loss: 2.45774, LR: 0.0002903, Tokens/sec: 182392.79\n",
      "Step: 2871, Training Loss: 2.34531, LR: 0.0002900, Tokens/sec: 182390.88\n",
      "Step: 2872, Training Loss: 2.17531, LR: 0.0002897, Tokens/sec: 182459.88\n",
      "Step: 2873, Training Loss: 2.46908, LR: 0.0002893, Tokens/sec: 182791.46\n",
      "Step: 2874, Training Loss: 2.62159, LR: 0.0002890, Tokens/sec: 182952.78\n",
      "Step: 2875, Training Loss: 2.16361, LR: 0.0002887, Tokens/sec: 181202.01\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 2875, Eval Loss: 2.29010\n",
      "Step: 2876, Training Loss: 1.87624, LR: 0.0002884, Tokens/sec: 182477.58\n",
      "Step: 2877, Training Loss: 2.34517, LR: 0.0002880, Tokens/sec: 182278.04\n",
      "Step: 2878, Training Loss: 2.33888, LR: 0.0002877, Tokens/sec: 181677.08\n",
      "Step: 2879, Training Loss: 2.26600, LR: 0.0002874, Tokens/sec: 182641.93\n",
      "Step: 2880, Training Loss: 2.56152, LR: 0.0002871, Tokens/sec: 182346.76\n",
      "Step: 2881, Training Loss: 2.11890, LR: 0.0002868, Tokens/sec: 182026.01\n",
      "Step: 2882, Training Loss: 2.32918, LR: 0.0002864, Tokens/sec: 182708.28\n",
      "Step: 2883, Training Loss: 2.12829, LR: 0.0002861, Tokens/sec: 180506.21\n",
      "Step: 2884, Training Loss: 2.28549, LR: 0.0002858, Tokens/sec: 182203.83\n",
      "Step: 2885, Training Loss: 2.33793, LR: 0.0002855, Tokens/sec: 182619.46\n",
      "Step: 2886, Training Loss: 2.33186, LR: 0.0002851, Tokens/sec: 182569.79\n",
      "Step: 2887, Training Loss: 1.74001, LR: 0.0002848, Tokens/sec: 182645.54\n",
      "Step: 2888, Training Loss: 2.41770, LR: 0.0002845, Tokens/sec: 182906.15\n",
      "Step: 2889, Training Loss: 2.59871, LR: 0.0002842, Tokens/sec: 182326.73\n",
      "Step: 2890, Training Loss: 2.54444, LR: 0.0002839, Tokens/sec: 182244.67\n",
      "Step: 2891, Training Loss: 1.99072, LR: 0.0002835, Tokens/sec: 182783.35\n",
      "Step: 2892, Training Loss: 2.38908, LR: 0.0002832, Tokens/sec: 181221.70\n",
      "Step: 2893, Training Loss: 2.23761, LR: 0.0002829, Tokens/sec: 181576.23\n",
      "Step: 2894, Training Loss: 2.47783, LR: 0.0002826, Tokens/sec: 182171.33\n",
      "Step: 2895, Training Loss: 2.28038, LR: 0.0002823, Tokens/sec: 180228.92\n",
      "Step: 2896, Training Loss: 2.37624, LR: 0.0002819, Tokens/sec: 182801.95\n",
      "Step: 2897, Training Loss: 2.61593, LR: 0.0002816, Tokens/sec: 182817.07\n",
      "Step: 2898, Training Loss: 2.27377, LR: 0.0002813, Tokens/sec: 182633.62\n",
      "Step: 2899, Training Loss: 2.40616, LR: 0.0002810, Tokens/sec: 182354.73\n",
      "Step: 2900, Training Loss: 2.22892, LR: 0.0002807, Tokens/sec: 182610.70\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 2900, Eval Loss: 2.34228\n",
      "Step: 2901, Training Loss: 1.94872, LR: 0.0002803, Tokens/sec: 181901.13\n",
      "Step: 2902, Training Loss: 2.28555, LR: 0.0002800, Tokens/sec: 182814.32\n",
      "Step: 2903, Training Loss: 2.14841, LR: 0.0002797, Tokens/sec: 182411.12\n",
      "Step: 2904, Training Loss: 2.07797, LR: 0.0002794, Tokens/sec: 182722.45\n",
      "Step: 2905, Training Loss: 2.48095, LR: 0.0002791, Tokens/sec: 182847.43\n",
      "Step: 2906, Training Loss: 1.83984, LR: 0.0002788, Tokens/sec: 181280.88\n",
      "Step: 2907, Training Loss: 2.39334, LR: 0.0002784, Tokens/sec: 182255.09\n",
      "Step: 2908, Training Loss: 2.30876, LR: 0.0002781, Tokens/sec: 182789.90\n",
      "Step: 2909, Training Loss: 2.32256, LR: 0.0002778, Tokens/sec: 181401.12\n",
      "Step: 2910, Training Loss: 1.95980, LR: 0.0002775, Tokens/sec: 182420.03\n",
      "Step: 2911, Training Loss: 2.23773, LR: 0.0002772, Tokens/sec: 182088.37\n",
      "Step: 2912, Training Loss: 2.47162, LR: 0.0002769, Tokens/sec: 181067.54\n",
      "Step: 2913, Training Loss: 2.01897, LR: 0.0002765, Tokens/sec: 179397.56\n",
      "Step: 2914, Training Loss: 2.21190, LR: 0.0002762, Tokens/sec: 181225.11\n",
      "Step: 2915, Training Loss: 2.44012, LR: 0.0002759, Tokens/sec: 182042.95\n",
      "Step: 2916, Training Loss: 2.29866, LR: 0.0002756, Tokens/sec: 181665.16\n",
      "Step: 2917, Training Loss: 1.95470, LR: 0.0002753, Tokens/sec: 182375.25\n",
      "Step: 2918, Training Loss: 2.23218, LR: 0.0002750, Tokens/sec: 180709.25\n",
      "Step: 2919, Training Loss: 2.44182, LR: 0.0002747, Tokens/sec: 182515.57\n",
      "Step: 2920, Training Loss: 2.13698, LR: 0.0002743, Tokens/sec: 182324.15\n",
      "Step: 2921, Training Loss: 2.49339, LR: 0.0002740, Tokens/sec: 182296.83\n",
      "Step: 2922, Training Loss: 2.35652, LR: 0.0002737, Tokens/sec: 182392.95\n",
      "Step: 2923, Training Loss: 2.14740, LR: 0.0002734, Tokens/sec: 182541.66\n",
      "Step: 2924, Training Loss: 2.67107, LR: 0.0002731, Tokens/sec: 181048.67\n",
      "Step: 2925, Training Loss: 2.51028, LR: 0.0002728, Tokens/sec: 182480.13\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 2925, Eval Loss: 2.41875\n",
      "Step: 2926, Training Loss: 1.74816, LR: 0.0002725, Tokens/sec: 181385.58\n",
      "Step: 2927, Training Loss: 1.97189, LR: 0.0002721, Tokens/sec: 182303.69\n",
      "Step: 2928, Training Loss: 2.51496, LR: 0.0002718, Tokens/sec: 182534.97\n",
      "Step: 2929, Training Loss: 2.72850, LR: 0.0002715, Tokens/sec: 182226.17\n",
      "Step: 2930, Training Loss: 2.04915, LR: 0.0002712, Tokens/sec: 181861.42\n",
      "Step: 2931, Training Loss: 1.90544, LR: 0.0002709, Tokens/sec: 182475.70\n",
      "Step: 2932, Training Loss: 2.19562, LR: 0.0002706, Tokens/sec: 181367.12\n",
      "Step: 2933, Training Loss: 2.16577, LR: 0.0002703, Tokens/sec: 182456.74\n",
      "Step: 2934, Training Loss: 2.05856, LR: 0.0002700, Tokens/sec: 182527.10\n",
      "Step: 2935, Training Loss: 3.44804, LR: 0.0002697, Tokens/sec: 182204.90\n",
      "Step: 2936, Training Loss: 2.79371, LR: 0.0002693, Tokens/sec: 182822.56\n",
      "Step: 2937, Training Loss: 2.17645, LR: 0.0002690, Tokens/sec: 182403.05\n",
      "Step: 2938, Training Loss: 2.10577, LR: 0.0002687, Tokens/sec: 181988.47\n",
      "Step: 2939, Training Loss: 2.21038, LR: 0.0002684, Tokens/sec: 183091.21\n",
      "Step: 2940, Training Loss: 1.86456, LR: 0.0002681, Tokens/sec: 182543.84\n",
      "Step: 2941, Training Loss: 2.22699, LR: 0.0002678, Tokens/sec: 182703.39\n",
      "Step: 2942, Training Loss: 2.15120, LR: 0.0002675, Tokens/sec: 182353.56\n",
      "Step: 2943, Training Loss: 2.30399, LR: 0.0002672, Tokens/sec: 182556.24\n",
      "Step: 2944, Training Loss: 2.37001, LR: 0.0002669, Tokens/sec: 181282.01\n",
      "Step: 2945, Training Loss: 2.40929, LR: 0.0002666, Tokens/sec: 182519.31\n",
      "Step: 2946, Training Loss: 2.61172, LR: 0.0002662, Tokens/sec: 183047.76\n",
      "Step: 2947, Training Loss: 2.13337, LR: 0.0002659, Tokens/sec: 182177.30\n",
      "Step: 2948, Training Loss: 2.76845, LR: 0.0002656, Tokens/sec: 182459.63\n",
      "Step: 2949, Training Loss: 2.02826, LR: 0.0002653, Tokens/sec: 182625.25\n",
      "Step: 2950, Training Loss: 1.83466, LR: 0.0002650, Tokens/sec: 181388.49\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 2950, Eval Loss: 2.23120\n",
      "Step: 2951, Training Loss: 1.88101, LR: 0.0002647, Tokens/sec: 182155.91\n",
      "Step: 2952, Training Loss: 1.78216, LR: 0.0002644, Tokens/sec: 182345.55\n",
      "Step: 2953, Training Loss: 2.22282, LR: 0.0002641, Tokens/sec: 182537.21\n",
      "Step: 2954, Training Loss: 1.62845, LR: 0.0002638, Tokens/sec: 182471.37\n",
      "Step: 2955, Training Loss: 1.69362, LR: 0.0002635, Tokens/sec: 182728.28\n",
      "Step: 2956, Training Loss: 1.74068, LR: 0.0002632, Tokens/sec: 182725.09\n",
      "Step: 2957, Training Loss: 2.68755, LR: 0.0002629, Tokens/sec: 183449.42\n",
      "Step: 2958, Training Loss: 1.86610, LR: 0.0002626, Tokens/sec: 181400.13\n",
      "Step: 2959, Training Loss: 2.01884, LR: 0.0002623, Tokens/sec: 182436.14\n",
      "Step: 2960, Training Loss: 2.31021, LR: 0.0002619, Tokens/sec: 182719.47\n",
      "Step: 2961, Training Loss: 2.76728, LR: 0.0002616, Tokens/sec: 182325.21\n",
      "Step: 2962, Training Loss: 2.09299, LR: 0.0002613, Tokens/sec: 182692.99\n",
      "Step: 2963, Training Loss: 2.03875, LR: 0.0002610, Tokens/sec: 176937.53\n",
      "Step: 2964, Training Loss: 2.07206, LR: 0.0002607, Tokens/sec: 182245.08\n",
      "Step: 2965, Training Loss: 2.01597, LR: 0.0002604, Tokens/sec: 182163.82\n",
      "Step: 2966, Training Loss: 1.93544, LR: 0.0002601, Tokens/sec: 182133.90\n",
      "Step: 2967, Training Loss: 2.38385, LR: 0.0002598, Tokens/sec: 181768.52\n",
      "Step: 2968, Training Loss: 2.38382, LR: 0.0002595, Tokens/sec: 182242.40\n",
      "Step: 2969, Training Loss: 2.34984, LR: 0.0002592, Tokens/sec: 182584.73\n",
      "Step: 2970, Training Loss: 1.88685, LR: 0.0002589, Tokens/sec: 181605.93\n",
      "Step: 2971, Training Loss: 2.00835, LR: 0.0002586, Tokens/sec: 182475.33\n",
      "Step: 2972, Training Loss: 2.23503, LR: 0.0002583, Tokens/sec: 183121.86\n",
      "Step: 2973, Training Loss: 2.44958, LR: 0.0002580, Tokens/sec: 182416.01\n",
      "Step: 2974, Training Loss: 2.05789, LR: 0.0002577, Tokens/sec: 182815.69\n",
      "Step: 2975, Training Loss: 1.98331, LR: 0.0002574, Tokens/sec: 182889.44\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 2975, Eval Loss: 2.14611\n",
      "Step: 2976, Training Loss: 2.49840, LR: 0.0002571, Tokens/sec: 181688.22\n",
      "Step: 2977, Training Loss: 2.08225, LR: 0.0002568, Tokens/sec: 182588.41\n",
      "Step: 2978, Training Loss: 1.50175, LR: 0.0002565, Tokens/sec: 182635.18\n",
      "Step: 2979, Training Loss: 1.97374, LR: 0.0002562, Tokens/sec: 181935.97\n",
      "Step: 2980, Training Loss: 2.22418, LR: 0.0002559, Tokens/sec: 182865.37\n",
      "Step: 2981, Training Loss: 2.31640, LR: 0.0002556, Tokens/sec: 181177.15\n",
      "Step: 2982, Training Loss: 2.26918, LR: 0.0002553, Tokens/sec: 182312.29\n",
      "Step: 2983, Training Loss: 1.97403, LR: 0.0002550, Tokens/sec: 182594.18\n",
      "Step: 2984, Training Loss: 2.07029, LR: 0.0002547, Tokens/sec: 182304.60\n",
      "Step: 2985, Training Loss: 2.35307, LR: 0.0002544, Tokens/sec: 182466.69\n",
      "Step: 2986, Training Loss: 2.04630, LR: 0.0002541, Tokens/sec: 182737.36\n",
      "Step: 2987, Training Loss: 2.33980, LR: 0.0002538, Tokens/sec: 181822.68\n",
      "Step: 2988, Training Loss: 2.16824, LR: 0.0002535, Tokens/sec: 182795.43\n",
      "Step: 2989, Training Loss: 2.48136, LR: 0.0002532, Tokens/sec: 182652.79\n",
      "Step: 2990, Training Loss: 2.14988, LR: 0.0002529, Tokens/sec: 182758.51\n",
      "Step: 2991, Training Loss: 2.01069, LR: 0.0002526, Tokens/sec: 182611.34\n",
      "Step: 2992, Training Loss: 1.53578, LR: 0.0002523, Tokens/sec: 183085.13\n",
      "Step: 2993, Training Loss: 1.88183, LR: 0.0002520, Tokens/sec: 181646.60\n",
      "Step: 2994, Training Loss: 1.89677, LR: 0.0002517, Tokens/sec: 182619.93\n",
      "Step: 2995, Training Loss: 2.63532, LR: 0.0002514, Tokens/sec: 183438.47\n",
      "Step: 2996, Training Loss: 2.07395, LR: 0.0002511, Tokens/sec: 182753.35\n",
      "Step: 2997, Training Loss: 1.85689, LR: 0.0002508, Tokens/sec: 182867.99\n",
      "Step: 2998, Training Loss: 2.39389, LR: 0.0002505, Tokens/sec: 182817.90\n",
      "Step: 2999, Training Loss: 2.41876, LR: 0.0002502, Tokens/sec: 181482.20\n",
      "Step: 3000, Training Loss: 1.52961, LR: 0.0002499, Tokens/sec: 183000.12\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 3000, Eval Loss: 2.28668\n",
      "Step: 3001, Training Loss: 2.50763, LR: 0.0002496, Tokens/sec: 176703.64\n",
      "Step: 3002, Training Loss: 2.24273, LR: 0.0002493, Tokens/sec: 179982.36\n",
      "Step: 3003, Training Loss: 1.86506, LR: 0.0002490, Tokens/sec: 181230.63\n",
      "Step: 3004, Training Loss: 2.67284, LR: 0.0002487, Tokens/sec: 181965.38\n",
      "Step: 3005, Training Loss: 1.89670, LR: 0.0002484, Tokens/sec: 181815.93\n",
      "Step: 3006, Training Loss: 2.25329, LR: 0.0002481, Tokens/sec: 181892.51\n",
      "Step: 3007, Training Loss: 2.04568, LR: 0.0002478, Tokens/sec: 180977.05\n",
      "Step: 3008, Training Loss: 1.74905, LR: 0.0002476, Tokens/sec: 182643.91\n",
      "Step: 3009, Training Loss: 2.01550, LR: 0.0002473, Tokens/sec: 182278.49\n",
      "Step: 3010, Training Loss: 1.65435, LR: 0.0002470, Tokens/sec: 182596.03\n",
      "Step: 3011, Training Loss: 1.59377, LR: 0.0002467, Tokens/sec: 182092.51\n",
      "Step: 3012, Training Loss: 2.19400, LR: 0.0002464, Tokens/sec: 181516.20\n",
      "Step: 3013, Training Loss: 2.00691, LR: 0.0002461, Tokens/sec: 180861.92\n",
      "Step: 3014, Training Loss: 2.35581, LR: 0.0002458, Tokens/sec: 182474.52\n",
      "Step: 3015, Training Loss: 2.01459, LR: 0.0002455, Tokens/sec: 182468.46\n",
      "Step: 3016, Training Loss: 1.63610, LR: 0.0002452, Tokens/sec: 182826.10\n",
      "Step: 3017, Training Loss: 2.04940, LR: 0.0002449, Tokens/sec: 182397.90\n",
      "Step: 3018, Training Loss: 2.41591, LR: 0.0002446, Tokens/sec: 181926.97\n",
      "Step: 3019, Training Loss: 2.49173, LR: 0.0002443, Tokens/sec: 181692.45\n",
      "Step: 3020, Training Loss: 2.28549, LR: 0.0002440, Tokens/sec: 181962.11\n",
      "Step: 3021, Training Loss: 1.55391, LR: 0.0002437, Tokens/sec: 182864.65\n",
      "Step: 3022, Training Loss: 2.03156, LR: 0.0002435, Tokens/sec: 182438.46\n",
      "Step: 3023, Training Loss: 1.81169, LR: 0.0002432, Tokens/sec: 182223.60\n",
      "Step: 3024, Training Loss: 2.32048, LR: 0.0002429, Tokens/sec: 182610.13\n",
      "Step: 3025, Training Loss: 1.85356, LR: 0.0002426, Tokens/sec: 181803.39\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 3025, Eval Loss: 2.26844\n",
      "Step: 3026, Training Loss: 2.83593, LR: 0.0002423, Tokens/sec: 182041.12\n",
      "Step: 3027, Training Loss: 2.12211, LR: 0.0002420, Tokens/sec: 181510.94\n",
      "Step: 3028, Training Loss: 1.90908, LR: 0.0002417, Tokens/sec: 182177.79\n",
      "Step: 3029, Training Loss: 1.52835, LR: 0.0002414, Tokens/sec: 182959.47\n",
      "Step: 3030, Training Loss: 2.26364, LR: 0.0002411, Tokens/sec: 182261.09\n",
      "Step: 3031, Training Loss: 1.50453, LR: 0.0002408, Tokens/sec: 182362.91\n",
      "Step: 3032, Training Loss: 2.63541, LR: 0.0002406, Tokens/sec: 182786.03\n",
      "Step: 3033, Training Loss: 1.85613, LR: 0.0002403, Tokens/sec: 181550.26\n",
      "Step: 3034, Training Loss: 1.97594, LR: 0.0002400, Tokens/sec: 182317.40\n",
      "Step: 3035, Training Loss: 2.24117, LR: 0.0002397, Tokens/sec: 182775.78\n",
      "Step: 3036, Training Loss: 1.67473, LR: 0.0002394, Tokens/sec: 183145.98\n",
      "Step: 3037, Training Loss: 2.22110, LR: 0.0002391, Tokens/sec: 182068.02\n",
      "Step: 3038, Training Loss: 1.68604, LR: 0.0002388, Tokens/sec: 182674.58\n",
      "Step: 3039, Training Loss: 1.76123, LR: 0.0002385, Tokens/sec: 181707.01\n",
      "Step: 3040, Training Loss: 2.07913, LR: 0.0002383, Tokens/sec: 181708.80\n",
      "Step: 3041, Training Loss: 1.77102, LR: 0.0002380, Tokens/sec: 182184.95\n",
      "Step: 3042, Training Loss: 1.85994, LR: 0.0002377, Tokens/sec: 182460.37\n",
      "Step: 3043, Training Loss: 2.21264, LR: 0.0002374, Tokens/sec: 182303.05\n",
      "Step: 3044, Training Loss: 2.38358, LR: 0.0002371, Tokens/sec: 182209.50\n",
      "Step: 3045, Training Loss: 1.79682, LR: 0.0002368, Tokens/sec: 181398.54\n",
      "Step: 3046, Training Loss: 2.06266, LR: 0.0002365, Tokens/sec: 182891.17\n",
      "Step: 3047, Training Loss: 2.30602, LR: 0.0002363, Tokens/sec: 183131.43\n",
      "Step: 3048, Training Loss: 1.96402, LR: 0.0002360, Tokens/sec: 182570.74\n",
      "Step: 3049, Training Loss: 2.46583, LR: 0.0002357, Tokens/sec: 182752.95\n",
      "Step: 3050, Training Loss: 2.16313, LR: 0.0002354, Tokens/sec: 182544.17\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 3050, Eval Loss: 2.14797\n",
      "Step: 3051, Training Loss: 2.20381, LR: 0.0002351, Tokens/sec: 175489.80\n",
      "Step: 3052, Training Loss: 2.22881, LR: 0.0002348, Tokens/sec: 181692.55\n",
      "Step: 3053, Training Loss: 2.14756, LR: 0.0002346, Tokens/sec: 182623.47\n",
      "Step: 3054, Training Loss: 1.76054, LR: 0.0002343, Tokens/sec: 182475.29\n",
      "Step: 3055, Training Loss: 2.01279, LR: 0.0002340, Tokens/sec: 182817.56\n",
      "Step: 3056, Training Loss: 2.26755, LR: 0.0002337, Tokens/sec: 180885.79\n",
      "Step: 3057, Training Loss: 2.57857, LR: 0.0002334, Tokens/sec: 182229.08\n",
      "Step: 3058, Training Loss: 1.78134, LR: 0.0002331, Tokens/sec: 182368.40\n",
      "Step: 3059, Training Loss: 1.80964, LR: 0.0002329, Tokens/sec: 181629.40\n",
      "Step: 3060, Training Loss: 2.26083, LR: 0.0002326, Tokens/sec: 182237.67\n",
      "Step: 3061, Training Loss: 1.70414, LR: 0.0002323, Tokens/sec: 182269.91\n",
      "Step: 3062, Training Loss: 1.75751, LR: 0.0002320, Tokens/sec: 181709.06\n",
      "Step: 3063, Training Loss: 2.41517, LR: 0.0002317, Tokens/sec: 182662.54\n",
      "Step: 3064, Training Loss: 2.20346, LR: 0.0002315, Tokens/sec: 182917.90\n",
      "Step: 3065, Training Loss: 2.20817, LR: 0.0002312, Tokens/sec: 182757.03\n",
      "Step: 3066, Training Loss: 1.95936, LR: 0.0002309, Tokens/sec: 177915.53\n",
      "Step: 3067, Training Loss: 2.09081, LR: 0.0002306, Tokens/sec: 179566.01\n",
      "Step: 3068, Training Loss: 2.68332, LR: 0.0002303, Tokens/sec: 179557.15\n",
      "Step: 3069, Training Loss: 2.25562, LR: 0.0002301, Tokens/sec: 182043.49\n",
      "Step: 3070, Training Loss: 2.07567, LR: 0.0002298, Tokens/sec: 182127.55\n",
      "Step: 3071, Training Loss: 1.83026, LR: 0.0002295, Tokens/sec: 182373.72\n",
      "Step: 3072, Training Loss: 1.72432, LR: 0.0002292, Tokens/sec: 182149.92\n",
      "Step: 3073, Training Loss: 1.59219, LR: 0.0002289, Tokens/sec: 183173.41\n",
      "Step: 3074, Training Loss: 1.73559, LR: 0.0002287, Tokens/sec: 181554.63\n",
      "Step: 3075, Training Loss: 2.14206, LR: 0.0002284, Tokens/sec: 182104.64\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 3075, Eval Loss: 2.20155\n",
      "Step: 3076, Training Loss: 2.20051, LR: 0.0002281, Tokens/sec: 181041.14\n",
      "Step: 3077, Training Loss: 2.26931, LR: 0.0002278, Tokens/sec: 182096.76\n",
      "Step: 3078, Training Loss: 1.87423, LR: 0.0002275, Tokens/sec: 182311.65\n",
      "Step: 3079, Training Loss: 2.55298, LR: 0.0002273, Tokens/sec: 182452.51\n",
      "Step: 3080, Training Loss: 1.97484, LR: 0.0002270, Tokens/sec: 181964.52\n",
      "Step: 3081, Training Loss: 2.09882, LR: 0.0002267, Tokens/sec: 182547.32\n",
      "Step: 3082, Training Loss: 2.06306, LR: 0.0002264, Tokens/sec: 181087.16\n",
      "Step: 3083, Training Loss: 2.02837, LR: 0.0002262, Tokens/sec: 182511.65\n",
      "Step: 3084, Training Loss: 1.86978, LR: 0.0002259, Tokens/sec: 183000.66\n",
      "Step: 3085, Training Loss: 2.44332, LR: 0.0002256, Tokens/sec: 182773.10\n",
      "Step: 3086, Training Loss: 1.79094, LR: 0.0002253, Tokens/sec: 181966.04\n",
      "Step: 3087, Training Loss: 1.74869, LR: 0.0002251, Tokens/sec: 182346.47\n",
      "Step: 3088, Training Loss: 1.84676, LR: 0.0002248, Tokens/sec: 181196.82\n",
      "Step: 3089, Training Loss: 1.92248, LR: 0.0002245, Tokens/sec: 183295.96\n",
      "Step: 3090, Training Loss: 2.39102, LR: 0.0002242, Tokens/sec: 182819.26\n",
      "Step: 3091, Training Loss: 2.16589, LR: 0.0002240, Tokens/sec: 182438.81\n",
      "Step: 3092, Training Loss: 1.83791, LR: 0.0002237, Tokens/sec: 182753.00\n",
      "Step: 3093, Training Loss: 1.80615, LR: 0.0002234, Tokens/sec: 182683.30\n",
      "Step: 3094, Training Loss: 2.16059, LR: 0.0002231, Tokens/sec: 182035.64\n",
      "Step: 3095, Training Loss: 2.57819, LR: 0.0002229, Tokens/sec: 182842.72\n",
      "Step: 3096, Training Loss: 2.06542, LR: 0.0002226, Tokens/sec: 182887.29\n",
      "Step: 3097, Training Loss: 1.79061, LR: 0.0002223, Tokens/sec: 182664.17\n",
      "Step: 3098, Training Loss: 2.25161, LR: 0.0002221, Tokens/sec: 183013.64\n",
      "Step: 3099, Training Loss: 1.87136, LR: 0.0002218, Tokens/sec: 182990.93\n",
      "Step: 3100, Training Loss: 2.11039, LR: 0.0002215, Tokens/sec: 181330.68\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 3100, Eval Loss: 2.10605\n",
      "Step: 3101, Training Loss: 2.21112, LR: 0.0002212, Tokens/sec: 181446.81\n",
      "Step: 3102, Training Loss: 2.10511, LR: 0.0002210, Tokens/sec: 182094.38\n",
      "Step: 3103, Training Loss: 2.08588, LR: 0.0002207, Tokens/sec: 182812.18\n",
      "Step: 3104, Training Loss: 1.82188, LR: 0.0002204, Tokens/sec: 182733.20\n",
      "Step: 3105, Training Loss: 1.88324, LR: 0.0002202, Tokens/sec: 182672.22\n",
      "Step: 3106, Training Loss: 1.98752, LR: 0.0002199, Tokens/sec: 182300.72\n",
      "Step: 3107, Training Loss: 1.95459, LR: 0.0002196, Tokens/sec: 182616.56\n",
      "Step: 3108, Training Loss: 1.79551, LR: 0.0002193, Tokens/sec: 181521.79\n",
      "Step: 3109, Training Loss: 2.16805, LR: 0.0002191, Tokens/sec: 182482.99\n",
      "Step: 3110, Training Loss: 1.71127, LR: 0.0002188, Tokens/sec: 182335.08\n",
      "Step: 3111, Training Loss: 2.45863, LR: 0.0002185, Tokens/sec: 182070.07\n",
      "Step: 3112, Training Loss: 2.04813, LR: 0.0002183, Tokens/sec: 182846.48\n",
      "Step: 3113, Training Loss: 2.10787, LR: 0.0002180, Tokens/sec: 182284.25\n",
      "Step: 3114, Training Loss: 2.19909, LR: 0.0002177, Tokens/sec: 181648.05\n",
      "Step: 3115, Training Loss: 1.70889, LR: 0.0002175, Tokens/sec: 182296.53\n",
      "Step: 3116, Training Loss: 1.42040, LR: 0.0002172, Tokens/sec: 183325.79\n",
      "Step: 3117, Training Loss: 2.39896, LR: 0.0002169, Tokens/sec: 182457.41\n",
      "Step: 3118, Training Loss: 2.29636, LR: 0.0002167, Tokens/sec: 182529.52\n",
      "Step: 3119, Training Loss: 2.21972, LR: 0.0002164, Tokens/sec: 182787.45\n",
      "Step: 3120, Training Loss: 1.92034, LR: 0.0002161, Tokens/sec: 181632.61\n",
      "Step: 3121, Training Loss: 1.75727, LR: 0.0002159, Tokens/sec: 182268.04\n",
      "Step: 3122, Training Loss: 2.50006, LR: 0.0002156, Tokens/sec: 182699.44\n",
      "Step: 3123, Training Loss: 2.03449, LR: 0.0002153, Tokens/sec: 182426.00\n",
      "Step: 3124, Training Loss: 2.13581, LR: 0.0002151, Tokens/sec: 182192.56\n",
      "Step: 3125, Training Loss: 1.67420, LR: 0.0002148, Tokens/sec: 182500.14\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 3125, Eval Loss: 2.00224\n",
      "Step: 3126, Training Loss: 2.02870, LR: 0.0002145, Tokens/sec: 181107.07\n",
      "Step: 3127, Training Loss: 1.74314, LR: 0.0002143, Tokens/sec: 182500.91\n",
      "Step: 3128, Training Loss: 2.00552, LR: 0.0002140, Tokens/sec: 182623.15\n",
      "Step: 3129, Training Loss: 1.61874, LR: 0.0002137, Tokens/sec: 182312.55\n",
      "Step: 3130, Training Loss: 2.19337, LR: 0.0002135, Tokens/sec: 182107.05\n",
      "Step: 3131, Training Loss: 1.83077, LR: 0.0002132, Tokens/sec: 180647.23\n",
      "Step: 3132, Training Loss: 2.61440, LR: 0.0002129, Tokens/sec: 182094.74\n",
      "Step: 3133, Training Loss: 1.94839, LR: 0.0002127, Tokens/sec: 182811.67\n",
      "Step: 3134, Training Loss: 2.20221, LR: 0.0002124, Tokens/sec: 182006.60\n",
      "Step: 3135, Training Loss: 1.88756, LR: 0.0002122, Tokens/sec: 182068.85\n",
      "Step: 3136, Training Loss: 1.50016, LR: 0.0002119, Tokens/sec: 182549.57\n",
      "Step: 3137, Training Loss: 2.23544, LR: 0.0002116, Tokens/sec: 181700.83\n",
      "Step: 3138, Training Loss: 2.38101, LR: 0.0002114, Tokens/sec: 182084.45\n",
      "Step: 3139, Training Loss: 2.01864, LR: 0.0002111, Tokens/sec: 182404.83\n",
      "Step: 3140, Training Loss: 2.15318, LR: 0.0002108, Tokens/sec: 182331.57\n",
      "Step: 3141, Training Loss: 1.75792, LR: 0.0002106, Tokens/sec: 182495.25\n",
      "Step: 3142, Training Loss: 1.76639, LR: 0.0002103, Tokens/sec: 182441.78\n",
      "Step: 3143, Training Loss: 1.80441, LR: 0.0002101, Tokens/sec: 182066.85\n",
      "Step: 3144, Training Loss: 2.51116, LR: 0.0002098, Tokens/sec: 182608.67\n",
      "Step: 3145, Training Loss: 2.06893, LR: 0.0002095, Tokens/sec: 183280.59\n",
      "Step: 3146, Training Loss: 2.00359, LR: 0.0002093, Tokens/sec: 182592.72\n",
      "Step: 3147, Training Loss: 1.89004, LR: 0.0002090, Tokens/sec: 182194.03\n",
      "Step: 3148, Training Loss: 2.21952, LR: 0.0002088, Tokens/sec: 182483.94\n",
      "Step: 3149, Training Loss: 1.92988, LR: 0.0002085, Tokens/sec: 181538.38\n",
      "Step: 3150, Training Loss: 3.30228, LR: 0.0002083, Tokens/sec: 182959.44\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 3150, Eval Loss: 2.07528\n",
      "Step: 3151, Training Loss: 1.99490, LR: 0.0002080, Tokens/sec: 181354.70\n",
      "Step: 3152, Training Loss: 2.09775, LR: 0.0002077, Tokens/sec: 181972.53\n",
      "Step: 3153, Training Loss: 1.96125, LR: 0.0002075, Tokens/sec: 182900.25\n",
      "Step: 3154, Training Loss: 1.97604, LR: 0.0002072, Tokens/sec: 182638.45\n",
      "Step: 3155, Training Loss: 2.05666, LR: 0.0002070, Tokens/sec: 182601.01\n",
      "Step: 3156, Training Loss: 1.67022, LR: 0.0002067, Tokens/sec: 182652.89\n",
      "Step: 3157, Training Loss: 1.51557, LR: 0.0002064, Tokens/sec: 181703.90\n",
      "Step: 3158, Training Loss: 1.82340, LR: 0.0002062, Tokens/sec: 183174.76\n",
      "Step: 3159, Training Loss: 2.18394, LR: 0.0002059, Tokens/sec: 182690.69\n",
      "Step: 3160, Training Loss: 1.34100, LR: 0.0002057, Tokens/sec: 180374.89\n",
      "Step: 3161, Training Loss: 1.96126, LR: 0.0002054, Tokens/sec: 182592.64\n",
      "Step: 3162, Training Loss: 1.60161, LR: 0.0002052, Tokens/sec: 182920.97\n",
      "Step: 3163, Training Loss: 1.98652, LR: 0.0002049, Tokens/sec: 180815.46\n",
      "Step: 3164, Training Loss: 2.09429, LR: 0.0002047, Tokens/sec: 183153.78\n",
      "Step: 3165, Training Loss: 2.02591, LR: 0.0002044, Tokens/sec: 182712.29\n",
      "Step: 3166, Training Loss: 2.63029, LR: 0.0002041, Tokens/sec: 182374.88\n",
      "Step: 3167, Training Loss: 1.97796, LR: 0.0002039, Tokens/sec: 182490.42\n",
      "Step: 3168, Training Loss: 1.65836, LR: 0.0002036, Tokens/sec: 182701.99\n",
      "Step: 3169, Training Loss: 2.00339, LR: 0.0002034, Tokens/sec: 182450.82\n",
      "Step: 3170, Training Loss: 2.44030, LR: 0.0002031, Tokens/sec: 182472.26\n",
      "Step: 3171, Training Loss: 1.69028, LR: 0.0002029, Tokens/sec: 183033.94\n",
      "Step: 3172, Training Loss: 2.41945, LR: 0.0002026, Tokens/sec: 182971.89\n",
      "Step: 3173, Training Loss: 1.85827, LR: 0.0002024, Tokens/sec: 183007.72\n",
      "Step: 3174, Training Loss: 2.07382, LR: 0.0002021, Tokens/sec: 182979.94\n",
      "Step: 3175, Training Loss: 1.83785, LR: 0.0002019, Tokens/sec: 182096.82\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 3175, Eval Loss: 2.04845\n",
      "Step: 3176, Training Loss: 2.14335, LR: 0.0002016, Tokens/sec: 182087.57\n",
      "Step: 3177, Training Loss: 2.24746, LR: 0.0002014, Tokens/sec: 182833.86\n",
      "Step: 3178, Training Loss: 1.88200, LR: 0.0002011, Tokens/sec: 182181.70\n",
      "Step: 3179, Training Loss: 1.94803, LR: 0.0002009, Tokens/sec: 182751.61\n",
      "Step: 3180, Training Loss: 1.69913, LR: 0.0002006, Tokens/sec: 182735.08\n",
      "Step: 3181, Training Loss: 2.26271, LR: 0.0002004, Tokens/sec: 182359.33\n",
      "Step: 3182, Training Loss: 1.91947, LR: 0.0002001, Tokens/sec: 182601.73\n",
      "Step: 3183, Training Loss: 1.66244, LR: 0.0001999, Tokens/sec: 181625.67\n",
      "Step: 3184, Training Loss: 1.82882, LR: 0.0001996, Tokens/sec: 182735.69\n",
      "Step: 3185, Training Loss: 2.60746, LR: 0.0001994, Tokens/sec: 182741.10\n",
      "Step: 3186, Training Loss: 2.10472, LR: 0.0001991, Tokens/sec: 182394.58\n",
      "Step: 3187, Training Loss: 2.32197, LR: 0.0001989, Tokens/sec: 182892.98\n",
      "Step: 3188, Training Loss: 2.02750, LR: 0.0001986, Tokens/sec: 182661.13\n",
      "Step: 3189, Training Loss: 1.71320, LR: 0.0001984, Tokens/sec: 182132.66\n",
      "Step: 3190, Training Loss: 2.16903, LR: 0.0001981, Tokens/sec: 183143.46\n",
      "Step: 3191, Training Loss: 1.66877, LR: 0.0001979, Tokens/sec: 182419.16\n",
      "Step: 3192, Training Loss: 2.10997, LR: 0.0001976, Tokens/sec: 182384.07\n",
      "Step: 3193, Training Loss: 2.13456, LR: 0.0001974, Tokens/sec: 182615.56\n",
      "Step: 3194, Training Loss: 1.81579, LR: 0.0001971, Tokens/sec: 183077.99\n",
      "Step: 3195, Training Loss: 2.37314, LR: 0.0001969, Tokens/sec: 181565.87\n",
      "Step: 3196, Training Loss: 2.15312, LR: 0.0001966, Tokens/sec: 181955.14\n",
      "Step: 3197, Training Loss: 1.89762, LR: 0.0001964, Tokens/sec: 183037.12\n",
      "Step: 3198, Training Loss: 2.52185, LR: 0.0001961, Tokens/sec: 182548.90\n",
      "Step: 3199, Training Loss: 1.64009, LR: 0.0001959, Tokens/sec: 182443.99\n",
      "Step: 3200, Training Loss: 1.95834, LR: 0.0001957, Tokens/sec: 182524.65\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 3200, Eval Loss: 1.98158\n",
      "Step: 3201, Training Loss: 1.83388, LR: 0.0001954, Tokens/sec: 181770.65\n",
      "Step: 3202, Training Loss: 2.45606, LR: 0.0001952, Tokens/sec: 182674.20\n",
      "Step: 3203, Training Loss: 2.18029, LR: 0.0001949, Tokens/sec: 182318.42\n",
      "Step: 3204, Training Loss: 1.63647, LR: 0.0001947, Tokens/sec: 182498.93\n",
      "Step: 3205, Training Loss: 2.17438, LR: 0.0001944, Tokens/sec: 182802.83\n",
      "Step: 3206, Training Loss: 2.00192, LR: 0.0001942, Tokens/sec: 181262.09\n",
      "Step: 3207, Training Loss: 1.68021, LR: 0.0001940, Tokens/sec: 182053.04\n",
      "Step: 3208, Training Loss: 2.10114, LR: 0.0001937, Tokens/sec: 182797.80\n",
      "Step: 3209, Training Loss: 2.04322, LR: 0.0001935, Tokens/sec: 182247.28\n",
      "Step: 3210, Training Loss: 1.83927, LR: 0.0001932, Tokens/sec: 177195.95\n",
      "Step: 3211, Training Loss: 2.01567, LR: 0.0001930, Tokens/sec: 182257.63\n",
      "Step: 3212, Training Loss: 1.97757, LR: 0.0001927, Tokens/sec: 180973.51\n",
      "Step: 3213, Training Loss: 1.90794, LR: 0.0001925, Tokens/sec: 182861.18\n",
      "Step: 3214, Training Loss: 1.98346, LR: 0.0001923, Tokens/sec: 182853.25\n",
      "Step: 3215, Training Loss: 1.74884, LR: 0.0001920, Tokens/sec: 182132.17\n",
      "Step: 3216, Training Loss: 2.29598, LR: 0.0001918, Tokens/sec: 182929.88\n",
      "Step: 3217, Training Loss: 1.85077, LR: 0.0001915, Tokens/sec: 182672.05\n",
      "Step: 3218, Training Loss: 1.92629, LR: 0.0001913, Tokens/sec: 181470.15\n",
      "Step: 3219, Training Loss: 1.99508, LR: 0.0001911, Tokens/sec: 181925.35\n",
      "Step: 3220, Training Loss: 1.74987, LR: 0.0001908, Tokens/sec: 182674.12\n",
      "Step: 3221, Training Loss: 1.77511, LR: 0.0001906, Tokens/sec: 181918.87\n",
      "Step: 3222, Training Loss: 1.93448, LR: 0.0001903, Tokens/sec: 182284.19\n",
      "Step: 3223, Training Loss: 1.58191, LR: 0.0001901, Tokens/sec: 181966.75\n",
      "Step: 3224, Training Loss: 2.17274, LR: 0.0001899, Tokens/sec: 181949.66\n",
      "Step: 3225, Training Loss: 1.42172, LR: 0.0001896, Tokens/sec: 182156.68\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 3225, Eval Loss: 2.09341\n",
      "Step: 3226, Training Loss: 1.75842, LR: 0.0001894, Tokens/sec: 180858.91\n",
      "Step: 3227, Training Loss: 2.07035, LR: 0.0001891, Tokens/sec: 181920.06\n",
      "Step: 3228, Training Loss: 1.29975, LR: 0.0001889, Tokens/sec: 182849.68\n",
      "Step: 3229, Training Loss: 1.94028, LR: 0.0001887, Tokens/sec: 182978.05\n",
      "Step: 3230, Training Loss: 2.35020, LR: 0.0001884, Tokens/sec: 182406.34\n",
      "Step: 3231, Training Loss: 1.39501, LR: 0.0001882, Tokens/sec: 182693.41\n",
      "Step: 3232, Training Loss: 1.76636, LR: 0.0001880, Tokens/sec: 181399.80\n",
      "Step: 3233, Training Loss: 1.74302, LR: 0.0001877, Tokens/sec: 183095.35\n",
      "Step: 3234, Training Loss: 1.81097, LR: 0.0001875, Tokens/sec: 183256.39\n",
      "Step: 3235, Training Loss: 2.02722, LR: 0.0001873, Tokens/sec: 182560.27\n",
      "Step: 3236, Training Loss: 2.06076, LR: 0.0001870, Tokens/sec: 182521.33\n",
      "Step: 3237, Training Loss: 2.10745, LR: 0.0001868, Tokens/sec: 181446.04\n",
      "Step: 3238, Training Loss: 2.27846, LR: 0.0001866, Tokens/sec: 181323.51\n",
      "Step: 3239, Training Loss: 1.61279, LR: 0.0001863, Tokens/sec: 181891.37\n",
      "Step: 3240, Training Loss: 1.56281, LR: 0.0001861, Tokens/sec: 182118.36\n",
      "Step: 3241, Training Loss: 1.87805, LR: 0.0001858, Tokens/sec: 182343.41\n",
      "Step: 3242, Training Loss: 1.71844, LR: 0.0001856, Tokens/sec: 182509.01\n",
      "Step: 3243, Training Loss: 2.10095, LR: 0.0001854, Tokens/sec: 183331.04\n",
      "Step: 3244, Training Loss: 2.14060, LR: 0.0001851, Tokens/sec: 181019.49\n",
      "Step: 3245, Training Loss: 1.60208, LR: 0.0001849, Tokens/sec: 173600.86\n",
      "Step: 3246, Training Loss: 1.62053, LR: 0.0001847, Tokens/sec: 182660.37\n",
      "Step: 3247, Training Loss: 1.88318, LR: 0.0001845, Tokens/sec: 181876.49\n",
      "Step: 3248, Training Loss: 2.14536, LR: 0.0001842, Tokens/sec: 181956.78\n",
      "Step: 3249, Training Loss: 1.83406, LR: 0.0001840, Tokens/sec: 181411.71\n",
      "Step: 3250, Training Loss: 1.84335, LR: 0.0001838, Tokens/sec: 180365.73\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 3250, Eval Loss: 2.02473\n",
      "Step: 3251, Training Loss: 1.73391, LR: 0.0001835, Tokens/sec: 171150.35\n",
      "Step: 3252, Training Loss: 2.19746, LR: 0.0001833, Tokens/sec: 176108.38\n",
      "Step: 3253, Training Loss: 1.85860, LR: 0.0001831, Tokens/sec: 179375.46\n",
      "Step: 3254, Training Loss: 1.66715, LR: 0.0001828, Tokens/sec: 180065.62\n",
      "Step: 3255, Training Loss: 1.87749, LR: 0.0001826, Tokens/sec: 180881.68\n",
      "Step: 3256, Training Loss: 1.85783, LR: 0.0001824, Tokens/sec: 181899.05\n",
      "Step: 3257, Training Loss: 1.84729, LR: 0.0001821, Tokens/sec: 181979.14\n",
      "Step: 3258, Training Loss: 1.81626, LR: 0.0001819, Tokens/sec: 180856.99\n",
      "Step: 3259, Training Loss: 1.74726, LR: 0.0001817, Tokens/sec: 181608.11\n",
      "Step: 3260, Training Loss: 1.77899, LR: 0.0001815, Tokens/sec: 183151.19\n",
      "Step: 3261, Training Loss: 1.60090, LR: 0.0001812, Tokens/sec: 182742.97\n",
      "Step: 3262, Training Loss: 2.14926, LR: 0.0001810, Tokens/sec: 183166.25\n",
      "Step: 3263, Training Loss: 1.88722, LR: 0.0001808, Tokens/sec: 182267.43\n",
      "Step: 3264, Training Loss: 1.96167, LR: 0.0001806, Tokens/sec: 181733.80\n",
      "Step: 3265, Training Loss: 2.45461, LR: 0.0001803, Tokens/sec: 182360.76\n",
      "Step: 3266, Training Loss: 2.16554, LR: 0.0001801, Tokens/sec: 182388.33\n",
      "Step: 3267, Training Loss: 2.00175, LR: 0.0001799, Tokens/sec: 182217.19\n",
      "Step: 3268, Training Loss: 2.17199, LR: 0.0001796, Tokens/sec: 182613.48\n",
      "Step: 3269, Training Loss: 1.70253, LR: 0.0001794, Tokens/sec: 182488.28\n",
      "Step: 3270, Training Loss: 1.79713, LR: 0.0001792, Tokens/sec: 181661.62\n",
      "Step: 3271, Training Loss: 1.44318, LR: 0.0001790, Tokens/sec: 182051.15\n",
      "Step: 3272, Training Loss: 2.03703, LR: 0.0001787, Tokens/sec: 182830.93\n",
      "Step: 3273, Training Loss: 1.65933, LR: 0.0001785, Tokens/sec: 182437.83\n",
      "Step: 3274, Training Loss: 1.88304, LR: 0.0001783, Tokens/sec: 182133.93\n",
      "Step: 3275, Training Loss: 1.81374, LR: 0.0001781, Tokens/sec: 183169.84\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 3275, Eval Loss: 1.94388\n",
      "Step: 3276, Training Loss: 1.61981, LR: 0.0001779, Tokens/sec: 181398.42\n",
      "Step: 3277, Training Loss: 1.52037, LR: 0.0001776, Tokens/sec: 181647.43\n",
      "Step: 3278, Training Loss: 1.58558, LR: 0.0001774, Tokens/sec: 182107.43\n",
      "Step: 3279, Training Loss: 1.64800, LR: 0.0001772, Tokens/sec: 181068.46\n",
      "Step: 3280, Training Loss: 1.98864, LR: 0.0001770, Tokens/sec: 182628.65\n",
      "Step: 3281, Training Loss: 1.69229, LR: 0.0001767, Tokens/sec: 180848.06\n",
      "Step: 3282, Training Loss: 2.27652, LR: 0.0001765, Tokens/sec: 182404.92\n",
      "Step: 3283, Training Loss: 1.56369, LR: 0.0001763, Tokens/sec: 182865.00\n",
      "Step: 3284, Training Loss: 1.76770, LR: 0.0001761, Tokens/sec: 182890.00\n",
      "Step: 3285, Training Loss: 1.21666, LR: 0.0001759, Tokens/sec: 182599.29\n",
      "Step: 3286, Training Loss: 1.73040, LR: 0.0001756, Tokens/sec: 182138.36\n",
      "Step: 3287, Training Loss: 1.70379, LR: 0.0001754, Tokens/sec: 181017.54\n",
      "Step: 3288, Training Loss: 1.97233, LR: 0.0001752, Tokens/sec: 183147.80\n",
      "Step: 3289, Training Loss: 2.20408, LR: 0.0001750, Tokens/sec: 182521.12\n",
      "Step: 3290, Training Loss: 1.97401, LR: 0.0001748, Tokens/sec: 182477.90\n",
      "Step: 3291, Training Loss: 1.89365, LR: 0.0001745, Tokens/sec: 182879.25\n",
      "Step: 3292, Training Loss: 2.15077, LR: 0.0001743, Tokens/sec: 182621.51\n",
      "Step: 3293, Training Loss: 1.57696, LR: 0.0001741, Tokens/sec: 181736.22\n",
      "Step: 3294, Training Loss: 1.78895, LR: 0.0001739, Tokens/sec: 182899.28\n",
      "Step: 3295, Training Loss: 1.99343, LR: 0.0001737, Tokens/sec: 182486.51\n",
      "Step: 3296, Training Loss: 1.77331, LR: 0.0001734, Tokens/sec: 182498.87\n",
      "Step: 3297, Training Loss: 1.72485, LR: 0.0001732, Tokens/sec: 182611.51\n",
      "Step: 3298, Training Loss: 2.00838, LR: 0.0001730, Tokens/sec: 182812.87\n",
      "Step: 3299, Training Loss: 1.65578, LR: 0.0001728, Tokens/sec: 181251.11\n",
      "Step: 3300, Training Loss: 1.70870, LR: 0.0001726, Tokens/sec: 182757.51\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 3300, Eval Loss: 1.85229\n",
      "Step: 3301, Training Loss: 1.97922, LR: 0.0001724, Tokens/sec: 180799.00\n",
      "Step: 3302, Training Loss: 2.17989, LR: 0.0001721, Tokens/sec: 182537.08\n",
      "Step: 3303, Training Loss: 1.67461, LR: 0.0001719, Tokens/sec: 182530.10\n",
      "Step: 3304, Training Loss: 1.88288, LR: 0.0001717, Tokens/sec: 182292.81\n",
      "Step: 3305, Training Loss: 1.64684, LR: 0.0001715, Tokens/sec: 183389.20\n",
      "Step: 3306, Training Loss: 1.75527, LR: 0.0001713, Tokens/sec: 183128.07\n",
      "Step: 3307, Training Loss: 1.85482, LR: 0.0001711, Tokens/sec: 181759.15\n",
      "Step: 3308, Training Loss: 1.69759, LR: 0.0001709, Tokens/sec: 182578.83\n",
      "Step: 3309, Training Loss: 2.00505, LR: 0.0001706, Tokens/sec: 182771.45\n",
      "Step: 3310, Training Loss: 1.67193, LR: 0.0001704, Tokens/sec: 183076.05\n",
      "Step: 3311, Training Loss: 1.31200, LR: 0.0001702, Tokens/sec: 183025.77\n",
      "Step: 3312, Training Loss: 2.38214, LR: 0.0001700, Tokens/sec: 182822.70\n",
      "Step: 3313, Training Loss: 2.06534, LR: 0.0001698, Tokens/sec: 181741.52\n",
      "Step: 3314, Training Loss: 1.94089, LR: 0.0001696, Tokens/sec: 182792.32\n",
      "Step: 3315, Training Loss: 1.48787, LR: 0.0001694, Tokens/sec: 182794.10\n",
      "Step: 3316, Training Loss: 1.77184, LR: 0.0001691, Tokens/sec: 182360.79\n",
      "Step: 3317, Training Loss: 2.13655, LR: 0.0001689, Tokens/sec: 182634.22\n",
      "Step: 3318, Training Loss: 1.77407, LR: 0.0001687, Tokens/sec: 182968.44\n",
      "Step: 3319, Training Loss: 1.37940, LR: 0.0001685, Tokens/sec: 181711.13\n",
      "Step: 3320, Training Loss: 1.73783, LR: 0.0001683, Tokens/sec: 182512.15\n",
      "Step: 3321, Training Loss: 1.54994, LR: 0.0001681, Tokens/sec: 182750.57\n",
      "Step: 3322, Training Loss: 1.72254, LR: 0.0001679, Tokens/sec: 182310.18\n",
      "Step: 3323, Training Loss: 2.22060, LR: 0.0001677, Tokens/sec: 183160.07\n",
      "Step: 3324, Training Loss: 1.92311, LR: 0.0001675, Tokens/sec: 182983.37\n",
      "Step: 3325, Training Loss: 2.44017, LR: 0.0001673, Tokens/sec: 181834.91\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 3325, Eval Loss: 1.84652\n",
      "Step: 3326, Training Loss: 2.02922, LR: 0.0001670, Tokens/sec: 181776.74\n",
      "Step: 3327, Training Loss: 1.71440, LR: 0.0001668, Tokens/sec: 182414.27\n",
      "Step: 3328, Training Loss: 1.77371, LR: 0.0001666, Tokens/sec: 182367.79\n",
      "Step: 3329, Training Loss: 1.60284, LR: 0.0001664, Tokens/sec: 182471.90\n",
      "Step: 3330, Training Loss: 1.67889, LR: 0.0001662, Tokens/sec: 182630.46\n",
      "Step: 3331, Training Loss: 1.60265, LR: 0.0001660, Tokens/sec: 182926.20\n",
      "Step: 3332, Training Loss: 1.86887, LR: 0.0001658, Tokens/sec: 182813.08\n",
      "Step: 3333, Training Loss: 1.87080, LR: 0.0001656, Tokens/sec: 181746.76\n",
      "Step: 3334, Training Loss: 2.00202, LR: 0.0001654, Tokens/sec: 183357.67\n",
      "Step: 3335, Training Loss: 2.50784, LR: 0.0001652, Tokens/sec: 183456.87\n",
      "Step: 3336, Training Loss: 2.00433, LR: 0.0001650, Tokens/sec: 182335.35\n",
      "Step: 3337, Training Loss: 1.56209, LR: 0.0001648, Tokens/sec: 182454.34\n",
      "Step: 3338, Training Loss: 1.42114, LR: 0.0001646, Tokens/sec: 183042.82\n",
      "Step: 3339, Training Loss: 1.36224, LR: 0.0001644, Tokens/sec: 181104.50\n",
      "Step: 3340, Training Loss: 1.89258, LR: 0.0001642, Tokens/sec: 182570.25\n",
      "Step: 3341, Training Loss: 1.26990, LR: 0.0001639, Tokens/sec: 183041.21\n",
      "Step: 3342, Training Loss: 1.74918, LR: 0.0001637, Tokens/sec: 182178.07\n",
      "Step: 3343, Training Loss: 1.71639, LR: 0.0001635, Tokens/sec: 182301.15\n",
      "Step: 3344, Training Loss: 1.46183, LR: 0.0001633, Tokens/sec: 183002.32\n",
      "Step: 3345, Training Loss: 1.75798, LR: 0.0001631, Tokens/sec: 181475.18\n",
      "Step: 3346, Training Loss: 1.55235, LR: 0.0001629, Tokens/sec: 182443.86\n",
      "Step: 3347, Training Loss: 1.91904, LR: 0.0001627, Tokens/sec: 182761.51\n",
      "Step: 3348, Training Loss: 1.56155, LR: 0.0001625, Tokens/sec: 182088.99\n",
      "Step: 3349, Training Loss: 1.51865, LR: 0.0001623, Tokens/sec: 182573.49\n",
      "Step: 3350, Training Loss: 1.85653, LR: 0.0001621, Tokens/sec: 182595.96\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 3350, Eval Loss: 1.86241\n",
      "Step: 3351, Training Loss: 1.64838, LR: 0.0001619, Tokens/sec: 181866.60\n",
      "Step: 3352, Training Loss: 1.35078, LR: 0.0001617, Tokens/sec: 182971.40\n",
      "Step: 3353, Training Loss: 1.92518, LR: 0.0001615, Tokens/sec: 182225.24\n",
      "Step: 3354, Training Loss: 1.77509, LR: 0.0001613, Tokens/sec: 182307.99\n",
      "Step: 3355, Training Loss: 2.01167, LR: 0.0001611, Tokens/sec: 182990.28\n",
      "Step: 3356, Training Loss: 1.75940, LR: 0.0001609, Tokens/sec: 181785.07\n",
      "Step: 3357, Training Loss: 2.24707, LR: 0.0001607, Tokens/sec: 182796.93\n",
      "Step: 3358, Training Loss: 1.70677, LR: 0.0001605, Tokens/sec: 182743.23\n",
      "Step: 3359, Training Loss: 1.67235, LR: 0.0001603, Tokens/sec: 182382.85\n",
      "Step: 3360, Training Loss: 1.77945, LR: 0.0001601, Tokens/sec: 183043.28\n",
      "Step: 3361, Training Loss: 1.63207, LR: 0.0001599, Tokens/sec: 183092.83\n",
      "Step: 3362, Training Loss: 1.53602, LR: 0.0001597, Tokens/sec: 181612.70\n",
      "Step: 3363, Training Loss: 1.68844, LR: 0.0001595, Tokens/sec: 182103.30\n",
      "Step: 3364, Training Loss: 1.77431, LR: 0.0001593, Tokens/sec: 183298.42\n",
      "Step: 3365, Training Loss: 1.24141, LR: 0.0001591, Tokens/sec: 183235.07\n",
      "Step: 3366, Training Loss: 1.77618, LR: 0.0001589, Tokens/sec: 182930.45\n",
      "Step: 3367, Training Loss: 1.65460, LR: 0.0001587, Tokens/sec: 183465.49\n",
      "Step: 3368, Training Loss: 1.44620, LR: 0.0001585, Tokens/sec: 181792.90\n",
      "Step: 3369, Training Loss: 1.44332, LR: 0.0001583, Tokens/sec: 182363.28\n",
      "Step: 3370, Training Loss: 2.51392, LR: 0.0001582, Tokens/sec: 182683.59\n",
      "Step: 3371, Training Loss: 2.22426, LR: 0.0001580, Tokens/sec: 182593.40\n",
      "Step: 3372, Training Loss: 1.94352, LR: 0.0001578, Tokens/sec: 182904.22\n",
      "Step: 3373, Training Loss: 1.60654, LR: 0.0001576, Tokens/sec: 182235.09\n",
      "Step: 3374, Training Loss: 1.57237, LR: 0.0001574, Tokens/sec: 181690.66\n",
      "Step: 3375, Training Loss: 1.51825, LR: 0.0001572, Tokens/sec: 182935.18\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 3375, Eval Loss: 1.81259\n",
      "Step: 3376, Training Loss: 1.40768, LR: 0.0001570, Tokens/sec: 181651.48\n",
      "Step: 3377, Training Loss: 1.82429, LR: 0.0001568, Tokens/sec: 182903.69\n",
      "Step: 3378, Training Loss: 1.64275, LR: 0.0001566, Tokens/sec: 182802.33\n",
      "Step: 3379, Training Loss: 1.24349, LR: 0.0001564, Tokens/sec: 182240.63\n",
      "Step: 3380, Training Loss: 1.58586, LR: 0.0001562, Tokens/sec: 182249.26\n",
      "Step: 3381, Training Loss: 1.88713, LR: 0.0001560, Tokens/sec: 182799.84\n",
      "Step: 3382, Training Loss: 1.80420, LR: 0.0001558, Tokens/sec: 181791.06\n",
      "Step: 3383, Training Loss: 1.52718, LR: 0.0001556, Tokens/sec: 182583.76\n",
      "Step: 3384, Training Loss: 1.74870, LR: 0.0001554, Tokens/sec: 183074.27\n",
      "Step: 3385, Training Loss: 1.69932, LR: 0.0001553, Tokens/sec: 183010.73\n",
      "Step: 3386, Training Loss: 1.57067, LR: 0.0001551, Tokens/sec: 182489.88\n",
      "Step: 3387, Training Loss: 1.73483, LR: 0.0001549, Tokens/sec: 182141.92\n",
      "Step: 3388, Training Loss: 2.06737, LR: 0.0001547, Tokens/sec: 182044.90\n",
      "Step: 3389, Training Loss: 1.90188, LR: 0.0001545, Tokens/sec: 182732.18\n",
      "Step: 3390, Training Loss: 2.02081, LR: 0.0001543, Tokens/sec: 182968.90\n",
      "Step: 3391, Training Loss: 1.92929, LR: 0.0001541, Tokens/sec: 182880.86\n",
      "Step: 3392, Training Loss: 1.78235, LR: 0.0001539, Tokens/sec: 183653.16\n",
      "Step: 3393, Training Loss: 1.99587, LR: 0.0001537, Tokens/sec: 183399.14\n",
      "Step: 3394, Training Loss: 1.40043, LR: 0.0001536, Tokens/sec: 181192.72\n",
      "Step: 3395, Training Loss: 1.82749, LR: 0.0001534, Tokens/sec: 182642.78\n",
      "Step: 3396, Training Loss: 1.78277, LR: 0.0001532, Tokens/sec: 183344.05\n",
      "Step: 3397, Training Loss: 1.74795, LR: 0.0001530, Tokens/sec: 182557.46\n",
      "Step: 3398, Training Loss: 2.13114, LR: 0.0001528, Tokens/sec: 182709.75\n",
      "Step: 3399, Training Loss: 2.15105, LR: 0.0001526, Tokens/sec: 182635.98\n",
      "Step: 3400, Training Loss: 1.69267, LR: 0.0001524, Tokens/sec: 181162.56\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 3400, Eval Loss: 1.76454\n",
      "Step: 3401, Training Loss: 1.74586, LR: 0.0001522, Tokens/sec: 183236.56\n",
      "Step: 3402, Training Loss: 1.81457, LR: 0.0001521, Tokens/sec: 182257.60\n",
      "Step: 3403, Training Loss: 1.61240, LR: 0.0001519, Tokens/sec: 182212.58\n",
      "Step: 3404, Training Loss: 2.15417, LR: 0.0001517, Tokens/sec: 182974.36\n",
      "Step: 3405, Training Loss: 1.71891, LR: 0.0001515, Tokens/sec: 182521.17\n",
      "Step: 3406, Training Loss: 1.63463, LR: 0.0001513, Tokens/sec: 181518.84\n",
      "Step: 3407, Training Loss: 1.91310, LR: 0.0001511, Tokens/sec: 182447.32\n",
      "Step: 3408, Training Loss: 1.78689, LR: 0.0001509, Tokens/sec: 181157.14\n",
      "Step: 3409, Training Loss: 1.47412, LR: 0.0001508, Tokens/sec: 182528.38\n",
      "Step: 3410, Training Loss: 2.02728, LR: 0.0001506, Tokens/sec: 182567.92\n",
      "Step: 3411, Training Loss: 1.38599, LR: 0.0001504, Tokens/sec: 182246.26\n",
      "Step: 3412, Training Loss: 2.04896, LR: 0.0001502, Tokens/sec: 182522.71\n",
      "Step: 3413, Training Loss: 1.79190, LR: 0.0001500, Tokens/sec: 182468.44\n",
      "Step: 3414, Training Loss: 1.89673, LR: 0.0001499, Tokens/sec: 181672.02\n",
      "Step: 3415, Training Loss: 2.05375, LR: 0.0001497, Tokens/sec: 182838.57\n",
      "Step: 3416, Training Loss: 1.90529, LR: 0.0001495, Tokens/sec: 183105.18\n",
      "Step: 3417, Training Loss: 1.93943, LR: 0.0001493, Tokens/sec: 182220.57\n",
      "Step: 3418, Training Loss: 1.76906, LR: 0.0001491, Tokens/sec: 182784.85\n",
      "Step: 3419, Training Loss: 1.84709, LR: 0.0001489, Tokens/sec: 183085.15\n",
      "Step: 3420, Training Loss: 1.49574, LR: 0.0001488, Tokens/sec: 181493.20\n",
      "Step: 3421, Training Loss: 1.92745, LR: 0.0001486, Tokens/sec: 183065.49\n",
      "Step: 3422, Training Loss: 2.22812, LR: 0.0001484, Tokens/sec: 182954.06\n",
      "Step: 3423, Training Loss: 2.12065, LR: 0.0001482, Tokens/sec: 182512.32\n",
      "Step: 3424, Training Loss: 1.76125, LR: 0.0001481, Tokens/sec: 182637.37\n",
      "Step: 3425, Training Loss: 1.77570, LR: 0.0001479, Tokens/sec: 182972.37\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 3425, Eval Loss: 1.65376\n",
      "Step: 3426, Training Loss: 2.27281, LR: 0.0001477, Tokens/sec: 181795.69\n",
      "Step: 3427, Training Loss: 2.53321, LR: 0.0001475, Tokens/sec: 183203.37\n",
      "Step: 3428, Training Loss: 1.40700, LR: 0.0001473, Tokens/sec: 182234.10\n",
      "Step: 3429, Training Loss: 1.44000, LR: 0.0001472, Tokens/sec: 182367.17\n",
      "Step: 3430, Training Loss: 2.06117, LR: 0.0001470, Tokens/sec: 183048.90\n",
      "Step: 3431, Training Loss: 2.16752, LR: 0.0001468, Tokens/sec: 181421.24\n",
      "Step: 3432, Training Loss: 1.57831, LR: 0.0001466, Tokens/sec: 182481.05\n",
      "Step: 3433, Training Loss: 1.72076, LR: 0.0001465, Tokens/sec: 183015.43\n",
      "Step: 3434, Training Loss: 1.43436, LR: 0.0001463, Tokens/sec: 182973.31\n",
      "Step: 3435, Training Loss: 2.22631, LR: 0.0001461, Tokens/sec: 182403.63\n",
      "Step: 3436, Training Loss: 2.13121, LR: 0.0001459, Tokens/sec: 182477.56\n",
      "Step: 3437, Training Loss: 1.75550, LR: 0.0001458, Tokens/sec: 181914.51\n",
      "Step: 3438, Training Loss: 1.88445, LR: 0.0001456, Tokens/sec: 182481.89\n",
      "Step: 3439, Training Loss: 1.65891, LR: 0.0001454, Tokens/sec: 183229.45\n",
      "Step: 3440, Training Loss: 1.24664, LR: 0.0001452, Tokens/sec: 182439.12\n",
      "Step: 3441, Training Loss: 1.86268, LR: 0.0001451, Tokens/sec: 182645.63\n",
      "Step: 3442, Training Loss: 1.54481, LR: 0.0001449, Tokens/sec: 182677.97\n",
      "Step: 3443, Training Loss: 1.90601, LR: 0.0001447, Tokens/sec: 181666.01\n",
      "Step: 3444, Training Loss: 2.04801, LR: 0.0001445, Tokens/sec: 183359.74\n",
      "Step: 3445, Training Loss: 2.07906, LR: 0.0001444, Tokens/sec: 183047.80\n",
      "Step: 3446, Training Loss: 2.53161, LR: 0.0001442, Tokens/sec: 182321.95\n",
      "Step: 3447, Training Loss: 1.94745, LR: 0.0001440, Tokens/sec: 183033.02\n",
      "Step: 3448, Training Loss: 1.93563, LR: 0.0001439, Tokens/sec: 182942.67\n",
      "Step: 3449, Training Loss: 1.78448, LR: 0.0001437, Tokens/sec: 182105.00\n",
      "Step: 3450, Training Loss: 1.62398, LR: 0.0001435, Tokens/sec: 182672.74\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 3450, Eval Loss: 1.71712\n",
      "Step: 3451, Training Loss: 1.42302, LR: 0.0001433, Tokens/sec: 180652.59\n",
      "Step: 3452, Training Loss: 1.66961, LR: 0.0001432, Tokens/sec: 182487.96\n",
      "Step: 3453, Training Loss: 1.75264, LR: 0.0001430, Tokens/sec: 183263.41\n",
      "Step: 3454, Training Loss: 1.57191, LR: 0.0001428, Tokens/sec: 182382.94\n",
      "Step: 3455, Training Loss: 1.94961, LR: 0.0001427, Tokens/sec: 182514.25\n",
      "Step: 3456, Training Loss: 1.65059, LR: 0.0001425, Tokens/sec: 183409.58\n",
      "Step: 3457, Training Loss: 1.86455, LR: 0.0001423, Tokens/sec: 175807.57\n",
      "Step: 3458, Training Loss: 1.66834, LR: 0.0001422, Tokens/sec: 182912.72\n",
      "Step: 3459, Training Loss: 1.61221, LR: 0.0001420, Tokens/sec: 183165.79\n",
      "Step: 3460, Training Loss: 1.93472, LR: 0.0001418, Tokens/sec: 182752.28\n",
      "Step: 3461, Training Loss: 1.75069, LR: 0.0001417, Tokens/sec: 182724.89\n",
      "Step: 3462, Training Loss: 1.54041, LR: 0.0001415, Tokens/sec: 182926.84\n",
      "Step: 3463, Training Loss: 1.22002, LR: 0.0001413, Tokens/sec: 181762.28\n",
      "Step: 3464, Training Loss: 1.58461, LR: 0.0001412, Tokens/sec: 182875.50\n",
      "Step: 3465, Training Loss: 1.41414, LR: 0.0001410, Tokens/sec: 182929.39\n",
      "Step: 3466, Training Loss: 1.62677, LR: 0.0001408, Tokens/sec: 181859.27\n",
      "Step: 3467, Training Loss: 1.55447, LR: 0.0001407, Tokens/sec: 182834.68\n",
      "Step: 3468, Training Loss: 1.21904, LR: 0.0001405, Tokens/sec: 182152.20\n",
      "Step: 3469, Training Loss: 1.81519, LR: 0.0001403, Tokens/sec: 181305.23\n",
      "Step: 3470, Training Loss: 2.03712, LR: 0.0001402, Tokens/sec: 182316.63\n",
      "Step: 3471, Training Loss: 1.70991, LR: 0.0001400, Tokens/sec: 182427.77\n",
      "Step: 3472, Training Loss: 1.85262, LR: 0.0001398, Tokens/sec: 183010.11\n",
      "Step: 3473, Training Loss: 1.66172, LR: 0.0001397, Tokens/sec: 182604.61\n",
      "Step: 3474, Training Loss: 1.66988, LR: 0.0001395, Tokens/sec: 182677.61\n",
      "Step: 3475, Training Loss: 1.58638, LR: 0.0001393, Tokens/sec: 181720.61\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 3475, Eval Loss: 1.61493\n",
      "Step: 3476, Training Loss: 1.26488, LR: 0.0001392, Tokens/sec: 181597.96\n",
      "Step: 3477, Training Loss: 1.69623, LR: 0.0001390, Tokens/sec: 182232.39\n",
      "Step: 3478, Training Loss: 1.65015, LR: 0.0001389, Tokens/sec: 182397.74\n",
      "Step: 3479, Training Loss: 1.51869, LR: 0.0001387, Tokens/sec: 182456.50\n",
      "Step: 3480, Training Loss: 1.47679, LR: 0.0001385, Tokens/sec: 181458.36\n",
      "Step: 3481, Training Loss: 1.54929, LR: 0.0001384, Tokens/sec: 182084.62\n",
      "Step: 3482, Training Loss: 1.61759, LR: 0.0001382, Tokens/sec: 181156.03\n",
      "Step: 3483, Training Loss: 1.28175, LR: 0.0001381, Tokens/sec: 180215.97\n",
      "Step: 3484, Training Loss: 1.50232, LR: 0.0001379, Tokens/sec: 181507.55\n",
      "Step: 3485, Training Loss: 1.80055, LR: 0.0001377, Tokens/sec: 181683.31\n",
      "Step: 3486, Training Loss: 1.89976, LR: 0.0001376, Tokens/sec: 181640.32\n",
      "Step: 3487, Training Loss: 1.82990, LR: 0.0001374, Tokens/sec: 182278.86\n",
      "Step: 3488, Training Loss: 1.32370, LR: 0.0001373, Tokens/sec: 182696.82\n",
      "Step: 3489, Training Loss: 1.79264, LR: 0.0001371, Tokens/sec: 180969.82\n",
      "Step: 3490, Training Loss: 2.12644, LR: 0.0001369, Tokens/sec: 182646.55\n",
      "Step: 3491, Training Loss: 1.82251, LR: 0.0001368, Tokens/sec: 175560.23\n",
      "Step: 3492, Training Loss: 1.23863, LR: 0.0001366, Tokens/sec: 181335.28\n",
      "Step: 3493, Training Loss: 1.69047, LR: 0.0001365, Tokens/sec: 182470.77\n",
      "Step: 3494, Training Loss: 1.31452, LR: 0.0001363, Tokens/sec: 181943.11\n",
      "Step: 3495, Training Loss: 1.37191, LR: 0.0001362, Tokens/sec: 180472.66\n",
      "Step: 3496, Training Loss: 1.18681, LR: 0.0001360, Tokens/sec: 181590.75\n",
      "Step: 3497, Training Loss: 1.54214, LR: 0.0001358, Tokens/sec: 182651.52\n",
      "Step: 3498, Training Loss: 1.53277, LR: 0.0001357, Tokens/sec: 182373.92\n",
      "Step: 3499, Training Loss: 1.49778, LR: 0.0001355, Tokens/sec: 182783.13\n",
      "Step: 3500, Training Loss: 1.50582, LR: 0.0001354, Tokens/sec: 182601.77\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 3500, Eval Loss: 1.75437\n",
      "Step: 3501, Training Loss: 1.87502, LR: 0.0001352, Tokens/sec: 179509.63\n",
      "Step: 3502, Training Loss: 1.55686, LR: 0.0001351, Tokens/sec: 181776.69\n",
      "Step: 3503, Training Loss: 1.24553, LR: 0.0001349, Tokens/sec: 181473.55\n",
      "Step: 3504, Training Loss: 1.88349, LR: 0.0001348, Tokens/sec: 182261.68\n",
      "Step: 3505, Training Loss: 1.14150, LR: 0.0001346, Tokens/sec: 182304.86\n",
      "Step: 3506, Training Loss: 1.51485, LR: 0.0001345, Tokens/sec: 181259.15\n",
      "Step: 3507, Training Loss: 1.73311, LR: 0.0001343, Tokens/sec: 182313.41\n",
      "Step: 3508, Training Loss: 1.63389, LR: 0.0001342, Tokens/sec: 182882.00\n",
      "Step: 3509, Training Loss: 1.76522, LR: 0.0001340, Tokens/sec: 183045.13\n",
      "Step: 3510, Training Loss: 1.70727, LR: 0.0001339, Tokens/sec: 182711.36\n",
      "Step: 3511, Training Loss: 1.34844, LR: 0.0001337, Tokens/sec: 182485.55\n",
      "Step: 3512, Training Loss: 1.68712, LR: 0.0001336, Tokens/sec: 181637.10\n",
      "Step: 3513, Training Loss: 1.89425, LR: 0.0001334, Tokens/sec: 182333.38\n",
      "Step: 3514, Training Loss: 1.68126, LR: 0.0001333, Tokens/sec: 182514.80\n",
      "Step: 3515, Training Loss: 1.73072, LR: 0.0001331, Tokens/sec: 182567.11\n",
      "Step: 3516, Training Loss: 1.66850, LR: 0.0001330, Tokens/sec: 183226.35\n",
      "Step: 3517, Training Loss: 1.92667, LR: 0.0001328, Tokens/sec: 182381.28\n",
      "Step: 3518, Training Loss: 1.42427, LR: 0.0001327, Tokens/sec: 181227.35\n",
      "Step: 3519, Training Loss: 1.46336, LR: 0.0001325, Tokens/sec: 182030.83\n",
      "Step: 3520, Training Loss: 2.22468, LR: 0.0001324, Tokens/sec: 183186.88\n",
      "Step: 3521, Training Loss: 1.44019, LR: 0.0001322, Tokens/sec: 182674.03\n",
      "Step: 3522, Training Loss: 1.46713, LR: 0.0001321, Tokens/sec: 182545.52\n",
      "Step: 3523, Training Loss: 1.23854, LR: 0.0001319, Tokens/sec: 182363.61\n",
      "Step: 3524, Training Loss: 1.61431, LR: 0.0001318, Tokens/sec: 181509.86\n",
      "Step: 3525, Training Loss: 1.23845, LR: 0.0001316, Tokens/sec: 181966.18\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 3525, Eval Loss: 1.75145\n",
      "Step: 3526, Training Loss: 1.89304, LR: 0.0001315, Tokens/sec: 181335.56\n",
      "Step: 3527, Training Loss: 1.94635, LR: 0.0001313, Tokens/sec: 183329.43\n",
      "Step: 3528, Training Loss: 1.75121, LR: 0.0001312, Tokens/sec: 183123.78\n",
      "Step: 3529, Training Loss: 1.58493, LR: 0.0001310, Tokens/sec: 181800.89\n",
      "Step: 3530, Training Loss: 1.74772, LR: 0.0001309, Tokens/sec: 181230.25\n",
      "Step: 3531, Training Loss: 1.86595, LR: 0.0001308, Tokens/sec: 181944.67\n",
      "Step: 3532, Training Loss: 1.88875, LR: 0.0001306, Tokens/sec: 181356.72\n",
      "Step: 3533, Training Loss: 2.07553, LR: 0.0001305, Tokens/sec: 181917.83\n",
      "Step: 3534, Training Loss: 1.57496, LR: 0.0001303, Tokens/sec: 182403.11\n",
      "Step: 3535, Training Loss: 0.96865, LR: 0.0001302, Tokens/sec: 182147.11\n",
      "Step: 3536, Training Loss: 1.99397, LR: 0.0001300, Tokens/sec: 182775.12\n",
      "Step: 3537, Training Loss: 1.90345, LR: 0.0001299, Tokens/sec: 182795.93\n",
      "Step: 3538, Training Loss: 1.71652, LR: 0.0001297, Tokens/sec: 182441.45\n",
      "Step: 3539, Training Loss: 1.52676, LR: 0.0001296, Tokens/sec: 182518.37\n",
      "Step: 3540, Training Loss: 1.54895, LR: 0.0001295, Tokens/sec: 183072.67\n",
      "Step: 3541, Training Loss: 1.38174, LR: 0.0001293, Tokens/sec: 182594.70\n",
      "Step: 3542, Training Loss: 1.27025, LR: 0.0001292, Tokens/sec: 183152.67\n",
      "Step: 3543, Training Loss: 1.61510, LR: 0.0001290, Tokens/sec: 182858.23\n",
      "Step: 3544, Training Loss: 1.70280, LR: 0.0001289, Tokens/sec: 182158.59\n",
      "Step: 3545, Training Loss: 1.57195, LR: 0.0001288, Tokens/sec: 182309.84\n",
      "Step: 3546, Training Loss: 1.59250, LR: 0.0001286, Tokens/sec: 182999.99\n",
      "Step: 3547, Training Loss: 1.92997, LR: 0.0001285, Tokens/sec: 182330.73\n",
      "Step: 3548, Training Loss: 2.11135, LR: 0.0001283, Tokens/sec: 183022.77\n",
      "Step: 3549, Training Loss: 1.28237, LR: 0.0001282, Tokens/sec: 183031.74\n",
      "Step: 3550, Training Loss: 1.69003, LR: 0.0001281, Tokens/sec: 181649.01\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 3550, Eval Loss: 1.46483\n",
      "Step: 3551, Training Loss: 1.59451, LR: 0.0001279, Tokens/sec: 181099.27\n",
      "Step: 3552, Training Loss: 1.50555, LR: 0.0001278, Tokens/sec: 181275.75\n",
      "Step: 3553, Training Loss: 1.80425, LR: 0.0001277, Tokens/sec: 182048.73\n",
      "Step: 3554, Training Loss: 1.86536, LR: 0.0001275, Tokens/sec: 182342.65\n",
      "Step: 3555, Training Loss: 1.50951, LR: 0.0001274, Tokens/sec: 182225.26\n",
      "Step: 3556, Training Loss: 1.75308, LR: 0.0001272, Tokens/sec: 182141.88\n",
      "Step: 3557, Training Loss: 1.55039, LR: 0.0001271, Tokens/sec: 182801.49\n",
      "Step: 3558, Training Loss: 2.23385, LR: 0.0001270, Tokens/sec: 181368.46\n",
      "Step: 3559, Training Loss: 1.56693, LR: 0.0001268, Tokens/sec: 182828.96\n",
      "Step: 3560, Training Loss: 1.58203, LR: 0.0001267, Tokens/sec: 182458.69\n",
      "Step: 3561, Training Loss: 1.78738, LR: 0.0001266, Tokens/sec: 182665.05\n",
      "Step: 3562, Training Loss: 1.59998, LR: 0.0001264, Tokens/sec: 182666.50\n",
      "Step: 3563, Training Loss: 1.45919, LR: 0.0001263, Tokens/sec: 182171.72\n",
      "Step: 3564, Training Loss: 1.51787, LR: 0.0001262, Tokens/sec: 181720.09\n",
      "Step: 3565, Training Loss: 1.56967, LR: 0.0001260, Tokens/sec: 182310.58\n",
      "Step: 3566, Training Loss: 1.86761, LR: 0.0001259, Tokens/sec: 183286.37\n",
      "Step: 3567, Training Loss: 2.04410, LR: 0.0001258, Tokens/sec: 182082.08\n",
      "Step: 3568, Training Loss: 1.64306, LR: 0.0001256, Tokens/sec: 182312.35\n",
      "Step: 3569, Training Loss: 1.68087, LR: 0.0001255, Tokens/sec: 182234.37\n",
      "Step: 3570, Training Loss: 1.90878, LR: 0.0001254, Tokens/sec: 181500.71\n",
      "Step: 3571, Training Loss: 2.16175, LR: 0.0001252, Tokens/sec: 182452.10\n",
      "Step: 3572, Training Loss: 1.89006, LR: 0.0001251, Tokens/sec: 182769.18\n",
      "Step: 3573, Training Loss: 1.70833, LR: 0.0001250, Tokens/sec: 182967.79\n",
      "Step: 3574, Training Loss: 1.83363, LR: 0.0001248, Tokens/sec: 182365.39\n",
      "Step: 3575, Training Loss: 1.23400, LR: 0.0001247, Tokens/sec: 182807.61\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 3575, Eval Loss: 1.67890\n",
      "Step: 3576, Training Loss: 1.60683, LR: 0.0001246, Tokens/sec: 181710.59\n",
      "Step: 3577, Training Loss: 1.75748, LR: 0.0001245, Tokens/sec: 182632.33\n",
      "Step: 3578, Training Loss: 1.96496, LR: 0.0001243, Tokens/sec: 182175.94\n",
      "Step: 3579, Training Loss: 1.86529, LR: 0.0001242, Tokens/sec: 181816.55\n",
      "Step: 3580, Training Loss: 2.26833, LR: 0.0001241, Tokens/sec: 182765.42\n",
      "Step: 3581, Training Loss: 1.38887, LR: 0.0001239, Tokens/sec: 180996.03\n",
      "Step: 3582, Training Loss: 1.35132, LR: 0.0001238, Tokens/sec: 181707.84\n",
      "Step: 3583, Training Loss: 1.48499, LR: 0.0001237, Tokens/sec: 182590.29\n",
      "Step: 3584, Training Loss: 1.35525, LR: 0.0001236, Tokens/sec: 182297.21\n",
      "Step: 3585, Training Loss: 1.31778, LR: 0.0001234, Tokens/sec: 182669.09\n",
      "Step: 3586, Training Loss: 1.46591, LR: 0.0001233, Tokens/sec: 182528.63\n",
      "Step: 3587, Training Loss: 1.96623, LR: 0.0001232, Tokens/sec: 181474.55\n",
      "Step: 3588, Training Loss: 1.46561, LR: 0.0001231, Tokens/sec: 182919.68\n",
      "Step: 3589, Training Loss: 1.39956, LR: 0.0001229, Tokens/sec: 183004.20\n",
      "Step: 3590, Training Loss: 1.69204, LR: 0.0001228, Tokens/sec: 182346.06\n",
      "Step: 3591, Training Loss: 1.29787, LR: 0.0001227, Tokens/sec: 182495.77\n",
      "Step: 3592, Training Loss: 1.65494, LR: 0.0001226, Tokens/sec: 182486.13\n",
      "Step: 3593, Training Loss: 1.77406, LR: 0.0001224, Tokens/sec: 181337.45\n",
      "Step: 3594, Training Loss: 1.51737, LR: 0.0001223, Tokens/sec: 182344.38\n",
      "Step: 3595, Training Loss: 1.40094, LR: 0.0001222, Tokens/sec: 182636.72\n",
      "Step: 3596, Training Loss: 1.39793, LR: 0.0001221, Tokens/sec: 182847.67\n",
      "Step: 3597, Training Loss: 1.93476, LR: 0.0001219, Tokens/sec: 182834.96\n",
      "Step: 3598, Training Loss: 1.68983, LR: 0.0001218, Tokens/sec: 182984.25\n",
      "Step: 3599, Training Loss: 1.19528, LR: 0.0001217, Tokens/sec: 181284.77\n",
      "Step: 3600, Training Loss: 1.49246, LR: 0.0001216, Tokens/sec: 182746.93\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 3600, Eval Loss: 1.56211\n",
      "Step: 3601, Training Loss: 1.68372, LR: 0.0001215, Tokens/sec: 182211.51\n",
      "Step: 3602, Training Loss: 1.50539, LR: 0.0001213, Tokens/sec: 182470.99\n",
      "Step: 3603, Training Loss: 1.67289, LR: 0.0001212, Tokens/sec: 182660.00\n",
      "Step: 3604, Training Loss: 1.35860, LR: 0.0001211, Tokens/sec: 182205.15\n",
      "Step: 3605, Training Loss: 1.37388, LR: 0.0001210, Tokens/sec: 182903.65\n",
      "Step: 3606, Training Loss: 1.48024, LR: 0.0001208, Tokens/sec: 182889.07\n",
      "Step: 3607, Training Loss: 1.58965, LR: 0.0001207, Tokens/sec: 181660.88\n",
      "Step: 3608, Training Loss: 1.77674, LR: 0.0001206, Tokens/sec: 182698.78\n",
      "Step: 3609, Training Loss: 1.48397, LR: 0.0001205, Tokens/sec: 182201.40\n",
      "Step: 3610, Training Loss: 1.48330, LR: 0.0001204, Tokens/sec: 182666.35\n",
      "Step: 3611, Training Loss: 1.97719, LR: 0.0001203, Tokens/sec: 182672.03\n",
      "Step: 3612, Training Loss: 2.05044, LR: 0.0001201, Tokens/sec: 183071.28\n",
      "Step: 3613, Training Loss: 1.16530, LR: 0.0001200, Tokens/sec: 181293.05\n",
      "Step: 3614, Training Loss: 1.72492, LR: 0.0001199, Tokens/sec: 182558.89\n",
      "Step: 3615, Training Loss: 1.27929, LR: 0.0001198, Tokens/sec: 183011.63\n",
      "Step: 3616, Training Loss: 2.15380, LR: 0.0001197, Tokens/sec: 182989.42\n",
      "Step: 3617, Training Loss: 1.54541, LR: 0.0001196, Tokens/sec: 182475.44\n",
      "Step: 3618, Training Loss: 1.37913, LR: 0.0001194, Tokens/sec: 182618.48\n",
      "Step: 3619, Training Loss: 1.72473, LR: 0.0001193, Tokens/sec: 181495.24\n",
      "Step: 3620, Training Loss: 1.10518, LR: 0.0001192, Tokens/sec: 182627.19\n",
      "Step: 3621, Training Loss: 2.07015, LR: 0.0001191, Tokens/sec: 183294.01\n",
      "Step: 3622, Training Loss: 1.42313, LR: 0.0001190, Tokens/sec: 182812.40\n",
      "Step: 3623, Training Loss: 1.37512, LR: 0.0001189, Tokens/sec: 182872.32\n",
      "Step: 3624, Training Loss: 1.19081, LR: 0.0001188, Tokens/sec: 183287.67\n",
      "Step: 3625, Training Loss: 1.27055, LR: 0.0001186, Tokens/sec: 181626.03\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 3625, Eval Loss: 1.50818\n",
      "Step: 3626, Training Loss: 1.25193, LR: 0.0001185, Tokens/sec: 182236.37\n",
      "Step: 3627, Training Loss: 1.28899, LR: 0.0001184, Tokens/sec: 182677.07\n",
      "Step: 3628, Training Loss: 1.27679, LR: 0.0001183, Tokens/sec: 182765.75\n",
      "Step: 3629, Training Loss: 1.91875, LR: 0.0001182, Tokens/sec: 182591.12\n",
      "Step: 3630, Training Loss: 1.91303, LR: 0.0001181, Tokens/sec: 182016.38\n",
      "Step: 3631, Training Loss: 1.57663, LR: 0.0001180, Tokens/sec: 182383.47\n",
      "Step: 3632, Training Loss: 1.91328, LR: 0.0001179, Tokens/sec: 182645.35\n",
      "Step: 3633, Training Loss: 1.33220, LR: 0.0001177, Tokens/sec: 181149.58\n",
      "Step: 3634, Training Loss: 1.40439, LR: 0.0001176, Tokens/sec: 183000.47\n",
      "Step: 3635, Training Loss: 1.40838, LR: 0.0001175, Tokens/sec: 182111.75\n",
      "Step: 3636, Training Loss: 1.85603, LR: 0.0001174, Tokens/sec: 180455.57\n",
      "Step: 3637, Training Loss: 1.24238, LR: 0.0001173, Tokens/sec: 181139.57\n",
      "Step: 3638, Training Loss: 1.44876, LR: 0.0001172, Tokens/sec: 182040.33\n",
      "Step: 3639, Training Loss: 1.56031, LR: 0.0001171, Tokens/sec: 180504.28\n",
      "Step: 3640, Training Loss: 1.91819, LR: 0.0001170, Tokens/sec: 181858.20\n",
      "Step: 3641, Training Loss: 1.29707, LR: 0.0001169, Tokens/sec: 182500.98\n",
      "Step: 3642, Training Loss: 1.78069, LR: 0.0001168, Tokens/sec: 182861.76\n",
      "Step: 3643, Training Loss: 1.34575, LR: 0.0001167, Tokens/sec: 180975.40\n",
      "Step: 3644, Training Loss: 1.34759, LR: 0.0001165, Tokens/sec: 182299.52\n",
      "Step: 3645, Training Loss: 1.40980, LR: 0.0001164, Tokens/sec: 180777.91\n",
      "Step: 3646, Training Loss: 1.78415, LR: 0.0001163, Tokens/sec: 181204.81\n",
      "Step: 3647, Training Loss: 1.36042, LR: 0.0001162, Tokens/sec: 181622.24\n",
      "Step: 3648, Training Loss: 1.49482, LR: 0.0001161, Tokens/sec: 181950.51\n",
      "Step: 3649, Training Loss: 1.29887, LR: 0.0001160, Tokens/sec: 182353.96\n",
      "Step: 3650, Training Loss: 1.41175, LR: 0.0001159, Tokens/sec: 183314.63\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 3650, Eval Loss: 1.46309\n",
      "Step: 3651, Training Loss: 1.46493, LR: 0.0001158, Tokens/sec: 181768.81\n",
      "Step: 3652, Training Loss: 1.26024, LR: 0.0001157, Tokens/sec: 182452.11\n",
      "Step: 3653, Training Loss: 1.48143, LR: 0.0001156, Tokens/sec: 180907.03\n",
      "Step: 3654, Training Loss: 1.29992, LR: 0.0001155, Tokens/sec: 182372.26\n",
      "Step: 3655, Training Loss: 1.43914, LR: 0.0001154, Tokens/sec: 182674.28\n",
      "Step: 3656, Training Loss: 1.25095, LR: 0.0001153, Tokens/sec: 179893.07\n",
      "Step: 3657, Training Loss: 1.65984, LR: 0.0001152, Tokens/sec: 182155.98\n",
      "Step: 3658, Training Loss: 1.60369, LR: 0.0001151, Tokens/sec: 182804.33\n",
      "Step: 3659, Training Loss: 1.17096, LR: 0.0001150, Tokens/sec: 182511.89\n",
      "Step: 3660, Training Loss: 1.36299, LR: 0.0001149, Tokens/sec: 182364.26\n",
      "Step: 3661, Training Loss: 1.71875, LR: 0.0001148, Tokens/sec: 182663.86\n",
      "Step: 3662, Training Loss: 1.58949, LR: 0.0001147, Tokens/sec: 181590.94\n",
      "Step: 3663, Training Loss: 1.36464, LR: 0.0001146, Tokens/sec: 182856.93\n",
      "Step: 3664, Training Loss: 1.24255, LR: 0.0001145, Tokens/sec: 183167.75\n",
      "Step: 3665, Training Loss: 1.52451, LR: 0.0001144, Tokens/sec: 183345.09\n",
      "Step: 3666, Training Loss: 1.40447, LR: 0.0001143, Tokens/sec: 183042.79\n",
      "Step: 3667, Training Loss: 1.66928, LR: 0.0001142, Tokens/sec: 182811.40\n",
      "Step: 3668, Training Loss: 1.25657, LR: 0.0001141, Tokens/sec: 181354.37\n",
      "Step: 3669, Training Loss: 1.38284, LR: 0.0001140, Tokens/sec: 182329.73\n",
      "Step: 3670, Training Loss: 1.93914, LR: 0.0001139, Tokens/sec: 182958.65\n",
      "Step: 3671, Training Loss: 1.78288, LR: 0.0001138, Tokens/sec: 182623.37\n",
      "Step: 3672, Training Loss: 1.44202, LR: 0.0001137, Tokens/sec: 182310.94\n",
      "Step: 3673, Training Loss: 1.22264, LR: 0.0001136, Tokens/sec: 182282.87\n",
      "Step: 3674, Training Loss: 1.47766, LR: 0.0001135, Tokens/sec: 182354.84\n",
      "Step: 3675, Training Loss: 1.41490, LR: 0.0001134, Tokens/sec: 183045.65\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 3675, Eval Loss: 1.56568\n",
      "Step: 3676, Training Loss: 1.34680, LR: 0.0001133, Tokens/sec: 180882.81\n",
      "Step: 3677, Training Loss: 1.18171, LR: 0.0001132, Tokens/sec: 182149.56\n",
      "Step: 3678, Training Loss: 1.42866, LR: 0.0001131, Tokens/sec: 182263.14\n",
      "Step: 3679, Training Loss: 1.57827, LR: 0.0001130, Tokens/sec: 182446.07\n",
      "Step: 3680, Training Loss: 1.79171, LR: 0.0001129, Tokens/sec: 181122.73\n",
      "Step: 3681, Training Loss: 1.25231, LR: 0.0001128, Tokens/sec: 181497.93\n",
      "Step: 3682, Training Loss: 1.22466, LR: 0.0001127, Tokens/sec: 180941.02\n",
      "Step: 3683, Training Loss: 1.19112, LR: 0.0001126, Tokens/sec: 182374.03\n",
      "Step: 3684, Training Loss: 1.38008, LR: 0.0001125, Tokens/sec: 182870.50\n",
      "Step: 3685, Training Loss: 1.33363, LR: 0.0001125, Tokens/sec: 183014.44\n",
      "Step: 3686, Training Loss: 2.13562, LR: 0.0001124, Tokens/sec: 182639.60\n",
      "Step: 3687, Training Loss: 1.54520, LR: 0.0001123, Tokens/sec: 182690.38\n",
      "Step: 3688, Training Loss: 1.47374, LR: 0.0001122, Tokens/sec: 182420.73\n",
      "Step: 3689, Training Loss: 1.45909, LR: 0.0001121, Tokens/sec: 182742.30\n",
      "Step: 3690, Training Loss: 1.29964, LR: 0.0001120, Tokens/sec: 183102.28\n",
      "Step: 3691, Training Loss: 1.67341, LR: 0.0001119, Tokens/sec: 182550.02\n",
      "Step: 3692, Training Loss: 1.07170, LR: 0.0001118, Tokens/sec: 182846.25\n",
      "Step: 3693, Training Loss: 1.73420, LR: 0.0001117, Tokens/sec: 182439.22\n",
      "Step: 3694, Training Loss: 1.74919, LR: 0.0001116, Tokens/sec: 182002.75\n",
      "Step: 3695, Training Loss: 1.29575, LR: 0.0001115, Tokens/sec: 181766.51\n",
      "Step: 3696, Training Loss: 1.36485, LR: 0.0001115, Tokens/sec: 183037.06\n",
      "Step: 3697, Training Loss: 1.52221, LR: 0.0001114, Tokens/sec: 182792.35\n",
      "Step: 3698, Training Loss: 1.64511, LR: 0.0001113, Tokens/sec: 182764.02\n",
      "Step: 3699, Training Loss: 1.32074, LR: 0.0001112, Tokens/sec: 183545.07\n",
      "Step: 3700, Training Loss: 1.56638, LR: 0.0001111, Tokens/sec: 181972.02\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 3700, Eval Loss: 1.57809\n",
      "Step: 3701, Training Loss: 1.64041, LR: 0.0001110, Tokens/sec: 181807.90\n",
      "Step: 3702, Training Loss: 1.43978, LR: 0.0001109, Tokens/sec: 181053.63\n",
      "Step: 3703, Training Loss: 1.56318, LR: 0.0001108, Tokens/sec: 182714.59\n",
      "Step: 3704, Training Loss: 1.60597, LR: 0.0001108, Tokens/sec: 177215.88\n",
      "Step: 3705, Training Loss: 1.21664, LR: 0.0001107, Tokens/sec: 182444.49\n",
      "Step: 3706, Training Loss: 1.10416, LR: 0.0001106, Tokens/sec: 182166.04\n",
      "Step: 3707, Training Loss: 2.14782, LR: 0.0001105, Tokens/sec: 181607.30\n",
      "Step: 3708, Training Loss: 1.55713, LR: 0.0001104, Tokens/sec: 180972.93\n",
      "Step: 3709, Training Loss: 1.58699, LR: 0.0001103, Tokens/sec: 182279.73\n",
      "Step: 3710, Training Loss: 1.71328, LR: 0.0001102, Tokens/sec: 182393.96\n",
      "Step: 3711, Training Loss: 1.16655, LR: 0.0001102, Tokens/sec: 182347.75\n",
      "Step: 3712, Training Loss: 1.42624, LR: 0.0001101, Tokens/sec: 182638.70\n",
      "Step: 3713, Training Loss: 1.54924, LR: 0.0001100, Tokens/sec: 181747.84\n",
      "Step: 3714, Training Loss: 1.70953, LR: 0.0001099, Tokens/sec: 181859.19\n",
      "Step: 3715, Training Loss: 1.20164, LR: 0.0001098, Tokens/sec: 182265.30\n",
      "Step: 3716, Training Loss: 1.40500, LR: 0.0001097, Tokens/sec: 182764.15\n",
      "Step: 3717, Training Loss: 1.25214, LR: 0.0001097, Tokens/sec: 182069.39\n",
      "Step: 3718, Training Loss: 1.28542, LR: 0.0001096, Tokens/sec: 182788.85\n",
      "Step: 3719, Training Loss: 1.33152, LR: 0.0001095, Tokens/sec: 181985.05\n",
      "Step: 3720, Training Loss: 1.34574, LR: 0.0001094, Tokens/sec: 181504.98\n",
      "Step: 3721, Training Loss: 1.27634, LR: 0.0001093, Tokens/sec: 182285.27\n",
      "Step: 3722, Training Loss: 1.12013, LR: 0.0001093, Tokens/sec: 182124.99\n",
      "Step: 3723, Training Loss: 1.56669, LR: 0.0001092, Tokens/sec: 182472.17\n",
      "Step: 3724, Training Loss: 2.07026, LR: 0.0001091, Tokens/sec: 181755.98\n",
      "Step: 3725, Training Loss: 1.52264, LR: 0.0001090, Tokens/sec: 182476.27\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 3725, Eval Loss: 1.39789\n",
      "Step: 3726, Training Loss: 1.38065, LR: 0.0001089, Tokens/sec: 180983.77\n",
      "Step: 3727, Training Loss: 1.20539, LR: 0.0001089, Tokens/sec: 182497.39\n",
      "Step: 3728, Training Loss: 1.89310, LR: 0.0001088, Tokens/sec: 182699.53\n",
      "Step: 3729, Training Loss: 1.38270, LR: 0.0001087, Tokens/sec: 182585.23\n",
      "Step: 3730, Training Loss: 1.60249, LR: 0.0001086, Tokens/sec: 182369.37\n",
      "Step: 3731, Training Loss: 1.26684, LR: 0.0001085, Tokens/sec: 181202.33\n",
      "Step: 3732, Training Loss: 2.13941, LR: 0.0001085, Tokens/sec: 182140.53\n",
      "Step: 3733, Training Loss: 1.33492, LR: 0.0001084, Tokens/sec: 182858.08\n",
      "Step: 3734, Training Loss: 2.06152, LR: 0.0001083, Tokens/sec: 182203.19\n",
      "Step: 3735, Training Loss: 1.88725, LR: 0.0001082, Tokens/sec: 182562.16\n",
      "Step: 3736, Training Loss: 1.77621, LR: 0.0001082, Tokens/sec: 182349.26\n",
      "Step: 3737, Training Loss: 1.37153, LR: 0.0001081, Tokens/sec: 181589.53\n",
      "Step: 3738, Training Loss: 1.21622, LR: 0.0001080, Tokens/sec: 181550.65\n",
      "Step: 3739, Training Loss: 1.32388, LR: 0.0001079, Tokens/sec: 182103.30\n",
      "Step: 3740, Training Loss: 1.26355, LR: 0.0001079, Tokens/sec: 181852.82\n",
      "Step: 3741, Training Loss: 0.96074, LR: 0.0001078, Tokens/sec: 181533.96\n",
      "Step: 3742, Training Loss: 1.42050, LR: 0.0001077, Tokens/sec: 181374.38\n",
      "Step: 3743, Training Loss: 1.67914, LR: 0.0001076, Tokens/sec: 180246.27\n",
      "Step: 3744, Training Loss: 1.03321, LR: 0.0001076, Tokens/sec: 181412.11\n",
      "Step: 3745, Training Loss: 1.84306, LR: 0.0001075, Tokens/sec: 181487.13\n",
      "Step: 3746, Training Loss: 1.37998, LR: 0.0001074, Tokens/sec: 182408.13\n",
      "Step: 3747, Training Loss: 1.24723, LR: 0.0001074, Tokens/sec: 181377.48\n",
      "Step: 3748, Training Loss: 1.40324, LR: 0.0001073, Tokens/sec: 181054.38\n",
      "Step: 3749, Training Loss: 1.49407, LR: 0.0001072, Tokens/sec: 180129.07\n",
      "Step: 3750, Training Loss: 1.04075, LR: 0.0001071, Tokens/sec: 182009.98\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 3750, Eval Loss: 1.48197\n",
      "Step: 3751, Training Loss: 2.06842, LR: 0.0001071, Tokens/sec: 178552.19\n",
      "Step: 3752, Training Loss: 1.52523, LR: 0.0001070, Tokens/sec: 181284.26\n",
      "Step: 3753, Training Loss: 1.18975, LR: 0.0001069, Tokens/sec: 181883.84\n",
      "Step: 3754, Training Loss: 1.05170, LR: 0.0001069, Tokens/sec: 180863.88\n",
      "Step: 3755, Training Loss: 1.43105, LR: 0.0001068, Tokens/sec: 180888.45\n",
      "Step: 3756, Training Loss: 1.67521, LR: 0.0001067, Tokens/sec: 180722.61\n",
      "Step: 3757, Training Loss: 1.36532, LR: 0.0001067, Tokens/sec: 179520.63\n",
      "Step: 3758, Training Loss: 1.42673, LR: 0.0001066, Tokens/sec: 180967.62\n",
      "Step: 3759, Training Loss: 1.60158, LR: 0.0001065, Tokens/sec: 181247.61\n",
      "Step: 3760, Training Loss: 1.56803, LR: 0.0001065, Tokens/sec: 180859.97\n",
      "Step: 3761, Training Loss: 1.22665, LR: 0.0001064, Tokens/sec: 181729.32\n",
      "Step: 3762, Training Loss: 1.00472, LR: 0.0001063, Tokens/sec: 181283.82\n",
      "Step: 3763, Training Loss: 1.74375, LR: 0.0001063, Tokens/sec: 180667.55\n",
      "Step: 3764, Training Loss: 1.53559, LR: 0.0001062, Tokens/sec: 181322.98\n",
      "Step: 3765, Training Loss: 0.74562, LR: 0.0001061, Tokens/sec: 181293.08\n",
      "Step: 3766, Training Loss: 1.42784, LR: 0.0001061, Tokens/sec: 181100.61\n",
      "Step: 3767, Training Loss: 1.50483, LR: 0.0001060, Tokens/sec: 181467.51\n",
      "Step: 3768, Training Loss: 1.50456, LR: 0.0001059, Tokens/sec: 182144.95\n",
      "Step: 3769, Training Loss: 1.42844, LR: 0.0001059, Tokens/sec: 180615.33\n",
      "Step: 3770, Training Loss: 1.20843, LR: 0.0001058, Tokens/sec: 181159.90\n",
      "Step: 3771, Training Loss: 1.63506, LR: 0.0001057, Tokens/sec: 181188.01\n",
      "Step: 3772, Training Loss: 1.09957, LR: 0.0001057, Tokens/sec: 181877.38\n",
      "Step: 3773, Training Loss: 1.30481, LR: 0.0001056, Tokens/sec: 181676.91\n",
      "Step: 3774, Training Loss: 1.60372, LR: 0.0001055, Tokens/sec: 181290.67\n",
      "Step: 3775, Training Loss: 1.56796, LR: 0.0001055, Tokens/sec: 179781.40\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 3775, Eval Loss: 1.41383\n",
      "Step: 3776, Training Loss: 1.21701, LR: 0.0001054, Tokens/sec: 180751.01\n",
      "Step: 3777, Training Loss: 1.37799, LR: 0.0001054, Tokens/sec: 181363.45\n",
      "Step: 3778, Training Loss: 2.13932, LR: 0.0001053, Tokens/sec: 181973.06\n",
      "Step: 3779, Training Loss: 1.27323, LR: 0.0001052, Tokens/sec: 181360.05\n",
      "Step: 3780, Training Loss: 1.53171, LR: 0.0001052, Tokens/sec: 180867.05\n",
      "Step: 3781, Training Loss: 1.43939, LR: 0.0001051, Tokens/sec: 180909.62\n",
      "Step: 3782, Training Loss: 1.23900, LR: 0.0001051, Tokens/sec: 181813.94\n",
      "Step: 3783, Training Loss: 1.15188, LR: 0.0001050, Tokens/sec: 179899.33\n",
      "Step: 3784, Training Loss: 1.31010, LR: 0.0001049, Tokens/sec: 181854.19\n",
      "Step: 3785, Training Loss: 1.76955, LR: 0.0001049, Tokens/sec: 181529.93\n",
      "Step: 3786, Training Loss: 1.42257, LR: 0.0001048, Tokens/sec: 181416.51\n",
      "Step: 3787, Training Loss: 1.43384, LR: 0.0001048, Tokens/sec: 181299.23\n",
      "Step: 3788, Training Loss: 1.23138, LR: 0.0001047, Tokens/sec: 181669.58\n",
      "Step: 3789, Training Loss: 1.56325, LR: 0.0001047, Tokens/sec: 181113.34\n",
      "Step: 3790, Training Loss: 1.76570, LR: 0.0001046, Tokens/sec: 181484.19\n",
      "Step: 3791, Training Loss: 1.52636, LR: 0.0001045, Tokens/sec: 182438.07\n",
      "Step: 3792, Training Loss: 1.06999, LR: 0.0001045, Tokens/sec: 181353.03\n",
      "Step: 3793, Training Loss: 1.18166, LR: 0.0001044, Tokens/sec: 182304.93\n",
      "Step: 3794, Training Loss: 1.03405, LR: 0.0001044, Tokens/sec: 181414.98\n",
      "Step: 3795, Training Loss: 1.35425, LR: 0.0001043, Tokens/sec: 180643.84\n",
      "Step: 3796, Training Loss: 1.58923, LR: 0.0001043, Tokens/sec: 181869.93\n",
      "Step: 3797, Training Loss: 1.59531, LR: 0.0001042, Tokens/sec: 181514.64\n",
      "Step: 3798, Training Loss: 1.66867, LR: 0.0001042, Tokens/sec: 181006.17\n",
      "Step: 3799, Training Loss: 1.49262, LR: 0.0001041, Tokens/sec: 181328.65\n",
      "Step: 3800, Training Loss: 1.19750, LR: 0.0001040, Tokens/sec: 181214.05\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 3800, Eval Loss: 1.49732\n",
      "Step: 3801, Training Loss: 1.41431, LR: 0.0001040, Tokens/sec: 180183.20\n",
      "Step: 3802, Training Loss: 1.26566, LR: 0.0001039, Tokens/sec: 180952.19\n",
      "Step: 3803, Training Loss: 1.22048, LR: 0.0001039, Tokens/sec: 180953.87\n",
      "Step: 3804, Training Loss: 0.96674, LR: 0.0001038, Tokens/sec: 181133.67\n",
      "Step: 3805, Training Loss: 1.11642, LR: 0.0001038, Tokens/sec: 181284.08\n",
      "Step: 3806, Training Loss: 1.17228, LR: 0.0001037, Tokens/sec: 180159.75\n",
      "Step: 3807, Training Loss: 1.46298, LR: 0.0001037, Tokens/sec: 180549.53\n",
      "Step: 3808, Training Loss: 1.43769, LR: 0.0001036, Tokens/sec: 181225.41\n",
      "Step: 3809, Training Loss: 1.25549, LR: 0.0001036, Tokens/sec: 181193.43\n",
      "Step: 3810, Training Loss: 1.45035, LR: 0.0001035, Tokens/sec: 181193.40\n",
      "Step: 3811, Training Loss: 1.43626, LR: 0.0001035, Tokens/sec: 181471.43\n",
      "Step: 3812, Training Loss: 1.44613, LR: 0.0001034, Tokens/sec: 180165.39\n",
      "Step: 3813, Training Loss: 1.32244, LR: 0.0001034, Tokens/sec: 181459.44\n",
      "Step: 3814, Training Loss: 1.36785, LR: 0.0001033, Tokens/sec: 181309.95\n",
      "Step: 3815, Training Loss: 1.09037, LR: 0.0001033, Tokens/sec: 181898.13\n",
      "Step: 3816, Training Loss: 1.47507, LR: 0.0001032, Tokens/sec: 181509.18\n",
      "Step: 3817, Training Loss: 1.38450, LR: 0.0001032, Tokens/sec: 181514.39\n",
      "Step: 3818, Training Loss: 1.58274, LR: 0.0001031, Tokens/sec: 180678.71\n",
      "Step: 3819, Training Loss: 1.62217, LR: 0.0001031, Tokens/sec: 176129.14\n",
      "Step: 3820, Training Loss: 1.01522, LR: 0.0001031, Tokens/sec: 178168.16\n",
      "Step: 3821, Training Loss: 1.25936, LR: 0.0001030, Tokens/sec: 180858.38\n",
      "Step: 3822, Training Loss: 1.23733, LR: 0.0001030, Tokens/sec: 181053.82\n",
      "Step: 3823, Training Loss: 1.63624, LR: 0.0001029, Tokens/sec: 181352.81\n",
      "Step: 3824, Training Loss: 1.15048, LR: 0.0001029, Tokens/sec: 180057.68\n",
      "Step: 3825, Training Loss: 1.00913, LR: 0.0001028, Tokens/sec: 181025.26\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 3825, Eval Loss: 1.36007\n",
      "Step: 3826, Training Loss: 1.22204, LR: 0.0001028, Tokens/sec: 180640.08\n",
      "Step: 3827, Training Loss: 1.31021, LR: 0.0001027, Tokens/sec: 180809.61\n",
      "Step: 3828, Training Loss: 1.01471, LR: 0.0001027, Tokens/sec: 181451.53\n",
      "Step: 3829, Training Loss: 1.27739, LR: 0.0001027, Tokens/sec: 181585.27\n",
      "Step: 3830, Training Loss: 1.32175, LR: 0.0001026, Tokens/sec: 181546.63\n",
      "Step: 3831, Training Loss: 1.71785, LR: 0.0001026, Tokens/sec: 181461.84\n",
      "Step: 3832, Training Loss: 1.03122, LR: 0.0001025, Tokens/sec: 180550.86\n",
      "Step: 3833, Training Loss: 1.09114, LR: 0.0001025, Tokens/sec: 181546.91\n",
      "Step: 3834, Training Loss: 1.60244, LR: 0.0001024, Tokens/sec: 181923.66\n",
      "Step: 3835, Training Loss: 1.18866, LR: 0.0001024, Tokens/sec: 181001.59\n",
      "Step: 3836, Training Loss: 1.65572, LR: 0.0001024, Tokens/sec: 181701.44\n",
      "Step: 3837, Training Loss: 0.82747, LR: 0.0001023, Tokens/sec: 181126.04\n",
      "Step: 3838, Training Loss: 1.16702, LR: 0.0001023, Tokens/sec: 180758.27\n",
      "Step: 3839, Training Loss: 1.10691, LR: 0.0001022, Tokens/sec: 181379.64\n",
      "Step: 3840, Training Loss: 1.20175, LR: 0.0001022, Tokens/sec: 181044.88\n",
      "Step: 3841, Training Loss: 1.56755, LR: 0.0001022, Tokens/sec: 181713.99\n",
      "Step: 3842, Training Loss: 1.28452, LR: 0.0001021, Tokens/sec: 181738.08\n",
      "Step: 3843, Training Loss: 1.29055, LR: 0.0001021, Tokens/sec: 180968.41\n",
      "Step: 3844, Training Loss: 1.27618, LR: 0.0001020, Tokens/sec: 180272.57\n",
      "Step: 3845, Training Loss: 1.34520, LR: 0.0001020, Tokens/sec: 181828.30\n",
      "Step: 3846, Training Loss: 1.37150, LR: 0.0001020, Tokens/sec: 181887.36\n",
      "Step: 3847, Training Loss: 1.36687, LR: 0.0001019, Tokens/sec: 181379.76\n",
      "Step: 3848, Training Loss: 1.18705, LR: 0.0001019, Tokens/sec: 182245.62\n",
      "Step: 3849, Training Loss: 1.43729, LR: 0.0001019, Tokens/sec: 181566.63\n",
      "Step: 3850, Training Loss: 1.19564, LR: 0.0001018, Tokens/sec: 180804.21\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 3850, Eval Loss: 1.55370\n",
      "Step: 3851, Training Loss: 1.42045, LR: 0.0001018, Tokens/sec: 180405.06\n",
      "Step: 3852, Training Loss: 1.51524, LR: 0.0001018, Tokens/sec: 180058.45\n",
      "Step: 3853, Training Loss: 1.35827, LR: 0.0001017, Tokens/sec: 181452.84\n",
      "Step: 3854, Training Loss: 1.26452, LR: 0.0001017, Tokens/sec: 181043.32\n",
      "Step: 3855, Training Loss: 1.28408, LR: 0.0001017, Tokens/sec: 181252.94\n",
      "Step: 3856, Training Loss: 1.17489, LR: 0.0001016, Tokens/sec: 181503.24\n",
      "Step: 3857, Training Loss: 1.14068, LR: 0.0001016, Tokens/sec: 181837.16\n",
      "Step: 3858, Training Loss: 1.49869, LR: 0.0001015, Tokens/sec: 180448.95\n",
      "Step: 3859, Training Loss: 1.29277, LR: 0.0001015, Tokens/sec: 181309.30\n",
      "Step: 3860, Training Loss: 1.44973, LR: 0.0001015, Tokens/sec: 182223.35\n",
      "Step: 3861, Training Loss: 1.32160, LR: 0.0001015, Tokens/sec: 181251.38\n",
      "Step: 3862, Training Loss: 1.74915, LR: 0.0001014, Tokens/sec: 181416.24\n",
      "Step: 3863, Training Loss: 2.05453, LR: 0.0001014, Tokens/sec: 180705.20\n",
      "Step: 3864, Training Loss: 1.51044, LR: 0.0001014, Tokens/sec: 180451.18\n",
      "Step: 3865, Training Loss: 1.46370, LR: 0.0001013, Tokens/sec: 182017.73\n",
      "Step: 3866, Training Loss: 1.34017, LR: 0.0001013, Tokens/sec: 181230.95\n",
      "Step: 3867, Training Loss: 1.32752, LR: 0.0001013, Tokens/sec: 181506.44\n",
      "Step: 3868, Training Loss: 1.83990, LR: 0.0001012, Tokens/sec: 180846.30\n",
      "Step: 3869, Training Loss: 1.01749, LR: 0.0001012, Tokens/sec: 180975.14\n",
      "Step: 3870, Training Loss: 1.63640, LR: 0.0001012, Tokens/sec: 180630.28\n",
      "Step: 3871, Training Loss: 1.19403, LR: 0.0001012, Tokens/sec: 180796.58\n",
      "Step: 3872, Training Loss: 1.42086, LR: 0.0001011, Tokens/sec: 181328.28\n",
      "Step: 3873, Training Loss: 1.15499, LR: 0.0001011, Tokens/sec: 180819.09\n",
      "Step: 3874, Training Loss: 1.36183, LR: 0.0001011, Tokens/sec: 181421.81\n",
      "Step: 3875, Training Loss: 1.15695, LR: 0.0001010, Tokens/sec: 181518.40\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 3875, Eval Loss: 1.41249\n",
      "Step: 3876, Training Loss: 1.28140, LR: 0.0001010, Tokens/sec: 179559.82\n",
      "Step: 3877, Training Loss: 1.10401, LR: 0.0001010, Tokens/sec: 180492.24\n",
      "Step: 3878, Training Loss: 1.27069, LR: 0.0001010, Tokens/sec: 181518.98\n",
      "Step: 3879, Training Loss: 1.56160, LR: 0.0001009, Tokens/sec: 180504.46\n",
      "Step: 3880, Training Loss: 1.54254, LR: 0.0001009, Tokens/sec: 181805.52\n",
      "Step: 3881, Training Loss: 1.25424, LR: 0.0001009, Tokens/sec: 180127.47\n",
      "Step: 3882, Training Loss: 1.44368, LR: 0.0001009, Tokens/sec: 181042.02\n",
      "Step: 3883, Training Loss: 1.25815, LR: 0.0001008, Tokens/sec: 181467.07\n",
      "Step: 3884, Training Loss: 1.35231, LR: 0.0001008, Tokens/sec: 180788.28\n",
      "Step: 3885, Training Loss: 1.66598, LR: 0.0001008, Tokens/sec: 181015.65\n",
      "Step: 3886, Training Loss: 1.34812, LR: 0.0001008, Tokens/sec: 180849.47\n",
      "Step: 3887, Training Loss: 1.24517, LR: 0.0001007, Tokens/sec: 179933.82\n",
      "Step: 3888, Training Loss: 1.35635, LR: 0.0001007, Tokens/sec: 181319.86\n",
      "Step: 3889, Training Loss: 1.49932, LR: 0.0001007, Tokens/sec: 181522.42\n",
      "Step: 3890, Training Loss: 1.23979, LR: 0.0001007, Tokens/sec: 181006.80\n",
      "Step: 3891, Training Loss: 1.54224, LR: 0.0001007, Tokens/sec: 180926.73\n",
      "Step: 3892, Training Loss: 1.23806, LR: 0.0001006, Tokens/sec: 181278.18\n",
      "Step: 3893, Training Loss: 1.00346, LR: 0.0001006, Tokens/sec: 180100.15\n",
      "Step: 3894, Training Loss: 1.34190, LR: 0.0001006, Tokens/sec: 181619.50\n",
      "Step: 3895, Training Loss: 1.24788, LR: 0.0001006, Tokens/sec: 181178.56\n",
      "Step: 3896, Training Loss: 1.30826, LR: 0.0001006, Tokens/sec: 181537.47\n",
      "Step: 3897, Training Loss: 0.95532, LR: 0.0001005, Tokens/sec: 181927.67\n",
      "Step: 3898, Training Loss: 1.28125, LR: 0.0001005, Tokens/sec: 182391.44\n",
      "Step: 3899, Training Loss: 1.60474, LR: 0.0001005, Tokens/sec: 180488.22\n",
      "Step: 3900, Training Loss: 1.31291, LR: 0.0001005, Tokens/sec: 181391.40\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 3900, Eval Loss: 1.33142\n",
      "Step: 3901, Training Loss: 1.42875, LR: 0.0001005, Tokens/sec: 171506.44\n",
      "Step: 3902, Training Loss: 0.88636, LR: 0.0001004, Tokens/sec: 179009.43\n",
      "Step: 3903, Training Loss: 1.18570, LR: 0.0001004, Tokens/sec: 180033.30\n",
      "Step: 3904, Training Loss: 1.43701, LR: 0.0001004, Tokens/sec: 180218.05\n",
      "Step: 3905, Training Loss: 1.37137, LR: 0.0001004, Tokens/sec: 180656.40\n",
      "Step: 3906, Training Loss: 1.25427, LR: 0.0001004, Tokens/sec: 180684.14\n",
      "Step: 3907, Training Loss: 0.87106, LR: 0.0001004, Tokens/sec: 179684.56\n",
      "Step: 3908, Training Loss: 1.30442, LR: 0.0001003, Tokens/sec: 181181.99\n",
      "Step: 3909, Training Loss: 0.95450, LR: 0.0001003, Tokens/sec: 181296.36\n",
      "Step: 3910, Training Loss: 1.26355, LR: 0.0001003, Tokens/sec: 181329.63\n",
      "Step: 3911, Training Loss: 1.45440, LR: 0.0001003, Tokens/sec: 180779.71\n",
      "Step: 3912, Training Loss: 1.21714, LR: 0.0001003, Tokens/sec: 181966.13\n",
      "Step: 3913, Training Loss: 1.49367, LR: 0.0001003, Tokens/sec: 180335.79\n",
      "Step: 3914, Training Loss: 0.96627, LR: 0.0001003, Tokens/sec: 181318.04\n",
      "Step: 3915, Training Loss: 1.21698, LR: 0.0001002, Tokens/sec: 181647.15\n",
      "Step: 3916, Training Loss: 1.48355, LR: 0.0001002, Tokens/sec: 181044.09\n",
      "Step: 3917, Training Loss: 1.48075, LR: 0.0001002, Tokens/sec: 181370.44\n",
      "Step: 3918, Training Loss: 1.96382, LR: 0.0001002, Tokens/sec: 181660.01\n",
      "Step: 3919, Training Loss: 1.60487, LR: 0.0001002, Tokens/sec: 180786.76\n",
      "Step: 3920, Training Loss: 1.40034, LR: 0.0001002, Tokens/sec: 180838.40\n",
      "Step: 3921, Training Loss: 1.15622, LR: 0.0001002, Tokens/sec: 181723.18\n",
      "Step: 3922, Training Loss: 0.99616, LR: 0.0001002, Tokens/sec: 181560.21\n",
      "Step: 3923, Training Loss: 1.23862, LR: 0.0001001, Tokens/sec: 181146.47\n",
      "Step: 3924, Training Loss: 1.03848, LR: 0.0001001, Tokens/sec: 181240.44\n",
      "Step: 3925, Training Loss: 1.66632, LR: 0.0001001, Tokens/sec: 180024.47\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 3925, Eval Loss: 1.33995\n",
      "Step: 3926, Training Loss: 1.38769, LR: 0.0001001, Tokens/sec: 181162.71\n",
      "Step: 3927, Training Loss: 1.06527, LR: 0.0001001, Tokens/sec: 181325.69\n",
      "Step: 3928, Training Loss: 1.10419, LR: 0.0001001, Tokens/sec: 182006.21\n",
      "Step: 3929, Training Loss: 1.25100, LR: 0.0001001, Tokens/sec: 181334.65\n",
      "Step: 3930, Training Loss: 1.45061, LR: 0.0001001, Tokens/sec: 181834.84\n",
      "Step: 3931, Training Loss: 1.04302, LR: 0.0001001, Tokens/sec: 181558.49\n",
      "Step: 3932, Training Loss: 1.19015, LR: 0.0001001, Tokens/sec: 182104.05\n",
      "Step: 3933, Training Loss: 1.21814, LR: 0.0001001, Tokens/sec: 180922.38\n",
      "Step: 3934, Training Loss: 0.81576, LR: 0.0001001, Tokens/sec: 181472.50\n",
      "Step: 3935, Training Loss: 1.25741, LR: 0.0001001, Tokens/sec: 182036.54\n",
      "Step: 3936, Training Loss: 1.43644, LR: 0.0001000, Tokens/sec: 181613.58\n",
      "Step: 3937, Training Loss: 1.45237, LR: 0.0001000, Tokens/sec: 181352.48\n",
      "Step: 3938, Training Loss: 1.39213, LR: 0.0001000, Tokens/sec: 181998.38\n",
      "Step: 3939, Training Loss: 1.29483, LR: 0.0001000, Tokens/sec: 180466.54\n",
      "Step: 3940, Training Loss: 1.70154, LR: 0.0001000, Tokens/sec: 182032.66\n",
      "Step: 3941, Training Loss: 1.56386, LR: 0.0001000, Tokens/sec: 181739.60\n",
      "Step: 3942, Training Loss: 1.25965, LR: 0.0001000, Tokens/sec: 181752.82\n",
      "Step: 3943, Training Loss: 0.98468, LR: 0.0001000, Tokens/sec: 181449.04\n",
      "Step: 3944, Training Loss: 1.51372, LR: 0.0001000, Tokens/sec: 181379.35\n",
      "Step: 3945, Training Loss: 1.42918, LR: 0.0001000, Tokens/sec: 180282.22\n",
      "Step: 3946, Training Loss: 1.30699, LR: 0.0001000, Tokens/sec: 180601.96\n",
      "Step: 3947, Training Loss: 1.22746, LR: 0.0001000, Tokens/sec: 181512.25\n",
      "Step: 3948, Training Loss: 0.92451, LR: 0.0001000, Tokens/sec: 182031.26\n",
      "Step: 3949, Training Loss: 1.42188, LR: 0.0001000, Tokens/sec: 181596.06\n",
      "Step: 3950, Training Loss: 1.53471, LR: 0.0001000, Tokens/sec: 181280.31\n",
      "Computing Eval loss, steps: 13\n",
      "Step: 3950, Eval Loss: 1.36981\n",
      "Step: 3951, Training Loss: 1.57132, LR: 0.0001000, Tokens/sec: 175757.31\n"
     ]
    }
   ],
   "source": [
    "trainer.train(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b74ac833c9795a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.save_checkpoint(\"think_shakespeare\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c2eccd3e2f5932b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_dict = torch.load(\"think_shakespeare/model.checkpoint.2025-02-22--23-04-54.pt\", weights_only=True)\n",
    "# model = ThinkTransformer(model_config)\n",
    "# model.load_state_dict(state_dict)\n",
    "# model.to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b5596eda083de0a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m input_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124mFirst Citizen:\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124mBefore we proceed any further, hear me speak.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124mSpeak, speak.\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m      9\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer([input_text], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m idx \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthink_r\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(idx)[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/smol-llama/think_model.py:355\u001b[0m, in \u001b[0;36mThinkTransformer.generate\u001b[0;34m(self, idx, temperature, top_k, max_new_tokens, think_r)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    353\u001b[0m     thought_embedding \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((thought_embedding, thought_embedding[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:, :]), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 355\u001b[0m logits, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_network(idx, thought_embedding)\n\u001b[1;32m    357\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :] \u001b[38;5;241m/\u001b[39m temperature\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m top_k \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "input_text = \"\"\"\n",
    "First Citizen:\n",
    "Before we proceed any further, hear me speak.\n",
    "\n",
    "All:\n",
    "Speak, speak.\n",
    "\"\"\".strip()\n",
    "\n",
    "input_ids = tokenizer([input_text], return_tensors=\"pt\")['input_ids'].to(\"cuda\")\n",
    "idx = model.generate(input_ids, temperature=0.01, top_k=5, max_new_tokens=64, think_r=256)\n",
    "print(tokenizer.batch_decode(idx)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d334ef6b19f83419",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
